- en: Chapter 7. Redundancy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章：冗余
- en: When building a proper VDI, it's imperative to understand all of the possible
    failure points within the solution so redundancy can be built in to mitigate any
    failures. While sizing a VDI incorrectly will cause a slow response time or poor
    end user experience, failing to build proper redundancy could render the solution
    unreachable. In a VDI solution there are physical failure points to consider such
    as network switches, power supplies, and hard drives. There are also software
    failure points such as the VMware vCenter Server, VMware View Connection Server,
    and the database server(s) to take into consideration.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建合适的 VDI 时，必须理解解决方案中的所有可能故障点，以便可以构建冗余来缓解任何故障。虽然不正确地调整 VDI 的规模会导致响应时间慢或最终用户体验差，但如果未能构建适当的冗余，可能会导致解决方案无法访问。在
    VDI 解决方案中，需要考虑物理故障点，如网络交换机、电源和硬盘。还需要考虑软件故障点，如 VMware vCenter Server、VMware View
    Connection Server 和数据库服务器。
- en: This chapter analyzes the potential points of failure within a VDI and offers
    up suggestions to provide redundancy for each component.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分析了 VDI 中可能出现的故障点，并提供了针对每个组件的冗余建议。
- en: Physical infrastructure
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物理基础设施
- en: In-depth coverage of designing a highly available virtual infrastructure is
    outside the scope of this book. However, understanding and utilizing the following
    VMware vSphere features are important to implement a robust VDI.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的范围不涉及高可用虚拟基础设施设计的深入内容。然而，理解并利用以下 VMware vSphere 功能对于实施稳健的 VDI（虚拟桌面基础设施）非常重要。
- en: VMware High Availability
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VMware 高可用性
- en: '**VMware High Availability (HA)** can be used to monitor and protect against
    physical host failures and can also be used to monitor and protect vDesktops themselves.
    VMware HA works by monitoring physical hosts in a given cluster. If a host is
    unable to communicate to the specific default gateway on a service console interface
    for 15 continuous seconds, an HA failover event is triggered. vSphere 5 also introduced
    Datastore Heartbeating, which is used when a network heartbeat failure has occurred.
    Datastore heartbeats provide an additional level of host isolation verification.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**VMware 高可用性（HA）**可用于监控和防止物理主机故障，也可以用于监控和保护 vDesktops 本身。VMware HA 通过监控给定集群中的物理主机来工作。如果主机无法在服务控制台接口上与特定默认网关通信超过
    15 秒，HA 故障转移事件将被触发。vSphere 5 还引入了数据存储心跳（Datastore Heartbeating），当发生网络心跳故障时使用。数据存储心跳提供了额外的主机隔离验证层次。'
- en: For more information on VMware HA, please refer to the *HA Deepdive* article
    by Duncan Epping at [http://www.yellow-bricks.com/vmware-high-availability-deepdiv/](http://www.yellow-bricks.com/vmware-high-availability-deepdiv/).
    You can also check it in his book *vSphere 5 Clustering Technical Deepdive*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 VMware HA 的更多信息，请参阅 Duncan Epping 的 *HA Deepdive* 文章，链接：[http://www.yellow-bricks.com/vmware-high-availability-deepdiv/](http://www.yellow-bricks.com/vmware-high-availability-deepdiv/)。你还可以在他的书籍
    *vSphere 5 Clustering Technical Deepdive* 中查看相关内容。
- en: A host is determined to be isolated when a host has stopped receiving heartbeats
    from other hosts in the cluster and the specified isolation address cannot be
    pinged.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当主机停止接收来自集群中其他主机的心跳信号且无法 ping 通指定的隔离地址时，该主机被认为是隔离的。
- en: If the Isolation Response for the HA cluster is set to **Leave Power On**, the
    vDesktops and other virtual machines on the host will remain powered on. Just
    because a host has lost network connectivity on its service console interface
    does not necessarily mean that the vDesktops have lost network connectivity.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 HA 集群的隔离响应设置为**保持开机**，则主机上的 vDesktops 和其他虚拟机将继续保持开机状态。仅仅因为主机在服务控制台接口上丧失了网络连接，并不意味着
    vDesktops 也丧失了网络连接。
- en: If the Isolation Response for the HA cluster is set to **Power Off**, the vDesktops
    and other virtual machines on the host will be powered off. This setting avoids
    the possibility of a split-brain scenario.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 HA 集群的隔离响应设置为**关机**，则主机上的 vDesktops 和其他虚拟机将会关闭电源。这个设置避免了可能出现的脑裂（split-brain）情况。
- en: With the advancements in vSphere 5, the host isolation events are highly verified
    and accurate and very likely indicate an actual host problem. Therefore, in VMware
    View solutions, it's preferred to set the Isolation Response to **Power Off**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 vSphere 5 的进步，主机隔离事件经过高度验证且准确无误，极有可能指示实际的主机问题。因此，在 VMware View 解决方案中，推荐将隔离响应设置为**关机**。
- en: 'If a specific host containing vDesktops has been isolated from the cluster,
    it will perform the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果包含 vDesktops 的特定主机已从集群中隔离，它将执行以下操作：
- en: All virtual machines and vDesktops will be powered off.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有虚拟机和 vDesktops 将被关闭。
- en: Users with active connections will be disconnected from their vDesktops.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有活动连接的用户将会断开与其 vDesktops 的连接。
- en: If the vDesktop is part of a persistent desktop pool, the user will be able
    to log back in once their specific vDesktop has been powered up and is online.
    The estimated outage time is 2 minutes.
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 vDesktop 是持久桌面池的一部分，用户将在其特定的 vDesktop 启动并上线后重新登录。预计停机时间为 2 分钟。
- en: If the vDesktop is part of a non-persistent desktop pool, the user will be able
    to log back in immediately to a vDesktop, assuming that there is an available
    vDesktop in the pool on a host that is currently online. The estimated outage
    time is less than or equal to 30 seconds.
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 vDesktop 是非持久桌面池的一部分，用户可以立即重新登录到 vDesktop，前提是当前在线的主机上池中有可用的 vDesktop。预计停机时间不超过
    30 秒。
- en: Do you even need VMware HA?
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你真的需要 VMware HA 吗？
- en: VMware HA provides a level of protection for host failures, whereby vDesktops
    residing on a host that has failed will automatically be powered up without intervention
    from the virtual infrastructure administrator.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: VMware HA 提供了一定的主机故障保护，当托管 vDesktops 的主机发生故障时，vDesktops 会在无需虚拟基础设施管理员干预的情况下自动启动。
- en: '![Do you even need VMware HA?](img/1124EN_07_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![你真的需要 VMware HA 吗？](img/1124EN_07_01.jpg)'
- en: If admission control is set to **strict**, the vDesktops will only be powered
    on if there are available resources on another host in the cluster.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果入场控制设置为**严格**，只有当集群中另一主机上有可用资源时，vDesktops 才会启动。
- en: VMware HA works by determining the slot size, or minimum amount of CPU and memory
    to support a failover of the most intensive virtual machine (or vDesktop).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: VMware HA 的工作原理是通过确定插槽大小，或支持最密集虚拟机（或 vDesktop）故障切换所需的最小 CPU 和内存量。
- en: For example, if vDesktop_A has 4 GHz of CPU and 2 GB of RAM while vDesktop_B
    has 1 GHz of CPU and 6 GB of RAM, the slot size will be 4 GHz of CPU and 6 GB
    of RAM (with additional calculations taken into consideration for memory overheard).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 vDesktop_A 拥有 4 GHz 的 CPU 和 2 GB 的内存，而 vDesktop_B 拥有 1 GHz 的 CPU 和 6 GB
    的内存，则插槽大小将为 4 GHz 的 CPU 和 6 GB 的内存（考虑到内存开销，还需进行额外计算）。
- en: In VMware View environments, there will likely be large quantities of vDesktops
    with identical specifications (for example, a Windows XP vDesktop with 2 GHz of
    CPU and 2 GB of RAM), therefore a concept known as **slot fragmentation** is primarily
    avoided. Slot fragmentation requires the collective availability of sufficient
    resources in a cluster to support virtual machines being powered on during an
    HA event, but the lack of sufficient resources on an individual physical host
    to support a virtual machine's requirements.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VMware View 环境中，通常会有大量具有相同规格的 vDesktops（例如，具有 2 GHz CPU 和 2 GB 内存的 Windows
    XP vDesktop），因此主要避免了一个名为**插槽碎片化**的概念。插槽碎片化要求集群中必须有足够的资源以支持在 HA 事件中启动虚拟机，但单个物理主机上没有足够的资源来支持虚拟机的需求。
- en: 'For more information on slot fragmentation, please see Duncan Epping''s very
    thorough article at the same URL: [http://www.yellow-bricks.com/vmware-high-availability-deepdiv/](http://www.yellow-bricks.com/vmware-high-availability-deepdiv/),
    as mentioned earlier.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解有关插槽碎片化的更多信息，请参阅 Duncan Epping 的非常详细的文章，网址如下：[http://www.yellow-bricks.com/vmware-high-availability-deepdiv/](http://www.yellow-bricks.com/vmware-high-availability-deepdiv/)，如前所述。
- en: '![Do you even need VMware HA?](img/1124EN_07_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![你真的需要 VMware HA 吗？](img/1124EN_07_02.jpg)'
- en: As of vSphere 4.1, HA also works in conjunction with DRS to free resource slots
    should slot fragmentation occur within a cluster. This would involve a failed
    server having to wait for virtual machines to be vMotion'd across hosts in the
    cluster until enough slots exist to power on necessary virtual machine(s).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从 vSphere 4.1 起，HA 还与 DRS 配合工作，当集群内出现插槽碎片化时，释放资源插槽。这将涉及到一个故障的服务器必须等待虚拟机在集群中的主机之间进行
    vMotion，直到有足够的插槽可用于启动必要的虚拟机。
- en: With a VMware View solution based on persistent vDesktops, HA should be used.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基于持久 vDesktops 的 VMware View 解决方案应该使用 HA。
- en: '![Do you even need VMware HA?](img/1124EN_07_03.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![你真的需要 VMware HA 吗？](img/1124EN_07_03.jpg)'
- en: In the preceding diagram, the end user is connected to a vDesktop on Host1\.
    Both Host1 and Host2 are part of the same cluster and can see the same shared
    storage. As illustrated in the diagram, the actual virtual disk files for the
    vDesktop reside on the shared storage and not on local storage within Host1.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，最终用户连接到了 Host1 上的 vDesktop。Host1 和 Host2 都是同一集群的一部分，并且可以访问相同的共享存储。正如图中所示，vDesktop
    的实际虚拟磁盘文件存储在共享存储中，而不是 Host1 的本地存储上。
- en: '![Do you even need VMware HA?](img/1124EN_07_04.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![你真的需要 VMware HA 吗？](img/1124EN_07_04.jpg)'
- en: As illustrated in the preceding diagram, when Host1 fails, the end user is disconnected
    from the vDesktop. In a persistent vDesktop solution, the end user is assigned
    to a specific vDesktop. In this case, the vDesktop is unavailable as it resides
    on Host1, which has just failed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，当 Host1 故障时，最终用户与 vDesktop 的连接会断开。在持久性 vDesktop 解决方案中，最终用户会被分配到特定的 vDesktop。在这种情况下，由于
    vDesktop 存储在 Host1 上，而 Host1 刚刚发生故障，因此 vDesktop 无法使用。
- en: The end user will be unable to work until the vDesktop is back online or the
    end user is manually assigned to another (available) vDesktop resource.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最终用户将无法工作，直到 vDesktop 恢复在线，或者最终用户被手动分配到另一个（可用的）vDesktop 资源。
- en: '![Do you even need VMware HA?](img/1124EN_07_05.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![你真的需要 VMware HA 吗？](img/1124EN_07_05.jpg)'
- en: As illustrated in the preceding diagram, VMware HA has powered the end user's
    vDesktop up on Host2, an available host in the cluster. By default, there is no
    notification to the end user that his/her vDesktop is now available, so the end
    user will need to try repeatedly for the time it takes vDesktop to come online
    on Host2\. This typically takes between 1 and 3 minutes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，VMware HA 已在 Host2 上启动了最终用户的 vDesktop，Host2 是集群中的可用主机。默认情况下，不会通知最终用户他的
    vDesktop 现在已可用，因此最终用户需要反复尝试，直到 vDesktop 在 Host2 上上线。这通常需要 1 到 3 分钟。
- en: Note
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: An advanced solution concept for persistent vDesktop environments is to monitor
    for individual vDesktop outages. If a user's persistent vDesktop is determined
    to be offline, an e-mail can be sent to the end user (who would likely receive
    it on his/her mobile device) letting him/her know that his/her vDesktop is currently
    unavailable but that the resolution is in progress. The same concept can then
    be used to detect when the vDesktop is back online and available (for example,
    by adding a 2 minute wait when a vDesktop enters the VMware Tool's OK state) and
    then notify the user that his/her vDesktop is now available.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于持久性 vDesktop 环境的高级解决方案概念是监控单个 vDesktop 的故障。如果发现某个用户的持久性 vDesktop 离线，可以向该用户发送电子邮件（用户可能会在手机上收到），告知其
    vDesktop 当前不可用，但问题正在解决中。然后，可以使用相同的概念检测 vDesktop 恢复在线并可用（例如，通过在 vDesktop 进入 VMware
    Tool 的 OK 状态时添加 2 分钟的等待时间），并通知用户他的 vDesktop 已经可用。
- en: '![Do you even need VMware HA?](img/1124EN_07_06.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![你真的需要 VMware HA 吗？](img/1124EN_07_06.jpg)'
- en: For solutions using non-persistent vDesktops, the use of VMware HA is the topic
    of greater debate. While non-persistent solutions rely on a pool of vDesktops
    spread across multiple hosts in a cluster, end users are not assigned to an individual
    vDesktop. When a host fails in a non-persistent solution, any end users connected
    to vDesktops on that specific host lose their connectivity. The end users can
    then reconnect to the VMware View environment, and, as long as another vDesktop
    is available, that user will successfully connect to a resource. This is because
    vDesktop assignment is done randomly at the time of log in when using non-persistent
    vDesktops.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用非持久性 vDesktop 的解决方案，VMware HA 的使用问题引起了更广泛的争议。虽然非持久性解决方案依赖于分布在集群中多个主机上的 vDesktop
    池，但最终用户并没有被分配到特定的 vDesktop。当主机在非持久性解决方案中发生故障时，任何连接到该特定主机上 vDesktop 的最终用户都会失去连接。此时，最终用户可以重新连接到
    VMware View 环境，只要有另一个 vDesktop 可用，用户将成功连接到资源。这是因为在使用非持久性 vDesktop 时，vDesktop 分配是在登录时随机完成的。
- en: Non-persistent example
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非持久性示例
- en: 'For this example, Company_A has 60 end users and has created a non-persistent
    vDesktop pool with the following settings:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，Company_A 有 60 个最终用户，并创建了一个非持久性 vDesktop 池，设置如下：
- en: Numbers of end users = 60
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终用户数量 = 60
- en: Desktop pool size (maximum number of desktops) = 60
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 桌面池大小（最大桌面数量） = 60
- en: Number of spare (powered on) desktops = 0
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备用（已开机）桌面数量 = 0
- en: Power setting = Always On
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电源设置 = 始终开启
- en: Provision all desktops up front
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前配置所有桌面
- en: With these settings, when the pool is initially built, it will automatically
    provision 60 vDesktops and power them on.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这些设置时，当池首次建立时，它将自动配置 60 个 vDesktop 并启动它们。
- en: The count is authoritatively held by VMware View in the ADAM database. If the
    pool's power settings are set to **Always On**, VMware View will create a pool
    of 60 vDesktops and immediately power all of them on. No matter what load exists
    from the end user community, 60 vDesktops will always be powered on. If 61 end
    users try to log in concurrently, 1 end user will be unable to access a resource.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该数量由 VMware View 在 ADAM 数据库中权威管理。如果池的电源设置为 **始终开启**，VMware View 将创建一个包含 60 个
    vDesktops 的池，并立即将其全部启动。无论最终用户社区的负载如何，60 个 vDesktops 总是会保持开启。如果 61 个最终用户尝试同时登录，1
    个最终用户将无法访问资源。
- en: Imagine a scenario where Host1 hosts 30 vDesktops and Host2 hosts 30 vDesktops.
    The desktop pool is configured to host 60 vDesktops.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有这样一种场景，Host1 托管着 30 个 vDesktops，而 Host2 托管着 30 个 vDesktops。桌面池配置为托管 60 个
    vDesktops。
- en: In this environment, if Host1 suddenly fails, the 30 vDesktops being hosted
    on Host1 enter an "Agent Unreachable" state. While the VMware View Connection
    Server has recognized that there are now 30 vDesktops that are unreachable, it
    does not provision 30 new vDesktops on the available hosts in the cluster (for
    example, Host2).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种环境中，如果 Host1 突然故障，托管在 Host1 上的 30 个 vDesktops 将进入“Agent Unreachable”状态。尽管
    VMware View 连接服务器已识别出现在有 30 个 vDesktops 无法访问，但它不会在集群中的可用主机（例如 Host2）上预配新的 30 个
    vDesktops。
- en: Therefore, without using HA to restart the vDesktops on another host, the pool's
    total number of vDesktops will be reduced. By using VMware HA, the pool's total
    number of vDesktops will not be reduced, although there could be a decrease in
    overall performance (if the ability to exceed available resources is allowed).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果没有使用 HA 将 vDesktops 在另一主机上重新启动，池中 vDesktops 的总数将会减少。通过使用 VMware HA，池中的
    vDesktops 总数将不会减少，尽管整体性能可能会下降（如果允许超出可用资源的能力）。
- en: There are two design paths for non-persistent vDesktop solutions. The first
    is to simply use VMware HA to ensure that any vDesktops that reside on a failed
    host are restarted on another available host in the cluster. This is likely the
    easiest configuration and results in 5 10 minutes of downtime (as vDesktops power
    up and enter a useable state).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非持久性 vDesktop 解决方案，有两条设计路径。第一种是简单地使用 VMware HA 来确保任何位于故障主机上的 vDesktops 能在集群中的另一个可用主机上重新启动。这可能是最简单的配置，通常会导致
    5 到 10 分钟的停机时间（因为 vDesktops 需要启动并进入可用状态）。
- en: The second design path is to design the desktop pool(s) with enough vDesktops
    to sustain a host failure. It is important to ensure that the number of used vDesktops
    does not exceed the legally licensed amount from VMware. However, by building
    a desktop pool with additional capacity (for example, extra 30 vDesktops), and
    outage of one host has minimal impact on the end user environment. For those users
    that were connected to a vDesktop on the failed host, they simply log back into
    the VMware View environment and connect to one of the already provisioned, already
    available, extra vDesktops.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种设计路径是为桌面池设计足够的 vDesktops，以应对主机故障。必须确保使用中的 vDesktops 数量不超过 VMware 授权的法定数量。然而，通过构建一个具有额外容量的桌面池（例如，额外的
    30 个 vDesktops），一个主机的停机对最终用户环境的影响将降至最低。对于那些连接到故障主机上 vDesktop 的用户，他们只需重新登录 VMware
    View 环境，连接到已预配好的、可用的额外 vDesktops。
- en: '![Non-persistent example](img/1124EN_07_07.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![非持久性示例](img/1124EN_07_07.jpg)'
- en: Using local storage
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用本地存储
- en: We have added a note about using local storage in this section. As will be covered
    later in this book, local storage is a viable option for certain VDI solutions.
    If the end user's vDesktop resides on local storage to Host1, during a host failure
    VMware HA would not be able to bring the vDesktop up on another host (for example,
    Host2) as other hosts would not have access to the virtual disk files residing
    on the local storage on Host1.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中我们添加了关于使用本地存储的说明。如本书后续将讨论的，本地存储是某些 VDI 解决方案的可行选项。如果最终用户的 vDesktop 存储在 Host1
    的本地存储上，在主机故障时，VMware HA 将无法将 vDesktop 启动到另一台主机上（例如 Host2），因为其他主机无法访问存储在 Host1
    本地存储上的虚拟磁盘文件。
- en: '![Using local storage](img/1124EN_07_08.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![使用本地存储](img/1124EN_07_08.jpg)'
- en: As illustrated in the preceding diagram, both Host1 and Host2 have a local **Virtual
    Machine File System (VMFS)** data store as well as access to a shared VMFS data
    store on the **Storage Area Network (SAN)**. If Host1 has an outage, any vDesktops
    or templates that were stored on the local VMFS data store on Host1 would be unavailable.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的示例图所示，Host1 和 Host2 都有本地 **虚拟机文件系统（VMFS）** 数据存储，并且能够访问 **存储区域网络（SAN）** 上的共享
    VMFS 数据存储。如果 Host1 发生故障，任何存储在 Host1 本地 VMFS 数据存储上的 vDesktop 或模板将无法使用。
- en: If a persistent solution was in use and vDesktops were placed on the local VMFS
    data store, end users would not have access to their vDesktops during a host outage.
    VMware HA would not matter as the vDesktops were not on shared storage, but instead
    on the local VMFS data store of the host.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用的是持久性解决方案，并且 vDesktop 被放置在本地 VMFS 数据存储中，那么在主机发生故障时，最终用户将无法访问他们的 vDesktop。VMware
    HA 将不起作用，因为 vDesktop 并未存储在共享存储上，而是存储在主机的本地 VMFS 数据存储中。
- en: Therefore, it is imperative to use a non-persistent solution when placing core
    virtual disks of a vDesktop on local storage as end users are not specifically
    assigned to a unique vDesktop. If the physical server hosting vDesktop_17 a persistent
    vDesktop assigned to employee User_LL were to fail, User_LL would not be able
    to connect to a desktop resource.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当将 vDesktop 的核心虚拟磁盘放置在本地存储时，必须使用非持久性解决方案，因为最终用户并未专门分配到特定的 vDesktop。如果托管 vDesktop_17
    的物理服务器发生故障，而该 vDesktop 是分配给员工 User_LL 的持久性 vDesktop，User_LL 将无法连接到桌面资源。
- en: VMware Distributed Resource Scheduling
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VMware 分布式资源调度（DRS）
- en: While **VMware Distributed Resource Scheduling (DRS)** does not provide resilience,
    it does minimize the potential impact during an unpredicted physical host failure.
    By balancing the processing load across all of the available hosts in a cluster,
    a host failure will have approximately the minimum impact possible. This is true
    for both virtual machines running a server OS and vDesktops. In a VMware View
    solution with virtualized VMware vCenter Server(s) and/or VMware View Connection
    Server(s), it is prudent to use VMware DRS to balance the load in the cluster
    and minimize the impact. For additional considerations, please refer to the *Anti-affinity*
    section explained next.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 **VMware 分布式资源调度 (DRS)** 不提供弹性，但它确实可以最小化在物理主机故障时的潜在影响。通过在集群中所有可用主机之间平衡处理负载，物理主机故障将会带来最小的影响。这对于运行服务器操作系统的虚拟机和
    vDesktop 都适用。在虚拟化的 VMware vCenter Server(s) 和/或 VMware View Connection Server(s)
    的 VMware View 解决方案中，使用 VMware DRS 来平衡集群中的负载并最小化影响是明智的。有关更多考虑事项，请参阅下文解释的 *反亲和性*
    部分。
- en: '![VMware Distributed Resource Scheduling](img/1124EN_07_09.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![VMware 分布式资源调度](img/1124EN_07_09.jpg)'
- en: In the example given in the preceding diagram, there are three hosts in a cluster
    without VMware DRS enabled. On the first host in the cluster, there are 70 vDesktops.
    On the second one, there are 10 vDesktops, and on the third host, there are 20
    vDesktops. As VMware DRS is not enabled, the load (and therefore the number of
    vDesktops) is not balanced across all of the available hosts in the cluster.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例图中，集群中有三台主机，并且没有启用 VMware DRS。在集群的第一台主机上，有 70 个 vDesktop；在第二台主机上，有 10
    个 vDesktop；而在第三台主机上，有 20 个 vDesktop。由于没有启用 VMware DRS，负载（因此 vDesktop 的数量）在集群中所有可用主机之间并未均衡分配。
- en: '![VMware Distributed Resource Scheduling](img/1124EN_07_10.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![VMware 分布式资源调度](img/1124EN_07_10.jpg)'
- en: Continuing the example, if the first host were to have an unpredicted outage,
    70 vDesktops would be impacted. If DRS was enabled and all vDesktops had roughly
    the same CPU consumption, approximately 34 vDesktops would be placed on each host.
    This will drastically reduce the number of end users that would experience an
    outage should a physical host fail.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 继续前面的示例，如果第一台主机发生了预料之外的故障，将有 70 个 vDesktop 受到影响。如果启用了 DRS，并且所有 vDesktop 的 CPU
    使用情况大致相同，那么每台主机上大约会分配 34 个 vDesktop。这将大大减少因物理主机故障而导致的最终用户受影响的数量。
- en: Anti-affinity
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反亲和性
- en: '**Affinity** and **Anti-affinity** are settings within VMware DRS that determine
    how virtual machines in a given cluster react to one another.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**亲和性**（Affinity）和 **反亲和性**（Anti-affinity）是 VMware DRS 中的设置，用于确定给定集群中虚拟机之间的相互关系。'
- en: '![Anti-affinity](img/1124EN_07_11.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![反亲和性](img/1124EN_07_11.jpg)'
- en: In the preceding diagram, DRS is enabled for the four-host cluster and set to
    **Automatic**. There are no affinity or anti-affinity rules set. The VMware View
    solution requires two View Connection Servers, both of which have been virtualized
    and placed in the aforementioned cluster.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，DRS 已启用并设置为**自动**模式，适用于四主机集群。没有设置亲和性或反亲和性规则。VMware View 解决方案需要两个 View
    连接服务器，这两个服务器都已虚拟化并放置在前述集群中。
- en: Through normal DRS activities, both View Connection Servers find themselves
    on Host1\. If Host1 were to have an outage, no new connections would be permitted
    into the VDI.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过正常的 DRS 操作，两个 View 连接服务器都被放置在 Host1 上。如果 Host1 发生故障，则不允许有新的连接进入 VDI。
- en: '![Anti-affinity](img/1124EN_07_12.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![Anti-affinity](img/1124EN_07_12.jpg)'
- en: In the previous illustration, the two View Connection Servers, View1 and View2
    have been placed in an anti-affinity rule, and they have opposing polarity. This
    rule states that the two View Connection Servers are never to reside on the same
    host as long as there are available hosts in the cluster.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示意图中，两个 View 连接服务器 View1 和 View2 已经设置了反亲和性规则，并且它们具有相反的极性。该规则表示，在集群中有可用主机时，两个
    View 连接服务器不能位于同一台主机上。
- en: With anti-affinity in place, a single host outage would not have the potential
    to bring down the entire VMware View Connection Server environment.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 配置反亲和性后，单一主机故障不会导致整个 VMware View 连接服务器环境崩溃。
- en: VMware vCenter Server
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VMware vCenter Server
- en: VMware View uses VMware vCenter for all provisioning tasks. Without a functioning
    VMware vCenter Server, it is impossible to create, refresh, recompose, rebalance,
    or delete vDesktops. Therefore, utmost importance must be placed on protecting
    the VMware vCenter Server(s) used in the solution.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: VMware View 使用 VMware vCenter 执行所有的配置任务。没有功能正常的 VMware vCenter Server，就无法创建、刷新、重新组合、重新平衡或删除
    vDesktops。因此，必须高度重视保护解决方案中使用的 VMware vCenter Server。
- en: 'There are two primary components to the VMware vCenter Server service. They
    are as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: VMware vCenter Server 服务有两个主要组件，具体如下：
- en: VMware vCenter Server service
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware vCenter Server 服务
- en: Backend database
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后端数据库
- en: The VMware vCenter Server service should be protected in a way that eliminates
    downtime or provides a **recovery time objective (RTO)** of less than one minute.
    For extremely active VDI, a prolonged downtime can result in an inability to provide
    desktop resources to requesting end users. The most robust way to protect the
    VMware vCenter Server service is VMware vCenter Server Heartbeat (more on this
    is mentioned next).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: VMware vCenter Server 服务应以一种消除停机时间或提供少于一分钟恢复时间目标（RTO）的方式进行保护。对于极为活跃的 VDI，长期停机可能导致无法向请求的终端用户提供桌面资源。保护
    VMware vCenter Server 服务的最强方法是 VMware vCenter Server Heartbeat（下面会进一步讨论）。
- en: The backend database used by VMware vCenter can reside on the same server (physical
    or virtual) or can reside on a separate database.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: VMware vCenter 使用的后端数据库可以驻留在同一台服务器（物理或虚拟）上，也可以驻留在独立的数据库服务器上。
- en: '| Component | Option 1 | Option 2 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 选项 1 | 选项 2 |'
- en: '| --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| vCenter Server | **Virtual** | Physical |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| vCenter Server | **虚拟** | 物理 |'
- en: '| Database location | On vCenter Server | **On one or more separate servers**
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 数据库位置 | 在 vCenter Server 上 | **在一个或多个独立的服务器上** |'
- en: '| Database protection | Backup solution | **Clustering solution** |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 数据库保护 | 备份解决方案 | **集群解决方案** |'
- en: Note
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The words in the preceding table that appear bold are the recommendations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述表格中，出现粗体字的部分是建议项。
- en: Cooperation from an organization's database team is important in VMware View
    solutions as a vCenter outage, which is typically database related, could wreak
    havoc on the VDI. Therefore, engaging with an organization's database team is
    a recommended part of the design process.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VMware View 解决方案中，来自组织数据库团队的合作非常重要，因为 vCenter 故障（通常与数据库相关）可能会对 VDI 造成严重影响。因此，建议在设计过程中与组织的数据库团队进行沟通。
- en: VMware vCenter Server Heartbeat
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VMware vCenter Server Heartbeat
- en: '**VMware vCenter Server Heartbeat (vCSH)** is a product from VMware that is
    powered by Neverfail. vCSH monitors and protects all of the necessary components
    of a functioning VMware vCenter Server infrastructure, including:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**VMware vCenter Server Heartbeat (vCSH)** 是 VMware 的一款产品，由 Neverfail 提供支持。vCSH
    监控并保护 VMware vCenter Server 基础架构中所有必要的组件，包括：'
- en: '**Server:** vCSH protects from a physical or virtual server failure or operating
    system fault (for example, Blue Screen of Death)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器：** vCSH 保护物理或虚拟服务器故障或操作系统故障（例如，蓝屏死机）'
- en: '**Network:** vCSH protects the network identity including IP address and DNS
    name of the vCenter Server'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络：** vCSH 保护网络身份，包括 vCenter Server 的 IP 地址和 DNS 名称。'
- en: '**Application:** vCSH protects the application environment specific to VMware
    vCenter Server service, and required files and registry entries'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用：** vCSH 保护与 VMware vCenter Server 服务相关的应用环境，以及所需的文件和注册表项。'
- en: '**Performance:** vCSH monitors the performance of the underlying physical or
    virtual server'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能：** vCSH 监控底层物理或虚拟服务器的性能。'
- en: '**Data:** vCSH monitors all data and relevant applications, and maintains a
    copy of the data, including database (if local)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据：** vCSH 监控所有数据和相关应用，并保持数据副本，包括数据库（如果是本地的）。'
- en: VMware vCenter Server Heartbeat requires two instances of VMware vCenter Server
    that are joined into a vCSH pair. The vCSH pair functions as a single VMware vCenter
    Server instance and shares the hostname, IP address, and other relevant information
    and configuration.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: VMware vCenter Server 心跳需要两个 VMware vCenter Server 实例，这两个实例会合并成一个 vCSH 配对。vCSH
    配对作为单一的 VMware vCenter Server 实例工作，共享主机名、IP 地址以及其他相关信息和配置。
- en: '![VMware vCenter Server Heartbeat](img/1124EN_07_13.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![VMware vCenter Server 心跳](img/1124EN_07_13.jpg)'
- en: vCSH replicates data asynchronously from the primary vCenter Server in the pair
    to the secondary vCenter Server in the pair. VMware vCenter is also aware of the
    VMware View Composer service to ensure that it's protected as well.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: vCSH 以异步方式将数据从配对中的主 vCenter Server 复制到次 vCenter Server。VMware vCenter 还会识别 VMware
    View Composer 服务，以确保它也受到保护。
- en: Why VMware vCenter Server Heartbeat should be used
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么应该使用 VMware vCenter Server 心跳
- en: For production environments, sizable deployments, or solutions with high criticality,
    VMware vCenter Server Heartbeat should always be used as it protects the most
    vulnerable component of the VDI.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产环境、大型部署或具有高关键性的解决方案，应该始终使用 VMware vCenter Server 心跳，因为它保护了 VDI 中最脆弱的组件。
- en: The storage array typically has high availability built-in with RAID, multiple
    storage processors, power supplies, fans, extra disks, and so on.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 存储阵列通常具有内建的高可用性，支持 RAID、多存储处理器、电源供应、风扇、额外磁盘等。
- en: The physical hosts are protected by multiple NICs, multiple power supplies,
    and VMware functionality, such as VMware HA.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 物理主机通过多个网卡、多重电源供应和 VMware 功能（如 VMware HA）来进行保护。
- en: The networking layer is protected by redundancy in hardware and network paths.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 网络层通过硬件和网络路径的冗余来进行保护。
- en: The View Connection Server is protected by duplicate instances, load balancing,
    and potentially by VMware Fault Tolerance.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: View 连接服务器通过重复实例、负载均衡，以及可能通过 VMware 故障容错来保护。
- en: However, as a single instance of VMware vCenter Server can be responsible for
    over a dozen physical hosts and potentially many hundreds of vDesktops, the protection
    of its state is of paramount importance.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于单个 VMware vCenter Server 实例可能负责超过十台物理主机，并且可能管理数百个 vDesktops，因此保护其状态至关重要。
- en: VMware vCenter Server Heartbeat has the ability to protect not only the VMware
    vCenter Server service, but also the vCenter database (if local) and the VMware
    View Composer service.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: VMware vCenter Server 心跳不仅能够保护 VMware vCenter Server 服务，还能保护 vCenter 数据库（如果是本地的）和
    VMware View Composer 服务。
- en: VMware View
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VMware View
- en: VMware View is responsible for processing incoming requests for vDesktops, interacting
    with VMware vCenter Server to provision, recompose, and delete vDesktops; as well
    as a variety of other tasks necessary for a properly functioning VDI.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: VMware View 负责处理来自 vDesktops 的请求，与 VMware vCenter Server 交互以配置、重组和删除 vDesktops；以及执行其他各种必要任务，以确保
    VDI 正常运行。
- en: Replica
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复制
- en: 'When installing the VMware View Connection Server, there are four installation
    types. They are as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 VMware View 连接服务器时，有四种安装类型。具体如下：
- en: Standard
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准
- en: Replica
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制
- en: Security
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全
- en: Transfer
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传输
- en: For the first VMware View Connection Server instance in a View Connection Server
    pool, the standard installation should be selected. However, to eliminate the
    View Connection Server as a single point of failure, a second (and additional)
    View Connection Server can be installed. Once a View Connection Server exists
    in the infrastructure, additional View Connection Servers can be joined to the
    original, forming a View Connection Server pool. To join a new View Connection
    Server to an existing View Connection Server or View Connection Server pool, select
    the **replica** installation mode.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 View Connection Server 池中的第一个 VMware View 连接服务器实例，应选择标准安装。然而，为了消除 View 连接服务器作为单点故障的风险，可以安装第二个（以及其他）View
    连接服务器。一旦 View 连接服务器存在于基础设施中，可以将额外的 View 连接服务器加入到原来的服务器，形成 View 连接服务器池。要将新的 View
    连接服务器加入现有的 View 连接服务器或 View 连接服务器池，请选择 **副本** 安装模式。
- en: When a replica View Connection Server instance is created, it copies the VMware
    View LDAP configuration from the existing View Connection Server instance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建副本 View 连接服务器实例时，它会从现有的 View 连接服务器实例复制 VMware View LDAP 配置。
- en: Load balancing
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负载均衡
- en: The VMware View Connection Servers are responsible for brokering the connection
    between an authorized end user and a vDesktop in the VDI. Therefore, if there
    are no available VMware View Connection Servers, no new connections can be made
    to the VDI. However, the existing connections will not be affected if there are
    no available VMware View Connection Servers.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: VMware View 连接服务器负责在授权的最终用户与 VDI 中的 vDesktop 之间建立连接。因此，如果没有可用的 VMware View 连接服务器，就无法建立新的连接到
    VDI。然而，如果没有可用的 VMware View 连接服务器，现有的连接将不受影响。
- en: '![Load balancing](img/1124EN_07_14.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![负载均衡](img/1124EN_07_14.jpg)'
- en: Therefore, it is important to protect the available View Connection Server(s)
    in the VDI. It is best practice to have a minimum of two VMware View Connection
    Server(s) (or potentially one protected by VMware Fault Tolerance). The easiest
    way to accomplish resilience for the VMware View Connection Servers is to use
    a load balancing solution. There are various load balancing solutions available,
    including **Microsoft Network Load Balancer (NLB)** and hardware appliances from
    companies like F5 (preferred).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，保护 VDI 中可用的 View 连接服务器非常重要。最佳做法是至少有两个 VMware View 连接服务器（或可能有一个通过 VMware 容错保护的服务器）。实现
    VMware View 连接服务器的弹性最简单的方法是使用负载均衡解决方案。市场上有多种负载均衡解决方案，包括 **Microsoft 网络负载均衡器 (NLB)**
    和来自 F5 等公司提供的硬件设备（首选）。
- en: A load balancing solution will create a virtual IP address that will be used
    by end users to connect to the VDI. Behind the virtual IP address will be the
    actual IP addresses of all of the VMware View Connection Servers in the load balancing
    pool. If a VMware View Connection Server is not responsive to a ping (for example),
    it will be removed from the load balancing pool to ensure that incoming end user
    requests are not routed to an unavailable View Connection Server.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡解决方案将创建一个虚拟 IP 地址，最终用户将通过该虚拟 IP 地址连接到 VDI。在虚拟 IP 地址背后，将是负载均衡池中所有 VMware
    View 连接服务器的实际 IP 地址。如果某个 VMware View 连接服务器没有响应 ping 请求（例如），它将被从负载均衡池中移除，以确保传入的最终用户请求不会被路由到不可用的
    View 连接服务器。
- en: Many load balancing solutions also offer the ability to monitor availability
    via HTTP GET and similar commands to ensure that not only is the server online
    but that it is also responsive to web-based requests.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 许多负载均衡解决方案还提供通过 HTTP GET 和类似命令监控可用性的功能，确保服务器不仅在线，而且对基于 Web 的请求做出响应。
- en: VMware Fault Tolerance
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VMware 容错
- en: '**VMware Fault Tolerance (FT)** can be used as an additional layer of protection
    for the VMware View Connection Server(s). VMware FT protects a VM by creating
    and maintaining a secondary VM that is identical to the primary one. The secondary
    VM is continuously available to replace the primary VM in case of failure of the
    host where the primary VM resides.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**VMware 容错 (FT)** 可以作为 VMware View 连接服务器的额外保护层。VMware FT 通过创建并维护一个与主虚拟机完全相同的副本虚拟机来保护虚拟机。副本虚拟机会持续可用，以便在主虚拟机所在主机发生故障时替代主虚拟机。'
- en: In VMware FT, there is no downtime (unlike VMware HA). VMware FT does have several
    limitations including supported hardware, number of vCPUs (currently 1), and so
    on.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VMware 容错中，不会有停机时间（与 VMware HA 不同）。但 VMware FT 确实有一些限制，包括支持的硬件、vCPU 数量（目前为
    1）等。
- en: VMware FT takes inputs and events that occur on the primary VM and transfer
    them to the secondary VM, which is running on a separate host in the cluster.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: VMware FT 会将主虚拟机上的输入和事件传递到副虚拟机，副虚拟机在集群中的另一台主机上运行。
- en: VMware FT does impact the virtual infrastructure design, as it requires a separate
    and dedicated NIC for FT Logging. The FT Logging and vMotion NICs must reside
    on separate subnets. Also, no more than four VMware FT-enabled virtual machine
    primaries or secondaries can reside on any single ESX host.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: VMware FT 确实会影响虚拟基础架构设计，因为它需要为 FT 日志记录配置单独的专用 NIC。FT 日志记录和 vMotion NIC 必须位于不同的子网中。此外，任何单一的
    ESX 主机上最多只能容纳四个启用 VMware FT 的虚拟机主虚拟机或副虚拟机。
- en: Design impact when using VMware FT
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 VMware FT 时的设计影响
- en: To properly illustrate the design impact of using VMware FT, the following example
    will be used.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确说明使用 VMware FT 的设计影响，将使用以下示例。
- en: Through a thorough analysis, it has been determined that CustomerA requires
    four VMware View Connection Servers to satisfy the demand of incoming requests.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 经过深入分析，已确定 CustomerA 需要四个 VMware View Connection Servers，以满足进入请求的需求。
- en: '![Design impact when using VMware FT](img/1124EN_07_15.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![使用 VMware FT 时的设计影响](img/1124EN_07_15.jpg)'
- en: As part of the solution, four VMware View Connection Servers are installed and
    configured, and placed behind a load balancer.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解决方案的一部分，安装并配置了四个 VMware View Connection Servers，并将其放置在负载均衡器后面。
- en: '![Design impact when using VMware FT](img/1124EN_07_16.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![使用 VMware FT 时的设计影响](img/1124EN_07_16.jpg)'
- en: If a VMware View Connection Server has a critical fault (for example, Blue Screen
    of Death), only three connection servers are available to the end users. There
    is a possibility that the three remaining connection servers cannot handle the
    load and some end users are unable to connect to the VDI.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个 VMware View Connection Server 发生严重故障（例如，蓝屏死机），则仅剩三个连接服务器可供最终用户使用。剩余的三个连接服务器可能无法处理负载，导致部分最终用户无法连接到
    VDI。
- en: '![Design impact when using VMware FT](img/1124EN_07_17.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![使用 VMware FT 时的设计影响](img/1124EN_07_17.jpg)'
- en: The only way to restore connectivity for a failed View Connection Server that's
    had a critical fault is to either restore from a previous backup or build a new
    View Connection Server, and add it to the load balanced pool, as illustrated in
    the preceding diagram.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复因故障导致的 View Connection Server 连接的唯一方法是从以前的备份中恢复，或者构建新的 View Connection Server，并将其添加到负载均衡池中，如前图所示。
- en: If a VMware View Connection Server resides on a host that has a failure, VMware
    HA will power the virtual machine up on an available host in the cluster. There
    may be several minutes of downtime for some end users (if the remaining three
    View Connection Servers are unable to handle the load). However, after 3 to 5
    minutes, full connectivity should be restored.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个 VMware View Connection Server 所在主机发生故障，VMware HA 将在集群中的可用主机上启动该虚拟机。部分最终用户可能会经历几分钟的停机时间（如果剩余的三个
    View Connection Servers 无法处理负载）。然而，3 到 5 分钟后，完整的连接应该恢复。
- en: '![Design impact when using VMware FT](img/1124EN_07_18.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![使用 VMware FT 时的设计影响](img/1124EN_07_18.jpg)'
- en: By leveraging VMware FT to protect the View Connection Servers, there will be
    zero impact should a physical host failure occur in this solution (assuming anti-affinity
    has separated primary and secondary VMs).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 VMware FT 保护 View Connection Servers，如果发生物理主机故障，这种解决方案将不会受到任何影响（假设反亲和性已将主虚拟机和副虚拟机分开）。
- en: '![Design impact when using VMware FT](img/1124EN_07_19.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![使用 VMware FT 时的设计影响](img/1124EN_07_19.jpg)'
- en: In the event of a physical host failure that impacts a View Connection Server,
    VMware FT immediately makes the secondary (shown in bottom row in the preceding
    diagram) active. Through technology from VMware vLockstep, the secondary is an
    identical copy of the primary that resided on the failed host. Once the failover
    has successfully occurred, the secondary is now marked as the primary. In addition,
    a new secondary is spawned from the newly appointed primary.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发生影响 View Connection Server 的物理主机故障，VMware FT 会立即使副虚拟机（如前图底行所示）变为活动状态。通过 VMware
    vLockstep 技术，副虚拟机是主虚拟机的完全副本，主虚拟机原本驻留在故障主机上。故障切换成功后，副虚拟机会被标记为主虚拟机。此外，将从新指定的主虚拟机创建新的副虚拟机。
- en: Note
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: However, VMware FT does not protect against guest operating system failures
    (for example, Blue Screen of Death). Therefore, coupling VMware FT with VM Monitoring
    via VMware HA is the most robust solution possible.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，VMware FT 并不防护来宾操作系统故障（例如，蓝屏死机）。因此，将 VMware FT 与 VMware HA 结合使用 VM 监控，是最强大的解决方案。
- en: While VMware FT is a useful technology in protecting VMware View Connection
    Servers, most virtualization architects would prefer to simply add an additional
    View Connection Server to the original design instead of adding the complexity
    of VMware FT.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 VMware FT 是保护 VMware View 连接服务器的有用技术，但大多数虚拟化架构师更倾向于仅仅向原设计中添加一个额外的 View 连接服务器，而不是增加
    VMware FT 的复杂性。
- en: Parent vDesktop and templates
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 父 vDesktop 和模板
- en: Virtual machine templates are used by VMware View when deploying vDesktops with
    the **full virtual machine** option selected. Standard virtual machines (not templates)
    are used by VMware View when deploying vDesktops with the **View Composer linked
    clones** option selected. For a virtual machine to be seen by View Composer, it
    must have at least one snapshot. View Composer deploys all vDesktops in the pool
    from the selected snapshot.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: VMware View 在部署选择了**完整虚拟机**选项的 vDesktops 时使用虚拟机模板。在选择了**View Composer 链接克隆**选项时，VMware
    View 使用标准虚拟机（而不是模板）部署 vDesktops。为了使虚拟机被 View Composer 识别，它必须至少有一个快照。View Composer
    从选定的快照部署池中的所有 vDesktops。
- en: It is important to understand that if the parent vDesktop (for Linked Clone
    pools) or the gold vDesktop template (for Full Desktop pools) are not available,
    then new vDesktops cannot be provisioned.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，如果父 vDesktop（对于 Linked Clone 池）或黄金 vDesktop 模板（对于 Full Desktop 池）不可用，则无法配置新的
    vDesktops。
- en: Templates
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模板
- en: Virtual machine templates are a bit confusing to protect. When creating a virtual
    machine template or adding a virtual machine template to the inventory, the administrator
    must select a specific host within a cluster. According to the **graphical user
    interface (GUI)**, "Choose a specific host within the cluster. On high-availability
    clusters and fully-manual dynamic workload management clusters, each template
    must be assigned to a specific host."
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机模板的保护稍显复杂。当创建虚拟机模板或将虚拟机模板添加到库存中时，管理员必须在集群中选择一个特定的主机。根据**图形用户界面（GUI）**，"选择集群中的特定主机。在高可用性集群和完全手动的动态工作负载管理集群中，每个模板必须分配给特定的主机。"
- en: Therefore, if the gold template for vDesktops resides on Host1 and if Host1
    experiences a failure, VMware HA will not recover this template. Instead, the
    original template will be shown as unavailable within vCenter. From this point,
    the original inventory entry in vCenter for the template can be removed and then
    the template can be re-added. This is possible because while the host is unavailable,
    the virtual machine template actually resides on shared storage (assuming that
    best practice was followed).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果 vDesktops 的黄金模板存储在 Host1 上，且 Host1 出现故障，VMware HA 将无法恢复该模板。相反，原始模板将在 vCenter
    中显示为不可用。从此时起，vCenter 中原始模板的库存条目可以被删除，然后模板可以重新添加。这是可能的，因为虽然主机不可用，但虚拟机模板实际上存储在共享存储中（假设遵循了最佳实践）。
- en: When assisting an organization with operational readiness, the preceding recovery
    process should be listed in a Standard Operating Procedure manual.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在协助组织进行操作准备时，前述的恢复过程应列入标准操作程序手册中。
- en: Parent vDesktops with snapshots
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有快照的父 vDesktops
- en: To protect the parent vDesktop and its snapshot, a simple clone virtual machine
    task will not suffice. This is because the clone task consolidates the snapshot
    tree and thus removes all snapshots associated with the base virtual machine.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要保护父 vDesktop 及其快照，仅仅执行一个简单的克隆虚拟机任务是不够的。这是因为克隆任务会合并快照树，从而删除所有与基础虚拟机相关的快照。
- en: '![Parent vDesktops with snapshots](img/1124EN_07_20.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![带有快照的父 vDesktops](img/1124EN_07_20.jpg)'
- en: Therefore, it is imperative that VMware HA should be used to protect the parent
    vDesktop. As the parent vDesktop is simply a virtual machine with snapshots (as
    opposed to a virtual machine template in the preceding scenario), VMware HA will
    change the ownership of the parent vDesktop to an available host in the cluster
    during a host outage.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，必须使用 VMware HA 来保护父 vDesktop。由于父 vDesktop 仅仅是一个带有快照的虚拟机（而不是前述场景中的虚拟机模板），在主机故障时，VMware
    HA 会将父 vDesktop 的所有权转移到集群中可用的主机上。
- en: User personas
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户角色
- en: For environments that leverage a user persona solution, such as Liquidware Labs
    ProfileUnity™, placing the user personas on a highly available network share is
    critical to ensure end user data is always available. By using the **Distributed
    File System (DFS)** service or **Distributed File System Replication (DFS-R)**
    service, a file share storing user personas will still be available in the event
    of a file server failure. In addition, with DFS-R, user personas can be replicated
    to other servers in the same site or other sites. DFS-R enables a VDI to provide
    **Continuity of Operations (COOP)** by ensuring that the file share containing
    the user personas has its data replicated offsite.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用用户角色解决方案的环境，例如 Liquidware Labs ProfileUnity™，将用户角色放置在高可用的网络共享上对于确保终端用户数据始终可用至关重要。通过使用**分布式文件系统
    (DFS)** 服务或**分布式文件系统复制 (DFS-R)** 服务，存储用户角色的文件共享即使在文件服务器发生故障时仍然可用。此外，使用 DFS-R，用户角色可以在同一站点或其他站点的其他服务器之间进行复制。DFS-R
    通过确保存储用户角色的文件共享将数据复制到异地，从而实现VDI的**持续运营 (COOP)**。
- en: Microsoft DFS also leverages Active Directory sites to ensure that an end user
    is retrieving their persona from the nearest server participating in the DFS/DFS-R
    group. In addition, site costing­ can be used to state which is the least expensive
    target selection for end users attempting to retrieve their user persona from
    a network share.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 微软 DFS 还利用 Active Directory 站点来确保终端用户从参与 DFS/DFS-R 组的最近服务器中获取他们的角色。此外，站点成本可以用于确定哪个是终端用户尝试从网络共享获取其用户角色时最便宜的目标选择。
- en: 'The following table shows a summary of the types of failures for all components:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了所有组件故障类型的汇总：
- en: '| Component | Type of failure | Protected by | Downtime | Notes |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 故障类型 | 保护方式 | 停机时间 | 备注 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| vCenter Server | Underlying physical host | VMware HA | Approximately 10
    minutes | During the outage, vDesktop tasks such as provision, Recompose, and
    so on are unavailable. vCenter may take longer to start (as opposed to View Connection
    Server) because of the database actions that are performed during an initial service
    start. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| vCenter 服务器 | 底层物理主机 | VMware HA | 大约 10 分钟 | 在故障期间，vDesktop 任务（如创建、重新合成等）将无法使用。由于在初始服务启动期间执行的数据库操作，vCenter
    可能需要更长时间才能启动（与 View 连接服务器不同）。 |'
- en: '| vCenter Server | Underlying physical host | vCenter Server Heartbeat | Less
    than 1 minute | It requires a second vCenter Server instance. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| vCenter 服务器 | 底层物理主机 | vCenter 服务器心跳 | 少于 1 分钟 | 需要第二个 vCenter 服务器实例。 |'
- en: '| vCenter Server | Operating system (Blue Screen of Death) | vCenter Server
    Heartbeat | Less than 1 minute | It requires a second vCenter Server instance.
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| vCenter 服务器 | 操作系统（蓝屏死机） | vCenter 服务器心跳 | 少于 1 分钟 | 需要第二个 vCenter 服务器实例。
    |'
- en: '| vCenter Database | Any | Clustering solution | Less than 1 minute | It requires
    two database servers. |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| vCenter 数据库 | 任何 | 集群解决方案 | 少于 1 分钟 | 需要两台数据库服务器。 |'
- en: '| vCenter Database | Database corruption | Backup/restore/snapshots | Varies
    | The time to restore depends on the solution used, speed of media, and throughput
    available. |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| vCenter 数据库 | 数据库损坏 | 备份/恢复/快照 | 变化 | 恢复时间取决于所使用的解决方案、媒体速度和可用的吞吐量。 |'
- en: '| View Connection Server | Underlying physical host | VMware HA | Approximately
    5 minutes | It requires multiple View Connection Servers behind a load balancer
    to mitigate impact to the end users. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| View 连接服务器 | 底层物理主机 | VMware HA | 大约 5 分钟 | 需要多个 View 连接服务器位于负载均衡器后，以减轻对终端用户的影响。
    |'
- en: '| View Connection Server | Underlying physical host | Load balancer | 0 minutes
    | It requires multiple View Connection Servers behind a load balancer to mitigate
    impact to the end users. Without VMware HA, the total number of inbound connections
    may be impacted. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| View 连接服务器 | 底层物理主机 | 负载均衡器 | 0 分钟 | 需要多个 View 连接服务器位于负载均衡器后，以减轻对终端用户的影响。如果没有
    VMware HA，可能会影响总的入站连接数。 |'
- en: '| View Connection Server | Underlying physical host | VMware FT | 0 minutes
    | It does not protect against guest operating system failures (see VM Monitoring
    with VMware HA). |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| View 连接服务器 | 底层物理主机 | VMware FT | 0 分钟 | 它不能保护客操作系统故障（参见 VMware HA 的虚拟机监控）。
    |'
- en: '| View Connection Server | View Connection Server service | Load balancer |
    0 minutes | It requires multiple View Connection Servers behind a load balancer
    to mitigate impact to the end users. |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| View 连接服务器 | View 连接服务器服务 | 负载均衡器 | 0 分钟 | 需要多个 View 连接服务器位于负载均衡器后，以减轻对终端用户的影响。
    |'
- en: '| View Connection Server | Operating system (Blue Screen of Death) | VM Monitoring
    with VMware HA | Approximately 5 minutes | It requires multiple View Connection
    Servers behind a load balancer to mitigate impact to the end users. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 视图连接服务器 | 操作系统（蓝屏死机） | VMware HA 的虚拟机监控 | 大约 5 分钟 | 需要多个视图连接服务器位于负载均衡器后，以减少对终端用户的影响。
    |'
- en: '| View Composer | Underlying physical host | VMware HA | Approximately 10 minutes
    | During the outage, vDesktop tasks such as provision, Recompose, and so on are
    unavailable. vCenter may take longer to start (as opposed to View Connection Server)
    because of the database actions that are performed during an initial service start.
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 视图编排器 | 底层物理主机 | VMware HA | 大约 10 分钟 | 在停机期间，vDesktop 任务（如配置、重组等）不可用。vCenter
    可能需要更长时间启动（与视图连接服务器不同），因为在初始服务启动过程中会执行数据库操作。 |'
- en: '| View Composer | Underlying physical host | vCenter Server Heartbeat | Less
    than 1 minute | It requires a second vCenter Server instance. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 视图编排器 | 底层物理主机 | vCenter 服务器心跳 | 少于 1 分钟 | 需要第二个 vCenter 服务器实例。 |'
- en: '| View Composer | Operating system (Blue Screen of Death) | vCenter Server
    Heartbeat | Less than 1 minute | It requires a second vCenter Server instance.
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 视图编排器 | 操作系统（蓝屏死机） | vCenter 服务器心跳 | 少于 1 分钟 | 需要第二个 vCenter 服务器实例。 |'
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Thus far, some of the most important design considerations (for example, persistent
    or non-persistent) have been addressed, as well as proper sizing of the overall
    VDI. Designing a VMware View solution that is highly resilient is also paramount
    to a production-quality solution. In the next chapter, the last major hurdle,
    storage design, will be discussed. An improperly sized VDI can result in a poor
    end user experience. A VDI without redundancy can result in unexpected outages
    and downtime. A VDI with improperly designed storage can not only result in poor
    end user experience but also significantly add to the overall cost of the VDI
    solution. As storage is often one of the expensive components, (if not the most
    expensive component) of a VDI solution, judicious sizing that will still meet
    the requirements is key.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，已经解决了一些最重要的设计考虑因素（例如，持久性或非持久性），以及整体 VDI 的适当大小。设计一个高度可靠的 VMware View 解决方案对于生产质量的解决方案至关重要。在下一章中，将讨论最后一个主要难题——存储设计。一个大小不当的
    VDI 可能会导致用户体验不佳。没有冗余的 VDI 可能会导致意外的停机和停机时间。一个存储设计不当的 VDI 不仅会导致用户体验差，还可能大大增加 VDI
    解决方案的整体成本。由于存储通常是 VDI 解决方案中最昂贵的组件之一（如果不是最昂贵的组件），因此合理的尺寸规划至关重要，必须在满足需求的前提下进行。
