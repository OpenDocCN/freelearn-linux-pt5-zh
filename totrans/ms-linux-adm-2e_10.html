<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-196"><a id="_idTextAnchor212"/>10</h1>
<h1 id="_idParaDest-197"><a id="_idTextAnchor213"/>Disaster Recovery, Diagnostics, and Troubleshooting</h1>
<p>In this chapter, you will learn how to do a system backup and restore in a disaster recovery scenario, and how to <strong class="bold">diagnose</strong> and <strong class="bold">troubleshoot</strong> a common array of problems. These are skills that each Linux system administrator needs to have if they wish to be prepared for worst-case scenarios such as power outages, theft, or hardware failure. The world’s IT backbone runs on Linux and we need to be prepared for anything that life throws at us.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>Planning for disaster recovery</li>
<li>Backing up and restoring the system</li>
<li>Introducing common Linux diagnostic tools for troubleshooting</li>
</ul>
<h1 id="_idParaDest-198"><a id="_idTextAnchor214"/>Technical requirements</h1>
<p>No special technical requirements are needed for this chapter, just a working installation of Linux on your system or even two different working systems on your local network for some of the examples used. Ubuntu and Fedora are equally suitable for this chapter’s exercises, but in this chapter, we’ll be using Ubuntu 22.04.2 LTS Server and Desktop editions.</p>
<h1 id="_idParaDest-199"><a id="_idTextAnchor215"/>Planning for disaster recovery</h1>
<p>Managing risks is an<a id="_idIndexMarker1568"/> important asset for every business or individual. The responsibility of this is tremendous for everyone involved in system administration. For all businesses, managing risks should be part of a wider <strong class="bold">risk management strategy</strong>. There <a id="_idIndexMarker1569"/>are various types of risks in IT, starting from natural hazards directly impacting data centers or business locations, all the way up to cyber security threats. IT’s footprint inside a company has exponentially grown in the last decade. Nowadays, there<a id="_idIndexMarker1570"/> is no activity that does not involve some sort of IT operations being behind it, be it inside small businesses, big corporations, government agencies, or the health and education public sectors, just to give a few examples. Each activity is unique in its own way, so it needs a specific type of assessment. Unfortunately, with regard to the information security field, risk management has largely evolved into a one-size-fits-all practice, based on checklists that should be implemented by IT management. Let’s begin with a brief introduction to risk management before moving on to learning how we can assess risk.</p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor216"/>A brief introduction to risk management</h2>
<p>What is<a id="_idIndexMarker1571"/> <strong class="bold">risk management</strong>? In a nutshell, it is comprised of specific operations that are set to mitigate any possible threat that could impact the overall continuity of a business. The risk management process is crucial for every IT department.</p>
<p>Risk management frameworks initially arose in the United States due to the <strong class="bold">Federal Information Systems Modernization Act</strong> (<strong class="bold">FISMA</strong>) laws, which started in 2002. This was the time<a id="_idIndexMarker1572"/> when the United States <strong class="bold">National Institute of Standards and Technology </strong>(<strong class="bold">NIST</strong>) began to create new standards and methods for<a id="_idIndexMarker1573"/> cyber security assessments among all US government agencies. Therefore, security certifications and compliances are of utmost importance for every Linux distribution provider that sees itself as a worthy competitor in the corporate and governmental space. Similar to the US certification bodies discussed previously, there are other agencies in the UK and Russia that develop specific security certifications. In this respect, all <a id="_idIndexMarker1574"/>major Linux <a id="_idIndexMarker1575"/>distributions from Red Hat, SUSE, and Canonical have certifications from NIST, the UK’s <strong class="bold">National Cyber Security Centre</strong> (<strong class="bold">NCSC</strong>), or Russia’s <strong class="bold">Federal Service for Technic and Export </strong><strong class="bold">Control</strong> (<strong class="bold">FSTEC</strong>).</p>
<p>The risk management framework, according to NIST SP 800-37r2 (see the official NIST website at <a href="https://csrc.nist.gov/publications/detail/sp/800-37/rev-2/final">https://csrc.nist.gov/publications/detail/sp/800-37/rev-2/final</a>), has seven steps, starting with preparing for the framework’s execution, up to monitoring the organization’s systems on a daily basis. We will not discuss those steps in detail; instead, we will provide a link at the end of this chapter for NIST’s official documentation. In a nutshell, the risk management framework is comprised of several important branches, such as the following:</p>
<ul>
<li><strong class="bold">Inventory</strong>: A thorough <a id="_idIndexMarker1576"/>inventory of all available systems that are on-premises, and a list of all software solutions</li>
<li><strong class="bold">System categorization</strong>: A<a id="_idIndexMarker1577"/>ssesses the impact level for each data type that’s used with regard to availability, integrity, and confidentiality</li>
<li><strong class="bold">Security control</strong>: Subject to detailed procedures with regard to hundreds of computer systems’ security – a compendium of NIST security controls can be found under SP800-53r4 (the following is a link to the official NIST website: <a href="https://csrc.nist.gov/publications/detail/sp/800-53/rev-4/final">https://csrc.nist.gov/publications/detail/sp/800-53/rev-4/final</a>)</li>
<li><strong class="bold">Risk assessment</strong>: A <a id="_idIndexMarker1578"/>series of steps that cover threat source identification, vulnerability identification, impact determination, information sharing, risk monitoring, and periodical updates</li>
<li><strong class="bold">System security plan</strong>: A <a id="_idIndexMarker1579"/>report based on every security control and how future actions are assessed, including their implementation and effectiveness</li>
<li><strong class="bold">Certification, accreditation, assessment, and authorization</strong>: The process of reviewing security<a id="_idIndexMarker1580"/> assessments and highlighting security issues and effective resolutions that are detailed in a future plan of action</li>
<li><strong class="bold">Plan of action</strong>: A<a id="_idIndexMarker1581"/> tool that’s used to track security weaknesses and apply the correct response procedures</li>
</ul>
<p>There are many types of risks when it comes to information technology, including hardware failure, software errors, spam and viruses, human error, and natural disasters (fires, floods, earthquakes, hurricanes, and so on). There are also risks of a more criminal nature, including security breaches, employee dishonesty, corporate espionage, or anything else that could be considered a cybercrime. These risks can be addressed by implementing an appropriate risk management strategy. As a basis, such a strategy should have five distinct steps:</p>
<ol>
<li><strong class="bold">Identifying risk</strong>: Identifying<a id="_idIndexMarker1582"/> possible threats and vulnerabilities that could impact your ongoing IT operations.</li>
<li><strong class="bold">Analyzing risk</strong>: Deciding how big or small it is, based on thorough studies.</li>
<li><strong class="bold">Evaluating risk</strong>: Evaluating the impact that it could have on your operations; the immediate action is to respond to the risk based on the impact it has. This calls for real actions to be performed at every level of your operations.</li>
<li><strong class="bold">Responding to risk</strong>: Activating your <strong class="bold">disaster recovery plans</strong> (<strong class="bold">DRPs</strong>), combined with<a id="_idIndexMarker1583"/> strategies for prevention and mitigation.</li>
<li><strong class="bold">Monitoring and reviewing risk</strong>: Triggering a drastic monitoring and reviewing strategy will ensure that all the IT teams know how to respond to the risk and have the tools and abilities to isolate it and enforce the company’s infrastructure.</li>
</ol>
<p>Risk assessment is extremely important to any business and should be taken very seriously by IT management. Now that we’ve tackled some concepts of risk management, it is time to explain what it really is.</p>
<h2 id="_idParaDest-201"><a id="_idTextAnchor217"/>Risk calculation</h2>
<p><strong class="bold">Risk assessment</strong>, also known as <strong class="bold">risk calculation</strong> or <strong class="bold">risk analysis</strong>, refers to <a id="_idIndexMarker1584"/>the action of finding and calculating solutions to possible threats and vulnerabilities. The following are some <a id="_idIndexMarker1585"/>basic terms you should know when you talk about risk impact:</p>
<ul>
<li>The <strong class="bold">annual loss expectancy</strong> (<strong class="bold">ALE</strong>) <a id="_idIndexMarker1586"/>defines the loss that’s expected in 1 year.</li>
<li>The <strong class="bold">single loss expectancy</strong> (<strong class="bold">SLE</strong>) <a id="_idIndexMarker1587"/>represents how much loss is expected at any given time.</li>
<li>The <strong class="bold">annual rate of occurrence</strong> (<strong class="bold">ARO</strong>) is the<a id="_idIndexMarker1588"/> likeliness of a risky event occurring within 1 year.</li>
<li>The <strong class="bold">risk calculation formula</strong> is <em class="italic">SLE x ARO = ALE</em>. There is a monetary value that each element of the<a id="_idIndexMarker1589"/> formula will provide, so the final result is also expressed as a monetary value. This is a formula that is useful to know.</li>
<li>The <strong class="bold">mean time between failures</strong> (<strong class="bold">MTBF</strong>) is used<a id="_idIndexMarker1590"/> to measure the time between anticipated and reparable failures.</li>
<li>The <strong class="bold">mean time to failure</strong> (<strong class="bold">MTTF</strong>) is the average time the system can operate before experiencing an irreparable failure.</li>
<li>The <strong class="bold">mean time to restore</strong> (<strong class="bold">MTTR</strong>) measures<a id="_idIndexMarker1591"/> the time needed to repair an affected system.</li>
<li>The <strong class="bold">recovery time objective</strong> (<strong class="bold">RTO</strong>) represents<a id="_idIndexMarker1592"/> the maximum time that’s allocated for downtime.</li>
<li>The <strong class="bold">recovery point objective</strong> (<strong class="bold">RPO</strong>) defines <a id="_idIndexMarker1593"/>the time when a system needs to be restored.</li>
</ul>
<p>Knowing those terms will help you understand risk assessments so that you can perform a well-documented assessment if or when needed. Risk assessment is based on two major types of actions (or better said, strategies):</p>
<ul>
<li><strong class="bold">Proactive actions</strong>:<ul><li><strong class="bold">Risk avoidance</strong>: Based <a id="_idIndexMarker1594"/>on risk identification and finding a quick solution to avoid its occurrence</li><li><strong class="bold">Risk mitigation</strong>: Based on <a id="_idIndexMarker1595"/>actions taken to reduce the occurrence of a possible risk</li><li><strong class="bold">Risk transference</strong>: Transferring<a id="_idIndexMarker1596"/> the risk’s possible outcome with an external entity</li><li><strong class="bold">Risk deterrence</strong>: <a id="_idIndexMarker1597"/>Based on specific systems and policies that should discourage any attacker from exploiting the system</li></ul></li>
<li><strong class="bold">Non-active actions</strong>:<ul><li><strong class="bold">Risk acceptance</strong>: Accepting<a id="_idIndexMarker1598"/> the risk if the other proactive actions could exceed the cost of the harm that’s done by the risk</li></ul></li>
</ul>
<p>The strategies described here can be applied to the risk associated with generic, on-premises computing, but nowadays, cloud computing is slowly and surely taking over the world. So, how could these risk strategies apply to cloud computing? In cloud computing, you use the infrastructure of a third party, but with your own data. Even though we will start discussing Linux in the cloud in <a href="B19682_14.xhtml#_idTextAnchor299"><em class="italic">Chapter 14</em></a>, <em class="italic">Short Introduction to</em><em class="italic"> Computing</em>, there are some concepts that we will introduce now. As we mentioned earlier, the cloud is taking the infrastructure operations from your on-premises environment to a larger player, such as Amazon, Microsoft, or Google. This could generally be seen as outsourcing. This means that some risks that were a threat when you were running services on-premises are now transferred to third parties.</p>
<p>There are three major cloud paradigms that are now buzzwords all over technology media:</p>
<ul>
<li><strong class="bold">Software as a service</strong> (<strong class="bold">SaaS</strong>): This is a software <a id="_idIndexMarker1599"/>solution for companies<a id="_idIndexMarker1600"/> looking to reduce<a id="_idIndexMarker1601"/> IT costs and rely on software subscriptions. Some <a id="_idIndexMarker1602"/>examples of<a id="_idIndexMarker1603"/> SaaS solutions are <strong class="bold">Slack</strong>, <strong class="bold">Microsoft 365</strong>, <strong class="bold">Google Apps</strong>, and <strong class="bold">Dropbox</strong>, among others.</li>
<li><strong class="bold">Platform as a service</strong> (<strong class="bold">PaaS</strong>): The way <a id="_idIndexMarker1604"/>you get software applications to your clients using another’s infrastructure, runtimes, and dependencies <a id="_idIndexMarker1605"/>is also <a id="_idIndexMarker1606"/>known as an application <a id="_idIndexMarker1607"/>platform. This<a id="_idIndexMarker1608"/> can be on a<a id="_idIndexMarker1609"/> public cloud, on a private cloud, or on a hybrid solution. Some<a id="_idIndexMarker1610"/> examples of PaaS are <strong class="bold">Microsoft Azure</strong>, <strong class="bold">AWS Lambda</strong>, <strong class="bold">Google App Engine</strong>, <strong class="bold">SAP Cloud Platform</strong>, <strong class="bold">Heroku</strong>, and <strong class="bold">Red </strong><strong class="bold">Hat OpenShift</strong>.</li>
<li><strong class="bold">Infrastructure as a service</strong> (<strong class="bold">IaaS</strong>): These <a id="_idIndexMarker1611"/>are services that are run online and provide high-level <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>). A notable<a id="_idIndexMarker1612"/> example is <strong class="bold">OpenStack</strong>.</li>
</ul>
<p>Details about all these technologies will be provided in <a href="B19682_14.xhtml#_idTextAnchor299"><em class="italic">Chapter 14</em></a>, <em class="italic">Short Introduction to</em><em class="italic"> Computing</em>, but for this chapter’s purpose, we have provided enough information. Major risks regarding cloud computing are concerned with data integration and compatibility. Those are among the risks that you must still overcome since most of the other risks are no longer your concern as they are transferred to the third party managing the infrastructure.</p>
<p>Risk calculation can be managed in different ways, depending on the IT scenario a company uses. When you’re using the on-premises scenario and you’re managing all the components in-house, risk assessments become quite challenging. When you’re using the IaaS, PaaS, and SaaS scenarios, risk assessment becomes less challenging as responsibilities are gradually transferred to an external entity.</p>
<p>Risk assessment should always be taken seriously by any individual concerned with the safety of their network and systems and by any IT manager. This is when a DRP comes into action. The foundation of a good DRP and strategy is having an effective risk assessment.</p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor218"/>Designing a DRP</h2>
<p>A DRP is <a id="_idIndexMarker1613"/>structured around the steps that should be taken when an incident occurs. In most cases, the DRP is part of a <strong class="bold">business continuity plan</strong>. This determines how a company should continue to operate based on a functioning infrastructure.</p>
<p>Every DRP needs to start from <a id="_idIndexMarker1614"/>an accurate <strong class="bold">hardware inventory</strong>, followed by a <strong class="bold">software applications inventory</strong> and a separate <strong class="bold">data inventory</strong>. The most important part of this is the strategy that’s designed to back up all the information that’s used.</p>
<p>In terms of the hardware that’s used, there must be a clear policy for standardized hardware. This will ensure that faulty hardware can easily be replaced. This kind of policy ensures that everything works and is optimized. Standardized hardware surely has good driver support, and this is very important in the Linux world. Nevertheless, using standardized hardware will tremendously limit practices such as <strong class="bold">bring your own device</strong> (<strong class="bold">BYOD</strong>), since employees <a id="_idIndexMarker1615"/>only need to use the hardware provided by their employer. Using standardized hardware comes with using specific software applications that have been set up and configured by the company’s IT department, with limited input available from the user.</p>
<p>The IT department’s responsibility is huge, and it plays an important role in designing the <strong class="bold">IT recovery strategies</strong> as part of a DRP. Key tolerances for downtime and loss of <a id="_idIndexMarker1616"/>data should be defined based on the <a id="_idIndexMarker1617"/>minimal acceptable RPO and RTO.</p>
<p>Deciding on the <strong class="bold">roles</strong> regarding who is responsible for what is another key step of a good DRP. This way, the response time for implementing the plan will be dramatically reduced and everyone will know their own responsibilities in case any risks arise. In this case, having a good <strong class="bold">communication strategy</strong> is critical. Enforcing clear procedures for every level of the organizational pyramid will provide clear communication, centralized decisions, and a succession plan for backup personnel.</p>
<p>DRPs need to be thoroughly tested at least twice a year to prove their efficiency. Unplanned downtime and outages can negatively impact a business, both on-premises and in any multi-cloud environment. Being prepared for worst-case scenarios is important. Therefore, in the following sections, we will show you some of the best tools and practices for backing up and restoring a Linux system.</p>
<h1 id="_idParaDest-203"><a id="_idTextAnchor219"/>Backing up and restoring the system</h1>
<p>Disasters can<a id="_idIndexMarker1618"/> occur at any time. Risk is everywhere. In this respect, backing up your <a id="_idIndexMarker1619"/>system is of utmost importance and needs to be done regularly. It is always better to practice good prevention than to recover from data loss and learn this the hard way.</p>
<p><strong class="bold">Backup</strong> and <strong class="bold">recovery</strong> need to be done based on a well-thought-out strategy and need to take the RTO and RPO factors into consideration. The RTO should answer basic questions such as how fast to recover lost data and how this will affect the business operations, while the RPO should answer questions such as how much of your data you can afford to lose.</p>
<p>There are different types <a id="_idIndexMarker1620"/>and methods<a id="_idIndexMarker1621"/> of backup. The following are some examples:</p>
<div><div><img alt="Figure 10.1 – Backup methods and types" src="img/B19682_10_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Backup methods and types</p>
<p>When doing a backup, keep the <a id="_idIndexMarker1622"/>following rules in mind:</p>
<ul>
<li>The <strong class="bold">321 rule</strong> means that you <a id="_idIndexMarker1623"/>should always have at least three copies of your data, with two copies on two different media at separate locations and one backup always being kept off-site (at a different geographical location). This is also known as the <strong class="bold">rule of three</strong>; it can be <a id="_idIndexMarker1624"/>adapted to anything, such as 312, 322, 311, or 323.</li>
<li><strong class="bold">Backup checking</strong> is extremely relevant and is overlooked most of the time. It checks the data’s integrity and usefulness.</li>
<li><strong class="bold">Clear and documented backup strategy and procedures</strong> are beneficial to everyone in the IT <a id="_idIndexMarker1625"/>team who is using the same practices.</li>
</ul>
<p>In the next section, we will look at some well-known tools for full Linux system backups, starting with the ones that are integrated inside the operating system to third-party solutions that are equally suited for both home and enterprise use.</p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor220"/>Disk cloning solutions</h2>
<p>A good option for a<a id="_idIndexMarker1626"/> backup is to clone the entire hard drive or several partitions that hold sensitive data. Linux <a id="_idIndexMarker1627"/>offers a plethora of versatile tools for this job. Among those are the <code>dd</code> command, the <code>ddrescue</code> command, and the <strong class="bold">Relax-and-Recover</strong> (<strong class="bold">ReaR</strong>) software tool. Let’s look at these in detail in the following sections.</p>
<h3>The dd command</h3>
<p>One of the <a id="_idIndexMarker1628"/>most well-known disk backup commands is the <code>dd</code> command. We discussed this previously in <a href="B19682_06.xhtml#_idTextAnchor124"><em class="italic">Chapter 6</em></a>, <em class="italic">Working with Disks and Filesystems</em>. Let’s recap how it is used in a backup and restore scenario. The <code>dd</code> command is used to copy block by block, regardless of the filesystem type, from a source to a destination.</p>
<p>Let’s learn how to clone an entire disk. We have a virtual machine on our system that has a 20 GB drive that we want to back up on a 128 GB USB pen drive. The procedures we are going to show you will work on bare metal too.</p>
<p>First, we will run the <code>sudo fdisk -l</code> command to verify that the disk sizes are correct. The output will show us information about both our local drive and the USB pen drive, among other information, depending on your system.</p>
<p>Now that we<a id="_idIndexMarker1629"/> know what the sizes are and that the source can fit into the destination, we will proceed to cloning the entire virtual disk. We will clone the source disk, <code>/dev/vda</code>, to the destination disk, <code>/dev/sda</code> (the operation could take a while):</p>
<div><div><img alt="Figure 10.2 – Using dd to clone an entire hard drive" src="img/B19682_10_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Using dd to clone an entire hard drive</p>
<p>The options shown in the preceding command are as follows:</p>
<ul>
<li><code>if=/dev/vda</code> represents the input file, which, in our case, is the source hard drive</li>
<li><code>of=/dev/sda</code> represents the output file, which is the destination USB drive</li>
<li><code>conv=noerror</code> represents the instruction that allows the command to continue ignoring errors</li>
<li><code>sync</code> represents the instruction to fill the input error blocks with zeros so that the data offset will always be synced</li>
<li><code>status=progress</code> shows <a id="_idIndexMarker1630"/>statistics about the transfer process</li>
</ul>
<p>Please keep in mind that this operation could take a while to finish. On our system, it took 200 minutes to complete. We took the preceding screenshot while the operation was at the beginning. In the following section, we will show you how to use <code>ddrescue</code>.</p>
<h3>The ddrescue command</h3>
<p>The <code>ddrescue</code> command <a id="_idIndexMarker1631"/>is yet another tool you can use to clone your disk. This tool copies from one device or file to another one, trying to copy only the good and healthy parts the first time. If your disk is failing, you might want to use <code>ddrescue</code> twice since, the first time, it will copy only the good sectors and map the errors to a destination file. The second time, it will copy only the bad sectors, so it is better to add an option for several read attempts just to be sure.</p>
<p>On Ubuntu, the <code>ddrescue</code> utility is not installed by default. To install it, use the following <code>apt</code> command:</p>
<pre class="console">
sudo apt install gddrescue</pre> <p>We will use <code>ddrescue</code> on<a id="_idIndexMarker1632"/> the same system we used previously and clone the same drive. The command for this is as follows:</p>
<pre class="console">
sudo ddrescue -n /dev/vda /dev/sda rescue.map --force</pre> <p> The output is as follows:</p>
<div><div><img alt="Figure 10.3 – Using ddrescue to clone the hard drive" src="img/B19682_10_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Using ddrescue to clone the hard drive</p>
<p>We used the <code>ddrescue</code> command with the <code>--force</code> option to make sure that everything on the <a id="_idIndexMarker1633"/>destination will be overwritten. This operation is time-consuming too, so be prepared for a lengthy wait. In our case, it took almost 1 hour to finish. Next, we will show you how to use another useful tool, the ReaR utility.</p>
<h3>Using ReaR</h3>
<p>ReaR is a <a id="_idIndexMarker1634"/>powerful disaster recovery and system migration tool written in Bash. It is used by enterprise-ready distributions such as RHEL and SLES, and can also be installed on Ubuntu. It was designed to be easy to use and set up. It is integrated with the local bootloader, with the <code>cron</code> scheduler, or monitoring tools<a id="_idIndexMarker1635"/> such as <strong class="bold">Nagios</strong>. For more details on this tool, visit the<a id="_idIndexMarker1636"/> official website at <a href="http://relax-and-recover.org/about/">http://relax-and-recover.org/about/</a>.</p>
<p>To install it on Ubuntu, use the following command:</p>
<pre class="console">
sudo apt install rear</pre> <p>Once the <a id="_idIndexMarker1637"/>packages have been installed, you will need to know the location of the main configuration file, which is <code>/etc/rear/local.conf</code>, and all the configuration options should be written inside it. ReaR makes ISO files by default, but it also supports Samba (CIFS), USB, and NFS as backup destinations. Next, we will show you how to use ReaR to back up to a local NFS server.</p>
<h4>Backing up to a local NFS server using ReaR</h4>
<p>As an example, we<a id="_idIndexMarker1638"/> will show you how to back up to an NFS server. As specified in the <em class="italic">Technical requirements</em> section, you would need to have at least two systems available on your network for this exercise: an NFS server set up on one of the machines (as a backup server) and a second system as the production machine to be backed up. ReaR should be installed on both of them. Perform the following steps:</p>
<ol>
<li>First, we must configure the NFS server accordingly (the operation is covered in <a href="B19682_13.xhtml#_idTextAnchor276"><em class="italic">Chapter 13</em></a>, <em class="italic">Configuring Linux Servers</em>). For extensive information on setting up an NFS server, please refer to <a href="B19682_13.xhtml#_idTextAnchor276"><em class="italic">Chapter 13</em></a>; here, we only cover it briefly. The configuration file for NFS is <code>/etc/exports</code> and it stores information about the share’s location. Before you add any new information about the ReaR backup share’s location, add a new directory. We will consider the <code>/home/export/</code> directory for our NFS setup. Inside that directory, we will create a new one for our ReaR backups. The command to create the new directory is as follows:<pre class="source-code">
<code>root</code>, ReaR will not have permission to write the backup to this location. Change the ownership using the following command:<pre class="source-code">
<code>/etc/exports</code> file with your favorite editor and add a <a id="_idIndexMarker1639"/>new line for the backup directory. The line should look like the following:<pre class="source-code">
/home/export/rear 192.168.124.0/24(rw,sync,no_subtree_check)</pre><p class="list-inset">Use your local network’s IP range, not the one we used.</p></li> <li>Once the new line has<a id="_idIndexMarker1640"/> been introduced, restart the NFS service and run the <code>exportfs</code> command using the <code>-</code><code>s</code> option:<pre class="source-code">
<code>/etc/rear/local.conf</code> file and add the lines shown in the following output. Use your own system’s IP address, not the one we used. The code should look like the following:<pre class="source-code">
<strong class="bold">OUTPUT=ISO</strong>
<strong class="bold">OUTPUT_URL=nfs://192.168.124.112/home/export/rear</strong>
<strong class="bold">BACKUP=NETFS</strong>
<code>OUTPUT</code>: The bootable image type, which, in our case, is <code>ISO</code></li><li><code>OUTPUT_URL</code>: The backup target, which can represent NFS, CIFS, FTP, RSYNC, or file</li><li><code>BACKUP</code>: The backup method used, which, in our case, is <code>NETFS</code>, the default ReaR method</li><li><code>BACKUP_URL</code>: The backup target’s location</li></ul></li> <li>Now, run the <code>mkbackup</code> command with the <code>-v</code> and <code>-</code><code>d</code> options:<pre class="source-code">
<strong class="bold">sudo rear -v -d mkbackup</strong></pre><p class="list-inset">The output will be large, so we will not show it to you here. The command will take a significant time to finish. Once it has finished, you can check the NFS directory to view its output. The backup should be in there.</p></li> </ol>
<p>There are <a id="_idIndexMarker1642"/>several files written on the NFS server. Among those, the one called <code>rear-neptune.iso</code> is the actual backup and the one that will be used in case a system restore is needed. There is also a file called <code>backup.tar.gz</code>, which contains all the files from our local machine.</p>
<p class="callout-heading">Important note</p>
<p class="callout">The naming convention of ReaR is as follows. The name will consist of the term <code>rear-</code>, followed by the system’s hostname and the <code>.iso</code> extension. Our system’s hostname is <code>neptune</code>, which is why the backup file is called <code>rear-neptune.iso</code> in our case.</p>
<p>Once the backup has been written on the NFS server, you will be able to restore the system by using a USB disk or DVD with the ISO image that was written on the NFS server.</p>
<h4>Backing up to USB using ReaR</h4>
<p>There is also<a id="_idIndexMarker1643"/> the option of directly backing up on the USB disk. Here are the steps to be followed:</p>
<ol>
<li>Insert a disk into the USB port and format it by using the following command:<pre class="source-code">
<strong class="bold">sudo rear format /dev/sda</strong></pre><p class="list-inset">The command will take a significant time to complete. The output is as follows:</p></li> </ol>
<div><div><img alt="Figure 10.4 – Formatting the USB disk with ReaR" src="img/B19682_10_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Formatting the USB disk with ReaR</p>
<ol>
<li value="2">Now, we need to modify the <code>/etc/rear/local.conf</code> file and adapt it so that it uses the USB as the backup destination. The new lines we will add should look as follows:<pre class="source-code">
OUTPUT=USB
BACKUP_URL="usb:///dev/disk/by-label/REAR-000"</pre></li> <li>To understand the last line of code, you can run the following command:<pre class="source-code">
<code>/dev/disk/by-label/REAR-000</code> is a link to <code>/dev/sda1</code> (in our case):</p></li> </ol>
<div><div><img alt="Figure 10.5 – Checking the URL location from the ReaR configuration file" src="img/B19682_10_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Checking the URL location from the ReaR configuration file</p>
<ol>
<li value="4">To back up the system on the USB disk, run the following command:<pre class="source-code">
<code>tar.gz</code> files will be on the USB drive.</p></li> <li>To recover the system, you will need to boot from the USB drive and select the first option, which says <code>Recover "hostname"</code>, where <code>"hostname"</code> is the hostname of the computer you backed up.</li>
</ol>
<p>System backup and recovery are two very important tasks that should be indispensable to any Linux system administrator. Knowing how to execute those tasks can save data, time, and money for both the company and the client. Minimal downtime and having a quick, effective response should be the most important assets on every <strong class="bold">Chief Technology Officer’s</strong> (<strong class="bold">CTO’s</strong>) table. Backup<a id="_idIndexMarker1645"/> and recovery strategies should always have a strong foundation in terms of good mitigation practices. In this respect, a strong diagnostics toolset and troubleshooting knowledge will always come in handy for every system administrator. This is why, in the next section, we will show you some of the best diagnostic tools in Linux.</p>
<h1 id="_idParaDest-205"><a id="_idTextAnchor221"/>Introducing common Linux diagnostic tools for troubleshooting</h1>
<p>The <a id="_idIndexMarker1646"/>openness of Linux is one of its best assets. This opened the door to an extensive number of solutions that can be used for any task at hand. Hence, many diagnostic tools are available to Linux system administrators. Depending on which part of your system you would like to diagnose, there are several tools available. Troubleshooting is essentially problem-solving based on diagnostics generated by specific tools. To reduce the number of diagnostic tools to cover, we will narrow down the issues to the following categories for this section:</p>
<ul>
<li>Boot issues</li>
<li>General system issues</li>
<li>Network issues</li>
<li>Hardware issues</li>
</ul>
<p>There are specific diagnostic tools for each of these categories. We will start by showing you some of the most widely used ones.</p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor222"/>Tools for troubleshooting boot issues</h2>
<p>To understand the issues<a id="_idIndexMarker1647"/> that may affect the boot process, it is important to know how the boot process works. We have not covered this in detail yet, so pay attention to everything that we will tell you.</p>
<h3>The boot process</h3>
<p>All the major Linux distributions, such as<a id="_idIndexMarker1648"/> Ubuntu, OpenSUSE, Debian, Fedora, and RHEL, use <code>systemd</code> as their default init system. Until GRUB2 initialization and the <code>systemd</code> startup were put in place, the Linux boot process had several more stages.</p>
<p>The boot order is as follows:</p>
<ol>
<li>The <strong class="bold">Basic Input Output System</strong> (<strong class="bold">BIOS</strong>) <strong class="bold">Power-On </strong><strong class="bold">Self-Test</strong> (<strong class="bold">POST</strong>)</li>
<li>GRUB2 bootloader initialization</li>
<li>GNU/Linux kernel initialization</li>
<li><code>systemd init</code> system initialization</li>
</ol>
<p>BIOS POST is a<a id="_idIndexMarker1650"/> process <a id="_idIndexMarker1651"/>specific to hardware initialization and testing, and it is similar for every PC, regardless of whether it is using Linux or Windows. The BIOS makes sure that every hardware component inside the PC is working properly. When the BIOS fails to start, there is usually a hardware problem or incompatibility issue. The BIOS searches for the disk’s boot record, such as<a id="_idIndexMarker1652"/> the <strong class="bold">master boot record</strong> (<strong class="bold">MBR</strong>) or <strong class="bold">GUID Partition Table</strong> (<strong class="bold">GPT</strong>), and loads <a id="_idIndexMarker1653"/>it into memory.</p>
<p>GRUB2 initialization is where Linux starts to kick in. This is the stage when the system loads the kernel into memory. It can choose between several different kernels in case there’s more than one operating system available. Once the kernel has been loaded into memory, it takes control of the boot process.</p>
<p>The kernel is a self-extracting archive. Once extracted, it runs into the memory and loads the <code>init</code> system, the parent of all the other processes on Linux.</p>
<p>The <code>init</code> system, called <code>systemd</code>, starts by mounting the filesystems and accessing all the available configuration files.</p>
<p>During the boot process, issues may appear. In the next section, we will discuss what to do if disaster strikes and your bootloader won’t start.</p>
<h3>Repairing GRUB2</h3>
<p>If GRUB2 breaks, you<a id="_idIndexMarker1654"/> will not be able to access your system. This calls for a GRUB repair. At this stage, a live bootable USB drive will save you. The following steps are an exercise that could count as an experiment. In our case, we have an Ubuntu 22.04 LTS Desktop edition live disk, and we will use it for this example. However, you can use any Linux distribution you like, not necessarily Ubuntu. The point is that you would need a live bootable USB drive with Linux on it. Here are the steps you should follow:</p>
<ol>
<li>Plug in the Ubuntu 22.04 live disk drive and boot the system.</li>
<li>Open the BIOS, select the bootable disk as the main boot device, and restart it.</li>
<li>Select <a id="_idIndexMarker1655"/>the <strong class="bold">Try </strong><strong class="bold">Ubuntu</strong> option.</li>
<li>Once inside the Ubuntu instance, open Terminal, enter the <code>sudo fdisk -l</code> command, and check your disks and partitions.</li>
<li>Select the<a id="_idIndexMarker1656"/> one that GRUB2 is installed on and use the following command (use the disk names as provided by your system; don’t copy/paste our example):<pre class="source-code">
<strong class="bold">sudo mount -t ext4 /dev/sda1 /mnt</strong></pre></li> <li> Install GRUB2 using the following commands:<pre class="source-code">
<strong class="bold">sudo chroot /mnt</strong>
<strong class="bold">grub-install /dev/sda</strong>
<strong class="bold">grub-install –recheck /dev/sda</strong>
<strong class="bold">update-grub</strong></pre></li> <li>Unmount the partition using the following commands:<pre class="source-code">
<strong class="bold">exit</strong>
<strong class="bold">sudo unmount /mnt</strong></pre></li> <li>Reboot the computer.</li>
</ol>
<p>Dealing with bootloaders is extremely sensitive. Pay attention to all the details and take care of all the commands you type in. If not, everything could go sideways. In the next section, we will show you some diagnostic tools for general system issues.</p>
<h2 id="_idParaDest-207"><a id="_idTextAnchor223"/>Tools for troubleshooting general system issues</h2>
<p>System issues can be <a id="_idIndexMarker1657"/>of different types and complexities. Knowing the tools to deal with them is of utmost importance. In this section, we will cover the default tools provided by the Linux distribution. Basic troubleshooting knowledge is necessary for any Linux system administrator as issues can – and will – occur during regular operations.</p>
<p>What could general system issues mean? Well, basically, these are issues regarding disk space, memory usage, system load, and running processes.</p>
<h3>Commands for disk-related issues</h3>
<p>Disks, be<a id="_idIndexMarker1658"/> they HDDs or SSDs, are an important part of the system. They provide the necessary space for your data, files, and software of any type, including the operating system. We will not discuss hardware-related issues as this will be the subject of a future section in this chapter called <em class="italic">Tools for troubleshooting hardware issues</em>. Instead, we will<a id="_idIndexMarker1659"/> cover issues related to <strong class="bold">disk space</strong>. The most common diagnostic tools for this are already installed on any Linux system, and they are represented by the following commands:</p>
<ul>
<li><code>du</code>: A utility that shows disk space utilization for files and directories</li>
<li><code>df</code>: A utility that shows the disk usage for directories</li>
</ul>
<p>The following is an example of using the <code>df</code> utility with the <code>-h</code> (human-readable) option:</p>
<pre class="source-code">
<strong class="bold">df -h</strong></pre> <p>If one of the disks runs out of space, it will be shown in the output. This is not an issue in our case, but the tool is still relevant for finding out which of the available disks is having issues with the free available space.</p>
<p>When a disk is full, or almost full, there are several fixes that can be applied. If you have to delete some of the files, we advise you to delete them from your <code>/home</code> directory. Try not to delete important system files. The following are some ideas for troubleshooting<a id="_idIndexMarker1660"/> available space issues:</p>
<ul>
<li>Delete the unnecessary files using the <code>rm</code> command (optionally using, with caution, the <code>-rf</code> option) or the <code>rmdir</code> command</li>
<li>Move files to an external drive (or to the cloud) using the <code>rsync</code> command</li>
<li>Find which directories use the most space in your <code>/</code><code>home</code> directory</li>
</ul>
<p>The following is an example of using the <code>du</code> utility to find the largest directories inside the <code>/home</code> directory. We are using two pipes to transfer the output of the <code>du</code> command to the <code>sort</code> command and finally to the <code>head</code> command with the option of <code>5</code> (because we want to show the five largest directories, not all of them):</p>
<div><div><img alt="Figure 10.6 – Finding the largest directories in your /home directory" src="img/B19682_10_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Finding the largest directories in your /home directory</p>
<p>Another troubleshooting scenario is with regard to the number of inodes being used, not with the space on a disk. In this case, you can use the <code>df -i</code> command to see whether you’ve run out of inodes:</p>
<div><div><img alt="Figure 10.7 – inode usage statistics" src="img/B19682_10_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – inode usage statistics</p>
<p>The output in the preceding screenshot shows basic information about the inode usage on the system. You will see the total numbers of inodes on different filesystems, how many inodes are in use (as a number and as a percentage), and how many are free.</p>
<p>Besides the commands shown here, which are the defaults for every Linux distribution, there are many other open source tools for disk space issues, such as <strong class="bold">pydf</strong>, <strong class="bold">parted</strong>, <strong class="bold">sfdisk</strong>, <strong class="bold">iostat</strong>, and the GUI-based <strong class="bold">GParted</strong> application.</p>
<p>In the next section, we will show you how to use commands to verify possible memory issues.</p>
<h3>Commands for memory usage issues</h3>
<p><code>free</code> and it can be accessed in any major distribution. In the following example, we will use the <code>-h</code> option for a human-readable output:</p>
<div><div><img alt="Figure 10.8 – Using the free command in Linux" src="img/B19682_10_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Using the free command in Linux</p>
<p>As shown in the preceding screenshot, using the <code>free</code> command (with the <code>-h</code> option for human-readable output) shows the following:</p>
<ul>
<li><code>total</code>: The total amount of memory</li>
<li><code>used</code>: The used memory, which is calculated as the total memory minus the buffered, cache, and free memory</li>
<li><code>free</code>: The free or unused memory</li>
<li><code>shared</code>: The memory used by <code>tmpfs</code></li>
<li><code>buff/cache</code>: The memory used by kernel buffers and the page cache</li>
<li><code>available</code>: The amount of memory available for new applications</li>
</ul>
<p>This way, you can find specific issues related to higher memory usage. Constantly checking memory usage on servers is important to see whether resources are being used efficiently.</p>
<p>Another way to check for memory usage is to use the <code>top</code> command, as shown in the following screenshot:</p>
<div><div><img alt="Figure 10.9 – Using the top command to check memory usage" src="img/B19682_10_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Using the top command to check memory usage</p>
<p>While using the <code>top</code> command, there are several sections available on screen. The output is dynamic, in the sense that it constantly changes, showing real-time information about the processes running on the system. The <code>memory</code> section shows information about <a id="_idIndexMarker1662"/>total memory used, as well as free and buffered memory. All the information is shown in megabytes by default so that it’s easier to read and understand.</p>
<p>Another command that shows information about memory (and other valuable system information) is <code>vmstat</code>:</p>
<div><div><img alt="Figure 10.10 – Using vmstat with no options" src="img/B19682_10_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – Using vmstat with no options</p>
<p>By default, <code>vmstat</code> shows information about processes, memory, swap, disk, and CPU usage. The memory information is shown starting from the second column and contains the following details:</p>
<ul>
<li><code>swpd</code>: How much virtual memory is being used</li>
<li><code>free</code>: How much memory is free</li>
<li><code>buff</code>: How much memory is being used for buffering</li>
<li><code>cache</code>: How much memory is being used for caching</li>
</ul>
<p>The <code>vmstat</code> command has <a id="_idIndexMarker1663"/>several options available. To learn about all the options and what all the columns from the output represent, visit the respective pages in the manual using the following command:</p>
<pre class="console">
man vmstat</pre> <p>The options that can be used with <code>vmstat</code> to show different information about memory are <code>-a</code> and <code>-s</code>. By using <code>vmstat -a</code>, the output will show the active and inactive memory:</p>
<div><div><img alt="Figure 10.11 – Using vmstat -a to show the active and inactive memory" src="img/B19682_10_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – Using vmstat -a to show the active and inactive memory</p>
<p>Using <code>vmstat -s</code> will show detailed memory, CPU, and disk statistics.</p>
<p>All the commands discussed in this section are essential for troubleshooting any memory issues. There might be others you can use, but these are the ones you will find by default on any Linux distribution.</p>
<p>Nevertheless, there <a id="_idIndexMarker1664"/>is one more that deserves to be mentioned in this section: the <code>sar</code> command. This can be installed in Ubuntu through the <code>sysstat</code> package. Therefore, install the package using the following command:</p>
<pre class="console">
sudo apt install sysstat</pre> <p>Once the package has been installed, to be able to use the <code>sar</code> command to show detailed statistics about the system’s memory usage, you will need to enable the <code>sysstat</code> service. It needs to be active to collect data. By default, the service runs every 10 minutes and saves the logs inside the <code>/var/log/sysstat/saXX</code> directory. Every directory is named after the day the service runs on. For example, if we were to run the <code>sar</code> command on April 25, the service would look for data inside <code>/var/log/sysstat/sa25</code>. We ran the <code>sar</code> command on April 25 before starting the service, and an error occurred. Thus, to enable data collection, first, we will start and enable the <code>sysstat</code> service, then we will run the application. With the <code>sar</code> command, you can generate different reports in real time. For example, if we want to generate a memory report times every two seconds, we will use the <code>-</code><code>r</code> option:</p>
<div><div><img alt="Figure 10.12 – Starting and enabling the service and running sar" src="img/B19682_10_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – Starting and enabling the service and running sar</p>
<p>The service’s name <a id="_idIndexMarker1665"/>is <code>sadc</code>) and it uses the <code>sysstat</code> name<a id="_idIndexMarker1666"/> for the package and service.</p>
<p class="callout-heading">Important note</p>
<p class="callout">In the event of a system reboot, the service might not restart by default, even though the preceding commands were executed. To overcome this, on Ubuntu, you should edit the /<code>etc/default/sysstat</code> file and change the <code>ENABLED</code> status from <code>false</code> to <code>true</code>.</p>
<p>The output shown in <em class="italic">Figure 10</em><em class="italic">.12</em> will write one line every two seconds, five times in a row, and an average line at the end. It is a powerful tool that can be used for more than just memory statistics. There are options for CPU and disk statistics as well.</p>
<p>Overall, in this section, we covered the most important tools to use for troubleshooting memory issues. In the next section, we will cover tools to use for general system load issues.</p>
<h3>Commands for system load issues</h3>
<p>Similar to what <a id="_idIndexMarker1667"/>we covered in the previous sections, in this section, we will discuss <code>top</code> command is one of the most widely used when we’re trying to determine the sluggishness of a system. All the other tools, such as <code>vmstat</code> and <code>sar</code>, can also be used for CPU and system load troubleshooting.</p>
<p>A basic command for troubleshooting system load is <code>uptime</code>:</p>
<div><div><img alt="Figure 10.13 – Using uptime for checking system load" src="img/B19682_10_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.13 – Using uptime for checking system load</p>
<p>The <code>uptime</code> output shows three values at the end. Those values represent the load averages for 1, 5, and 15 minutes. The load average can give you a fair image of what happens with the system’s processes.</p>
<p>If you have a single CPU system, a load average of <code>1</code> means that that CPU is under full load. If the number is higher, this means that the load is much higher than the CPU can handle, and this will probably put a lot of stress on your system. Because of this, processes will take longer to execute and the system’s overall performance will be affected.</p>
<p>A high load average means that there are applications that run multiple threads simultaneously at once. Nevertheless, some load issues are not only the result of an overcrowded CPU – they can be the combined effect of CPU load, disk I/O load, and memory load. In this case, the Swiss Army knife for troubleshooting system load issues is the <code>top</code> command. The output of the <code>top</code> command constantly changes in real time, based on the system’s load.</p>
<p>By default, <code>top</code> sorts <a id="_idIndexMarker1668"/>processes by how much CPU they use. It runs in interactive mode and, sometimes, the output is difficult to see on the screen. You can redirect the output to a file and use the command in batch mode using the <code>-b</code> option. This mode only updates the command a specified number of times. To run <code>top</code> in batch mode, run the following command:</p>
<pre class="console">
top -b -n 1 | tee top-command-output</pre> <p>The <code>top</code> command could be a little intimidating for inexperienced Linux users. The output is highlighted in the following screenshot:</p>
<div><div><img alt="Figure 10.14 – The output of the top command" src="img/B19682_10_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.14 – The output of the top command</p>
<p>Let’s look at what the output means:</p>
<ul>
<li><code>us</code>: User CPU time</li>
<li><code>sy</code>: System CPU time</li>
<li><code>ni</code>: Nice CPU time</li>
<li><code>id</code>: Idle CPU time</li>
<li><code>wa</code>: Input/output wait time</li>
<li><code>hi</code>: CPU hardware interrupts time</li>
<li><code>si</code>: CPU software interrupts time</li>
<li><code>st</code>: CPU steal time</li>
</ul>
<p>Another useful tool for <a id="_idIndexMarker1669"/>troubleshooting CPU usage and hard drive input/output time is <code>iostat</code>:</p>
<div><div><img alt="Figure 10.15 – The output of iostat" src="img/B19682_10_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.15 – The output of iostat</p>
<p>The CPU statistics are similar to the ones from the output of the <code>top</code> command shown earlier. The I/O statistics are shown below the CPU statistic and here is what each column represents:</p>
<ul>
<li><code>tps</code>: Transfers per second to the device (I/O requests)</li>
<li><code>kB_read/s</code>: Amount of data read from the device (in terms of the number of blocks – kilobytes)</li>
<li><code>kB_wrtn/s</code>: Amount of data written to the device (in terms of the number of blocks – kilobytes)</li>
<li><code>kB_dscd/s</code>: Amount of data discarded for the device (in kilobytes)</li>
<li><code>kB_read</code>: Total number of blocks read</li>
<li><code>kB_wrtn</code>: Total number of blocks written</li>
<li><code>kB_dscd</code>: Total number of blocks discarded</li>
</ul>
<p>For more details<a id="_idIndexMarker1670"/> about the <code>iostat</code> command, read the respective manual pages by using the following command:</p>
<pre class="console">
man iostat</pre> <p>Besides the <code>iostat</code> command, there is another one that you could use, called <code>iotop</code>. It is not installed by default on Ubuntu, but you can install it with the following command:</p>
<pre class="console">
sudo apt install iotop</pre> <p>Once the package has been installed, you will need <code>sudo</code> privileges to run it:</p>
<pre class="console">
sudo iotop</pre> <p>The output is as shown in the following screenshot:</p>
<div><div><img alt="Figure 10.16 – Running the iotop command" src="img/B19682_10_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.16 – Running the iotop command</p>
<p>You can<a id="_idIndexMarker1671"/> also run the <code>sysstat</code> service to troubleshoot system load issues, similar to how we used it for troubleshooting memory issues.</p>
<p>By default, <code>sar</code> will output the CPU statistics for the current day:</p>
<div><div><img alt="Figure 10.17 – Running sar for CPU load troubleshooting" src="img/B19682_10_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.17 – Running sar for CPU load troubleshooting</p>
<p>In the preceding screenshot, <code>sar</code> ran five times, every two seconds. Our local network servers are not under heavy load at this time, but you can imagine that the output would be different when the command is run on a heavily used server. As we pointed out in the previous section, the <code>sar</code> command has several options that could prove useful in finding solutions to potential problems. Run the <code>man sar</code> command to view the manual page containing all the available options.</p>
<p>There are <a id="_idIndexMarker1672"/>many other tools that could be used for general system troubleshooting. We barely scratched the surface of this subject with the tools shown in this section. We advise you to search for more tools designed for general system troubleshooting if you feel the need to do so. Otherwise, the ones presented here are sufficient for you to generate a viable report about possible system issues.</p>
<p>Network-specific issues will be covered in the next section.</p>
<h2 id="_idParaDest-208"><a id="_idTextAnchor224"/>Tools for troubleshooting network issues</h2>
<p>Quite often, due<a id="_idIndexMarker1673"/> to the complexities of a network, issues tend to appear. Networks are essential for everyday living. We use them everywhere, from our wireless smartwatch to our smartphone, to our computer, and up to the cloud. Everything is connected worldwide to make our lives better and a systems administrator’s life a little bit harder. In this interconnected world, things can go sideways quite easily, and network issues need troubleshooting.</p>
<p>Troubleshooting <a id="_idIndexMarker1674"/><strong class="bold">network issues</strong> is almost 80% of a system administrator’s job – probably even more. This number is not backed up by any official studies but more of a hands-on experience insight. Since most of the server and cloud issues are related to networking, an optimal working network means reduced downtime and happy clients and system administrators.</p>
<p>The tools we will cover in this section are the defaults on all major Linux distributions. All these tools were discussed in <a href="B19682_07.xhtml#_idTextAnchor139"><em class="italic">Chapter 7</em></a>, <em class="italic">Networking with Linux</em>, and <a href="B19682_09.xhtml#_idTextAnchor194"><em class="italic">Chapter 9</em></a>, <em class="italic">Securing Linux</em>, or will be discussed in <a href="B19682_13.xhtml#_idTextAnchor276"><em class="italic">Chapter 13</em></a>, <em class="italic">Configuring Linux Servers</em>, so we will only name them again from a problem-solving standpoint. Let’s break down the tools we should use on specific TCP/IP layers. Remember how many layers there are in the TCP/IP model? There are five layers available, and we will start from layer 1. As a good practice, troubleshooting a network is best done through the stack, starting from the application layer all the way to the physical layer.</p>
<h3>Diagnosing the physical layer (layer 1)</h3>
<p>One of the basic testing tools and one of the first to be used by most system administrators is the <code>ping</code> command. The <a id="_idIndexMarker1675"/>name comes from <code>-c</code> option of the <code>ping</code> command. The output is as follows:</p>
<div><div><img alt="Figure 10.18 – Running a basic test using the ping command" src="img/B19682_10_18.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.18 – Running a basic test using the ping command</p>
<p>Ping is sending simple ICMP packets to the destination (in our case, it was <a href="http://google.com">google.com</a>) and is waiting for a response. Once it is received and no packets are lost, this means that everything is working fine. The <code>ping</code> command can be used to test connections to local network systems, as well as remote networks. It is the first tool that’s used to test and isolate possible problems.</p>
<p>There are times when a simple test with the <code>ping</code> command is not enough. In this case, another versatile command is the <code>ip</code> command. You<a id="_idIndexMarker1677"/> can use it to check whether there are any issues with the <a id="_idIndexMarker1678"/>physical layer:</p>
<pre class="console">
ip link show</pre> <p>You will see the output as follows:</p>
<div><div><img alt="Figure 10.19 – Showing the state of the physical interfaces with the ip command" src="img/B19682_10_19.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.19 – Showing the state of the physical interfaces with the ip command</p>
<p>In the preceding screenshot, you can see that the Ethernet interface is running well (<code>state UP</code>). As we are running on a virtual machine, we do not have a wireless connection. If we were to use a laptop, for example, the wireless connection would have been visible, showing something such as <code>wlp0s20f3</code>.</p>
<p>If any of the interfaces is not working, for example, the wireless one, the output of the preceding command would show <code>state DOWN</code>. In our case, we will bring the wireless interface up using the following command:</p>
<pre class="console">
ip link set wlp0s20f3 up</pre> <p>Once executed, you can check the state of the interface by running the <code>ip</code> command once again:</p>
<pre class="console">
ip link show</pre> <p>If you have direct access to a bare metal system, maybe a server, you can directly check whether the wires are connected. If, by any chance, you are using a wireless connection (not recommended), you will need to use<a id="_idIndexMarker1679"/> the <code>ip</code> command.</p>
<p>Another useful <a id="_idIndexMarker1680"/>tool for layer 1 is <code>ethtool</code>. On Ubuntu 22.04.2 LTS, it is installed by default. To check the Ethernet interface, run the following command by using the connection’s name (you’ve seen it while using the <code>ip</code> command):</p>
<pre class="console">
ethtool enp1s0</pre> <p>By using <code>ethtool</code>, we<a id="_idIndexMarker1681"/> can check whether a connection has negotiated the correct speed. In the output of the command (which we will not show here), you would see that the system has correctly negotiated a full 1,000 Mbps full-duplex connection, for example (it may differ in your case). In the next section, we will show you how to diagnose the layer 2 stack.</p>
<h3>Diagnosing the data link layer (layer 2)</h3>
<p>The second<a id="_idIndexMarker1682"/> layer in the TCP/IP stack is called the <code>ip</code> command and <a id="_idIndexMarker1684"/>the <code>arp</code> command. The <code>arp</code> command, which comes from<a id="_idIndexMarker1685"/> the <code>arp</code> command is<a id="_idIndexMarker1686"/> available through the <code>net-tools</code> package. First, proceed and install it using the following command:</p>
<pre class="console">
sudo apt install net-tools</pre> <p>To check the entries inside the ARP table, you can use the <code>arp</code> command with the <code>-a</code> (all) option. As we are running on a virtual machine, we will have only one entry. As a comparison, we run the same command on our host system, this time having three entries, one for the wireless connection, one for the wired connection, and another for the virtual bridge connection used by KVM. The following is an example:</p>
<div><div><img alt="Figure 10.20 – Using the arp command to map the ARP entries" src="img/B19682_10_20.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.20 – Using the arp command to map the ARP entries</p>
<p>The output of the <code>arp</code> command will show all the connected devices, with details about their IP and MAC addresses. Note that the MAC addresses have been blurred for privacy reasons.</p>
<p>Similar to the <code>arp</code> command, you<a id="_idIndexMarker1687"/> can use the <code>ip neighbor show</code> command, as shown in the following screenshot. We will use our host system, not the virtual machine:</p>
<div><div><img alt="Figure 10.21 – Listing the ARP entries using the ip command" src="img/B19682_10_21.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.21 – Listing the ARP entries using the ip command</p>
<p>The <code>ip</code> command can be used to delete entries from the ARP list, like so:</p>
<pre class="console">
ip neighbor delete 192.168.124.112 dev virbr0</pre> <p>We used the command to delete the virtual bridge connection, using its IP address and name from the list as shown in <em class="italic">Figure 10</em><em class="italic">.21</em>.</p>
<p>Both the <code>arp</code> and <code>ip</code> commands have a similar output. They are powerful commands and very useful for troubleshooting possible layer 2 issues. In the next section, we will show you how to diagnose the layer 3 stack.</p>
<h3>Diagnosing the internet layer (layer 3)</h3>
<p>On the internet layer, layer 3, we <a id="_idIndexMarker1688"/>are working with IP addresses only. We already know about the tools to use here, such as the <code>ip</code> command, the <code>ping</code> command, the <code>traceroute</code> command, and the <code>nslookup</code> command. Since we’ve already covered the <code>ip</code> and <code>ping</code> commands, we will only discuss how to use <code>traceroute</code> and <code>nslookup</code> here. The <code>traceroute</code> command is not installed by default in Ubuntu. You will have to install it using the following command:</p>
<pre class="console">
sudo apt install traceroute</pre> <p>The <code>nslookup</code> package is already available in Ubuntu by default. First, to check for the routing table to see the list of gateways for different routes, we can use the <code>ip route </code><code>show</code> command:</p>
<div><div><img alt="Figure 10.22 – Showing the routing table using the “ip route show” command" src="img/B19682_10_22.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.22 – Showing the routing table using the “ip route show” command</p>
<p>The <code>ip route show</code> command is showing<a id="_idIndexMarker1689"/> the default gateway. An issue would be if it is missing or incorrectly configured.</p>
<p>The <code>traceroute</code> tool is <a id="_idIndexMarker1690"/>used to<a id="_idIndexMarker1691"/> check the path of traffic from the source to the destination. The following output shows the path of packets traveling from our local gateway to Google’s servers:</p>
<div><div><img alt="Figure 10.23 – Using traceroute for path tracing" src="img/B19682_10_23.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.23 – Using traceroute for path tracing</p>
<p>Packets don’t usually have the same route when they’re sending and coming back to the source. Packets are sent to gateways to be processed and sent to the destination on a certain route. When packets exceed the local network, their route can be inaccurately represented by the <code>traceroute</code> tool, since the packets it relies on could be filtered by many of the gateways on the path (<em class="italic">ICMP TTL Exceeded</em> packets are generally filtered).</p>
<p>Similar to <code>traceroute</code>, there is a newer tool called <code>tracepath</code>. It is installed by default on Ubuntu and is a replacement for <code>traceroute</code>. Both <code>traceroute</code> and <code>tracepath</code> use the UDP ports for tracking by default. The <code>tracepath</code> command can also be used with the <code>-n</code> option to show the IP address instead of the hostname. The following is an example of this:</p>
<div><div><img alt="Figure 10.24 – Using the tracepath command" src="img/B19682_10_24.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.24 – Using the tracepath command</p>
<p>Checking further<a id="_idIndexMarker1692"/> network issues could lead to faulty DNS resolution, where a host can only be accessed by the IP address and not by the hostname. To troubleshoot this, even if it is not a layer 3 protocol, you could use the <code>nslookup</code> command, combined with the <code>ping</code> command.</p>
<p>If the <code>nslookup</code> command has the same IP output as the <code>ping</code> command, it means that everything is fine. If a different IP address shows up in the <a id="_idIndexMarker1693"/>output, then you have an issue with your host’s configuration. In the next section, we will show you how to diagnose both the layer 4 and layer 5 stacks.</p>
<h3>Diagnosing the transport and application layers (layers 4 and 5)</h3>
<p>The last two <a id="_idIndexMarker1694"/>layers, layer 4 (<code>ss</code> command (where <code>ss</code> is short for <strong class="bold">socket statistics</strong>).</p>
<p>The <code>ss</code> command<a id="_idIndexMarker1698"/> is a recent replacement for <code>netstat</code> and is used to see the list of all network sockets. As such, a list can have a significant size, so you could use several command options to reduce it.</p>
<p>For example, you could use the <code>-t</code> option to see only the TCP sockets, the <code>-u</code> option for UDP sockets, and <code>-x</code> for Unix sockets. Thus, to see TCP and UDP socket<a id="_idIndexMarker1699"/> information, we will use<a id="_idIndexMarker1700"/> the <code>ss</code> command with the <code>-t</code> option. Furthermore, to see all the listening sockets on your system, you can use the <code>-l</code> option. This one, combined with the <code>-u</code> and <code>-t</code> options, will show you all the UDP and TCP listening sockets on your system. The following is an excerpt from a much longer list:</p>
<div><div><img alt="Figure 10.25 – Using the ss command to list TCP and UDP sockets and ﻿all listening sockets" src="img/B19682_10_25.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.25 – Using the ss command to list TCP and UDP sockets and all listening sockets</p>
<p>The <code>ss</code> command is<a id="_idIndexMarker1701"/> important for <a id="_idIndexMarker1702"/>network troubleshooting when you want to verify the available sockets and the ones that are in the <code>LISTEN</code> state, for example. Another use is for the <code>TIME_WAIT</code> state, and in this case, you can use the command as shown in the following screenshot:</p>
<div><div><img alt="Figure 10.26 – Showing the TIME_WAIT ports with the ss command" src="img/B19682_10_26.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.26 – Showing the TIME_WAIT ports with the ss command</p>
<p>We used the <code>ss</code> command with the <code>-o</code> option and the <code>state time-wait</code> argument. The <code>TIME_WAIT</code> state is achieved when a socket closes for a short time.</p>
<p>For more information on this state, you can visit the following link: <a href="https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux">https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux</a>.</p>
<p>The <code>ss</code> tool should not be missing from a system administrator’s toolbox. Layer 5, the application layer, comprises protocols used by applications, and we will remember protocols such as the <strong class="bold">Dynamic Host Configuration Protocol</strong> (<strong class="bold">DHCP</strong>), the <strong class="bold">Hypertext Transfer Protocol</strong> (<strong class="bold">HTTP</strong>), and the <strong class="bold">File Transfer Protocol</strong> (<strong class="bold">FTP</strong>). Since<a id="_idIndexMarker1703"/> diagnosing layer 5 is<a id="_idIndexMarker1704"/> mainly an application troubleshooting <a id="_idIndexMarker1705"/>process, <a id="_idTextAnchor225"/>this <a id="_idIndexMarker1706"/>will not be <a id="_idIndexMarker1707"/>covered in this section.</p>
<p>In the next section, we will discuss troubleshooting hardware issues.</p>
<h2 id="_idParaDest-209"><a id="_idTextAnchor226"/>Tools for troubleshooting hardware issues</h2>
<p>The first step in<a id="_idIndexMarker1708"/> troubleshooting hardware issues is to check your hardware. A very good tool to see details about the system’s hardware is the <code>dmidecode</code> command. This command is used to read details about each hardware component in a human-readable format. Each piece of hardware has a specific DMI code, depending on its type. This code is specific to the <strong class="bold">System Management Basic Input/Output System</strong> (<strong class="bold">SMBIOS</strong>). There are 45 codes that are used by the SMBIOS. More information<a id="_idIndexMarker1709"/> about the codes can be obtained at <a href="https://www.thegeekstuff.com/2008/11/how-to-get-hardware-information-on-linux-using-dmidecode-command/">https://www.thegeekstuff.com/2008/11/how-to-get-hardware-information-on-linux-using-dmidecode-command/</a>.</p>
<p>To view details <a id="_idIndexMarker1710"/>about the system’s memory, you can use the <code>dmidecode</code> command with the <code>-t</code> option (from <code>TYPE</code>) and code <code>17</code>, which corresponds<a id="_idIndexMarker1711"/> to the memory device code from SMBIOS. An example from our virtual machine is as follows:</p>
<div><div><img alt="Figure 10.27 – Using dmidecode to view information about memory" src="img/B19682_10_27.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.27 – Using dmidecode to view information about memory</p>
<p>To see details<a id="_idIndexMarker1712"/> about other hardware components, use the command with the specific code.</p>
<p>Other quick troubleshooting tools include commands such as <code>lspci</code>, <code>lsblk</code>, and <code>lscpu</code>. The<a id="_idIndexMarker1713"/> output of the <code>lsblk</code> command shows information about the disks and partitions being <a id="_idIndexMarker1714"/>used on the system. The <code>lscpu</code> command<a id="_idIndexMarker1715"/> will show details about the CPU.</p>
<p>Also, when you’re troubleshooting hardware issues, taking a quick look at the kernel’s logs could prove useful. To do this, use the <code>dmesg</code> command. You can use the <code>dmesg | more</code> command to have more control over the output.</p>
<p>As you’ve seen in this section, hardware<a id="_idIndexMarker1716"/> troubleshooting is just as important and challenging as all other types of troubleshooting. Solving hardware-related issues is an integral part of any system administrator’s job. This involves constantly checking hardware components, replacing the faulty parts with new ones, and making sure that they run smoothly.</p>
<h1 id="_idParaDest-210"><a id="_idTextAnchor227"/>Summary</h1>
<p>In this chapter, we emphasized the importance of disaster recovery planning, backup and restore strategies, and troubleshooting various system issues. Every system administrator should be able to put their knowledge into practice when disaster strikes. Different types of failures will eventually hit the running servers, so solutions should be found as soon as possible to ensure minimal downtime and to prevent data loss.</p>
<p>This chapter represented the culmination of the <em class="italic">Advanced Linux Administration</em> section of this book. In the next chapter, we will introduce you to <strong class="bold">server administration</strong>, with emphasis on KVM virtual machine management, Docker containers, and different types of Linux server configuration.</p>
<h1 id="_idParaDest-211"><a id="_idTextAnchor228"/>Questions</h1>
<p>Troubleshooting is problem-solving at its best. Before we dive into the server section, let’s test your troubleshooting knowledge:</p>
<ol>
<li>Try to draft a DRP for your private network or small business.</li>
<li>Back up your entire system using the 321 rule.</li>
<li>Find out what your system’s top 10 processes are that use CPU the most.</li>
<li>Find out what your system’s top 10 processes are that use RAM the most.</li>
</ol>
<h1 id="_idParaDest-212"><a id="_idTextAnchor229"/>Further reading</h1>
<ul>
<li>Ubuntu LTS official documentation: <a href="https://ubuntu.com/server/docs">https://ubuntu.com/server/docs</a></li>
<li>RHEL official documentation: <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/</a></li>
<li>SUSE official documentation: <a href="https://documentation.suse.com/">https://documentation.suse.com/</a></li>
</ul>
</div>


<div><h1 id="_idParaDest-213" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor230"/>Part 3:Server Administration</h1>
<p>In this third part, you will learn about advanced Linux server administration tasks by setting up different types of servers, as well as working with and managing virtual machines and Docker containers.</p>
<p>This part has the following chapters:</p>
<ul>
<li><a href="B19682_11.xhtml#_idTextAnchor231"><em class="italic">Chapter 11</em></a>, <em class="italic">Working with Virtual Machines</em></li>
<li><a href="B19682_12.xhtml#_idTextAnchor257"><em class="italic">Chapter 12</em></a>, <em class="italic">Managing Containers with Docker</em></li>
<li><a href="B19682_13.xhtml#_idTextAnchor276"><em class="italic">Chapter 13</em></a>, <em class="italic">Configuring Linux Servers</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</body></html>