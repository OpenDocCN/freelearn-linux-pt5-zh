<html><head></head><body>
		<div id="_idContainer016">
			<h1 id="_idParaDest-39"><em class="italic"><a id="_idTextAnchor039"/>Chapter 3</em>: Dealing with Kernel Core Helpers</h1>
			<p>The Linux kernel<a id="_idIndexMarker133"/> is a standalone piece of software—as you'll see in this chapter—that does not depend on any external library as it implements any functionalities it needs to use (from list management to compression algorithms, everything is implemented from scratch). It implements any mechanism you may encounter in modern libraries and even more, such as compression, string functions, and so on. We will walk step by step through the most important aspects of such capabilities.</p>
			<p> In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Linux kernel locking mechanisms and shared resources</li>
				<li>Dealing with kernel waiting, sleeping, and delay mechanisms</li>
				<li>Understanding Linux kernel time management</li>
				<li>Implementing work-deferring mechanisms</li>
				<li>Kernel interrupt handling</li>
			</ul>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor040"/>Linux kernel locking mechanisms and shared resources</h1>
			<p>A resource<a id="_idIndexMarker134"/> is said to be shared when it is accessible by several contenders, whether exclusively or not. When it is exclusive, access must be synchronized so that only the allowed contender(s) may own the resource. Such resources might be memory locations or<a id="_idIndexMarker135"/> peripheral devices, and the contenders might be processors, processes, or threads. The operating system performs mutual exclusion by <a id="_idIndexMarker136"/>atomically modifying a variable that holds the current state of the resource, making this visible to all contenders that might access the variable at the same time. Atomicity guarantees the modification to be entirely successful, or not successful at all. Modern operating systems nowadays rely on hardware (which should allow atomic operations) to implement synchronization, though a simple system may ensure atomicity by disabling interrupts (and avoiding scheduling) around the critical code section.</p>
			<p>We can enumerate two synchronization mechanisms, as follows:</p>
			<ul>
				<li><strong class="bold">Locks</strong>: Used <a id="_idIndexMarker137"/>for mutual exclusion. When one contender holds the lock, no other can hold it (others are excluded). The most known locks in the kernel are spinlocks and mutexes.</li>
				<li><strong class="bold">Conditional variables</strong>: For<a id="_idIndexMarker138"/> waiting for a change. These are implemented differently in the kernel, as we will see later.</li>
			</ul>
			<p>When it comes to locking, it is up to the hardware to allow such synchronizations by means of atomic operations, which the kernel uses to implement locking facilities. Synchronization <a id="_idIndexMarker139"/>primitives are data structures used for coordinating access to shared resources. Because only one contender can hold the lock (and thus access the shared resource), it might perform an arbitrary operation on the resource associated with the lock, which would appear to be atomic to others.</p>
			<p>Apart from dealing with the <a id="_idIndexMarker140"/>exclusive ownership of a given shared resource, there are situations where it is better to wait for the state of the resource to change—for example, waiting for a list to contain at least one object (its state then passes from empty to not empty) or for a task to complete (a <strong class="bold">direct memory access</strong> (<strong class="bold">DMA</strong>) transaction, for example). The Linux kernel does<a id="_idIndexMarker141"/> not implement conditional variables. From user space, we could think of conditional variables for both situations, but to achieve the same or even better, the kernel provides the following mechanisms:</p>
			<ul>
				<li><strong class="bold">Wait queue</strong>: To <a id="_idIndexMarker142"/>wait for a change—designed to work in concert with locks</li>
				<li><strong class="bold">Completion queue</strong>: To wait <a id="_idIndexMarker143"/>for the completion of a given computation, mostly used with DMAs</li>
			</ul>
			<p>All the aforementioned mechanisms are supported by the Linux kernel and exposed to drivers by means of a reduced set of <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>) (which significantly ease their use for developers), which we will discuss in the coming sections.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/>Spinlocks</h2>
			<p>A spinlock<a id="_idIndexMarker144"/> is a hardware-based locking primitive that depends on hardware capabilities to provide atomic operations (such as <strong class="source-inline">test_and_set</strong>, which in a non-atomic implementation would result in read, modify, and write operations). It is the simplest and the base locking primitive, working as described in the following scenario. </p>
			<p>When <em class="italic">CPUB</em> is running, and task <em class="italic">B</em> wants to acquire the spinlock (task <em class="italic">B</em> calls the spinlock's locking function), and this <a id="_idIndexMarker145"/>spinlock is already held by another <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>) (let's say <em class="italic">CPUA</em> running task <em class="italic">A</em>, which has already called this spinlock's locking function), then <em class="italic">CPUB</em> will simply spin around a <strong class="source-inline">while</strong> loop (thus blocking task <em class="italic">B</em>) until the other CPU releases the lock (task <em class="italic">A</em> calls the spinlock's release function). This spinning will only happen on multi-core machines (hence the use case described previously, involving more than one CPU) because, on a single-core machine, it cannot happen (the task either holds the spinlock and proceeds or never runs until the lock is released). A spinlock is said to be a lock held by a CPU, in contrast to a mutex (which we will discuss in the next section of this chapter), which is a lock held by a task.</p>
			<p>A spinlock operates by disabling the scheduler on the local CPU (that is, the CPU running the task that called the spinlock's locking API). This also means that a task currently running on that CPU cannot be preempted except <a id="_idIndexMarker146"/>by <strong class="bold">interrupt requests</strong> (<strong class="bold">IRQs</strong>) if they are not disabled on the local CPU (more on this later). In other words, spinlocks protect resources that only one CPU can take/access at a time. This makes spinlocks suitable<a id="_idIndexMarker147"/> for <strong class="bold">symmetrical multiprocessing</strong> (<strong class="bold">SMP</strong>) safety and for executing atomic tasks.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Not only do spinlocks take advantage of hardware atomic functions. In the Linux kernel, for example, preemption status depends on a per-CPU variable that, if it equals <strong class="source-inline">0</strong>, means preemption is enabled; if it is greater than 0, this means preemption is disabled (<strong class="source-inline">schedule()</strong> becomes inoperative). Thus, disabling preemption (<strong class="source-inline">preempt_disable()</strong>) consists of adding 1 to the current per-CPU variable (<strong class="source-inline">preempt_count</strong>, actually), while <strong class="source-inline">preempt_enable()</strong> subtracts <strong class="source-inline">1</strong> from the variable, checks whether the new value is <strong class="source-inline">0</strong>, and calls <strong class="source-inline">schedule()</strong>. Those addition/subtraction operations are atomic and thus rely on the CPU being able to provide atomic addition/subtraction functions.</p>
			<p>A <a id="_idIndexMarker148"/>spinlock is created either statically using a <strong class="source-inline">DEFINE_SPINLOCK</strong> macro, as illustrated here, or dynamically by calling <strong class="source-inline">spin_lock_init()</strong> on an uninitialized spinlock:</p>
			<p class="source-code">static DEFINE_SPINLOCK(my_spinlock);</p>
			<p>To understand how this works, we just must look at the definition of this macro in <strong class="source-inline">include/linux/spinlock_types.h</strong>, as follows:</p>
			<p class="source-code">#define DEFINE_SPINLOCK(x) spinlock_t x = \</p>
			<p class="source-code">                                 __SPIN_LOCK_UNLOCKED(x)</p>
			<p>This can be used as follows:</p>
			<p class="source-code">static DEFINE_SPINLOCK(foo_lock);</p>
			<p>After this, the spinlock will be accessible through its name <strong class="source-inline">foo_lock</strong>, and its address would be <strong class="source-inline">&amp;foo_lock</strong>.</p>
			<p>However, for dynamic (runtime) allocation, it's better to embed the spinlock into a bigger structure, allocating memory for this structure and then calling <strong class="source-inline">spin_lock_init()</strong> on the spinlock element, as illustrated in the following code snippet:</p>
			<p class="source-code">struct bigger_struct {</p>
			<p class="source-code">    spinlock_t lock;</p>
			<p class="source-code">    unsigned int foo;</p>
			<p class="source-code">    [...]</p>
			<p class="source-code">};</p>
			<p class="source-code">static struct bigger_struct *fake_init_function()</p>
			<p class="source-code">{</p>
			<p class="source-code">    struct bigger_struct *bs;</p>
			<p class="source-code">    bs = kmalloc(sizeof(struct bigger_struct), GFP_KERNEL);</p>
			<p class="source-code">    if (!bs)</p>
			<p class="source-code">        return -ENOMEM;</p>
			<p class="source-code">    spin_lock_init(&amp;bs-&gt;lock);</p>
			<p class="source-code">    return bs;</p>
			<p class="source-code">}</p>
			<p>It's<a id="_idIndexMarker149"/> better to use <strong class="source-inline">DEFINE_SPINLOCK</strong> whenever possible. This offers compile-time initialization and requires fewer lines of code, with no real drawback. In this step, we can lock/unlock the spinlock using <strong class="source-inline">spin_lock()</strong> and <strong class="source-inline">spin_unlock()</strong> inline functions, both defined in <strong class="source-inline">include/linux/spinlock.h</strong>, as follows:</p>
			<p class="source-code">static __always_inline void spin_unlock(spinlock_t *lock)</p>
			<p class="source-code">static __always_inline void spin_lock(spinlock_t *lock)</p>
			<p>That said, there are some known limitations in using spinlocks this way. Though a spinlock prevents preemption on the local CPU, it does not prevent this CPU from being hogged by an interrupt (thus executing this interrupt's handler). Imagine a situation where the CPU holds a "spinlock" on behalf of task A in order to protect a given resource, and an interrupt occurs. The CPU will stop its current task and branch to this interrupt handler. So far, so good. Now, imagine if this IRQ handler needs to acquire this same spinlock (you probably already guessed that the resource is shared with the interrupt handler). It will infinitely spin in place, trying to acquire a lock already locked by a task that it has preempted. This situation will result in a deadlock, for sure.</p>
			<p>To address this issue, the Linux kernel provides <strong class="source-inline">_irq</strong> variant functions for spinlocks, which, in addition to disabling/enabling preemption, also disable/enable interrupts on the local CPU. These functions are <strong class="source-inline">spin_lock_irq()</strong> and <strong class="source-inline">spin_unlock_irq()</strong>, defined as follows:</p>
			<p class="source-code">static void spin_unlock_irq(spinlock_t *lock)</p>
			<p class="source-code">static void spin_lock_irq(spinlock_t *lock)</p>
			<p>We might think that this solution is sufficient, but it isn't. The <strong class="source-inline">_irq</strong> variant partially solves the problem. Imagine interrupts are already disabled on the processor before your code starts locking; when you call <strong class="source-inline">spin_unlock_irq()</strong>, you will not just release the lock but enable interrupts also, but probably in an erroneous manner since there is no way for <strong class="source-inline">spin_unlock_irq()</strong> to know which interrupts were enabled before locking and which were not.</p>
			<p>Let's consider the following example:</p>
			<ul>
				<li>Let's say interrupt <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> were disabled before a spinlock was acquired, and <strong class="source-inline">z</strong> was not.</li>
				<li><strong class="source-inline">spin_lock_irq()</strong> will disable the interrupts (<strong class="source-inline">x</strong>, <strong class="source-inline">y</strong>, and <strong class="source-inline">z</strong> are now disabled) and take the lock.</li>
				<li><strong class="source-inline">spin_unlock_irq()</strong> will enable the interrupts. <strong class="source-inline">x</strong>, <strong class="source-inline">y</strong>, and <strong class="source-inline">z</strong> find themselves all enabled, which was not the case before acquiring the lock. This is where the problem lies.</li>
			</ul>
			<p>This <a id="_idIndexMarker150"/>makes <strong class="source-inline">spin_lock_irq()</strong> unsafe when called from IRQs off-context as its counterpart <strong class="source-inline">spin_unlock_irq()</strong> will dumbly enable IRQs, with the risk of enabling those that were not enabled while <strong class="source-inline">spin_lock_irq()</strong> was invoked. It makes sense to use <strong class="source-inline">spin_lock_irq()</strong> only when you know that interrupts are enabled—that is, you are sure nothing else might have disabled interrupts on the local CPU.</p>
			<p>Now, imagine if you save the interrupts' status in a variable before acquiring the lock and restore them exactly as they were while releasing—there would be no further issues at all. To achieve this, the kernel provides <strong class="source-inline">_irqsave</strong> variant functions that behave exactly like the <strong class="source-inline">_irq</strong> ones, with saving and restoring interrupts status features in addition. These are <strong class="source-inline">spin_lock_irqsave()</strong> and <strong class="source-inline">spin_lock_irqrestore()</strong>, defined as follows:</p>
			<p class="source-code">spin_lock_irqsave(spinlock_t *lock, unsigned long flags)</p>
			<p class="source-code">spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="source-inline">spin_lock()</strong> and all its variants automatically call <strong class="source-inline">preempt_disable()</strong>, which disables preemption on the local CPU, while <strong class="source-inline">spin_unlock()</strong> and its variants call <strong class="source-inline">preempt_enable()</strong>, which tries to enable preemption (Yes—tries!!! It depends on whether other spinlocks are locked, which would affect the value of the preemption counter), and which internally calls <strong class="source-inline">schedule()</strong> if enabled (depending on the current value of the counter, whose current value should be <strong class="source-inline">0</strong>). <strong class="source-inline">spin_unlock()</strong> is then a preemption point and might re-enable preemption.</p>
			<h3>Disabling preemption versus disabling interrupts</h3>
			<p>Though disabling interrupts<a id="_idIndexMarker151"/> may prevent kernel preemption (scheduler tick disabled), nothing prevents the protected section from invoking the scheduler (<strong class="source-inline">schedule()</strong> function). A lot of kernel functions indirectly invoke the scheduler, such as those dealing with spinlocks. As a result, even a simple <strong class="source-inline">printk()</strong> function may invoke the scheduler since it deals with the spinlock that protects the kernel message buffer. The kernel disables or enables the scheduler (and, thus, preemption) by increasing or decreasing a kernel global and per-CPU variable (which defaults to <strong class="source-inline">0</strong>, meaning <em class="italic">enabled</em>) called <strong class="source-inline">preempt_count</strong>. When this variable is greater than <strong class="source-inline">0</strong> (which is checked by the <strong class="source-inline">schedule()</strong> function), the scheduler simply returns and does nothing. This variable is incremented at each invocation of a <strong class="source-inline">spin_lock*</strong> family function. On the other side, releasing a spinlock (any <strong class="source-inline">spin_unlock*</strong> family function) decrements it from <strong class="source-inline">1</strong>, and whenever it reaches <strong class="source-inline">0</strong>, the scheduler is invoked, meaning that your critical section would not be that atomic.</p>
			<p>Thus, only disabling interrupts protects you from kernel preemption only in cases where the protected code does not trigger preemption itself. That said, code that locked a spinlock may not sleep as there would be no way to wake it up (remember—timer interrupts and/or schedulers are disabled on the local CPU).</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor042"/>Mutexes</h2>
			<p>A mutex<a id="_idIndexMarker152"/> is the second and last locking primitive we will discuss in this chapter. It behaves exactly like a spinlock, with the only difference being that your code can sleep. If you try to lock a mutex that is already held by another task, your task will find itself suspended and woken up only when the mutex is released. No spinning this time, meaning that the CPU can do something else while your task waits in a sleeping state. As I mentioned previously, a spinlock is a lock held by a CPU. A mutex, on the other hand, is a lock held by a task.</p>
			<p>A mutex <a id="_idIndexMarker153"/>is a simple data structure that embeds a wait queue (to put contenders to sleep) and a spinlock to protect access to this wait queue, as illustrated in the following code snippet:</p>
			<p class="source-code">struct mutex {</p>
			<p class="source-code">    atomic_long_t owner;</p>
			<p class="source-code">    spinlock_t wait_lock;</p>
			<p class="source-code">#ifdef CONFIG_MUTEX_SPIN_ON_OWNER</p>
			<p class="source-code">    struct optimistic_spin_queue osq; /* Spinner MCS lock */</p>
			<p class="source-code">#endif</p>
			<p class="source-code">    struct list_head wait_list;</p>
			<p class="source-code">[...]</p>
			<p class="source-code">};</p>
			<p>In the preceding code snippet, elements used only in debugging mode have been removed for the sake of readability. However, as we can see, mutexes are built on top of spinlocks. <strong class="source-inline">owner</strong> represents the process that owns (holds) the lock. <strong class="source-inline">wait_list</strong> is the list in which the mutex's contenders are put to sleep. <strong class="source-inline">wait_lock</strong> is the spinlock that protects <strong class="source-inline">wait_list</strong> manipulation (removal or insertion of contenders to sleep in it). It helps to keep <strong class="source-inline">wait_list</strong> coherent on SMP systems.</p>
			<p>The mutex APIs can be found in the <strong class="source-inline">include/linux/mutex.h</strong> header file. Prior to acquiring and releasing a mutex, it must be initialized. As for other kernel core data structures, there is a static initialization, as shown here:</p>
			<p class="source-code">static DEFINE_MUTEX(my_mutex);</p>
			<p>Here is a definition of the <strong class="source-inline">DEFINE_MUTEX()</strong> macro:</p>
			<p class="source-code">#define DEFINE_MUTEX(mutexname) \</p>
			<p class="source-code">struct mutex mutexname = __MUTEX_INITIALIZER(mutexname)</p>
			<p>A second approach the kernel offers is dynamic initialization, possible thanks to a call to a <strong class="source-inline">__mutex_init()</strong> low-level function, which is actually wrapped by a much more user-friendly macro, <strong class="source-inline">mutex_init()</strong>. You can see this in action in the following code snippet:</p>
			<p class="source-code">struct fake_data {</p>
			<p class="source-code">    struct i2c_client *client;</p>
			<p class="source-code">    u16 reg_conf;</p>
			<p class="source-code">    struct mutex mutex;</p>
			<p class="source-code">};</p>
			<p class="source-code">static int fake_probe(struct i2c_client *client)</p>
			<p class="source-code">{</p>
			<p class="source-code">[...]</p>
			<p class="source-code">    mutex_init(&amp;data-&gt;mutex);</p>
			<p class="source-code">[...]</p>
			<p class="source-code">}</p>
			<p>Acquiring (aka locking) a <a id="_idIndexMarker154"/>mutex is as simple as calling one of the following three functions:</p>
			<p class="source-code">void mutex_lock(struct mutex *lock);</p>
			<p class="source-code">int mutex_lock_interruptible(struct mutex *lock);</p>
			<p class="source-code">int mutex_lock_killable(struct mutex *lock);</p>
			<p>If the mutex is free (unlocked), your task will immediately acquire it without going to sleep. Otherwise, your task will be put to sleep in a manner that depends on the locking function you use. With <strong class="source-inline">mutex_lock()</strong>, your task will be put in an uninterruptible sleep state (<strong class="source-inline">TASK_UNINTERRUPTIBLE</strong>) while waiting for the mutex to be released (if it is held by another task). <strong class="source-inline">mutex_lock_interruptible()</strong> will put your task in an interruptible sleep state, in which the sleep can be interrupted by any signal. <strong class="source-inline">mutex_lock_killable()</strong> will allow your sleeping task to be interrupted only by signals that actually kill the task. Each of these functions returns <strong class="source-inline">0</strong> if the lock has been acquired successfully. Moreover, interruptible variants return <strong class="source-inline">-EINTR</strong> when the locking attempt was interrupted by a signal.</p>
			<p>Whichever locking function is used, the mutex owner (and only the owner) should release the mutex using <strong class="source-inline">mutex_unlock()</strong>, defined as follows:</p>
			<p class="source-code">void mutex_unlock(struct mutex *lock);</p>
			<p>If it is worth checking the status of the mutex, you can use <strong class="source-inline">mutex_is_locked()</strong>, as follows:</p>
			<p class="source-code">static bool mutex_is_locked(struct mutex *lock)</p>
			<p>This function simply checks if the<a id="_idIndexMarker155"/> mutex owner is <strong class="source-inline">NULL</strong> and returns <strong class="source-inline">true</strong> if so or <strong class="source-inline">false</strong> otherwise.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">It is recommended to use <strong class="source-inline">mutex_lock()</strong> only when you can guarantee the mutex will not be held for a long time. If not, you should use an interruptible variant instead.</p>
			<p>There are specific rules while using mutexes. The most important ones are enumerated in the <strong class="source-inline">include/linux/mutex.h</strong> kernel mutex API header file, and some of these are outlined here:</p>
			<ul>
				<li>A mutex can be held by one and only one task at a time.</li>
				<li>Once held, the mutex can only be unlocked by the owner (that is, the task that locked it).</li>
				<li>Multiple, recursive, or nested locks/unlocks are not allowed.</li>
				<li>A mutex object must be initialized via the API. It must not be initialized by copying nor by using <strong class="source-inline">memset</strong>, just as held mutexes must not be reinitialized.</li>
				<li>A task that holds a mutex may not exit, just as memory areas where held locks reside must not be freed.</li>
				<li>Mutexes may not be used in hardware or software interrupt contexts such as tasklets and timers.</li>
			</ul>
			<p>All this makes mutexes suitable for the following cases:</p>
			<ul>
				<li>Locking only in the user context</li>
				<li>If the protected resource is not accessed from an IRQ handler and the operations need not be atomic</li>
			</ul>
			<p>However, it may be <a id="_idIndexMarker156"/>cheaper (in terms of CPU cycles) to use spinlocks for very small critical sections since the spinlock only suspends the scheduler and starts spinning, compared to the cost of using a mutex, which needs to suspend the current task and insert it into the mutex's wait queue, requiring the scheduler to switch to another task and rescheduling the sleeping task once the mutex is released.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor043"/>Trylock methods</h2>
			<p>There are cases where <a id="_idIndexMarker157"/>we may need to acquire the lock only if it is not already held by another contender elsewhere. Such methods try to acquire the lock and immediately (without spinning if we are using a spinlock, nor sleeping if we are using a mutex) return a status value, showing whether the lock has been successfully locked or not.</p>
			<p>Both spinlock and mutex APIs provide a trylock method. These are, respectively, <strong class="source-inline">spin_trylock()</strong> and <strong class="source-inline">mutex_trylock()</strong>, the latter of which you can see here. Both methods return 0 on failure (the lock is already locked) or 1 on success (lock acquired). Thus, it makes sense to use these functions along with an <strong class="source-inline">if</strong> statement:</p>
			<p class="source-code">int mutex_trylock(struct mutex *lock)</p>
			<p><strong class="source-inline">spin_trylock()</strong> actually targets spinlocks. It will lock the spinlock if it is not already locked, just as the <strong class="source-inline">spin_lock()</strong> method does. However, it immediately returns <strong class="source-inline">0</strong> without spinning in cases where the spinlock is already locked. You can see it in action here:</p>
			<p class="source-code">static DEFINE_SPINLOCK(foo_lock);</p>
			<p class="source-code">[...]</p>
			<p class="source-code">static void foo(void)</p>
			<p class="source-code">{</p>
			<p class="source-code">    [...]</p>
			<p class="source-code">    if (!spin_trylock(&amp;foo_lock)) {</p>
			<p class="source-code">        /* Failure! the spinlock is already locked */</p>
			<p class="source-code">        [...]</p>
			<p class="source-code">        return;</p>
			<p class="source-code">    }</p>
			<p class="source-code">    /*</p>
			<p class="source-code">    * reaching this part of the code means that the</p>
			<p class="source-code">    * spinlock has been successfully locked</p>
			<p class="source-code">    */</p>
			<p class="source-code">    [...]</p>
			<p class="source-code">    spin_unlock(&amp;foo_lock);</p>
			<p class="source-code">    [...]</p>
			<p class="source-code">}</p>
			<p>On the other<a id="_idIndexMarker158"/> hand, <strong class="source-inline">mutex_trylock()</strong> targets mutexes. It will lock the mutex if it is not already locked, just as the <strong class="source-inline">mutex_lock()</strong> method does. However, it immediately returns <strong class="source-inline">0</strong> without sleeping in cases where the mutex is already locked. You can see an example of this in the following code snippet:</p>
			<p class="source-code">static DEFINE_MUTEX(bar_mutex);</p>
			<p class="source-code">[...]</p>
			<p class="source-code">static void bar (void)</p>
			<p class="source-code">{</p>
			<p class="source-code">    [...]</p>
			<p class="source-code">    if (!mutex_trylock(&amp;bar_mutex)){</p>
			<p class="source-code">        /* Failure! the mutex is already locked */</p>
			<p class="source-code">        [...]</p>
			<p class="source-code">        return;</p>
			<p class="source-code">    }</p>
			<p class="source-code">    /*</p>
			<p class="source-code">     * reaching this part of the code means that the</p>
			<p class="source-code">     * mutex has been successfully acquired</p>
			<p class="source-code">     */</p>
			<p class="source-code">    [...]</p>
			<p class="source-code">    mutex_unlock(&amp;bar_mutex);</p>
			<p class="source-code">    [...]</p>
			<p class="source-code">}</p>
			<p>In the preceding <a id="_idIndexMarker159"/>excerpt, <strong class="source-inline">mutex_trylock()</strong> is used along with an <strong class="source-inline">if</strong> statement so that the driver can adapt its behavior.</p>
			<p>Now that we are done with the trylock variant, let's switch to a totally different concept—learning how to explicitly delay execution. </p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor044"/>Dealing with kernel waiting, sleeping, and delay mechanisms</h1>
			<p>The term <em class="italic">sleeping</em> in<a id="_idIndexMarker160"/> this section refers to a mechanism by which a task (on behalf of the running kernel code) voluntarily relaxes the processor, with the possibility of another task being scheduled. While simple sleeping would consist of a task sleeping and being awakened after a given duration (to passively delay an operation, for example), there are sleeping mechanisms based on external events (such as data availability). Simple sleeps are implemented in the kernel using dedicated APIs; waking up from such sleeps is implicit (handled by the kernel itself) after the duration expires. The other sleeping mechanism is conditioned on an event and the waking-up is explicit (another task must explicitly wake us up based on a condition, else we sleep forever) unless a sleeping timeout is specified. This mechanism is implemented in the kernel using the concept of wait queues. That said, both sleep APIs and wait queues implement what we can call passive waiting. The difference between the two is how the waking-up process occurs.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>Wait queue</h2>
			<p>The kernel scheduler <a id="_idIndexMarker161"/>manages a list of tasks to run (tasks in a <strong class="source-inline">TASK_RUNNING</strong> state), known as a runqueue. On the other hand, sleeping tasks, whether interruptible or not (in a <strong class="source-inline">TASK_INTERRUPTIBLE</strong> or <strong class="source-inline">TASK_UNINTERRUPTIBLE</strong> state), have their own queues, known as wait queues.</p>
			<p>Wait queues are a higher-level mechanism essentially used to process blocking <strong class="bold">input/output</strong> (<strong class="bold">I/O</strong>), to wait for a condition to be <strong class="source-inline">true</strong>, to wait for a given event to occur, or to sense data or resource availability. To understand how they work, let's have a look at the following structure in <strong class="source-inline">include/linux/wait.h</strong>:</p>
			<p class="source-code">struct wait_queue_head {</p>
			<p class="source-code">    spinlock_t lock;</p>
			<p class="source-code">    struct list_head head;</p>
			<p class="source-code">};</p>
			<p>A wait queue is nothing but a list (with sleeping processes in it waiting to be awakened) and a spinlock to protect access to this list. We can use a wait queue when more than one process wants to sleep, waiting for one or more events to occur in order to be awakened. The head member is actually a list of processes waiting for the event(s). Each process that wants to sleep while waiting for the event to occur puts itself it this list before going to sleep. While a process is in the list, it is called wait queue entry. When an event occurs, one or more processes on the list are woken up and moved off the list.</p>
			<p>We can declare and initialize a wait queue in two ways. The first method is to statically use <strong class="source-inline">DECLARE_WAIT_QUEUE_HEAD</strong>, as follows:</p>
			<p class="source-code">DECLARE_WAIT_QUEUE_HEAD(my_event);</p>
			<p>Alternatively, we can dynamically use <strong class="source-inline">init_waitqueue_head()</strong>, as follows:</p>
			<p class="source-code">wait_queue_head_t my_event;</p>
			<p class="source-code">init_waitqueue_head(&amp;my_event);</p>
			<p>Any process that wants to sleep while waiting for <strong class="source-inline">my_event</strong> to occur can invoke either <strong class="source-inline">wait_event_interruptible()</strong> or <strong class="source-inline">wait_event()</strong>. Most of the time, the event is just the fact that a resource becomes available, thus it makes sense for a process to go to sleep only after a first check of the availability of that resource. To make things easy, these functions both take an expression in place of the second argument so that the process is put to sleep only if the expression evaluates <strong class="source-inline">false</strong>, as illustrated in the following code snippet:</p>
			<p class="source-code">wait_event(&amp;my_event, (event_occured == 1));</p>
			<p class="source-code">/* or */</p>
			<p class="source-code">wait_event_interruptible(&amp;my_event, (event_occured == 1));</p>
			<p><strong class="source-inline">wait_event()</strong> or <strong class="source-inline">wait_event_interruptible()</strong> simply evaluates the condition when called. If the <a id="_idIndexMarker162"/>condition is <strong class="source-inline">false</strong>, the process is put into either a <strong class="source-inline">TASK_UNINTERRUPTIBLE</strong> or a <strong class="source-inline">TASK_INTERRUPTIBLE</strong> (for the <strong class="source-inline">_interruptible</strong> variant) state and removed from the runqueue.</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="source-inline">wait_event()</strong> puts the process into an exclusive wait, aka uninterruptible sleep, and can't thus be interrupted by the signal. It should be used only for critical tasks. Interruptible functions are recommended in most situations.</p>
			<p>There may be cases where you need not only the condition to be <strong class="source-inline">true</strong> but to time out after a certain waiting duration. You can address such cases using <strong class="source-inline">wait_event_timeout()</strong>, whose prototype is shown here:</p>
			<p class="source-code">wait_event_timeout(wq_head, condition, timeout)</p>
			<p>This function has two behaviors, depending on the timeout having elapsed or not. These are outlined here:</p>
			<ul>
				<li><strong class="bold">Timeout elapsed</strong>: The function returns <strong class="source-inline">0</strong> if the condition is evaluated to <strong class="source-inline">false</strong> or <strong class="source-inline">1</strong> if it is evaluated to <strong class="source-inline">true</strong>.</li>
				<li><strong class="bold">Timeout not elapsed yet</strong>: The function returns the remaining time (in jiffies—at least 1) if the condition is evaluated to <strong class="source-inline">true</strong>.</li>
			</ul>
			<p>The time unit for timeout is a<a id="_idIndexMarker163"/> jiffy. There are convenient APIs to convert convenient time units such as milliseconds and microseconds to jiffies, defined as follows: </p>
			<p class="source-code">unsigned long msecs_to_jiffies(const unsigned int m)</p>
			<p class="source-code">unsigned long usecs_to_jiffies(const unsigned int u)</p>
			<p>After a change on any variable that could affect the result of the wait condition, you must call the appropriate <strong class="source-inline">wake_up*</strong> family function. That being said, in order to wake up a process sleeping on a wait queue, you should call either <strong class="source-inline">wake_up()</strong>, <strong class="source-inline">wake_up_all()</strong>, <strong class="source-inline">wake_up_interruptible()</strong>, or <strong class="source-inline">wake_up_interruptible_all()</strong>. Whenever you call any of these functions, the condition is re-evaluated again. If the condition is <strong class="source-inline">true</strong> at that time, then a process (or all processes for the <strong class="source-inline">_all()</strong> variant) in the wait queue will be awakened, and its (their) state set to <strong class="source-inline">TASK_RUNNING</strong>; otherwise, (the condition is <strong class="source-inline">false</strong>), nothing happens. The following code snippet illustrates this concept:</p>
			<p class="source-code">wake_up(&amp;my_event);</p>
			<p class="source-code">wake_up_all(&amp;my_event);</p>
			<p class="source-code">wake_up_interruptible(&amp;my_event);</p>
			<p class="source-code">wake_up_interruptible_all(&amp;my_event);</p>
			<p>In the preceding code snippet, <strong class="source-inline">wake_up()</strong> will wake only one process from the wait queue, while <strong class="source-inline">wake_up_all()</strong> will wake all processes from the wait queue. On the other hand, <strong class="source-inline">wake_up_interruptible()</strong> will wake only one process from the wait queue that is in interruptible sleep, and <strong class="source-inline">wake_up_interruptible_all()</strong> will wake all processes <a id="_idIndexMarker164"/>from the wait queue that are in interruptible sleep.</p>
			<p>Because they can be interrupted by signals, you should check the return value of the <strong class="source-inline">_interruptible</strong> variants. A nonzero value means your sleep has been interrupted by some sort of signal, and the driver should return <strong class="source-inline">ERESTARTSYS</strong>, as illustrated in the following code snippet:</p>
			<p class="source-code">#include &lt;linux/module.h&gt;</p>
			<p class="source-code">#include &lt;linux/init.h&gt;</p>
			<p class="source-code">#include &lt;linux/sched.h&gt;</p>
			<p class="source-code">#include &lt;linux/time.h&gt;</p>
			<p class="source-code">#include &lt;linux/delay.h&gt;</p>
			<p class="source-code">#include&lt;linux/workqueue.h&gt;</p>
			<p class="source-code">static DECLARE_WAIT_QUEUE_HEAD(my_wq);</p>
			<p class="source-code">static int condition = 0;</p>
			<p class="source-code">/* declare a work queue*/</p>
			<p class="source-code">static struct work_struct wrk;</p>
			<p class="source-code">static void work_handler(struct work_struct *work)</p>
			<p class="source-code">{</p>
			<p class="source-code">    pr_info("Waitqueue module handler %s\n", __FUNCTION__);</p>
			<p class="source-code">    msleep(5000);</p>
			<p class="source-code">    pr_info("Wake up the sleeping module\n");</p>
			<p class="source-code">    condition = 1;</p>
			<p class="source-code">    wake_up_interruptible(&amp;my_wq);</p>
			<p class="source-code">}</p>
			<p class="source-code">static int __init my_init(void)</p>
			<p class="source-code">{</p>
			<p class="source-code">    pr_info("Wait queue example\n");</p>
			<p class="source-code">    INIT_WORK(&amp;wrk, work_handler);</p>
			<p class="source-code">    schedule_work(&amp;wrk);</p>
			<p class="source-code">    pr_info("Going to sleep %s\n", __FUNCTION__);</p>
			<p class="source-code">    if (wait_event_interruptible(my_wq, condition != 0)) {</p>
			<p class="source-code">        pr_info("Our sleep has been interrupted\n");</p>
			<p class="source-code">        return -ERESTARTSYS;</p>
			<p class="source-code">    }</p>
			<p class="source-code">    pr_info("woken up by the work job\n");</p>
			<p class="source-code">    return 0;</p>
			<p class="source-code">}</p>
			<p class="source-code">void my_exit(void)</p>
			<p class="source-code">{</p>
			<p class="source-code">    pr_info("waitqueue example cleanup\n");</p>
			<p class="source-code">}</p>
			<p class="source-code">module_init(my_init)</p>
			<p class="source-code">module_exit(my_exit);</p>
			<p class="source-code">MODULE_AUTHOR("John Madieu &lt;john.madieu@gmail.com&gt;");</p>
			<p class="source-code">MODULE_LICENSE("GPL");</p>
			<p>In the preceding example, we have<a id="_idIndexMarker165"/> used the <strong class="source-inline">msleep()</strong> API, which will be explained shortly. Back to the behavior of the code—the current process (actually, <strong class="source-inline">insmod</strong>) will be put to sleep in the wait queue for 5 seconds and woken up by the work handler. The <strong class="source-inline">dmesg</strong> output is shown here:</p>
			<p class="source-code">[342081.385491] Wait queue example</p>
			<p class="source-code">[342081.385505] Going to sleep my_init</p>
			<p class="source-code">[342081.385515] Waitqueue module handler work_handler</p>
			<p class="source-code">[342086.387017] Wake up the sleeping module</p>
			<p class="source-code">[342086.387096] woken up by the work job</p>
			<p class="source-code">[342092.912033] waitqueue example cleanup</p>
			<p>Now that we are comfortable with the concept of wait queue, which allows us to put processes to sleep and wait for these to be awakened, let's learn another simple sleeping mechanism that simply consists of delaying the execution flow unconditionally.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor046"/>Simple sleeping in the kernel</h2>
			<p>This<a id="_idIndexMarker166"/> simple sleeping can also be referred to<a id="_idIndexMarker167"/> as <strong class="bold">passive delay</strong> because the task sleeps (allowing the CPU to schedule another task) while waiting, in contrast to active delay, which is a busy wait as the task waits by wasting the CPU clock and thus consuming resources. Before using sleeping APIs, the driver must include <strong class="source-inline">#include &lt;linux/delay&gt;</strong>, which would make the following function available:</p>
			<p class="source-code">usleep_range(unsigned long min, unsigned long max)</p>
			<p class="source-code">msleep(unsigned long msecs)</p>
			<p class="source-code">msleep(unsigned long msecs)</p>
			<p class="source-code">msleep_interruptible(unsigned long msecs)</p>
			<p>In the preceding APIs, <strong class="source-inline">msecs</strong> is the number of milliseconds of sleep. <strong class="source-inline">min</strong> and <strong class="source-inline">max</strong> are the minimum and upper bounds of sleeping in microseconds.</p>
			<p>The <strong class="source-inline">usleep_range()</strong> API relies<a id="_idIndexMarker168"/> on <strong class="bold">high-resolution timers</strong> (<strong class="bold">hrtimers</strong>), and it is recommended to use this sleep for a few ~<strong class="source-inline">µsecs</strong> or small <strong class="source-inline">msecs</strong> (between 10 microseconds and 20 milliseconds), avoiding the busy-wait loop of <strong class="source-inline">udelay()</strong>.</p>
			<p><strong class="source-inline">msleep*()</strong> APIs <a id="_idIndexMarker169"/>are backed by <strong class="source-inline">jiffies</strong>/legacy timers. You should use this for larger milliseconds of sleep (10 milliseconds or more). This API sets the current task to <strong class="source-inline">TASK_UNINTERRUPTIBLE</strong>, whereas <strong class="source-inline">msleep_interruptible()</strong> sets the current task to <strong class="source-inline">TASK_INTERRUPTIBLE</strong> before scheduling the sleep. In short, the difference is whether the sleep can be ended early by a signal. It is recommended to use <strong class="source-inline">msleep()</strong> unless you have a need for the interruptible variant.</p>
			<p>APIs in this section must be used for inserting delays in a non-atomic context exclusively.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>Kernel delay or busy waiting</h2>
			<p>First, the term "delay" in this <a id="_idIndexMarker170"/>section can be considered as busy waiting as the task actively waits (corresponding to the <strong class="source-inline">for()</strong> or the <strong class="source-inline">while()</strong> loop), consuming CPU resources, in contrast to sleep, which is a passive delay as the task sleeps while waiting.</p>
			<p>Even for busy loop <a id="_idIndexMarker171"/>waiting, the driver must include <strong class="source-inline">#include &lt;linux/delay&gt;</strong>, which would make the following APIs available as well:</p>
			<p class="source-code">ndelay(unsigned long nsecs)</p>
			<p class="source-code">udelay(unsigned long usecs)</p>
			<p class="source-code">mdelay(unsigned long msecs)</p>
			<p>The advantage of such APIs is that they can be used in both atomic and non-atomic contexts.</p>
			<p>The precision<a id="_idIndexMarker172"/> of <strong class="source-inline">ndelay</strong> depends on how accurate your timer is (not always the case on an embedded <strong class="bold">system on a chip</strong>, or <strong class="bold">SoC</strong>). <strong class="source-inline">ndelay</strong>-level precision may not actually exist on many non-PC devices. Instead, you are more likely to come across the following:</p>
			<ul>
				<li><strong class="source-inline">udelay</strong>: This API is <a id="_idIndexMarker173"/>busy-wait loop-based. It will busy wait for enough loop cycles to achieve the desired delay. You should use this function if you need to sleep for a few <strong class="source-inline">µsecs</strong> (&lt; ~10 microseconds). It is recommended to use this API even for sleeping less than 10 us because, on slower systems (some embedded SoCs), the overhead of setting up hrtimers for <strong class="source-inline">usleep</strong> may not be worth it. Such an evaluation will obviously depend on your specific situation, but it is something to be aware of.</li>
				<li><strong class="source-inline">mdelay</strong>: This is<a id="_idIndexMarker174"/> a macro wrapper around <strong class="source-inline">udelay</strong> to account for possible overflow when passing large arguments to <strong class="source-inline">udelay</strong>. In general, the use of <strong class="source-inline">mdelay</strong> is discouraged, and code should be refactored to allow for the use of <strong class="source-inline">msleep</strong>.</li>
			</ul>
			<p>At this step, we are done with Linux kernel sleeping or delay mechanisms. We should be able to design and implement a time-slice-managed execution flow. We can now get deeper into the way the Linux kernel manages time.</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor048"/>Understanding Linux kernel time management</h1>
			<p>Time is one of the most<a id="_idIndexMarker175"/> used resources in computer systems, right after memory. It is used to do almost everything: timer, sleep, scheduling, and many other tasks.</p>
			<p>The Linux kernel includes software timer concepts to enable kernel functions to be invoked at a later time. </p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>The concepts of clocksource, clockevent, and tick device</h2>
			<p>In the original Linux timer implementation, the main hardware timer was mainly used for timekeeping. It was also programmed to fire interrupts periodically at HZ frequency, whose corresponding period is<a id="_idIndexMarker176"/> called a <strong class="bold">jiffy</strong> (both are explained later in this chapter, in the <em class="italic">Jiffies and HZ</em> section). Each of these interrupts generated every 1/HZ second was (and still is) referred to as a tick. Throughout this <a id="_idIndexMarker177"/>section, the term <em class="italic">tick</em> will refer to the interrupt generated at a 1/HZ period.</p>
			<p>The whole system time management (either from the kernel or user space) was bound to jiffies, which is also a global variable in the kernel, incremented at each tick. In addition to incrementing the value of <strong class="source-inline">jiffies</strong> (on top of which a timer wheel was implemented), the tick handler was also responsible for processes scheduling, statistic updating, and profiling.</p>
			<p>Starting from kernel version 2.6.21, as a first improvement, hrtimers implementation was merged (and is now available through <strong class="source-inline">CONFIG_HIGH_RES_TIMERS</strong>). This feature was (and still is) transparent and has come with <strong class="source-inline">hrtimer</strong> timers as a functionality of its<a id="_idIndexMarker178"/> own, with a new data type, <strong class="source-inline">ktime_t</strong>, which is used to keep time value on a nanosecond basis. The former legacy (tick-based and low-res) timer implementation remained, however. The improvement converted the <strong class="source-inline">nanosleep()</strong>, <strong class="bold">interval timers</strong> (<strong class="bold">itimers</strong>), and <strong class="bold">Portable Operating System Interface</strong> (<strong class="bold">POSIX</strong>) timers to<a id="_idIndexMarker179"/> rely on high-resolution timers, resulting in better granularity and accuracy than with the current jiffy resolution. The unification of <strong class="source-inline">nanosleep()</strong> and <strong class="source-inline">clock_nanosleep()</strong> was made possible by the conversion of <strong class="source-inline">nanosleep()</strong> and POSIX timers. Without this improvement, the best accuracy that could be obtained for timer events would be 1 jiffy, whose duration depends on the value of <strong class="source-inline">HZ</strong> in the kernel.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">That said, high-resolution timer implementation is an independent feature. hrtimer timers can be enabled whatever the platform, but whether they work in high-resolution mode or not depends on the underlying hardware timer. Otherwise, the system is said to be <a id="_idIndexMarker180"/>in <strong class="bold">low-resolution</strong> (<strong class="bold">low-res</strong>) mode.</p>
			<p>Improvements have been done all along the kernel evolutions until the generic clockevent interface came in, with the concepts of clock source, clock event, and tick devices. This completely changed time management in the kernel and influenced CPU power management.</p>
			<h3>The clocksource framework and clock source devices</h3>
			<p>A clock source<a id="_idIndexMarker181"/> is a monotonic, atomic, and free-running counter. This can be considered<a id="_idIndexMarker182"/> as a timer that acts as a free-running counter that provides time stamping and read access to the monotonically increasing time value. The common operation performed on clock source devices is reading the counter's value.</p>
			<p>In the kernel, there is a <strong class="source-inline">clocksource_list</strong> global list that tracks the clock source devices registered with the system, enqueued ordered by rating. This allows the Linux kernel to know about all registered clock source devices and switch to a clock source with a better rating and features. For instance, the <strong class="source-inline">__clocksource_select()</strong> function is invoked after each registration of a new clock source, which ensures that the best clock source is always selected. Clock sources are registered with either <strong class="source-inline">clocksource_mmio_init()</strong> or <strong class="source-inline">clocksource_register_hz()</strong> (you can <strong class="source-inline">grep</strong> these words). However, clock source device drivers are in <strong class="source-inline">drivers/clocksource/</strong> in kernel sources.</p>
			<p>On a running Linux system, the most intuitive way to list clock source devices that are registered with the framework is by looking for the word <strong class="source-inline">clocksource</strong> in the kernel log message buffer, as shown here:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B17934_03_001.jpg" alt="Figure 3.1 – System clocksource list&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – System clocksource list</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In the preceding output logs (from a Pi 4), the <strong class="source-inline">jiffies</strong> clock source is a jiffy granularity-based and always provided clock source registered by the kernel as <strong class="source-inline">clocksource_jiffies</strong> in <strong class="source-inline">kernel/time/jiffies.c</strong>, with the lowest valid rating value (that is, used as the last resort). On the x86 platform, this clock source is refined and renamed into <strong class="source-inline">refined-jiffies</strong>—see the <strong class="source-inline">register_refined_jiffies()</strong> function call in <strong class="source-inline">arch/x86/kernel/setup.c</strong>.</p>
			<p>However, the preferred way (especially where the <strong class="source-inline">dmesg</strong> buffer has rotated or has been cleared) to enumerate the available <a id="_idIndexMarker183"/>clock source on a running Linux system is by reading the content of the <strong class="source-inline">available_clocksource</strong> file in <strong class="source-inline">/sys/devices/system/clocksource/clocksource0/</strong>, as shown in the following code snippet (on a Pi 4):</p>
			<p class="source-code">root@raspberrypi4-64-d0:~# cat  /sys/devices/system/clocksource/clocksource0/available_clocksource </p>
			<p class="source-code">arch_sys_counter </p>
			<p class="source-code">root@raspberrypi4-64-d0:~#</p>
			<p>On an i.MX6 board, we have<a id="_idIndexMarker184"/> the following:</p>
			<p class="source-code">root@udoo-labcsmart:~# cat  /sys/devices/system/clocksource/clocksource0/available_clocksource </p>
			<p class="source-code">mxc_timer1 </p>
			<p class="source-code">root@udoo-labcsmart:~#</p>
			<p>To check the currently used clock source, you can use the following code:</p>
			<p class="source-code">root@raspberrypi4-64-d0:~# cat  /sys/devices/system/clocksource/clocksource0/current_clocksource </p>
			<p class="source-code">arch_sys_counter</p>
			<p class="source-code">root@raspberrypi4-64-d0:~#</p>
			<p>On my x86 machine, we have the following for both available clock sources and the currently used one:</p>
			<p class="source-code">jma@labcsmart:~$ cat /sys/devices/system/clocksource/clocksource0/available_clocksource</p>
			<p class="source-code">tsc hpet acpi_pm </p>
			<p class="source-code">jma@labcsmart:~$ cat /sys/devices/system/clocksource/clocksource0/current_clocksource </p>
			<p class="source-code">tsc</p>
			<p class="source-code">jma@labcsmart:~$</p>
			<p>To change the <a id="_idIndexMarker185"/>current clock source, you can echo the name of one of the available<a id="_idIndexMarker186"/> clock sources into the <strong class="source-inline">current_clocksource</strong> file, like this:</p>
			<p class="source-code">jma@labcsmart:~$ echo acpi_pm &gt;  /sys/devices/system/clocksource/clocksource0/current_clocksource </p>
			<p class="source-code">jma@labcsmart:~$</p>
			<p>Changing the current clock source must be done with caution since the current clock source selected by the kernel during the boot is always the best one.</p>
			<h4>Linux kernel timekeeping</h4>
			<p>One of the main goals of the<a id="_idIndexMarker187"/> clock source device is feeding the timekeeper. There can be multiple clock sources in a system, but the timekeeper will choose the one with the highest precision to use. The timekeeper needs to obtain the value of the clock source periodically to update the system time, which is usually updated during the tick processing, as illustrated in the following diagram:</p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B17934_03_002.jpg" alt="Figure 3.2 – Linux kernel timekeeper implementation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Linux kernel timekeeper implementation</p>
			<p>The <a id="_idIndexMarker188"/>timekeeper provides several types of time: <strong class="bold">xtime</strong>, <strong class="bold">monotonic time</strong>, <strong class="bold">raw monotonic time</strong>, and <strong class="bold">boot time</strong>, which are outlined in more detail here:</p>
			<ul>
				<li><strong class="bold">xtime</strong>: This is<a id="_idIndexMarker189"/> wall (real) time, which represents the current time as given by <a id="_idIndexMarker190"/>the <strong class="bold">Real-Time Clock</strong> (<strong class="bold">RTC</strong>) chip.</li>
				<li><strong class="bold">Monotonic time</strong>: The <a id="_idIndexMarker191"/>cumulative time since the system is turned on, but does not count the time the system sleeps.</li>
				<li><strong class="bold">Raw monotonic time</strong>: This <a id="_idIndexMarker192"/>has the same meaning as monotonic time, but it is purer and will not be affected by <strong class="bold">Network Time Protocol</strong> (<strong class="bold">NTP</strong>) time <a id="_idIndexMarker193"/>adjustment.</li>
				<li><strong class="bold">Boot time</strong>: This adds <a id="_idIndexMarker194"/>the time the system spent sleeping to the monotonic time, which gives the total time after the system is powered on.</li>
			</ul>
			<p>The following table shows the <a id="_idIndexMarker195"/>different types of time and their kernel getter functions:</p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/Table_01.jpg" alt="Table 3.1 – Linux kernel timekeeping functions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 3.1 – Linux kernel timekeeping functions</p>
			<p>Now that we are familiar with the Linux kernel timekeeping mechanisms and APIs, we are free to learn another concept involved in this time management—the clockevent framework.</p>
			<h3>The clockevent framework and clock event devices</h3>
			<p>Before the concept of clockevent was introduced, the locality of hardware timers was not considered. The clock source/event hardware was programmed to periodically generate <strong class="source-inline">HZ</strong> ticks (interrupts) per second, the interval between each tick being a jiffy. With the introduction of <a id="_idIndexMarker196"/>clockevent/source in the kernel, the interruption of the clock became abstracted as an event. The main function of the clockevent framework is to distribute the clock interrupts (events) and set the next trigger condition. It is a generic framework for next-event interrupt programming.</p>
			<p>A clock event device<a id="_idIndexMarker197"/> is a device that can fire interrupts and allow us to program when the next interrupt (an event) will poke in the future. Each clock event device driver must provide a <strong class="source-inline">set_next_event</strong> function (or <strong class="source-inline">set_next_ktime</strong> in the case of an hrtimer-backed clock event device), which is used by the framework when it comes to using the underlying clock event device to program the next interrupt.</p>
			<p>Clock event devices are orthogonal to clock source devices. This is probably why their drivers are in the same place (and, sometimes, in the same compilation unit) as clock source device drivers—that is, in <strong class="source-inline">drivers/clocksource</strong>. On most platforms, the same hardware and register range may be used for the clock event and for the clock source, but they are essentially different things. This is the case, for example, with the BCM2835 System Timer, which is a memory-mapped peripheral found on the BCM2835 used in the Raspberry Pi. It has a 64-bit free-running counter that runs at 1 <strong class="bold">megahertz</strong> (<strong class="bold">MHz</strong>), as well as four distinct "output compare registers" that can be used to schedule interrupts. In such cases, the driver usually registers the clock source and the clock event device in the same compilation unit.</p>
			<p>On a running Linux system, the available clock event devices can by listed from the <strong class="source-inline">/sys/devices/system/clockevents/</strong> directory. Here is an example on a Pi 4:</p>
			<p class="source-code">root@raspberrypi4-64-d0:~# ls /sys/devices/system/clockevents/         </p>
			<p class="source-code">broadcast    clockevent1  clockevent3  uevent</p>
			<p class="source-code">clockevent0  clockevent2  power</p>
			<p class="source-code">root@raspberrypi4-64-d0:~#</p>
			<p>On a dual-core i.MX6 running system, we have the following:</p>
			<p class="source-code">root@udoo-labcsmart:~# ls /sys/devices/system/clockevents/</p>
			<p class="source-code">broadcast    clockevent0  clockevent1  consumers    power        suppliers    uevent</p>
			<p class="source-code">root@empair-labcsmart:~#</p>
			<p>And finally, on my height core machine, we have the following:</p>
			<p class="source-code">jma@labcsmart:~$ ls /sys/devices/system/clockevents/</p>
			<p class="source-code">broadcast  clockevent0  clockevent1  clockevent2  clockevent3  clockevent4  clockevent5  clockevent6  clockevent7  power  uevent</p>
			<p class="source-code">jma@labcsmart:~$</p>
			<p>From the preceding listings of available<a id="_idIndexMarker198"/> clock event devices on the system, we can say the following:</p>
			<ul>
				<li>There are as many clock event devices as CPUs on the system (allowing per-CPU clock devices, thus involving timer locality).</li>
				<li>There is always a strange directory, <strong class="source-inline">broadcast</strong>. We will discuss this particular timer in the next sections.</li>
			</ul>
			<p>To know the underlying timer of a given clock event device, you can read the content of <strong class="source-inline">current_device</strong> in the clock event directory. We'll now look at some examples on three different machines.</p>
			<p>On the i.MX 6 platform, we have the following:</p>
			<p class="source-code">root@udoo-labcsmart:~# cat /sys/devices/system/clockevents/clockevent0/current_device </p>
			<p class="source-code">local_timer</p>
			<p class="source-code">root@udoo-labcsmart:~# cat /sys/devices/system/clockevents/clockevent1/current_device </p>
			<p class="source-code">local_timer</p>
			<p>On the Pi 4, we have the following:</p>
			<p class="source-code">root@raspberrypi4-64-d0:~# cat /sys/devices/system/clockevents/clockevent2/current_device</p>
			<p class="source-code">arch_sys_timer</p>
			<p class="source-code">root@raspberrypi4-64-d0:~# cat /sys/devices/system/clockevents/clockevent3/current_device</p>
			<p class="source-code">arch_sys_timer</p>
			<p>On my x86 running machine, we have the following:</p>
			<p class="source-code">jma@labcsmart:~$ cat /sys/devices/system/clockevents/clockevent0/current_device</p>
			<p class="source-code">lapic-deadline</p>
			<p class="source-code">jma@labcsmart:~$ cat /sys/devices/system/clockevents/clockevent1/current_device</p>
			<p class="source-code">lapic-deadline</p>
			<p>For the sake of readability, the choice has been made to read two entries only, and from what we have read, we can conclude the following:</p>
			<ul>
				<li>Clock event devices<a id="_idIndexMarker199"/> are backed by the same hardware timer, which is different from the hardware timer backing the clock source device.</li>
				<li>At least two hardware timers are needed to support the high-resolution timer interface, one playing the clock source role and one (ideally per-CPU) baking clock event devices. </li>
			</ul>
			<p>A clock event device can be configured to work in either one-shot mode or in periodic mode, as outlined here:</p>
			<ul>
				<li>In periodic mode, it is configured to generate a tick every 1/HZ second and does all the things the legacy (low-resolution) timer-based tick did, such as updating jiffies, accounting CPU time, and so on. In other words, in periodic mode, it is used the same way as the legacy low-resolution timer was, but it is run out of the new infrastructure.</li>
				<li>One-shot mode makes the hardware generate a tick after a specific number of cycles from the current time. It is mostly used to program the next interrupt that will wake the CPU before it goes idle.</li>
			</ul>
			<p>To track the operating mode of a clock event device, the concept of a tick device was introduced. This is further explained in the next section.</p>
			<h3>Tick devices</h3>
			<p>Tick devices <a id="_idIndexMarker200"/>are software extensions of clock event devices to provide a continuous stream of tick events that happen at regular time intervals. A tick device is automatically created by the kernel when a new clock event device is registered and always selects the best clock event device. It then goes without saying that a tick device is bound to a clock event device and that a tick device is backed by a clock event device.</p>
			<p>The definition of a tick-device data structure is shown in the following code snippet:</p>
			<p class="source-code">struct tick_device {</p>
			<p class="source-code">    struct clock_event_device *evtdev;</p>
			<p class="source-code">    enum tick_device_mode mode;</p>
			<p class="source-code">};</p>
			<p>In this data structure, <strong class="source-inline">evtdev</strong> is the clock event device that is abstracted by the tick device. <strong class="source-inline">mode</strong> is used to track the working mode of the underlying clock event. Therefore, when a tick device is said to be in periodic mode, it also means that the underlying clock event device is configured to work in this mode. The following diagram illustrates this:</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B17934_03_003.jpg" alt="Figure 3.3 – Clockevent and tick-device correlation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – Clockevent and tick-device correlation</p>
			<p>A tick device can either be global to the system or local (per-CPU tick device). Whether a tick device must be global or not is decided by the framework, by selecting one local tick device based on the features of the underlying clock event device of this tick device. The descriptions of each type of tick device is as follows:</p>
			<ul>
				<li>A per-CPU tick device<a id="_idIndexMarker201"/> is used to provide local CPU functionality <a id="_idIndexMarker202"/>such as process accounting, profiling, and—obviously—CPU local periodic tick (in periodic mode) and CPU local next event interrupt (non-periodic mode), for CPU local hrtimers management (see the <strong class="source-inline">update_process_times()</strong> function to learn how all that is handled). In the timer core code, there is a <strong class="source-inline">tick_cpu_device</strong> per-CPU variable that represents the instance of the tick device for each CPU in the system.</li>
				<li>A global tick device<a id="_idIndexMarker203"/> is responsible for providing the period ticks that <a id="_idIndexMarker204"/>mainly run the <strong class="source-inline">do_timer()</strong> and <strong class="source-inline">update_wall_time()</strong> functions. Thus, the first one updates the global <strong class="source-inline">jiffies</strong> value and updates the system load average, while the latter updates the wall time (which is stored in <strong class="source-inline">xtime</strong>, which records the time difference from January 1, 1970, to now), and runs any dynamic timers that have expired (for instance, running local process timers). In the timer core code, there is the <strong class="source-inline">tick_do_timer_cpu</strong> global variable, which holds the CPU number whose tick device has the role of the global tick device—the one that executes <strong class="source-inline">do_timer()</strong>. There is another global variable, <strong class="source-inline">tick_next_period</strong>, which keeps track of the next time the global tick device will fire.<p class="callout-heading">Note</p><p class="callout">This also means that the <strong class="source-inline">jiffies</strong> variable is always managed from one core at a time, but its function management affinity can jump from one core to the another as and when <strong class="source-inline">tick_do_timer_cpu</strong> changes.</p></li>
			</ul>
			<p>From its interrupt routine, the driver of the underlying clock event device must invoke <strong class="source-inline">evtdev-&gt;event_handler()</strong>, which is the default handler of the clock device installed by the framework. While it is transparent for the device driver, this handler is set by the framework depending on other parameters, as follows: two kernel configuration options (<strong class="source-inline">CONFIG_HIGH_RES_TIMERS</strong> and <strong class="source-inline">CONFIG_NO_HZ</strong>), the underlying hardware timer resolution, and whether the tick device is operating in dynamic mode or one-shot mode.</p>
			<p><strong class="source-inline">NO_HZ</strong> is the kernel option<a id="_idIndexMarker205"/> enabling dynamic tick support, and <strong class="source-inline">HIGH_RES_TIMERS</strong> allows the use of hrtimer APIs. With hrtimers enabled, the base code is still tick-driven, but the periodic tick interrupt is replaced by timers under hrtimers (<strong class="source-inline">softirq</strong> is called in the timer softirq context). However, whether the hrtimers will work in high-resolution mode depends on the underlying hardware timer being high-resolution or not. If not, the hrtimers will be fed by the old low-resolution tick-based timer.</p>
			<p>A tick device can operate in either one-shot mode or periodic mode. In periodic mode, the framework uses a per-CPU hrtimer via a control structure to emulate the ticks so that the base code is still tick-driven, but the periodic tick interrupt is replaced by timers under hrtimers embedded in the control structure. This control structure is a <strong class="source-inline">tick_sched</strong> struct, defined as follows:</p>
			<p class="source-code">struct tick_sched {</p>
			<p class="source-code">    struct hrtimer               sched_timer;</p>
			<p class="source-code">    enum tick_nohz_mode          nohz_mode;</p>
			<p class="source-code">[...]</p>
			<p class="source-code">};</p>
			<p>Then, a per-CPU instance of this control structure is declared, as follows:</p>
			<p class="source-code">static DEFINE_PER_CPU(struct tick_sched, tick_cpu_sched);</p>
			<p>This per-CPU instance allows a per-CPU tick emulation, which will drive the low-res timer processing via the <strong class="source-inline">sched_timer</strong> element, periodically reprogrammed to the next-low-res-timer-expires interval. This seems, however, obvious since each CPU has its own runqueue and ready processes list to manage.</p>
			<p>A <strong class="source-inline">tick_sched</strong> element can be configured by the framework to work in three different modes, described as follows:</p>
			<ul>
				<li><strong class="source-inline">NOHZ_MODE_INACTIVE</strong>: This mode means no dynamic tick and no hrtimers support. It is the state the system is in during initialization. In this mode, the local per-CPU tick-device event handler is <strong class="source-inline">tick_handle_periodic()</strong>, and entering <strong class="source-inline">idle</strong> will be interrupted by the tick timer interrupt.</li>
				<li><strong class="source-inline">NOHZ_MODE_LOWRES</strong>: This is also the <strong class="source-inline">lowres</strong> mode, which means a dynamic tick enabled in low-resolution mode. It means no high-resolution hardware timer has been found on the<a id="_idIndexMarker206"/> system to allow hrtimers to work in high-resolution mode and that they work in low-precision mode, which has the same precision as the low-precision timer (<strong class="bold">software</strong> (<strong class="bold">SW</strong>) local timer), which is based on the tick. In this mode, the local per-CPU tick-device event handler is <strong class="source-inline">tick_nohz_handler()</strong>, and entering <strong class="source-inline">idle</strong> will not be interrupted by the tick timer interrupt.</li>
				<li><strong class="source-inline">NOHZ_MODE_HIGHRES</strong>: This is also the <strong class="source-inline">highres</strong> mode. In this mode, both dynamic tick and hrtimer "high-resolution" modes are enabled. The local per-CPU tick-device event handler is <strong class="source-inline">hrtimer_interrupt()</strong>. Here, hrtimers work in high-precision mode, which has the same accuracy as the hardware timer (<strong class="bold">hardware</strong> (<strong class="bold">HW</strong>) local timer), which is much greater than the tick accuracy of the low-precision timer. In order to support the high-precision mode of hrtimers, hrtimers directly uses the one-shot mode of <strong class="source-inline">tick_device</strong>, and the conventional tick timer is converted into a sub-timer of hrtimer.</li>
			</ul>
			<p>Tick core-related source code in the kernel is in the <strong class="source-inline">kernel/time/</strong> directory, implemented in <strong class="source-inline">tick-*.c</strong> files. These are <strong class="source-inline">tick-broadcast.c</strong>, <strong class="source-inline">tick-common.c</strong>, <strong class="source-inline">tick-broadcast-hrtimer.c</strong>, <strong class="source-inline">tick-legacy.c</strong>, <strong class="source-inline">tick-oneshot.c</strong>, and <strong class="source-inline">tick-sched.c</strong>.</p>
			<h3>Broadcast tick device</h3>
			<p>On platforms implementing <a id="_idIndexMarker207"/>CPU power management, most (if not all) of the hardware timers backing clock event devices will be turned off in some <strong class="source-inline">CPUidle</strong> states. To keep the software timers functional, the kernel relies on an always-on clock event device (that is, backed by an always-on timer) to be present in the platform to relay the interrupt signaling when the timer expires. This always-on timer is called a <strong class="bold">broadcast clock device</strong>. In a <a id="_idIndexMarker208"/>nutshell, a tick broadcast is used to wake idle CPUs. It is represented by the <strong class="source-inline">broadcast</strong> directory in <strong class="source-inline">/sys/devices/system/clockevents/</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">A broadcast tick device is able to wake up any CPU by issuing an <strong class="bold">inter-processor interrupt</strong> (<strong class="bold">IPI</strong>) (discussed in <a href="B17934_13_Epub.xhtml#_idTextAnchor194"><em class="italic">Chapter 13</em></a>, <em class="italic">Demystifying the Kernel IRQ Framework</em>) to that CPU. See <strong class="source-inline">wake_up_nohz_cpu()</strong>, which is used for this purpose.</p>
			<p>To see the underlying <a id="_idIndexMarker209"/>timer backing the broadcast device, you can read the <strong class="source-inline">current_device</strong> variable in its directory.</p>
			<p>On the x86 platform, we have the following output:</p>
			<p class="source-code">jma@labcsmart:~$ cat /sys/devices/system/clockevents/broadcast/current_device</p>
			<p class="source-code">hpet</p>
			<p>The Pi 4 output is shown here:</p>
			<p class="source-code">root@raspberrypi4-64-d0:~# cat /sys/devices/system/clockevents/broadcast/current_device</p>
			<p class="source-code">bc_hrtimer</p>
			<p>Finally, the i.MX 6 broadcast device is backed by the following timer:</p>
			<p class="source-code">root@udoo-labcsmart:~# cat /sys/devices/system/clockevents/broadcast/current_device </p>
			<p class="source-code">mxc_timer1</p>
			<p>From the preceding output showing the timer backing the broadcast device, we can conclude that clock source, clock event, and broadcast device timers are all different.</p>
			<p>Whether a tick device can be used as a broadcast device or not is decided by the <strong class="source-inline">tick_install_broadcast_device()</strong> core function, invoked at each tick-device registration. This function will exclude tick devices with <strong class="source-inline">CLOCK_EVT_FEAT_C3STOP</strong> flags set (which means the underlying clock event device's timer stops in the <strong class="source-inline">C3</strong> idle state) and rely on other criteria (such as supporting one-shot mode—that is, having the <strong class="source-inline">CLOCK_EVT_FEAT_ONESHOT</strong> flag set). Finally, the <strong class="source-inline">tick_broadcast_device</strong> global variable defined in <strong class="source-inline">kernel/time/tick-broadcast.c</strong> contains the tick device that has the role of a broadcast device. When a tick device is selected as a broadcast device, its next event handler is set to <strong class="source-inline">tick_handle_periodic_broadcast()</strong>, instead of to <strong class="source-inline">tick_handle_periodic()</strong>.</p>
			<p>There are, however, platforms implementing CPU core gating that do not have an always-on hardware timer. For such platforms, the kernel provides a kernel hrtimer-based clock event device that is unconditionally registered upon boot (and can be chosen as a tick broadcast device) with the lowest possible rating value so that any broadcast-capable hardware clock event device present on the system will be chosen in preference to the tick broadcast device.</p>
			<p>This hrtimer-backed clock event device<a id="_idIndexMarker210"/> relies on a dynamically chosen CPU (such that if there is a CPU about to enter into deep sleep with its wake-up time earlier than the hrtimer expiration time, this CPU becomes the new broadcast CPU) to be always powered up. This CPU will then relay the timer interrupt to CPUs in deep-idle states through its hardware local timer device. It is implemented in <strong class="source-inline">kernel/time/tick-broadcast-hrtimer.c</strong>, registered as <strong class="source-inline">ce_broadcast_hrtimer</strong> with the <strong class="source-inline">name</strong> field set to <strong class="source-inline">bc_hrtimer</strong>. It is, for instance, as you can see, the broadcast tick device used by the Pi 4 platform.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">It goes without saying that having an always-on CPU has implications for power management platform capabilities and makes <strong class="source-inline">CPUidle</strong> suboptimal since at least a CPU is kept always in a shallow idle state by the kernel to relay timer interrupts. It is a trade-off between CPU power management and a high-resolution timer interface, which at least leaves the kernel with a functional system with some working power management capabilities.</p>
			<h3>Understanding the sched_clock function</h3>
			<p><strong class="source-inline">sched_clock()</strong> is a kernel <a id="_idIndexMarker211"/>timekeeping and timestamping function that returns the number of nanoseconds since the system started. It is weakly defined (to allow its overriding by architecture or platform code) in <strong class="source-inline">kernel/sched/clock.c</strong>, as follows:</p>
			<p class="source-code">unsigned long long __weak sched_clock(void)</p>
			<p>It is, for instance, the function that provides a timestamp to <strong class="source-inline">printk()</strong> or is invoked when using <strong class="source-inline">ktime_get_boottime()</strong> or related kernel APIs. It defaults to a jiffy-backed implementation (which could affect scheduling accuracy). If overridden, the new implementation must return a 64-bit monotonic timestamp in nanoseconds that represents the number of nanoseconds since the last reboot. Most platforms achieve this by directly reading the timer registers. On platforms that lack timers, this feature is implemented with the same timer as the one used to back the main clock source device. This is the case on Raspberry Pi, for example. When this is the case, the registers to read the main clock source device value and the registers from where the <strong class="source-inline">sched_clock()</strong> value comes are the same: see <strong class="source-inline">drivers/clocksource/bcm2835_timer.c</strong>.</p>
			<p>The timer driving <strong class="source-inline">sched_clock()</strong> has to be very fast compared with the clock source timers. As its name <a id="_idIndexMarker212"/>states, it is mainly used by the scheduler, which means it is called much more often. If you have to do trade-offs between accuracy compared to the clock source, you may sacrifice accuracy for speed in <strong class="source-inline">sched_clock()</strong>.</p>
			<p>When <strong class="source-inline">sched_clock()</strong> is not overridden directly, the kernel time core provides a <strong class="source-inline">sched_clock_register()</strong> helper to supply a platform-dependent timer reading function as well as a rating value. Anyway, this timer reading function will end up in the <strong class="source-inline">cd</strong> kernel time framework variable, which is of type <strong class="source-inline">struct clock_data</strong> (assuming that the rate of the new underlying timer is greater than the rate of the timer driving the current function).</p>
			<h3>Dynamic tick/tickless kernel</h3>
			<p>Dynamic ticks<a id="_idIndexMarker213"/> are the logical consequence of migration to a high-resolution timer interface. Before they were introduced, periodic ticks periodically issued interrupts (<strong class="source-inline">HZ</strong> times per second) to drive the operating system. This kept the system awake even when there were no tasks or timer handlers to run. With this approach, long rests were impossible for the CPUs, which had to wake for no real purpose.</p>
			<p>The dynamic tick mechanism came with a solution, allowing periodic ticks to be stopped during certain time intervals to save power. With this new approach, periodic ticks are enabled back only when some tasks need to be performed; otherwise, they are disabled.</p>
			<p>How does it work? When a CPU has no more tasks to run (that is, the idle task is scheduled on this CPU), only two events can create new work to do after the CPU goes idle: the expiration of one of the internal kernel timers, which is predictable, or the completion of an I/O operation. When a CPU enters an idle state, the timer framework examines the next scheduled timer event and, if it is later than the next periodic tick, it reprograms the per-CPU clock event device to this later event. This will allow the idle CPU to enter longer idle sleeps without being interrupted unnecessarily by a periodic tick.</p>
			<p>There are, however, systems with low power states where even the per-CPU clock event device would stop. On such platforms, it is the broadcast tick device that is programmed with the next future event.</p>
			<p>Just before a CPU <a id="_idIndexMarker214"/>enters such an idle state (the <strong class="source-inline">do_idle()</strong> function), it calls into the tick broadcast framework (<strong class="source-inline">tick_nohz_idle_enter()</strong>), and the periodic tick of its <strong class="source-inline">tick_device</strong> variable is disabled (see <strong class="source-inline">tick_nohz_idle_stop_tick()</strong>). This CPU is then added to a list of CPUs to be woken up by setting the bit corresponding to this CPU in the <strong class="source-inline">tick_broadcast_mask</strong> "broadcast map" variable, which is a bitmap that represents a list of processors that are in a sleeping mode. Then, the framework calculates the time at which this CPU must be woken up (its next event time); if this time is earlier than the time at which <strong class="source-inline">tick_broadcast_device</strong> is currently programmed, the time at which <strong class="source-inline">tick_broadcast_device</strong> should interrupt is updated to reflect the new value, and this new value is programmed into the clock event device backing the broadcast tick device. The <strong class="source-inline">tick_cpu_device</strong> variable of the CPU that is about to enter a deep idle state is now put in shutdown mode, which means that it is no longer functional.</p>
			<p>The foregoing procedures are repeated each<a id="_idIndexMarker215"/> time a CPU enters a deep idle state, and the <strong class="source-inline">tick_broadcast_device</strong> variable is programmed to fire at the earliest of the wake-up times of the CPUs in deep idle states.</p>
			<p>When the tick broadcast device next event pokes, it will look into the bitmask of sleeping CPUs, looking for the CPU(s) owning the timer(s) that might have expired and will send an IPI to any remote CPU in this bitmask that might host an expired timer.</p>
			<p>If, however, a CPU leaves the idle state upon an interrupt (the architecture code calls <strong class="source-inline">handle_IRQ()</strong>, which indirectly calls <strong class="source-inline">tick_irq_enter()</strong>), this CPU tick device is enabled (first in one-shot mode), and before it performs any task, the <strong class="source-inline">tick_nohz_irq_enter()</strong> function is called to ensure that <strong class="source-inline">jiffies</strong> are up to date so that the interrupt handler does not have to deal with a stale jiffy value, and then it resumes the periodic tick, which is kept active until the next call to <strong class="source-inline">tick_nohz_idle_stop_tick()</strong> (which is essentially called from <strong class="source-inline">do_idle()</strong>).</p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/>Using standard kernel low-precision (low-res) timers</h2>
			<p>Standard (now legacy and also referred to as low-resolution) timers are kernel timers operating on <a id="_idIndexMarker216"/>the granularity of <strong class="source-inline">jiffies</strong>. The resolution of these timers is bound to the resolution of the regular system tick, which depends on the architecture and configuration (that is, <strong class="source-inline">CONFIG_HZ</strong> or, simply, <strong class="source-inline">HZ</strong>) used by the kernel to control the scheduling and execution of a function at a certain point in the future (based on jiffies).</p>
			<h3>Jiffies and HZ</h3>
			<p>A jiffy<a id="_idIndexMarker217"/> is a kernel unit of time whose duration depends on the value of <strong class="source-inline">HZ</strong>, which represents the incrementation frequency of the <strong class="source-inline">jiffies</strong> variable in the kernel. Each increment event is called a tick. The clock source based on <strong class="source-inline">jiffies</strong> is the lowest common denominator clock source that should function on any system.</p>
			<p>Since the <strong class="source-inline">jiffies</strong> variable is incremented <strong class="source-inline">HZ</strong> times every second, if <strong class="source-inline">HZ = 1,000</strong>, then it is incremented 1,000 times per second (that is, one tick every 1/1,000 seconds, or 1 millisecond). On most <strong class="bold">Advanced RISC Machines</strong> (<strong class="bold">ARM</strong>) kernel configurations, <strong class="source-inline">HZ</strong> defaults <a id="_idIndexMarker218"/>to <strong class="source-inline">100</strong>, while it defaults to <strong class="source-inline">250</strong> on x86, which would result in a resolution of 10 milliseconds or 4 milliseconds.</p>
			<p>Here are different <strong class="source-inline">HZ</strong> values on two running systems:</p>
			<p class="source-code">jma@labcsmart:~$ grep ‘CONFIG_HZ=' /boot/config-$(uname -r)</p>
			<p class="source-code">CONFIG_HZ=250</p>
			<p class="source-code">jma@labcsmart:~$</p>
			<p>The preceding code has been executed on a running x86 machine. On an ARM running machine, we have the following:</p>
			<p class="source-code">root@udoo-labcsmart:~# zcat /proc/config.gz |grep CONFIG_HZ</p>
			<p class="source-code">CONFIG_HZ_100=y</p>
			<p class="source-code">root@udoo-labcsmart:~#</p>
			<p>The preceding code says the current <strong class="source-inline">HZ</strong> value is <strong class="source-inline">100</strong>.</p>
			<h3>Kernel timer APIs</h3>
			<p>A timer<a id="_idIndexMarker219"/> is <a id="_idIndexMarker220"/>represented in the kernel as an instance of <strong class="source-inline">struct timer_list</strong>, defined as follows:</p>
			<p class="source-code">struct timer_list {</p>
			<p class="source-code">    struct hlist_node entry;</p>
			<p class="source-code">    unsigned long expires;</p>
			<p class="source-code">    void (*function)(struct timer_list *);</p>
			<p class="source-code">    u32 flags;</p>
			<p class="source-code">);</p>
			<p>In the preceding data structure, <strong class="source-inline">expires</strong> is an absolute value in jiffies that defines when this timer will expire in the future. <strong class="source-inline">entry</strong> is internally used by the kernel to track this timer in a per-CPU global list of timers. <strong class="source-inline">flags</strong> are OR'ed bitmasks that represent the timer flags, such as the way the timer is managed and the CPU on which the callback will be scheduled, and <strong class="source-inline">function</strong> is the callback to be executed when this timer expires.</p>
			<p>You can dynamically define a timer using <strong class="source-inline">timer_setup()</strong> or statically create one with <strong class="source-inline">DEFINE_TIMER()</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In Linux kernel versions prior to 4.15, <strong class="source-inline">setup_timer()</strong> was used as the dynamic variant.</p>
			<p>Here are the definitions of both macros:</p>
			<p class="source-code">void timer_setup( struct timer_list *timer,        \</p>
			<p class="source-code">           void (*function)( struct timer_list *), \</p>
			<p class="source-code">           unsigned int flags);</p>
			<p class="source-code">#define DEFINE_TIMER(_name, _function) [...]</p>
			<p>After the timer has been initialized, you must set its expiration delay before starting it using one of the following APIs:</p>
			<p class="source-code">int mod_timer(struct timer_list *timer,</p>
			<p class="source-code">               unsigned long expires);</p>
			<p class="source-code">void add_timer(struct timer_list *timer)</p>
			<p>The <strong class="source-inline">mod_timer()</strong> function<a id="_idIndexMarker221"/> is used to either set an initial expiration delay or to update its value on an active timer, which means that calling this function on an inactive timer will activate this timer.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Activating a timer here means arming and queueing this timer. That said, when a timer is just armed, queued, and counting down, waiting for its expiration before running the callback function, it is said to be pending.</p>
			<p>You should prefer this function over <strong class="source-inline">add_timer()</strong>, which is another function to start inactive timers exclusively. Before calling <strong class="source-inline">add_timer()</strong>, you must have set the timer expiration delay and the callback as follows:</p>
			<p class="source-code">my_timer.expires = jiffies + ((12 * HZ) / 10); /* 1.2s */</p>
			<p class="source-code">add_timer(&amp;my_timer);</p>
			<p>The value <strong class="source-inline">mod_timer()</strong> returns depends on the state of the timer prior to it being invoked. Calling <strong class="source-inline">mod_timer()</strong> on an inactive timer returns <strong class="source-inline">0</strong> on success, while it returns <strong class="source-inline">1</strong> when successfully invoked on a pending timer or a timer whose callback function is being executed. This means it is totally safe to execute <strong class="source-inline">mod_timer()</strong> from the timer callback. When invoked on an active timer, it is equivalent to <strong class="source-inline">del_timer(timer); timer-&gt;expires = expires; add_timer(timer);</strong>.</p>
			<p>Once done with the timer, it can be released or cancelled using one of the following functions: </p>
			<p class="source-code">int del_timer(struct timer_list *timer);</p>
			<p class="source-code">int del_timer_sync(struct timer_list *timer);</p>
			<p><strong class="source-inline">del_timer()</strong> removes (dequeues) the <strong class="source-inline">timer</strong> object from the timer management queue. On success, it returns a different value depending on whether it is invoked on an inactive timer or on an active timer. In the first case, it returns <strong class="source-inline">0</strong>, while it returns <strong class="source-inline">1</strong> in the latter case, even if the function callback of this timer is currently being executed. </p>
			<p>Let's consider the following execution flow, where a timer is being deleted on a CPU while its callback is being executed on another CPU:</p>
			<p class="source-code">mainline (CPUx)                  handler(CPUy)</p>
			<p class="source-code">==============                   =============</p>
			<p class="source-code">                                 enter xxx_timer()</p>
			<p class="source-code">del_timer()</p>
			<p class="source-code">kfree(some_resource)</p>
			<p class="source-code">                                 access(some_resource)</p>
			<p>In the preceding code <a id="_idIndexMarker222"/>snippet, using <strong class="source-inline">del_timer()</strong> does not guarantee that the callback is not running anymore. Here is another example:</p>
			<p class="source-code">mainline (CPUx)            handler(CPUy)</p>
			<p class="source-code">==============             =============</p>
			<p class="source-code">                           enter xxx_timer()</p>
			<p class="source-code"> del_timer()</p>
			<p class="source-code"> kfree(timer)</p>
			<p class="source-code">                           mod_timer(timer)</p>
			<p>When <strong class="source-inline">del_timer()</strong> returns, it only guarantees that the timer is deactivated and unqueued, ensuring that it will not be executed in the future. However, on a multiprocessing machine, the timer function might already be executing on another processor. <strong class="source-inline">del_timer_sync()</strong> should be used in such cases, which will deactivate the timer and wait for any executing handler to exit before returning. This function will check each processor to make sure that the given timer is not currently running there. By using <strong class="source-inline">del_timer_sync()</strong> in the preceding race condition examples, <strong class="source-inline">kfree()</strong> could be invoked without worrying about the resource being used in the callback or not. You should almost always use <strong class="source-inline">del_timer_sync()</strong> instead of <strong class="source-inline">del_timer()</strong>. The driver must not hold a lock preventing the handler's completion; otherwise, it will result in a deadlock. This makes the <strong class="source-inline">del_timer()</strong> context agnostic as it is asynchronous, while <strong class="source-inline">del_timer_sync()</strong> is to be used in a non-atomic context exclusively.</p>
			<p>Moreover, for sanity purposes, we can independently check whether the timer is pending or not using the following API:</p>
			<p class="source-code">int timer_pending(const struct timer_list *timer);</p>
			<p>This function checks whether this timer is armed and pending.</p>
			<p>The following code snippet <a id="_idIndexMarker223"/>shows a basic usage of standard kernel timers:</p>
			<p class="source-code">#include &lt;linux/init.h&gt;</p>
			<p class="source-code">#include &lt;linux/kernel.h&gt;</p>
			<p class="source-code">#include &lt;linux/module.h&gt;</p>
			<p class="source-code">#include &lt;linux/timer.h&gt;</p>
			<p class="source-code">static struct timer_list my_timer;</p>
			<p class="source-code">void my_timer_callback(struct timer_list *t)</p>
			<p class="source-code">{</p>
			<p class="source-code">    pr_info("Timer callback&amp;; called\n");</p>
			<p class="source-code">}</p>
			<p class="source-code"> </p>
			<p class="source-code">static int __init my_init(void)</p>
			<p class="source-code">{</p>
			<p class="source-code">    int retval;</p>
			<p class="source-code">    pr_info("Timer module loaded\n");</p>
			<p class="source-code">    timer_setup(&amp;my_timer, my_timer_callback, 0);</p>
			<p class="source-code">    pr_info("Setup timer to fire in 500ms (%ld)\n",</p>
			<p class="source-code">              jiffies);</p>
			<p class="source-code">    retval = mod_timer(&amp;my_timer,</p>
			<p class="source-code">                        jiffies + msecs_to_jiffies(500));</p>
			<p class="source-code">    if (retval)</p>
			<p class="source-code">        pr_info("Timer firing failed\n");</p>
			<p class="source-code"> </p>
			<p class="source-code">    return 0;</p>
			<p class="source-code">}</p>
			<p class="source-code"> </p>
			<p class="source-code">static void my_exit(void)</p>
			<p class="source-code">{</p>
			<p class="source-code">    int retval;</p>
			<p class="source-code">    retval = del_timer(&amp;my_timer);</p>
			<p class="source-code">    /* Is timer still active (1) or no (0) */</p>
			<p class="source-code">    if (retval)</p>
			<p class="source-code">        pr_info("The timer is still in use...\n");</p>
			<p class="source-code">    pr_info("Timer module unloaded\n");</p>
			<p class="source-code">}</p>
			<p class="source-code">module_init(my_init);</p>
			<p class="source-code">module_exit(my_exit);</p>
			<p class="source-code">MODULE_AUTHOR("John Madieu &lt;john.madieu@gmail.com&gt;");</p>
			<p class="source-code">MODULE_DESCRIPTION("Standard timer example");</p>
			<p class="source-code">MODULE_LICENSE("GPL");</p>
			<p>In the preceding example, we demonstrated a basic usage of standard timers. We request 500 milliseconds of timeout. That said, the unit of time of this kind of timer is a jiffy. So, in order to pass a timeout value in a human format (seconds or milliseconds), you must use conversion helpers. You can see some here:</p>
			<p class="source-code">unsigned long msecs_to_jiffies(const unsigned int m)</p>
			<p class="source-code">unsigned long usecs_to_jiffies(const unsigned int u)</p>
			<p class="source-code">unsigned long timespec64_to_jiffies(</p>
			<p class="source-code">                  const struct timespec64 *value);</p>
			<p>With the preceding helper <a id="_idIndexMarker224"/>functions, you should not expect any accuracy better than a jiffy. For example, using <strong class="source-inline">usecs_to_jiffies(100)</strong> will return a jiffy. The returned value is rounded up to the closest jiffy value.</p>
			<p>In order to pass additional arguments to the timer callback, the preferred way is to embed them as elements into a structure together with the timer and use the <strong class="source-inline">from_timer()</strong> macro on the element to retrieve the bigger structure, from which you can access each element. This macro is defined as follows:</p>
			<p class="source-code">#define from_timer(var, callback_timer, timer_fieldname) \</p>
			<p class="source-code">    container_of(callback_timer, typeof(*var), timer_fieldname) </p>
			<p>As an example, let's consider we need to pass two elements to the timer callback: the first one is of type <strong class="source-inline">struct sometype</strong> and the second one is an integer. In order to pass arguments, we define an additional structure, as follows:</p>
			<p class="source-code">struct fake_data {</p>
			<p class="source-code">    struct timer_list timer;</p>
			<p class="source-code">    struct sometype foo;</p>
			<p class="source-code">    int bar;</p>
			<p class="source-code">};</p>
			<p>After this, we pass the embedded timer to the setup function, as follows:</p>
			<p class="source-code">struct fake_data *fd = alloc_init_fake_data();</p>
			<p class="source-code">timer_setup(&amp;fd-&gt;timer, timer_callback, 0);</p>
			<p>Later in the callback, you must use the <strong class="source-inline">from_timer</strong> variable to retrieve the bigger structure from which you can access arguments. Here is an example of this in use:</p>
			<p class="source-code">void timer_callback(struct timer_list *t)</p>
			<p class="source-code">{</p>
			<p class="source-code">    struct fake_data *fd = from_timer(fd, t, timer);</p>
			<p class="source-code">    sometype data = fd-&gt;data;</p>
			<p class="source-code">    int var = fd-&gt;bar;</p>
			<p class="source-code">[...]</p>
			<p class="source-code">}</p>
			<p>In the preceding code <a id="_idIndexMarker225"/>snippet, we described how to pass data to the timer callback and how to get this data, using a container structure. By default, a pointer to <strong class="source-inline">timer_list</strong> is passed to the callback function, instead of the <strong class="source-inline">unsigned long</strong> data type in versions prior to 4.15.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>High-resolution timers (hrtimers) </h2>
			<p>While the legacy<a id="_idIndexMarker226"/> timer implementation is bound to ticks, high-precision timers provide us with nanosecond-level and tick-agnostic timing accuracy to meet the urgent need for precise time applications or kernel drivers. This has been introduced in kernel v2.6.16 and can be enabled in the build using the <strong class="source-inline">CONFIG_HIGH_RES_TIMERS</strong> option in the kernel configuration.</p>
			<p>While the standard timer interface keeps/represents time values in <strong class="source-inline">jiffies</strong>, the high-resolution timer interface came with a new data type, allowing us to keep the time value: <strong class="source-inline">ktime_t</strong>, which is a simple 64-bit scalar.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Prior to kernel 3.17, the <strong class="source-inline">ktime_t</strong> type was represented differently on 32- or 64-bit machines. On 64-bit CPUs, it was represented as a plain 64-bit nanosecond value as it is nowadays all over the kernel, while it was represented as a two-32-bit-fields data structure ([<strong class="source-inline">seconds – nanoseconds</strong>] pair) on 32-bit CPUs.</p>
			<p>Hrtimer APIs require a <strong class="source-inline">#include &lt;linux/hrtimer.h&gt;</strong> header. That said, in the header file, the structure that characterizes a high-resolution timer is defined as follows:</p>
			<p class="source-code">struct hrtimer {</p>
			<p class="source-code">    ktime_t                 _softexpires;</p>
			<p class="source-code">    enum hrtimer_restart    (*function)(struct hrtimer *);</p>
			<p class="source-code">    struct hrtimer_clock_base    *base;</p>
			<p class="source-code">    u8                     state;</p>
			<p class="source-code">[...]</p>
			<p class="source-code">};</p>
			<p>The elements in the data structure have been shortened to the strict minimum to cover the needs of the book. For the rest, we'll now look at their meaning.</p>
			<p>Before using the<a id="_idIndexMarker227"/> hrtimer, it must be initialized with <strong class="source-inline">hrtimer_init()</strong>, defined as follows:</p>
			<p class="source-code">void hrtimer_init(struct hrtimer *timer,</p>
			<p class="source-code">                  clockid_t which_clock,</p>
			<p class="source-code">                  enum hrtimer_mode mode);</p>
			<p>In the preceding function, <strong class="source-inline">timer</strong> is a pointer to the hrtimer to initialize. <strong class="source-inline">clock_id</strong> tells which type of clock must be used to feed this hrtimer. The following are common options:</p>
			<ul>
				<li><strong class="source-inline">CLOCK_REALTIME</strong>: This selects the real-time time—that is, the wall time. If the system time changes, it can affect this timer.</li>
				<li><strong class="source-inline">CLOCK_MONOTONIC</strong>: This is an incremental time, not affected by system changes. However, it stops incrementing when the system goes to sleep or suspends.</li>
				<li><strong class="source-inline">CLOCK_BOOTTIME</strong>: The running time of the system. Similar to <strong class="source-inline">CLOCK_MONOTONIC</strong>, the difference is that it includes sleep time. When suspended, it will still increase.</li>
			</ul>
			<p>In the preceding code snippet, the <strong class="source-inline">mode</strong> parameter tells how the hrtimer should be working. Here are some possible options:</p>
			<ul>
				<li> <strong class="source-inline">HRTIMER_MODE_ABS</strong>: This means that this timer expires after an absolute specified time.</li>
				<li><strong class="source-inline">HRTIMER_MODE_REL</strong>: This timer expires after a specified relative from now.</li>
				<li><strong class="source-inline">HRTIMER_MODE_PINNED</strong>: This hrtimer is bound to a CPU. This is only considered when starting the hrtimer so that the hrtimer fires and executes the callback on the same CPU on which it is queued.</li>
				<li><strong class="source-inline">HRTIMER_MODE_ABS_PINNED</strong>: This is a combination of the first and the third flags.</li>
				<li><strong class="source-inline">HRTIMER_MODE_REL_PINNED</strong>: This is a combination of the second and third flags.</li>
			</ul>
			<p>After the hrtimer<a id="_idIndexMarker228"/> has been initialized, it must be assigned a callback function that will be executed upon the timer expiration. The following code snippet shows the expected prototype:</p>
			<p class="source-code">enum hrtimer_restart callback(struct hrtimer *h);</p>
			<p><strong class="source-inline">hrtimer_restart</strong> is the type to be returned by the callback. It must be either <strong class="source-inline">HRTIMER_NORESTART</strong> to indicate that the timer must not be restarted (used to perform a one-shot operation) or <strong class="source-inline">HRTIMER_RESTART</strong> to indicate that the timer must be restarted (to simulate periodical mode). In the first case, when returning <strong class="source-inline">HRTIMER_NORESTART</strong>, the driver will have to explicitly restart the timer (using <strong class="source-inline">hrtimer_start()</strong>, for example) if need be. When returning <strong class="source-inline">HRTIMER_RESTART</strong>, the timer restart is implicit as it will be handled by the kernel. However, the driver needs to reset the timeout before returning from the callback. In order to do so, the driver can use <strong class="source-inline">hrtimer_forward()</strong>, defined as follows:</p>
			<p class="source-code">u64 hrtimer_forward(struct hrtimer *timer,</p>
			<p class="source-code">                    ktime_t now, ktime_t interval)</p>
			<p>In the preceding code snippet, <strong class="source-inline">timer</strong> is the hrtimer to forward, <strong class="source-inline">now</strong> is the point from where the timer must be forwarded, and <strong class="source-inline">interval</strong> is how long in the future the timer must be forwarded. Do, however, note that this only updates the timer expiry value and does not requeue the timer.</p>
			<p>The <strong class="source-inline">now</strong> parameter can be obtained in different ways, either by using <strong class="source-inline">ktime_get()</strong>, which would return the current monotonic clock time or with <strong class="source-inline">hrtimer_get_expires()</strong>, which would return the time when the timer is supposed to expire before forwarding. This is illustrated in the following code snippet:</p>
			<p class="source-code">hrtimer_forward(hrtimer, ktime_get(), ms_to_ktime(500));</p>
			<p class="source-code">/* or */</p>
			<p class="source-code">hrtimer_forward(handle, hrtimer_get_expires(handle),</p>
			<p class="source-code">                 ns_to_ktime(450));</p>
			<p>In the first line of the<a id="_idIndexMarker229"/> preceding example, the hrtimer is forwarded 500 milliseconds from the current time, while in the second line, it is forwarded 450 nanoseconds from the time when it was supposed to expire. The first line in the example is equivalent to <strong class="source-inline">hrtimer_forward_now()</strong>, which forwards the hrtimer to a specified time from the current time (from now). It is declared as follows:</p>
			<p class="source-code">u64 hrtimer_forward_now(struct hrtimer *timer,</p>
			<p class="source-code">                          ktime_t interval)</p>
			<p>Now that the timer has been set up and its callback defined, it can be armed (started) using <strong class="source-inline">hrtimer_start()</strong>, which has the following prototype:</p>
			<p class="source-code">int hrtimer_start(struct hrtimer *timer, ktime_t time,</p>
			<p class="source-code">                    const enum hrtimer_mode mode);</p>
			<p>In the preceding code snippet, <strong class="source-inline">mode</strong> represents the timer expiry mode, and it should be either <strong class="source-inline">HRTIMER_MODE_ABS</strong> for an absolute time value or <strong class="source-inline">HRTIMER_MODE_REL</strong> for a time value relative to now. This parameter must be consistent with the initialization mode parameter. The <strong class="source-inline">timer</strong> parameter is a pointer to the initialized hrtimer. Finally, <strong class="source-inline">time</strong> is the expiry time of the hrtimer. Since it is of type <strong class="source-inline">ktime_t</strong>, various helper functions allow us to generate a <strong class="source-inline">ktime_t</strong> element from various input time units. These are shown here:</p>
			<p class="source-code">ktime_t ktime_set(const s64 secs,</p>
			<p class="source-code">                  const unsigned long nsecs);</p>
			<p class="source-code">ktime_t ns_to_ktime(u64 ns);</p>
			<p class="source-code">ktime_t ms_to_ktime(u64 ms);</p>
			<p>In the preceding list, <strong class="source-inline">ktime_set()</strong> generates a <strong class="source-inline">ktime_t</strong> element from a given number of seconds and nanoseconds. <strong class="source-inline">ns_to_ktime()</strong> or <strong class="source-inline">ms_to_ktime()</strong> generate a <strong class="source-inline">ktime_t</strong> element from a given number of nanoseconds or milliseconds, respectively.</p>
			<p>You may also be interested in returning the number of nano-/microseconds, given a <strong class="source-inline">ktime_t</strong> input element using the following functions: </p>
			<p class="source-code">s64 ktime_to_ns(const ktime_t kt)</p>
			<p class="source-code">s64 ktime_to_us(const ktime_t kt)</p>
			<p>Moreover, given one or two <strong class="source-inline">ktime_t</strong> elements, you can perform some arithmetical operations using the following helpers:</p>
			<p class="source-code">ktime_t ktime_sub(const ktime_t lhs, const ktime_t rhs);</p>
			<p class="source-code">ktime_t ktime_sub(const ktime_t lhs, const ktime_t rhs);</p>
			<p class="source-code">ktime_t ktime_add(const ktime_t add1, const ktime_t add2);</p>
			<p class="source-code">ktime_t ktime_add_ns(const ktime_t kt, u64 nsec);</p>
			<p>To subtract or<a id="_idIndexMarker230"/> add <strong class="source-inline">ktime</strong> objects, you can use <strong class="source-inline">ktime_sub()</strong> and <strong class="source-inline">ktime_add()</strong>, respectively. <strong class="source-inline">ktime_add_ns()</strong> increments a <strong class="source-inline">ktime_t</strong> element by a specified number of nanoseconds. <strong class="source-inline">ktime_add_us()</strong> is another variant for microseconds. For subtraction, <strong class="source-inline">ktime_sub_ns()</strong> and <strong class="source-inline">ktime_sub_us()</strong> can be used.</p>
			<p>After calling <strong class="source-inline">hrtimer_start()</strong>, the hrtimer will be armed (activated) and enqueued in a (time-ordered) per-CPU bucket, waiting for its expiration. This bucket will be local to the CPU on which <strong class="source-inline">hrtimer_start()</strong> has been invoked, but it is not guaranteed that the callback will run on this CPU (migration might happen). For a CPU-bound hrtimer, you should use a <strong class="source-inline">*_PINNED</strong> mode variant when you initialize the hrtimer.</p>
			<p>An enqueued hrtimer is always started. Once the timer expires, its callback is invoked, and depending on the return value, the hrtimer can be requeued or not. In order to cancel a timer, drivers can use <strong class="source-inline">hrtimer_cancel()</strong> or <strong class="source-inline">hrtimer_try_to_cancel()</strong>, declared as follows:</p>
			<p class="source-code">int hrtimer_cancel(struct hrtimer *timer);</p>
			<p class="source-code">int hrtimer_try_to_cancel(struct hrtimer *timer);</p>
			<p>Both functions return <strong class="source-inline">0</strong> when the timer is not active during the call. <strong class="source-inline">hrtimer_try_to_cancel()</strong> will return <strong class="source-inline">1</strong> if the timer is active (running but not executing the callback function) and has been successfully canceled or will fail, returning <strong class="source-inline">-1</strong> if the callback function is being executed. On the other hand, <strong class="source-inline">hrtimer_cancel()</strong> will cancel the timer if the callback function is not running yet or will wait for it to finish if it is being executed. When <strong class="source-inline">hrtimer_cancel()</strong> returns, the caller can be guaranteed that the timer is no <a id="_idIndexMarker231"/>longer active and that its expiration function is not running.</p>
			<p>Drivers can, however, independently check whether the hrtimer callback is still running with the following code:</p>
			<p class="source-code">int hrtimer_callback_running(struct hrtimer *timer);</p>
			<p>For instance, <strong class="source-inline">hrtimer_try_to_cancel()</strong> internally calls <strong class="source-inline">hrtimer_callback_running()</strong> and returns <strong class="source-inline">-1</strong> if the callback is running.</p>
			<p>Let's write a module example to put our hrtimer knowledge into practice. We first start by writing the callback function, as follows:</p>
			<p class="source-code">#include &lt;linux/module.h&gt;</p>
			<p class="source-code">#include &lt;linux/kernel.h&gt;</p>
			<p class="source-code">#include &lt;linux/hrtimer.h&gt;</p>
			<p class="source-code">#include &lt;linux/ktime.h&gt;</p>
			<p class="source-code">static struct hrtimer hr_timer;</p>
			<p class="source-code">static enum hrtimer_restart timer_callback(struct hrtimer *timer)</p>
			<p class="source-code">{</p>
			<p class="source-code">    pr_info("Hello from timer!\n");</p>
			<p class="source-code">#ifdef PERIODIC_MS_500</p>
			<p class="source-code">    hrtimer_forward_now(timer, ms_to_ktime(500));</p>
			<p class="source-code">    return HRTIMER_RESTART;</p>
			<p class="source-code">#else</p>
			<p class="source-code">    return HRTIMER_NORESTART;</p>
			<p class="source-code">#endif</p>
			<p class="source-code">}</p>
			<p>In the preceding hrtimer callback function, we can decide to run in one-shot mode or periodic mode. For periodic mode, the user must define <strong class="source-inline">PERIODIC_MS_500</strong>, in which case the timer will be forwarded 500 milliseconds in the future from the current hrtimer clock base time before being requeued.</p>
			<p>Then, the rest of the <a id="_idIndexMarker232"/>module implementation looks like this:</p>
			<p class="source-code">static int __init hrtimer_module_init(void)</p>
			<p class="source-code">{;</p>
			<p class="source-code">    ktime_t init_time;</p>
			<p class="source-code">    init_time = ktime_set(1, 1000);</p>
			<p class="source-code">    hrtimer_init(&amp;hr_timer, CLOCK_MONOTONIC,</p>
			<p class="source-code">                   HRTIMER_MODE_REL);</p>
			<p class="source-code">    hr_timer.function = &amp;timer_callback;</p>
			<p class="source-code">    hrtimer_start(&amp;hr_timer, init_time, HRTIMER_MODE_REL);</p>
			<p class="source-code">    return 0;</p>
			<p class="source-code">}</p>
			<p class="source-code">static void __exit hrtimer_module_exit(void) {</p>
			<p class="source-code">    int ret;</p>
			<p class="source-code">    ret = hrtimer_cancel(&amp;hr_timer);</p>
			<p class="source-code">    if (ret)</p>
			<p class="source-code">        pr_info("Our timer is still in use...\n");</p>
			<p class="source-code">     pr_info("Uninstalling hrtimer module\n");</p>
			<p class="source-code">}</p>
			<p class="source-code">module_init(hrtimer_module_init);</p>
			<p class="source-code">module_exit(hrtimer_module_exit);</p>
			<p>In the preceding implementation, we generated an initial <strong class="source-inline">ktime_t</strong> element of 1 second and 1,000 nanoseconds—that is, 1 second and 1 millisecond, which is used as initial expiration duration. When the <a id="_idIndexMarker233"/>hrtimer expires for the first time, our callback is invoked. If <strong class="source-inline">PERIODIC_MS_500</strong> is defined, the hrtimer will be forwarded to 500 milliseconds later, and the callback will be periodically invoked (every 500 milliseconds) after the initial invocation; otherwise, it is a one-shot invocation.</p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor052"/>Implementing work-deferring mechanisms</h1>
			<p>Deferring<a id="_idIndexMarker234"/> is a method by which <a id="_idIndexMarker235"/>you schedule a piece of work to be executed in the future. It's a way to report an action later. Obviously, the kernel provides facilities to implement such a mechanism; it allows you to defer functions, whatever their type, to be called and executed later. There are three of them in the kernel, as outlined here:</p>
			<ul>
				<li><strong class="bold">Softirqs</strong>: Executed<a id="_idIndexMarker236"/> in an atomic context</li>
				<li><strong class="bold">Tasklets</strong>: Executed <a id="_idIndexMarker237"/>in an atomic context</li>
				<li><strong class="bold">Workqueues</strong>: Executed <a id="_idIndexMarker238"/>in a process context</li>
			</ul>
			<p>In the next three sections, we will learn in detail the implementation of each of them.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor053"/>Softirqs</h2>
			<p>As the name <a id="_idIndexMarker239"/>suggests, <strong class="bold">softirq</strong> stands for <strong class="bold">software interrupt</strong>. Such a handler can preempt all other tasks on the system but the hardware IRQ handlers since it is executed with IRQs enabled. Softirqs are intended to be used for high-frequency threaded job scheduling. Network and block devices are the only two subsystems in the kernel that make direct use of softirqs. Even though softirq handlers run with interrupts enabled, they cannot sleep, and any shared data needs proper locking. The softirq APIs are implemented in <strong class="source-inline">kernel/softirq.c</strong> in the kernel source tree, and drivers that wish to use this API need to include <strong class="source-inline">&lt;linux/interrupt.h&gt;</strong>.</p>
			<p>Softirqs are represented by <strong class="source-inline">struct softirq_action</strong> structures and are defined as follows:</p>
			<p class="source-code">struct softirq_action {</p>
			<p class="source-code">    void (*action)(struct softirq_action *);</p>
			<p class="source-code">};</p>
			<p>This structure embeds a pointer to the function to run when the softirq is raised. Thus, the prototype of your softirq handler should look like this:</p>
			<p class="source-code">void softirq_handler(struct softirq_action *h)</p>
			<p>Running a<a id="_idIndexMarker240"/> softirq handler results in executing this action function, which has only one parameter: a pointer to the corresponding <strong class="source-inline">softirq_action</strong> structure. You can register the softirq handler at runtime by means of the <strong class="source-inline">open_softirq()</strong> function, as illustrated here:</p>
			<p class="source-code">void open_softirq(int nr,</p>
			<p class="source-code">                  void (*action)(struct softirq_action *))</p>
			<p><strong class="source-inline">nr</strong> represents the softirq index, which is also considered as the softirq priority (where 0 is the highest). <strong class="source-inline">action</strong> is a pointer to the softirq handler. Possible indexes are enumerated in the following code snippet: </p>
			<p class="source-code">enum</p>
			<p class="source-code">{</p>
			<p class="source-code">    HI_SOFTIRQ=0,   /* High-priority tasklets */</p>
			<p class="source-code">    TIMER_SOFTIRQ,  /* Timers */</p>
			<p class="source-code">    NET_TX_SOFTIRQ, /* Send network packets */</p>
			<p class="source-code">    NET_RX_SOFTIRQ, /* Receive network packets */</p>
			<p class="source-code">    BLOCK_SOFTIRQ,  /* Block devices */</p>
			<p class="source-code">    BLOCK_IOPOLL_SOFTIRQ, /* Block devices with I/O polling</p>
			<p class="source-code">                           * blocked on other CPUs */</p>
			<p class="source-code">    TASKLET_SOFTIRQ,/* Normal Priority tasklets */</p>
			<p class="source-code">    SCHED_SOFTIRQ,  /* Scheduler */</p>
			<p class="source-code">    HRTIMER_SOFTIRQ,/* High-resolution timers */</p>
			<p class="source-code">    RCU_SOFTIRQ,    /* RCU locking */</p>
			<p class="source-code">    NR_SOFTIRQS     /* This only represent the number</p>
			<p class="source-code">                     * of softirqs type, 10 actually */</p>
			<p class="source-code">};</p>
			<p>Softirqs <a id="_idIndexMarker241"/>with lower indexes (highest priority) run before those with higher indexes (lowest priority). The name of all the available softirqs in the kernel are listed in the following array:</p>
			<p class="source-code">const char * const softirq_to_name[NR_SOFTIRQS] = {</p>
			<p class="source-code">    "HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "BLOCK_IOPOLL", </p>
			<p class="source-code">    "TASKLET", "SCHED", "HRTIMER", "RCU"</p>
			<p class="source-code">};</p>
			<p>It's easy to check some in the output of the <strong class="source-inline">/proc/softirqs</strong> virtual file, as follows:</p>
			<p class="source-code">root@udoo-labcsmart:~# cat /proc/softirqs</p>
			<p class="source-code">                    CPU0       CPU1       </p>
			<p class="source-code">          HI:       3535          1</p>
			<p class="source-code">       TIMER:    4211589    4748893</p>
			<p class="source-code">      NET_TX:    1277827         39</p>
			<p class="source-code">      NET_RX:    1665450          0</p>
			<p class="source-code">       BLOCK:       1978        201</p>
			<p class="source-code">    IRQ_POLL:          0          0</p>
			<p class="source-code">     TASKLET:     455761         33</p>
			<p class="source-code">       SCHED:    4212802    4750408</p>
			<p class="source-code">     HRTIMER:          3          0</p>
			<p class="source-code">         RCU:     438826     286874</p>
			<p class="source-code">root@udoo-labcsmart:~#</p>
			<p>A <strong class="source-inline">NR_SOFTIRQS</strong>-entry array of <strong class="source-inline">struct softirq_action</strong> is declared in <strong class="source-inline">kernel/softirq.c</strong>, as follows:</p>
			<p class="source-code">static struct softirq_action softirq_vec[NR_SOFTIRQS] ;</p>
			<p>Each entry in this array may contain one—and only one—softirq. Consequently, there can be a maximum of <strong class="source-inline">NR_SOFTIRQS</strong> (actually, 10 in v5.10, the last stable version at the time of writing) registered<a id="_idIndexMarker242"/> softirqs. The following code snippet from <strong class="source-inline">kernel/softirq.c</strong> illustrates this:</p>
			<p class="source-code">void open_softirq(int nr,</p>
			<p class="source-code">                  void (*action)(struct softirq_action *))</p>
			<p class="source-code">{</p>
			<p class="source-code">    softirq_vec[nr].action = action;</p>
			<p class="source-code">}</p>
			<p>A concrete example is the network subsystem, which registers the softirqs it needs (in <strong class="source-inline">net/core/dev.c</strong>), as follows:</p>
			<p class="source-code">open_softirq(NET_TX_SOFTIRQ, net_tx_action);</p>
			<p class="source-code">open_softirq(NET_RX_SOFTIRQ, net_rx_action);</p>
			<p>Before a registered softirq can execute, it should be activated/scheduled. In order to do so, you have to call <strong class="source-inline">raise_softirq()</strong> or <strong class="source-inline">raise_softirq_irqoff()</strong> (if interrupts are already off), as illustrated in the following code snippet:</p>
			<p class="source-code">void __raise_softirq_irqoff(unsigned int nr)</p>
			<p class="source-code">void raise_softirq_irqoff(unsigned int nr)</p>
			<p class="source-code">void raise_softirq(unsigned int nr)</p>
			<p>The first function simply sets the appropriate bit in the per-CPU softirq bitmap (the <strong class="source-inline">__softirq_pending</strong> field in the <strong class="source-inline">struct irq_cpustat_t</strong> data structure allocated per CPU in <strong class="source-inline">kernel/softirq.c</strong>), as follows:</p>
			<p class="source-code">irq_cpustat_t irq_stat[NR_CPUS] ____cacheline_aligned;</p>
			<p class="source-code">EXPORT_SYMBOL(irq_stat);</p>
			<p>When the flag is checked, this allows it to run. This function is described here for study purposes and should not be used directly.</p>
			<p><strong class="source-inline">raise_softirq_irqoff</strong> needs to be called with interrupts disabled. First, it internally calls <strong class="source-inline">__raise_softirq_irqoff()</strong>, described previously, to activate the softirq. Afterward, it <a id="_idIndexMarker243"/>checks whether it has been called from within an interrupt (either hardirq or softirq) context by mean of an <strong class="source-inline">in_interrupt()</strong> macro (which simply returns the value of <strong class="source-inline">current_thread_info( )-&gt;preempt_count</strong>, where 0 means preemption-enabled, stating that we are not in an interrupt context, and a &gt; 0 value means we are in an interrupt context). If <strong class="source-inline">in_interrupt() &gt; 0</strong>, this does nothing when we are in an interrupt context because softirq flags are checked on the exit path of any I/O IRQ handler (see <strong class="source-inline">asm_do_IRQ()</strong> for ARM or <strong class="source-inline">do_IRQ()</strong> for x86 platforms, which makes a call to <strong class="source-inline">irq_exit()</strong>). Here, softirqs run in an interrupt context. However, if <strong class="source-inline">in_interrupt() == 0</strong>, it invokes <strong class="source-inline">wakeup_softirqd()</strong>, responsible for waking the local CPU <strong class="source-inline">ksoftirqd</strong> thread up (it schedules it, actually) in order to ensure the softirq runs soon but in a process context this time.</p>
			<p><strong class="source-inline">raise_softirq</strong>, on the other hand, first calls <strong class="source-inline">local_irq_save()</strong> (which disables interrupts on the local processor after saving its current interrupt flags). It then calls <strong class="source-inline">raise_softirq_irqoff()</strong>, described previously, in order to schedule the softirq on the local CPU (remember—this function must be invoked with IRQs disabled on the local CPU). Finally, it calls <strong class="source-inline">local_irq_restore()</strong> in order to restore the previously saved interrupt flags.</p>
			<p>Here are a few things to remember about softirqs:</p>
			<ul>
				<li>A softirq can never preempt another softirq. Only hardware interrupts can. Softirqs are executed at a high priority, with scheduler preemption disabled but IRQs enabled. This makes softirqs suitable for the most time-critical and important deferred processing on the system.</li>
				<li>While a handler runs on a CPU, other softirqs on this CPU are disabled. Softirqs run concurrently, however. While a softirq is running, another softirq (even the same one) can run on another processor. This is one of the main advantages of softirqs over hardirqs and is the reason why they are used in the networking subsystem, which may require heavy CPU power.</li>
				<li>Softirqs are <a id="_idIndexMarker244"/>mostly scheduled in the return path of hardware interrupt handlers. If any is scheduled out of the interrupt context, it will run in a process context if it is still pending when the local <strong class="source-inline">ksoftirqd</strong> thread is given to the CPU. Their execution may be triggered in the following cases:<ul><li>By the local per-CPU timer interrupt (on SMP system only, with <strong class="source-inline">CONFIG_SMP</strong> enabled). See <strong class="source-inline">timer_tick()</strong>, <strong class="source-inline">update_process_times()</strong>, and <strong class="source-inline">run_local_timers()</strong>.</li><li>By a call to the <strong class="source-inline">local_bh_enable()</strong> function (mostly invoked by the network subsystem for handling packet-receiving/-transmitting softirqs).</li><li>On the exit path of any I/O IRQ handler (see <strong class="source-inline">do_IRQ</strong>, which makes a call to <strong class="source-inline">irq_exit()</strong>, in turn invoking <strong class="source-inline">invoke_softirq()</strong>.</li><li>When the local <strong class="source-inline">ksoftirqd</strong> thread is given to the CPU (aka awakened).</li></ul></li>
			</ul>
			<p>The actual kernel function responsible for walking through the softirqs' pending bitmap and running them is <strong class="source-inline">__do_softirq()</strong>, defined in <strong class="source-inline">kernel/softirq.c</strong>. This function is always invoked with interrupts disabled on the local CPU. It does the following tasks:</p>
			<ul>
				<li>Once invoked, the function first saves the current per-CPU pending softirqs' bitmap in a variable named <strong class="source-inline">pending</strong>, and locally disables softirqs by means of <strong class="source-inline">__local_bh_disable_ip</strong>.</li>
				<li>It then resets the current per-CPU pending bitmask (which has already been saved) and then re-enables interrupts (softirqs run with interrupts enabled).</li>
				<li>After this, it enters a <strong class="source-inline">while</strong> loop, checking for pending softirqs in the saved bitmap. If there is no softirq pending, it will execute the handler of each pending softirq, taking care to increment their execution statistics.</li>
				<li>After all pending IRQ handlers have been executed (we are out of the <strong class="source-inline">while </strong>loop), <strong class="source-inline">__do_softirq()</strong> again reads the per-CPU pending bitmask in order to check if any softirqs were scheduled again when it was in the <strong class="source-inline">while</strong> loop. If there are any pending softirqs, the whole process will restart (based on a <strong class="source-inline">goto</strong> loop), starting from <em class="italic">Step 2</em>. This helps in handling, for example, softirqs that rescheduled themselves.</li>
			</ul>
			<p>However, <strong class="source-inline">__do_softirq()</strong> will not repeat if one of the following conditions occurs:</p>
			<ul>
				<li>It has already repeated up to <strong class="source-inline">MAX_SOFTIRQ_RESTART</strong> times, which is set to 10 in <strong class="source-inline">kernel/softirq.c</strong>. This is the limit of the softirqs' processing loop, not the upper bound of the previously described <strong class="source-inline">while</strong> loop.</li>
				<li>It has hogged<a id="_idIndexMarker245"/> the CPU more than <strong class="source-inline">MAX_SOFTIRQ_TIME</strong>, which is set to 2 milliseconds (<strong class="source-inline">msecs_to_jiffies(2)</strong>) in <strong class="source-inline">kernel/softirq.c</strong>, since this would prevent the scheduler from being enabled.</li>
			</ul>
			<p>If one of the two aforementioned situations occurs, <strong class="source-inline">__do_softirq()</strong> will break its loop and call <strong class="source-inline">wakeup_softirqd()</strong> in order to wake the local <strong class="source-inline">ksoftirqd</strong> thread, which will later execute the pending softirqs in a process context. Since <strong class="source-inline">do_softirq</strong> is called at many points in the kernel, it is likely that another invocation of <strong class="source-inline">__do_softirqs</strong> handles pending softirqs before the <strong class="source-inline">ksoftirqd</strong> thread has a chance to run.</p>
			<h3>Some words about ksoftirqd</h3>
			<p><strong class="source-inline">ksoftirqd</strong> is a <a id="_idIndexMarker246"/>per-CPU kernel thread raised to handle unserviced software interrupts. It is spawned early during the kernel boot process, as stated in <strong class="source-inline">kernel/softirq.c</strong> and shown here:</p>
			<p class="source-code">static __init int spawn_ksoftirqd(void)</p>
			<p class="source-code">{</p>
			<p class="source-code">    cpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, "softirq:dead",</p>
			<p class="source-code">                          NULL, takeover_tasklets);</p>
			<p class="source-code">    BUG_ON(smpboot_register_percpu_thread(&amp;softirq_threads));</p>
			<p class="source-code">    return 0;</p>
			<p class="source-code">}</p>
			<p class="source-code">early_initcall(spawn_ksoftirqd);</p>
			<p>After running the top command <a id="_idIndexMarker247"/>in the preceding code snippet, we can see some <strong class="source-inline">ksoftirqd/&lt;n&gt;</strong> entries, where <strong class="source-inline">&lt;n&gt;</strong> is the logical CPU index of the CPU running the <strong class="source-inline">ksoftirqd</strong> thread. As <strong class="source-inline">ksoftirqd</strong> threads run in a process context, they are equal to classic processes/threads, so they compete for the CPU. <strong class="source-inline">ksoftirqd</strong> threads hogging CPUs for a long time may indicate a system under heavy load.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor054"/>Tasklets</h2>
			<p>Before starting to discuss<a id="_idIndexMarker248"/> about <strong class="bold">tasklets</strong>, you must notice that these are scheduled for removal in the Linux kernel, thus the purpose of this section is purely for pedagogic reasons, to help you understanding their use in older kernel modules. Consequently, you must not use these in your developments.</p>
			<p>Tasklets are bottom halves that are built on top of <strong class="source-inline">HI_SOFTIRQ</strong> and <strong class="source-inline">TASKLET_SOFTIRQ</strong> <strong class="bold">softirqs</strong>, with the exception that the <strong class="source-inline">HI_SOFTIRQ</strong>-based tasklets run before <strong class="source-inline">TASKLET_SOFTIRQ</strong>-based ones. Simply put, tasklets are softirqs and obey the same rules. Unlike softirqs, however, two of the same tasklets never run concurrently. The tasklet API is quite basic and intuitive.</p>
			<p>Tasklets are represented by a <strong class="source-inline">struct tasklet_struct</strong> structure defined in <strong class="source-inline">&lt;linux/interrupt.h&gt;</strong>. Each instance of this structure represents a unique tasklet, as illustrated in the following code snippet:</p>
			<p class="source-code">struct tasklet_struct</p>
			<p class="source-code">{</p>
			<p class="source-code">	struct tasklet_struct *next;</p>
			<p class="source-code">	unsigned long state;</p>
			<p class="source-code">	atomic_t count;</p>
			<p class="source-code">	bool use_callback;</p>
			<p class="source-code">	union {</p>
			<p class="source-code">		void (*func)(unsigned long data);</p>
			<p class="source-code">		void (*callback)(struct tasklet_struct *t);</p>
			<p class="source-code">	};</p>
			<p class="source-code">	unsigned long data;</p>
			<p class="source-code">};</p>
			<p>Though this API is scheduled for removal, it has been slightly modernized as compared to its legacy implementation. The callback function is stored in the <strong class="source-inline">callback()</strong> field rather than <strong class="source-inline">func()</strong>, which is kept for compatibility with the old implementation. This new callback simply takes a pointer to the <strong class="source-inline">tasklet_struct</strong> structure as its one argument. The handler will be executed by the underlying softirq. It is the equivalent of <strong class="source-inline">action</strong> to a softirq, with the same prototype and the same argument meaning. <strong class="source-inline">data</strong> will be passed as its sole argument.</p>
			<p>Whether <strong class="source-inline">callback()</strong> handler or <strong class="source-inline">func()</strong> handler is executed depends on the way the tasklet is initialized. A tasklet can be statically initialized using either <strong class="source-inline">DECLARE_TASKLET()</strong> macro or <strong class="source-inline">DECLARE_TASKLET_OLD()</strong> macro. These macros are defined as follows:</p>
			<p class="source-code">#define DECLARE_TASKLET_OLD(name, _func)       \</p>
			<p class="source-code">    struct tasklet_struct name = {             \</p>
			<p class="source-code">    .count = ATOMIC_INIT(0),            	   \</p>
			<p class="source-code">    .func = _func,                    	        \</p>
			<p class="source-code">}</p>
			<p class="source-code">#define DECLARE_TASKLET(name, _callback)       \</p>
			<p class="source-code">     struct tasklet_struct name = {            \</p>
			<p class="source-code">     .count = ATOMIC_INIT(0),                  \</p>
			<p class="source-code">     .callback = _callback,                    \</p>
			<p class="source-code">     .use_callback = true,                     \</p>
			<p class="source-code">}</p>
			<p>From what we can see, by using <strong class="source-inline">DECLARE_TASKLET_OLD()</strong>, the legacy<a id="_idIndexMarker249"/> implementation is kept and <strong class="source-inline">func()</strong> is used as the callback. Therefore, the prototype of the provided handler must be as follows:</p>
			<p class="source-code">void foo(unsigned long data);</p>
			<p>By using <strong class="source-inline">DECLARE_TASKLET()</strong>, the <strong class="source-inline">callback</strong> field is used as the handler and <strong class="source-inline">use_callback</strong> filed is set to <strong class="source-inline">true</strong> (this is because the tasklet core checks this value to determine the handler that must be invoked). In this case, the protype of the callback is as follows:</p>
			<p class="source-code">void foo(struct tasklet_struct *t)</p>
			<p>In the previous snipped, <strong class="source-inline">t</strong> pointer is passed by the tasklet core while invoking the handler. It will point to your tasklet. Since a pointer to the tasklet is passed as argument to the callback, it is common to embed the tasklet object within a larger, user-specific structure, the pointer to which can be obtained with the <strong class="source-inline">container_of()</strong> macro. In order to do so, you should rather use the dynamic initialization, which can be achieved thanks to <strong class="source-inline">tasklet_setup()</strong> function, defined as follows:</p>
			<p class="source-code">void tasklet_setup(struct tasklet_struct *t,</p>
			<p class="source-code">     void (*callback)(struct tasklet_struct *));</p>
			<p>According to the previous prototype, we can guess that by using the dynamic initialization, we have no choice but to use the new implementation where <strong class="source-inline">callback</strong> field is used as the tasklet handler.</p>
			<p>Using static or dynamic method depends <a id="_idIndexMarker250"/>on what you need to achieve, for example, if you want the tasklet to be unique for the whole module or to be private per probed device, or even more, if you need to have a direct or indirect reference to the tasklet.</p>
			<p>By default, an initialized tasklet is runnable when it is scheduled: <em class="italic">it is said to be enabled</em>. <strong class="source-inline">DECLARE_TASKLET_DISABLED</strong> is an alternative to statically initialize default-disabled tasklets. There is no such alternative for a dynamically initialized tasklet, unless you invoke <strong class="source-inline">tasklet_disable()</strong> on this tasklet after it has been dynamically initialized. A disabled tasklet will require the <strong class="source-inline">tasklet_enable()</strong> function to be invoked to make this tasklet runnable. Tasklets are scheduled (similar to raising the softirq) via the <strong class="source-inline">tasklet_schedule()</strong> and <strong class="source-inline">tasklet_hi_schedule()</strong> functions. You can use <strong class="source-inline">tasklet_disable()</strong> API to disable a scheduled or running tasklet. This function disables the tasklet and returns only when the tasklet has terminated its execution (assuming it was running). After this, the tasklet can still be scheduled, but it will not run on the CPU until it is enabled again. The asynchronous variant <strong class="source-inline">tasklet_disable_nosync()</strong> can be used too, which returns immediately, even if the termination has not occurred. Moreover, a tasklet that has been disabled several times should be enabled the same number of times (this is enforced and verified by the kernel thanks to the <strong class="source-inline">count</strong> field in the <strong class="source-inline">struct tasklet</strong> object). The definition of the previously mentioned tasklet APIs is illustrated in the following snippet:</p>
			<p class="source-code">DECLARE_TASKLET(name, _callback)</p>
			<p class="source-code">DECLARE_TASKLET_DISABLED(name, _callback);</p>
			<p class="source-code">DECLARE_TASKLET_OLD(name, func);</p>
			<p class="source-code">void tasklet_setup(struct tasklet_struct *t,</p>
			<p class="source-code">     void (*callback)(struct tasklet_struct *));</p>
			<p class="source-code">void tasklet_enable(struct tasklet_struct *t);</p>
			<p class="source-code">void tasklet_disable(struct tasklet_struct *t);</p>
			<p class="source-code">void tasklet_schedule(struct tasklet_struct *t);</p>
			<p class="source-code">void tasklet_hi_schedule(struct tasklet_struct *t);</p>
			<p>The kernel maintains normal-priority and high-priority tasklets in two per-CPU queues (each CPU maintains its low- and high-priority queue pair). <strong class="source-inline">tasklet_schedule()</strong> adds the tasklet into the normal priority list of the CPU on which it is invoked, scheduling the associated softirq with a <strong class="source-inline">TASKLET_SOFTIRQ</strong> flag. With <strong class="source-inline">tasklet_hi_schedule()</strong>, the tasklet is added into the high-priority list (still of the list on which it is invoked), scheduling the associated softirq with a <strong class="source-inline">HI_SOFTIRQ</strong> flag. When the tasklet is scheduled, its <strong class="source-inline">TASKLET_STATE_SCHED</strong> flag is set, and the tasklet is queued for execution. At the time of execution, a <strong class="source-inline">TASKLET_STATE_RUN</strong> flag is set, and the <strong class="source-inline">TASKLET_STATE_SCHED</strong> state is removed, thus making the tasklet re-schedulable during its execution, either by the tasklet itself or from within an interrupt handler.</p>
			<p><strong class="bold">High priority tasklets</strong> are meant to be used for soft interrupt handlers with low latency requirements. Calling <strong class="source-inline">tasklet_schedule()</strong> on a tasklet already scheduled and whose execution has not started yet will do nothing, resulting in the tasklet being executed only once. A tasklet can reschedule itself, and you can safely call <strong class="source-inline">tasklet_schedule()</strong> in a tasklet. High priority tasklets are always executed before normal ones and should then be used carefully, else you may increase system latency. Stopping a tasklet is as simple as calling <strong class="source-inline">tasklet_kill()</strong>, as illustrated in the following code snippet, which will prevent the tasklet from running again, or waiting for its completion before killing it if the tasklet is currently scheduled to run. If a tasklet re-schedules itself, you should first prevent the tasklet from re-scheduling itself prior to calling this function:</p>
			<p class="source-code">void tasklet_kill(struct tasklet_struct *t);</p>
			<h3>Writing your tasklet handler</h3>
			<p>All that being said, let's <a id="_idIndexMarker251"/> see some use cases, as follows:</p>
			<p class="source-code"># #include &lt;linux/init.h&gt;</p>
			<p class="source-code">#include &lt;linux/module.h&gt;</p>
			<p class="source-code">#include &lt;linux/kernel.h&gt;</p>
			<p class="source-code">#include &lt;linux/interrupt.h&gt;    /* for tasklets api */</p>
			<p class="source-code">/* Tasklet handler, that just prints the handler name */</p>
			<p class="source-code">void tasklet_function(struct tasklet_struct *t)</p>
			<p class="source-code">{</p>
			<p class="source-code">    pr_info("running %s\n", __func__);</p>
			<p class="source-code">}</p>
			<p class="source-code">DECLARE_TASKLET(my_tasklet, tasklet_function);</p>
			<p class="source-code">static int __init my_init(void)</p>
			<p class="source-code">{</p>
			<p class="source-code">    /* Schedule the handler */</p>
			<p class="source-code">    tasklet_schedule(&amp;my_tasklet);</p>
			<p class="source-code">    pr_info("tasklet example\n");</p>
			<p class="source-code">    return 0;</p>
			<p class="source-code">}</p>
			<p class="source-code">void my_exit( void )</p>
			<p class="source-code">{</p>
			<p class="source-code">    /* Stop the tasklet before we exit */</p>
			<p class="source-code">    tasklet_kill(&amp;my_tasklet);</p>
			<p class="source-code">    pr_info("tasklet example cleanup\n");</p>
			<p class="source-code">    return;</p>
			<p class="source-code">}</p>
			<p class="source-code">module_init(my_init);</p>
			<p class="source-code">module_exit(my_exit);</p>
			<p class="source-code">MODULE_AUTHOR("John Madieu &lt;john.madieu@gmail.com&gt;");</p>
			<p class="source-code">MODULE_LICENSE("GPL");</p>
			<p>In the preceding  <a id="_idIndexMarker252"/>code snippet, we statically declare our <strong class="source-inline">my_tasklet</strong> tasklet and the function supposed to be invoked when this tasklet is scheduled. Since we did not used the <strong class="source-inline">_OLD</strong> variant, we defined the handler prototype the same as <strong class="source-inline">callback</strong> field in the tasklet object.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Tasklet API is deprecated, and you should consider using threaded IRQs instead.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor055"/>Workqueues</h2>
			<p>Added since Linux kernel 2.6, the <a id="_idIndexMarker253"/>most used and simple deferring mechanism is a workqueue. As a deferring mechanism, it takes an opposite approach to the others we've seen, running only in a preemptible context. It is the only choice when sleeping is required unless you implicitly create a kernel thread or unless you are using a threaded interrupt. That said, workqueues are built on top of kernel threads, and for this simple reason, we will not implicitly cover kernel threads in this book. </p>
			<p>At the core of the workqueue subsystem, there are two data structures that fairly well explain the concept behind this, as follows:</p>
			<ul>
				<li>The work to be deferred (referred to as a work item), represented in the kernel by instances of <strong class="source-inline">struct work_struct</strong>, which indicates the handler function to run. If you need a delay before the work runs after it has been submitted to the workqueue, the kernel provides a <strong class="source-inline">struct delayed_work</strong> instance instead. A work item is a simple structure that only contains a pointer to the function that is to be scheduled for asynchronous execution. To summarize, we can enumerate two types of work item structures, as follows:<ul><li>A <strong class="source-inline">work_struct</strong> structure, which schedules a task to run as soon as possible when the system allows it</li><li>A <strong class="source-inline">delayed_work</strong> structure, which schedules a task to run after at least a given time interval</li></ul></li>
				<li>The workqueue itself, which is represented by a <strong class="source-inline">struct workqueue_struct</strong> instance and is the structure onto which work is placed. It is a queue of work items.</li>
			</ul>
			<p>Apart from these data structures, there are two generic terms you should be familiar with, as follows:</p>
			<ul>
				<li><strong class="bold">Worker threads</strong>, which <a id="_idIndexMarker254"/>are dedicated threads that execute and pull the functions off the queue, one by one, one after the other.</li>
				<li><strong class="bold">Worker pools</strong>: This is <a id="_idIndexMarker255"/>a collection of worker threads (a thread pool) that are used to better manage the worker threads.</li>
			</ul>
			<p>The first step in using workqueues consists of creating a work item, represented by <strong class="source-inline">struct work_struct</strong> or <strong class="source-inline">struct delayed_work</strong> for the delayed variant, and defined in <strong class="source-inline">linux/workqueue.h</strong>. The kernel provides either a <strong class="source-inline">DECLARE_WORK</strong> macro to statically declare and initialize a work structure or dynamically uses an <strong class="source-inline">INIT_WORK</strong> macro. If you need delayed work, you can use the <strong class="source-inline">INIT_DELAYED_WORK</strong> macro for dynamic allocation and initialization or <strong class="source-inline">DECLARE_DELAYED_WORK</strong> for a static one. You can see the macros in action in the following code snippet:</p>
			<p class="source-code">DECLARE_WORK(name, function)</p>
			<p class="source-code">DECLARE_DELAYED_WORK(name, function)</p>
			<p class="source-code">INIT_WORK(work, func );</p>
			<p class="source-code">INIT_DELAYED_WORK( work, func);</p>
			<p>Here is what our <a id="_idIndexMarker256"/>work item structure looks like:</p>
			<p class="source-code">struct work_struct {</p>
			<p class="source-code">    atomic_long_t data;</p>
			<p class="source-code">    struct list_head entry;</p>
			<p class="source-code">    work_func_t func;</p>
			<p class="source-code">};</p>
			<p class="source-code">struct delayed_work {</p>
			<p class="source-code">    struct work_struct work;</p>
			<p class="source-code">    struct timer_list timer;</p>
			<p class="source-code">    struct workqueue_struct *wq;</p>
			<p class="source-code">    int cpu;</p>
			<p class="source-code">};</p>
			<p>The <strong class="source-inline">func</strong> field, which is of type <strong class="source-inline">work_func_t</strong>, tells us a bit more about the header of a <strong class="source-inline">work</strong> function, as illustrated here:</p>
			<p class="source-code">typedef void (*work_func_t)(struct work_struct *work);</p>
			<p><strong class="source-inline">work</strong> is an input parameter that corresponds to the work structure to be scheduled. If you submitted delayed work, it would correspond to the <strong class="source-inline">delayed_work.work</strong> field. It would then be necessary to use the <strong class="source-inline">to_delayed_work()</strong> function in order to get the underlying delayed work structure, as illustrated in the following code snippet:</p>
			<p class="source-code">struct delayed_work *to_delayed_work(</p>
			<p class="source-code">                struct work_struct *work)</p>
			<p>Workqueue infrastructure<a id="_idIndexMarker257"/> allows drivers to create a dedicated kernel thread (a workqueue) called a worker thread to run work functions. A new workqueue can be created with the following functions:</p>
			<p class="source-code">struct workqueue_struct *create_workqueue(const char *name)</p>
			<p class="source-code">struct workqueue_struct *create_singlethread_workqueue(</p>
			<p class="source-code">                                          const char *name)</p>
			<p><strong class="source-inline">create_workqueue()</strong> creates a dedicated thread (a worker thread) per CPU on the system. For example, on an 8-core system, it will result in 8 kernel threads created to run the works submitted to your workqueue. Unless you have strong reasons to create a thread per CPU, you should prefer the single thread variant. In most cases, a single system kernel thread should be enough. In this case, you should use <strong class="source-inline">create_singlethread_workqueue()</strong> instead, which creates—as its name states—a single-threaded workqueue. Either normal or delayed works can be enqueued onto the same queue. In order to schedule works on your created workqueue, you can use either <strong class="source-inline">queue_work()</strong> or <strong class="source-inline">queue_delayed_work()</strong>, depending on the nature of the work. These functions are defined as follows:</p>
			<p class="source-code">bool queue_work(struct workqueue_struct *wq,</p>
			<p class="source-code">                 struct work_struct *work)</p>
			<p class="source-code">bool queue_delayed_work(struct workqueue_struct *wq,</p>
			<p class="source-code">                        struct delayed_work *dwork,</p>
			<p class="source-code">                        unsigned long delay)</p>
			<p>These functions return <strong class="source-inline">false</strong> if the work was already on a queue and <strong class="source-inline">true</strong> otherwise. <strong class="source-inline">queue_dalayed_work()</strong> is to be used to schedule a work item (a delayed one) for later execution after a given delay. The time unit for the delay is a jiffy. There are, however, APIs to convert milliseconds and microseconds to jiffies, defined as follows:</p>
			<p class="source-code">unsigned long msecs_to_jiffies(const unsigned int m)</p>
			<p class="source-code">unsigned long usecs_to_jiffies(const unsigned int u)</p>
			<p>The following example uses 200 milliseconds as a delay:</p>
			<p class="source-code">queue_delayed_work(my_wq, &amp;drvdata-&gt;tx_work,</p>
			<p class="source-code">                  usecs_to_jiffies(200));</p>
			<p>You should not expect this<a id="_idIndexMarker258"/> delay to be accurate as the delay will be rounded up to the closest jiffy value. Thus, even when requesting 200 us, you should expect a jiffy. Submitted work items can be canceled by calling either <strong class="source-inline">cancel_delayed_work()</strong>, <strong class="source-inline">cancel_delayed_work_sync()</strong>, or <strong class="source-inline">cancel_work_sync()</strong>. These cancelation functions are defined as follows:</p>
			<p class="source-code">bool cancel_work_sync(struct work_struct *work)</p>
			<p class="source-code">bool cancel_delayed_work(struct delayed_work *dwork)</p>
			<p class="source-code">bool cancel_delayed_work_sync(struct delayed_work *dwork)</p>
			<p><strong class="source-inline">cancel_work_sync()</strong> synchronously cancels the given work—in other words, it cancels work and waits for its execution to finish. The kernel guarantees <strong class="source-inline">work</strong> not to be pending or executing on any CPU on return from this function, even if the work migrates to another workqueue or requeues itself. It returns <strong class="source-inline">true</strong> if <strong class="source-inline">work</strong> was pending and <strong class="source-inline">false</strong> otherwise.</p>
			<p><strong class="source-inline">cancel_delayed_work()</strong> asynchronously cancels a delayed entry. It returns <strong class="source-inline">true</strong> (actually, a nonzero value) if <strong class="source-inline">dwork</strong> was pending and canceled and <strong class="source-inline">false</strong> if it wasn't pending, probably because it is actually running, and thus might still be running after <strong class="source-inline">cancel_delayed_work()</strong> has returned. To make sure the work really ran to its end, you may want to use <strong class="source-inline">flush_workqueue()</strong>, which flushes every work item in the given queue, or <strong class="source-inline">cancel_delayed_work_sync()</strong>, which is the synchronous version of <strong class="source-inline">cancel_delayed_work()</strong>.</p>
			<p>When you are done with a workqueue, you should destroy it with <strong class="source-inline">destroy_workqueue()</strong>, as illustrated here:</p>
			<p class="source-code">void flush_workqueue(struct worksqueue_struct * queue); </p>
			<p class="source-code">void destroy_workqueue(structure workqueque_struct *queue);</p>
			<p>While waiting for any pending work to execute, the <strong class="source-inline">_sync</strong> variant functions sleep and thus can be called only from a process context.</p>
			<h3>Kernel-global workqueue – the shared queue</h3>
			<p>In most situations, your <a id="_idIndexMarker259"/>code does not necessarily need the performance of its own dedicated set of threads, and because <strong class="source-inline">create_workqueue()</strong> creates one worker thread for each CPU, it may be a bad idea to use it on very large multi-CPU systems. You may then want to use the kernel shared queue, which already has its set of kernel threads pre-allocated (early during the boot, via the <strong class="source-inline">workqueue_init_early()</strong> function) for running works.</p>
			<p>This kernel-global workqueue is the so-called <strong class="source-inline">system_wq</strong> workqueue, defined in <strong class="source-inline">kernel/workqueue.c</strong>. There is actually one instance per CPU, each backed by a dedicated thread named <strong class="source-inline">events/n</strong>, where <strong class="source-inline">n</strong> is the processor number (or index) to which the thread is bound.</p>
			<p>You can queue a work item in the default system workqueue using one of the following functions:</p>
			<p class="source-code">int schedule_work(struct work_struct *work);</p>
			<p class="source-code">int schedule_delayed_work(struct delayed_work *dwork,</p>
			<p class="source-code">                          unsigned long delay);</p>
			<p class="source-code">int schedule_work_on(int cpu,</p>
			<p class="source-code">               struct work_struct *work);</p>
			<p class="source-code">int schedule_delayed_work_on(int cpu,</p>
			<p class="source-code">                struct delayed_work *dwork,</p>
			<p class="source-code">                unsigned long delay);</p>
			<p><strong class="source-inline">schedule_work()</strong> immediately schedules the work that will be executed as soon as possible after the worker thread on the current processor wakes up. With <strong class="source-inline">schedule_delayed_work()</strong>, the work will be put in the queue in the future, after the delay timer ticks. <strong class="source-inline">_on</strong> variants are used to schedule the work on a specific CPU (which does not absolutely need to be the current one). Each of these functions queue works on the system's shared workqueue, <strong class="source-inline">system_wq</strong>, defined in <strong class="source-inline">kernel/workqueue.c</strong> as follows:</p>
			<p class="source-code">struct workqueue_struct *system_wq __read_mostly;</p>
			<p class="source-code">EXPORT_SYMBOL(system_wq);</p>
			<p>You should also note that since this system workqueue is shared, you should not queue works which can run for too long, otherwise it may slow down other contender works, which could then wait for longer than they should before being executed.</p>
			<p>In order to flush the kernel-global workqueue—that is, ensure a given batch of work is completed—we can use <strong class="source-inline">flush_scheduled_work()</strong>, as follows:</p>
			<p class="source-code">void flush_scheduled_work(void);</p>
			<p><strong class="source-inline">flush_scheduled_work()</strong> is a<a id="_idIndexMarker260"/> wrapper that calls <strong class="source-inline">flush_workqueue()</strong> on <strong class="source-inline">system_wq</strong>. Note that there may be works in the <strong class="source-inline">system_wq</strong> workqueue that you have not submitted and have no control over. Flushing this workqueue entirely is thus overkill, and it is recommended to run <strong class="source-inline">cancel_delayed_work_sync()</strong> or <strong class="source-inline">cancel_work_sync()</strong> instead.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Unless you have a strong reason for creating a dedicated thread, the default (kernel-global) thread is preferred.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor056"/>Workqueues' new generation</h2>
			<p>The original (now legacy) workqueue <a id="_idIndexMarker261"/>implementation used two kinds of workqueues: those with a single thread system-wide, and those with a thread per CPU. However, for a growing number of CPUs, this led to some limitations, as outlined here:</p>
			<ul>
				<li>On very large systems, the kernel could run out of <strong class="bold">process identifiers</strong> (<strong class="bold">PIDs</strong>) (defaulting to 32,000) just at the boot, even before the <strong class="source-inline">init</strong> process started.</li>
				<li>Multithreaded workqueues provided poor concurrency management as their threads compete for the CPU with each other threads on the system. As there were more CPU contenders, this introduced some overhead—that is, more context switches than necessary.</li>
				<li>The consumption of far more resources than were really needed.</li>
			</ul>
			<p>Moreover, subsystems that needed a dynamic or fine-grained level of concurrency had to implement their own thread pools. As a result of this, a new workqueue API has been designed, and the legacy workqueue API (<strong class="source-inline">create_workqueue()</strong>,  <strong class="source-inline">create_singlethread_workqueue()</strong>, and <strong class="source-inline">create_freezable_workqueue()</strong>) is scheduled for removal, though they are actually wrappers around the new one, the so-called <strong class="bold">Concurrency Managed Workqueue</strong> (<strong class="bold">cmwq</strong>), using per-CPU<a id="_idIndexMarker262"/> worker pools shared by all workqueues in order to automatically provide a dynamic and flexible level of concurrency, abstracting such details for the API users.</p>
			<h3>cmwq</h3>
			<p>cmwq<a id="_idIndexMarker263"/> is an upgrade of workqueue APIs. Using this new API implies you are choosing between the <strong class="source-inline">alloc_workqueue()</strong> function and the <strong class="source-inline">alloc_ordered_workqueue()</strong> macro to create a workqueue. They both allocate a workqueue and return a pointer to it on success, and <strong class="source-inline">NULL</strong> on failure. The returned workqueue can be freed using the <strong class="source-inline">destroy_workqueue()</strong> function. You can see an illustration of the code in the following snippet:</p>
			<p class="source-code">struct workqueue_struct *alloc_workqueue(const char *fmt,</p>
			<p class="source-code">                             unsigned int flags,</p>
			<p class="source-code">                             int max_active, ...);</p>
			<p class="source-code">#define alloc_ordered_workqueue(fmt, flags, args...) [...]</p>
			<p class="source-code">void destroy_workqueue(struct workqueue_struct *wq)</p>
			<p><strong class="source-inline">fmt</strong> is the <strong class="source-inline">printf</strong> format for the name of the workqueue, and <strong class="source-inline">args...</strong> are arguments for <strong class="source-inline">fmt</strong>.</p>
			<p><strong class="source-inline">destroy_workqueue()</strong> is to be called on the workqueue once you are done with it. All works currently pending will be done first before the kernel truly destroys the workqueue. <strong class="source-inline">alloc_workqueue()</strong> creates a workqueue based on <strong class="source-inline">max_active</strong>, which defines the concurrency level by limiting the number of works (tasks, actually) that can be executing (workers in a runnable state) simultaneously from this workqueue on any given CPU. For example, a <strong class="source-inline">max_active</strong> value of 5 would mean at most 5 work items of this workqueue can be executing at the same time per CPU. On the other hand, <strong class="source-inline">alloc_ordered_workqueue()</strong> creates a <a id="_idIndexMarker264"/>workqueue that processes each work item one by one in queued order (that is, <strong class="bold">first-in, first-out</strong> (<strong class="bold">FIFO</strong>) order).</p>
			<p><strong class="source-inline">flags</strong> controls how and when work items are queued, assigned execution resources, scheduled, and executed. There are various flags used in this new API on which we should spend some time, as follows:</p>
			<ul>
				<li><strong class="source-inline">WQ_UNBOUND</strong>: Legacy <a id="_idIndexMarker265"/>workqueues had a worker thread per CPU and were designed to run tasks on the CPU where they were submitted. The kernel scheduler had no choice but to always schedule a worker on the CPU on which it was defined. With this approach, even a single workqueue was able to prevent a CPU from idling and being turned off, which leads to increased power consumption or poor scheduling policies. <strong class="source-inline">WQ_UNBOUND</strong> turns off the previously-described behavior. Work items are <a id="_idIndexMarker266"/>not bound to the CPU anymore, hence the name <strong class="bold">unbound workqueues</strong>. There is no more locality, and the scheduler can reschedule workers on any CPU as it sees fit. The scheduler has the last word now and can balance CPU load, especially for long and sometimes CPU-intensive works.</li>
				<li><strong class="source-inline">WQ_MEM_RECLAIM</strong>: This<a id="_idIndexMarker267"/> flag is to be set for workqueues that need to guarantee forward progress during the memory reclaim path (when free memory is running dangerously low; the system is said to be under memory pressure). In this case, <strong class="source-inline">GFP_KERNEL</strong> allocations may block and deadlock the entire workqueue. The workqueue is then guaranteed to have at least a ready-to-use worker thread—a so-called rescuer thread—reserved for it, regardless of memory pressure, so that it can progress forward. There's one rescuer thread allocated for each workqueue that has this flag set.</li>
			</ul>
			<p>Let's consider a situation where we have three work items (<strong class="source-inline">w1</strong>, <strong class="source-inline">w2</strong>, and <strong class="source-inline">w3</strong>) in our workqueue <strong class="source-inline">W</strong>. <strong class="source-inline">w1</strong> does some work and then waits for <strong class="source-inline">w3</strong> to complete (let's say it depends on the computation result of <strong class="source-inline">w3</strong>). Afterward, <strong class="source-inline">w2</strong> (which is independent of the others) does some <strong class="source-inline">kmalloc()</strong> allocation (<strong class="source-inline">GFP_KERNEL</strong>) and, oops—there's not enough memory. While <strong class="source-inline">w2</strong> is blocked, it still occupies <strong class="source-inline">W</strong>'s workqueue. This results in <strong class="source-inline">w3</strong> not being able to run, even though there is no dependency between <strong class="source-inline">w2</strong> and <strong class="source-inline">w3</strong>. As there is not enough memory available, there would be no way of allocating a new thread to run <strong class="source-inline">w3</strong>. A pre-allocated thread would for sure solve this problem, not by magically allocating the memory for <strong class="source-inline">w2</strong>, but by running <strong class="source-inline">w3</strong> so that <strong class="source-inline">w1</strong> can continue its job, and so on. <strong class="source-inline">w2</strong> will continue its progression as soon as possible when there is enough available memory to allocate. This pre-allocated thread is a so-called <strong class="bold">rescuer thread</strong>. You must<a id="_idIndexMarker268"/> set the <strong class="source-inline">WQ_MEM_RECLAIM</strong> flag if you think the workqueue is likely to be used in the memory reclaim path. This flag replaces the legacy <strong class="source-inline">WQ_RESCUER</strong> flag as of this commit: <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=493008a8e475771a2126e0ce95a73e35b371d277">https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=493008a8e475771a2126e0ce95a73e35b371d277</a>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Memory reclaim<a id="_idIndexMarker269"/> is a Linux mechanism on a memory allocation path that consists of allocating memory only after throwing the current content of that memory somewhere else.</p>
			<ul>
				<li><strong class="source-inline">WQ_FREEZABLE</strong>: This<a id="_idIndexMarker270"/> flag is used for power-management purposes. A workqueue with this flag set will be frozen when the system is suspended or hibernates. On the freezing path, all current work items of the worker(s) will be processed. When the freeze is complete, no new work item will be executed until the system is unfrozen. Filesystem-related workqueue(s) may use this flag (that is, to ensure that modifications made on files are pushed to the disk or create a hibernation image on the freezing path and that no modification is made on-disk after the hibernation image has been created. In this situation, non-freezable items or doing things differently could lead to filesystem corruption. As an example, all <strong class="bold">Extents File System</strong> (<strong class="bold">XFS</strong>) internal workqueues<a id="_idIndexMarker271"/> have this flag set (see <strong class="source-inline">fs/xfs/xfs_super.c</strong>) to ensure no further changes are made on-disk once the freezer infrastructure freezes kernel threads and creates a hibernation image. You should definitely not have this flag set if your workqueue can run tasks as part of the hibernation/suspend/resume process of the system. More information on this topic can be found in <strong class="source-inline">Documentation/power/freezing-of-tasks.txt</strong> and by having a look at the <strong class="source-inline">freeze_workqueues_begin()</strong> and <strong class="source-inline">thaw_workqueues()</strong> internal kernel functions.</li>
				<li><strong class="source-inline">WQ_HIGHPRI</strong>: Tasks<a id="_idIndexMarker272"/> with this flag set run immediately and do not wait for the CPU to become available. This flag is used for workqueues that queue work items requiring a high priority for execution. Such workqueues have worker threads with a high priority level (lower nice value). In the earlier days of the cmwq, high-priority work items were just queued at the head of a global normal priority worklist so that they could immediately run. Nowadays, there are no interactions between normal-priority and high-priority workqueues as each has its own worklist and its own worker pool. Work items of high-priority workqueues are queued to the high-priority worker pool of the target CPU. Tasks in this workqueue should not block much. Use this flag if you do not want your work item competing for CPU with normal- or lower-priority tasks. Crypto and block devices subsystems use this, for example.</li>
				<li><strong class="source-inline">WQ_CPU_INTENSIVE</strong>: Work<a id="_idIndexMarker273"/> items of CPU-intensive workqueues may burn a lot of CPU cycles and do not participate in workqueue concurrency management. Instead, as with any other task, their execution is regulated by the system scheduler, which makes this flag handy for bound work items that may consume a lot of CPU time. Though the system scheduler controls their execution, concurrency management controls the start of their execution, and runnable non-CPU-intensive work items might cause CPU-intensive work items to be delayed. The <strong class="source-inline">crypto</strong> and <strong class="source-inline">dm-crypt</strong> subsystems use such workqueues. To prevent such tasks from delaying the execution of other non-CPU-intensive work items, they will not be considered when the workqueue code determines whether the CPU is available or not.</li>
			</ul>
			<p>To be compliant and feature-compatible with the old workqueue API, the following mappings are done to keep this API compatible with the original one:</p>
			<ul>
				<li><strong class="source-inline">create_workqueue(name)</strong> is mapped onto <strong class="source-inline">alloc_workqueue(name,WQ_MEM_RECLAIM, 1)</strong></li>
				<li><strong class="source-inline">create_singlethread_workqueue(name)</strong> is mapped onto <strong class="source-inline">alloc_ordered_workqueue(name, WQ_MEM_RECLAIM)</strong></li>
				<li><strong class="source-inline">create_freezable_workqueue(name)</strong> is mapped onto <strong class="source-inline">alloc_workqueue(name,WQ_FREEZABLE | WQ_UNBOUND|WQ_MEM_RECLAIM, 1)</strong></li>
			</ul>
			<p>To summarize, <strong class="source-inline">alloc_ordered_workqueue()</strong> actually replaces <strong class="source-inline">create_freezable_workqueue()</strong> and <strong class="source-inline">create_singlethread_workqueue()</strong> (as per this commit: <a href="https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=81dcaf6516d8">https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=81dcaf6516d8</a>). Workqueues allocated with <strong class="source-inline">alloc_ordered_workqueue()</strong> are unbound and have <strong class="source-inline">max_active</strong> set to <strong class="source-inline">1</strong>.</p>
			<p>When it comes to scheduling items in a workqueue, the work items that have been queued to a specific CPU using <strong class="source-inline">queue_work_on()</strong> will execute on that CPU. Work items queued via <strong class="source-inline">queue_work()</strong> will prefer the queueing CPU, but locality is not guaranteed.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Note that <strong class="source-inline">schedule_work()</strong> is a wrapper that calls <strong class="source-inline">queue_work()</strong> on the system workqueue (<strong class="source-inline">system_wq</strong>), while <strong class="source-inline">schedule_work_on()</strong> is a wrapper around <strong class="source-inline">queue_work_on()</strong>. Also, keep in mind the following: <strong class="source-inline">system_wq = alloc_workqueue("events", 0, 0);</strong>. You can have a look at the <strong class="source-inline">workqueue_init_early()</strong> function in <strong class="source-inline">kernel/workqueue.c</strong> in kernel sources to see how other system-wide workqueues are created.</p>
			<p>We are done with the new Linux kernel workqueue management implementation—that is, cmwq. Because workqueues can be used to defer works from interrupt handlers, we can move on to the next section and learn how to handle interrupts from the Linux kernel.</p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor057"/>Kernel interrupt handling</h1>
			<p>Apart from servicing <a id="_idIndexMarker274"/>processes and user requests, another job of the Linux kernel is managing and speaking with hardware. This is either from the CPU to the device or from the device to the CPU and is achieved by means of interrupts. An interrupt is a signal sent to the processor by an external hardware device requesting immediate attention. Prior to an interrupt being visible to the CPU, this interrupt should be enabled by the interrupt controller, which is a device on its own whose main job consists of routing interrupts to CPUs.</p>
			<p>The Linux kernel allows the provision of handlers for interrupts we are interested in so that when those interrupts are triggered, our handlers are executed. </p>
			<p>An interrupt is how a device halts the kernel, telling it that something interesting or important has happened. These are called IRQs on Linux systems. The main advantage interrupts offer is to avoid device polling. It is up to the device to tell if there is a change in its state; it is not up to us to poll it.</p>
			<p>To be notified when an interrupt occurs, you need to register with that IRQ, providing a function called an interrupt handler that will be called every time that interrupt is raised.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor058"/>Designing and registering an interrupt handler</h2>
			<p>When an interrupt handler is executed, it runs with interrupts disabled on the local CPU. This involves respecting <a id="_idIndexMarker275"/>certain constraints while designing an <strong class="bold">interrupt service routine</strong> (<strong class="bold">ISR</strong>), as outlined here:</p>
			<ul>
				<li><strong class="bold">Execution time</strong>: As <a id="_idIndexMarker276"/>IRQ handlers run with interrupts disabled on the local CPU, the code must be as short and small as possible and fast enough to assure a fast re-enabling of the previously disabled CPU-local interrupts in order not to miss any further occurring IRQs. Time-consuming IRQ handlers may considerably alter the real-time properties of the system and slow it down.</li>
				<li><strong class="bold">Execution context</strong>: Since <a id="_idIndexMarker277"/>interrupt handlers are executed in an atomic context, sleeping (or other mechanisms that may sleep—such as mutexes—copying data from kernel to user space or vice versa, and so on) is forbidden. Any part of the code requiring or involving sleeping must be deferred into another, safer context (that is, a process context).</li>
			</ul>
			<p>An IRQ handler <a id="_idIndexMarker278"/>needs to be given two arguments: the interrupt line to<a id="_idIndexMarker279"/> install the handler for, and a <strong class="bold">unique device ID</strong> (<strong class="bold">UDI</strong>) of the peripheral (mostly used as a context data structure—that is, a pointer to the per-device or private structure of the associated hardware device)—as illustrated here:</p>
			<p class="source-code">typedef irqreturn_t (*irq_handler_t)(int, void *);</p>
			<p>The device driver wishing to register an interrupt handler for a given IRQ should call <strong class="source-inline">devm_request_irq()</strong>, defined in <strong class="source-inline">&lt;linux/interrupt.h&gt;</strong> as follows:</p>
			<p class="source-code">devm_request_irq(struct device *dev, unsigned int irq,</p>
			<p class="source-code">                  irq_handler_t handler,</p>
			<p class="source-code">                  unsigned long irqflags, </p>
			<p class="source-code">                  onst char *devname, void *dev_id)</p>
			<p>The preceding function argument list, <strong class="source-inline">dev</strong>, is the device responsible for the IRQ line, <strong class="source-inline">irq</strong> represents the interrupt line (that is, the interrupt number of the issuing device) to register <strong class="source-inline">handler</strong> for. Prior to validating the request, the kernel will make sure the requested interrupt is valid and that it is not already assigned to another device unless both devices request this <strong class="source-inline">irq</strong> line to be shared (with help of <strong class="source-inline">flags</strong>). <strong class="source-inline">handler</strong> is the function pointer to the interrupt handler, and <strong class="source-inline">flags</strong> represent the interrupt flags. <strong class="source-inline">name</strong> is an <strong class="bold">American Standard Code for Information Interchange</strong> (<strong class="bold">ASCII</strong>) string that should namely describe the interrupt, and <strong class="source-inline">dev</strong> should be unique to each registered handler and cannot be <strong class="source-inline">NULL</strong> for shared IRQs since it is used by the kernel IRQ core to identify the device. A common way of using it is to provide a pointer to the device structure or a pointer to any per-device (and potentially useful to the handler) data structure, since when an interrupt occurs, both the interrupt line (<strong class="source-inline">irq</strong>) and this parameter will be passed to the registered handler, which can use this data as context data for further processing.</p>
			<p><strong class="source-inline">flags</strong> mangles the <a id="_idIndexMarker280"/>state or the behavior of the IRQ line or its handler by means of the following masks, which can be OR'ed to form a final desired bitmask according to your needs:</p>
			<p class="source-code">#define IRQF_SHARED 0x00000080</p>
			<p class="source-code">#define IRQF_PROBE_SHARED 0x00000100</p>
			<p class="source-code">#define IRQF_NOBALANCING 0x00000800</p>
			<p class="source-code">#define IRQF_IRQPOLL 0x00001000</p>
			<p class="source-code">#define IRQF_ONESHOT 0x00002000</p>
			<p class="source-code">#define IRQF_NO_SUSPEND 0x00004000</p>
			<p class="source-code">#define IRQF_FORCE_RESUME 0x00008000</p>
			<p class="source-code">#define IRQF_NO_THREAD 0x00010000</p>
			<p class="source-code">#define IRQF_EARLY_RESUME 0x000200002</p>
			<p class="source-code">#define IRQF_COND_SUSPEND 0x00040000</p>
			<p>Note that <strong class="source-inline">flags</strong> can be <strong class="source-inline">0</strong> as well. Let's now explain some important flags—we leave the rest for the user<a id="_idIndexMarker281"/> to explore in <strong class="source-inline">include/linux/interrupt.h</strong>, but these are the ones we'll look at in more detail:</p>
			<ul>
				<li><strong class="source-inline">IRQF_NOBALANCING</strong> excludes <a id="_idIndexMarker282"/>the interrupt from IRQ balancing, which is a mechanism that consists of distributing/relocating interrupts across CPUs, with a goal of increasing performance. It is kind of preventing the CPU affinity of that IRQ from being changed. This flag may be useful to provide a flexible setup for clock source or clock event devices, to prevent misattribution of the event to the wrong core. This flag is meaningful on multi-core systems only.</li>
				<li><strong class="source-inline">IRQF_IRQPOLL</strong>: This <a id="_idIndexMarker283"/>flag allows the implementation of an <strong class="source-inline">irqpoll</strong> mechanism, intended to fix interrupt problems, meaning this handler should be added to the list of known interrupt handlers that can be looked for when a given interrupt is not handled.</li>
				<li><strong class="source-inline">IRQF_ONESHOT</strong>: Normally, the<a id="_idIndexMarker284"/> actual interrupt line being serviced is re-enabled after its hardirq handler completes, whether it awakes a threaded handler or not. This flag keeps the interrupt line disabled after the hardirq handler finishes. It must be set on threaded interrupts (we will discuss this later) for which the interrupt line must remain disabled until the threaded handler has completed, after which it will be re-enabled.</li>
				<li><strong class="source-inline">IRQF_NO_SUSPEND</strong> does<a id="_idIndexMarker285"/> not disable the IRQ during system hibernation/suspension. It does mean the interrupt is able to wake up the system from a suspended state. Such IRQs may be timer interrupts that may trigger and need to be handled even during system suspension. The whole IRQ line is affected by this flag such that if the IRQ is shared, every registered handler for this shared line will be executed, not only the one that installed this flag. You should avoid as much as possible using <strong class="source-inline">IRQF_NO_SUSPEND</strong> and <strong class="source-inline">IRQF_SHARED</strong> at the same time.</li>
				<li><strong class="source-inline">IRQF_FORCE_RESUME</strong> enables<a id="_idIndexMarker286"/> the IRQ in the system resume path even if <strong class="source-inline">IRQF_NO_SUSPEND</strong> is set.</li>
				<li><strong class="source-inline">IRQF_NO_THREAD</strong> prevents<a id="_idIndexMarker287"/> the interrupt handler from being threaded. This flag overrides the kernel <strong class="source-inline">threadirqs</strong> command-line option that forces every interrupt to be threaded. This flag has been introduced to address the non-threadability of some interrupts (for example, timers, which cannot be threaded even when all interrupt handlers are forced to be threaded).</li>
				<li><strong class="source-inline">IRQF_TIMER</strong> marks<a id="_idIndexMarker288"/> this handler as being specific to system timer<a id="_idIndexMarker289"/> interrupts. It helps not to disable the timer IRQ during system suspension to ensure normal resumption and not thread them when full preemption (that is, <strong class="source-inline">PREEMPT_RT</strong>) is enabled. It is just an alias for <strong class="source-inline">IRQF_NO_SUSPEND | IRQF_NO_THREAD</strong>.</li>
				<li><strong class="source-inline">IRQF_EARLY_RESUME</strong> resumes<a id="_idIndexMarker290"/> IRQ early at resume time of <strong class="bold">system core</strong> (<strong class="bold">syscore</strong>) operations instead of at device resume time. The following link points to the message of the commit introducing its support: <a href="https://lkml.org/lkml/2013/11/20/89">https://lkml.org/lkml/2013/11/20/89</a>.</li>
				<li><strong class="source-inline">IRQF_SHARED</strong> allows <a id="_idIndexMarker291"/>for the sharing of the interrupt line among several devices. However, each device driver that needs to share the given interrupt line must set with this flag; otherwise, the handler registration will fail.</li>
			</ul>
			<p>We must also consider the <strong class="source-inline">irqreturn_t</strong> return type of interrupt handlers since it may involve further actions after the return of the handler. Possible return values are listed here:</p>
			<ul>
				<li><strong class="source-inline">IRQ_NONE</strong>: On a shared interrupt line, once the interrupt occurs, the kernel IRQ core successively walks through handlers registered for this line and executes them in the order they have been registered. The driver then has the responsibility to check whether it is its device that issued the interrupt. If the interrupt does not come<a id="_idIndexMarker292"/> from its device, it must return <strong class="source-inline">IRQ_NONE</strong> to instruct the kernel to call the next registered interrupt handler. This return value is mostly used on shared interrupt lines since it informs the kernel that the interrupt does not come from our device. However, if 99,900 of the previous 100,000 interrupts of a given IRQ line have not been handled, the kernel then assumes that this IRQ is stuck in some manner, drops a diagnostic, and tries to turn the IRQ off. For more information on this, you can have a look at the <strong class="source-inline">__report_bad_irq()</strong> function in the kernel source tree.</li>
				<li><strong class="source-inline">IRQ_HANDLED</strong>: This value should be returned if the interrupt has been handled successfully. On a threaded IRQ, this value does acknowledge the interrupt (at a controller level) without waking the thread handler up.</li>
				<li><strong class="source-inline">IRQ_WAKE_THREAD</strong>: On a thread IRQ handler, this value must be returned by the hard-IRQ handler to wake the handler thread. In this case, <strong class="source-inline">IRQ_HANDLED</strong> must only be returned by that threaded handler, previously registered with <strong class="source-inline">devm_request_threaded_irq()</strong>. We will discuss this later in the chapter.<p class="callout-heading">Note</p><p class="callout">You should never re-enable IRQs from within your IRQ handler as this would involve allowing "interrupt reentrancy".</p></li>
			</ul>
			<p><strong class="source-inline">devm_request_irq()</strong> is the managed version of <strong class="source-inline">request_irq()</strong>, defined as follows:</p>
			<p class="source-code">int request_irq(unsigned int irq, irq_handler_t handler,</p>
			<p class="source-code">                unsigned long flags, const char *name,</p>
			<p class="source-code">                void *dev) </p>
			<p>They both <a id="_idIndexMarker293"/>have the same variable meanings. If the driver used the managed version, the IRQ core will take care of releasing the resources. In other cases, such as at the unloading path or when the device leaves, the driver will have to release the IRQ resources by unregistering the interrupt handler using <strong class="source-inline">free_irq()</strong>, declared as follows:</p>
			<p class="source-code">void free_irq(unsigned int irq, void *dev_id)</p>
			<p><strong class="source-inline">free_irq()</strong> removes the handler (identified by <strong class="source-inline">dev_id</strong> when it comes to shared interrupts) and disables the line. If the interrupt line is shared, the handler is just removed from the list of handlers for this <strong class="source-inline">irq</strong>, and the interrupt line is disabled in the future when the last handler is removed. Moreover, if possible, your code must make sure the interrupt is really disabled on the card it drives before calling this function, since omitting this may lead to spurious IRQ.</p>
			<p>There are a few things worth mentioning here about interrupts that you should never forget, as follows:</p>
			<ul>
				<li>On Linux systems, when the handler of an IRQ is being executed by a CPU, all interrupts are disabled on that CPU and the interrupt being serviced is masked on all the other cores. This means interrupt handlers need not be reentrant because the same interrupt will never be received until the current handler has completed. However, all other interrupts but the serviced one remain enabled (or, should we say, unchanged) on other cores, so other interrupts keep being serviced, though the current line is always disabled, as well as further interrupts on the local CPU. As a result, the same interrupt handler is never invoked concurrently to service a nested interrupt. This makes writing your interrupt handler a lot easier.</li>
				<li>Critical regions that need to run with interrupts disabled should be limited as much as possible. To remember this, tell yourself that your interrupt handler has interrupted other code and needs to give the CPU back.</li>
				<li>The interrupt context has its own (fixed and quite low) stack size. It thus totally makes sense to disable IRQs while running an ISR as reentrancy could cause stack overflow if too many preemptions happen.</li>
				<li>Interrupt handlers <a id="_idIndexMarker294"/>cannot block; they do not run in a process context. Thus, you may not perform the following operations from within an interrupt handler:<ul><li>You cannot transfer data to/from user space since this may block.</li><li>You cannot sleep or rely on code that may lead to sleep, such as invoking <strong class="source-inline">wait_event()</strong>, memory allocation with any flag other than <strong class="source-inline">GFP_ATOMIC</strong>, or using a mutex/semaphore. The threaded handler can handle this.</li><li>You cannot trigger or call <strong class="source-inline">schedule()</strong>.<p class="callout-heading">Note</p><p class="callout">If a device issues an IRQ while this IRQ is disabled (or masked) at a controller level, it will not be processed at all (masked in the flow handler), but an interrupt will instantaneously occur if it is still pending (at a device level) when the IRQ is enabled (or unmasked).</p><p class="callout">The concept of non-reentrancy for an interrupt means that, if an interrupt is already in an active state, it cannot enter it again until the active status is cleared.</p></li></ul></li>
			</ul>
			<h3>Understanding the concept of top and bottom halves</h3>
			<p>External devices send interrupt requests to the CPU either to signal a particular event or request a service. As stated in the previous section, bad interrupt management may considerably increase a system's latency and decrease its real-time properties. We also stated that interrupt processing—that is, the hard-IRQ handler at least—must be very fast not only to keep the system responsive but also not to miss other interrupt events.</p>
			<p>The idea here is to split the interrupt handler into two parts. The first part (a function, actually) will run in a so-called hard-IRQ context with interrupts disabled, and will perform the minimum required work (such as doing some quick sanity checks—essentially, time-sensitive tasks, read/write hardware registers, and fast processing of this data and acknowledging interrupts to the device that raised it). This first part is the so-called <strong class="bold">top half</strong> on <a id="_idIndexMarker295"/>Linux systems. The top-half would then schedule a thread handler, which would run a <a id="_idIndexMarker296"/>so-called <strong class="bold">bottom-half</strong> function, with interrupts re-enabled, and which is the second part of the interrupt. The bottom half could then perform time-consuming operations (such as buffer processing) and tasks that may sleep, as it runs in a thread.</p>
			<p>This splitting would considerably increase system responsiveness as the time spent with IRQs disabled is reduced to its minimum, and since bottom halves are run in kernel threads, they compete for the CPU with other processes on the runqueue. Moreover, they may have their real-time properties set. The top half is actually the handler registered using <strong class="source-inline">devm_request_irq()</strong>. When using <strong class="source-inline">devm_request_threaded_irq()</strong>, as we will see in the next section, the top half is the first handler given to the function. </p>
			<p>As described previously in the <em class="italic">Implementing work-deferring mechanisms</em> section, a bottom half represents nowadays any task (or work) scheduled from within an interrupt handler. Bottom halves are designed using work-deferring mechanisms, which we have seen previously.</p>
			<p>Depending on which one you choose, it may run in a (software) interrupt context or in a process context. These are softirqs, tasklets, workqueues, and threaded IRQs.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Tasklets and softirqs have nothing to do with the "thread interrupt" mechanism since they run in their own special (atomic) contexts.</p>
			<p>Since softirq handlers run at a high priority with the scheduler preemption disabled, not relinquishing CPU to processes/threads until they complete, care must be taken while using them for bottom-half delegation. As nowadays the quantum allocated for a particular process may vary, there is no strict rule for how long the softirq handler should take to complete in order not to slow the system down as the kernel would not be able to give CPU time to other processes. I would say no longer than half a jiffy.</p>
			<p>The hard IRQ (top half, actually) must be as fast as possible, and most of the time, just reading and writing in I/O memory. Any other computation should be deferred in the bottom half, whose main goal is to perform any time-consuming and not interrupt-related work not performed by the top half. There is no clear guideline on the repartition of work between the top and bottom halves. Here is some advice:</p>
			<ul>
				<li>Hardware-related or time-sensitive work can be performed in the top half.</li>
				<li>If the work really need not be interrupted, it can be performed in the top half.</li>
				<li>From my point of view, everything else can be deferred and thus performed in the bottom half, which will run with interrupts enabled and when the system is less busy.</li>
				<li>If the hard IRQ is fast enough to process and acknowledge interrupts within a few microseconds consistently, then there is absolutely no need to use bottom-half delegations.</li>
			</ul>
			<h3>Working with threaded IRQ handlers</h3>
			<p>Threaded interrupt handlers<a id="_idIndexMarker297"/> were introduced to reduce the time spent in interrupt handlers and defer the rest of the work (that is, processing) out into kernel threads. So, the top half (hard IRQ) would consist of quick sanity checks such as ensuring whether the interrupt comes from its device and waking the bottom half accordingly. A threaded interrupt handler runs in its own thread, either in the thread of their parent (if they have one) or in a separate kernel thread. Moreover, the dedicated kernel thread can have its real-time priority set, though it runs at normal real-time priority (that is, <strong class="source-inline">MAX_USER_RT_PRIO/2</strong>, as you can see in the <strong class="source-inline">setup_irq_thread()</strong> function in <strong class="source-inline">kernel/irq/manage.c</strong>).</p>
			<p>The general rule behind threaded interrupts is simple: keep the hard-IRQ handler as minimal as possible and defer as much work to the kernel thread as possible (preferably, all work). You should use <strong class="source-inline">devm_request_threaded_irq()</strong> if you wish to request a threaded interrupt handling. Here is its prototype:</p>
			<p class="source-code">devm_request_threaded_irq(struct device *dev, unsigned int irq,</p>
			<p class="source-code">                  irq_handler_t handler, irq_handler_t thread_fn,</p>
			<p class="source-code">                  unsigned long irqflags, const char *devname,</p>
			<p class="source-code">                  void *dev_id);</p>
			<p>This function accepts two special parameters on which we should spend some time, <strong class="source-inline">handler</strong>, and <strong class="source-inline">thread_fn</strong>. They are outlined in more detail here:</p>
			<ul>
				<li><strong class="source-inline">handler</strong> immediately<a id="_idIndexMarker298"/> runs when the interrupt occurs, in an interrupt context, and acts as a hard-IRQ handler. Its job usually consists of reading the interrupt cause (in the device's status register) to determine whether or how to handle the interrupt (this is frequent <a id="_idIndexMarker299"/>on <strong class="bold">memory-mapped I/O</strong> (<strong class="bold">MMIO</strong>) devices). If the interrupt does not come from our device, this function should return <strong class="source-inline">IRQ_NONE</strong>. This return value usually only makes sense on shared interrupt lines.</li>
			</ul>
			<p>If this hard-IRQ handler can finish interrupt processing fast enough (this is not a universal rule, but let's say no longer than a half of jiffy—that is, not longer than 500 µs if <strong class="source-inline">CONFIG_HZ</strong>, which defines the value of a jiffy, is set to <strong class="source-inline">1000</strong>) for some set of interrupt causes, it should return <strong class="source-inline">IRQ_HANDLED</strong> after processing in order to acknowledge the interrupts. Interrupt processing that does not fall in this time lapse should be deferred in the thread IRQ handlers. In this case, the hard-IRQ handler should return <strong class="source-inline">IRQ_WAKE_THREAD</strong> to awake the threaded handler. Returning <strong class="source-inline">IRQ_WAKE_THREAD</strong> makes sense only when the <strong class="source-inline">thread_fn</strong> handler is also provided.</p>
			<ul>
				<li><strong class="source-inline">thread_fn</strong> is the threaded handler added to the scheduler runqueue when the hard-IRQ handler function returns <strong class="source-inline">IRQ_WAKE_THREAD</strong>. If <strong class="source-inline">thread_fn</strong> is <strong class="source-inline">NULL</strong> while the handler is set and returns <strong class="source-inline">IRQ_WAKE_THREAD</strong>, nothing happens at the return path of the hard-IRQ handler but a simple warning message (we can see that in the <strong class="source-inline">__irq_wake_thread()</strong> function in the kernel sources). As <strong class="source-inline">thread_fn</strong> competes for the CPU with other processes on the runqueue, it may be executed immediately or later in the future when the system has less load. This function should return <strong class="source-inline">IRQ_HANDLED</strong> when it has completed the interrupt handling. After that, the associated kernel thread will be taken off the runqueue and put in a blocked state until woken up again by the hard-IRQ function.</li>
			</ul>
			<p>A default hard-IRQ handler will<a id="_idIndexMarker300"/> be installed by the kernel if <strong class="source-inline">handler</strong> is <strong class="source-inline">NULL</strong> and <strong class="source-inline">thread_fn != NULL</strong>. This is the default primary handler. It does nothing but return <strong class="source-inline">IRQ_WAKE_THREAD</strong> to wake up the associated kernel thread that will execute the <strong class="source-inline">thread_fn</strong> handler.</p>
			<p>It is implemented as follows:</p>
			<p class="source-code">/* Default primary interrupt handler for threaded</p>
			<p class="source-code"> * interrupts. Assigned as primary handler when</p>
			<p class="source-code"> * request_threaded_irq is called with handler == NULL.</p>
			<p class="source-code"> * Useful for oneshot interrupts.</p>
			<p class="source-code"> */</p>
			<p class="source-code">static irqreturn_t irq_default_primary_handler(int irq,</p>
			<p class="source-code">                                         void *dev_id)</p>
			<p class="source-code">{</p>
			<p class="source-code">    return IRQ_WAKE_THREAD;</p>
			<p class="source-code">}</p>
			<p class="source-code">int request_threaded_irq(unsigned int irq,</p>
			<p class="source-code">          irq_handler_t handler, irq_handler_t thread_fn,</p>
			<p class="source-code">          unsigned long irqflags, const char *devname,</p>
			<p class="source-code">          void *dev_id)</p>
			<p class="source-code">{</p>
			<p class="source-code">[...]</p>
			<p class="source-code">    if (!handler) {</p>
			<p class="source-code">        if (!thread_fn)</p>
			<p class="source-code">            return -EINVAL;</p>
			<p class="source-code">        handler = irq_default_primary_handler;</p>
			<p class="source-code">    }</p>
			<p class="source-code">[...]</p>
			<p class="source-code">}</p>
			<p class="source-code">EXPORT_SYMBOL(request_threaded_irq);</p>
			<p>This makes it possible to move the execution of interrupt handlers entirely to the process context, thus preventing buggy drivers (buggy IRQ handlers, actually) from breaking the whole system and reducing interrupt latency.</p>
			<p>In new kernel releases, <strong class="source-inline">request_irq()</strong> simply wraps <strong class="source-inline">request_threaded_irq()</strong> with the <strong class="source-inline">thread_fn</strong> parameter set to <strong class="source-inline">NULL</strong> (the same goes for the <strong class="source-inline">devm_</strong> variant).</p>
			<p>Note that the interrupt<a id="_idIndexMarker301"/> is acknowledged at an interrupt controller level when you return from the hard-IRQ handler (whatever the return value is), thus allowing you to take other interrupts into account. In such a situation, if the interrupt hasn't been acknowledged at the device level, the interrupt will fire again and again, resulting in stack overflows (or being stuck in the hard-IRQ handler forever) for level-triggered interrupts since the issuing device will still have the interrupt line asserted.</p>
			<p>For threaded interrupt implementation, when drivers needed to run the bottom half in a thread, they had to mask the interrupt at device level from the hard-interrupt handler. This required accessing the issuing device, which is not, however, always possible for devices sitting on slow buses (such I2C <a id="_idIndexMarker302"/>or <strong class="bold">Serial Peripheral Interface</strong> (<strong class="bold">SPI</strong>) buses, for example) because such buses require a thread context. With the introduction of <strong class="source-inline">IRQF_ONESHOT</strong>, this operation is not mandatory anymore as it helps keep the IRQ disabled at a controller level even when the threaded handler runs. Drivers must, however, clear the device interrupt in the threaded handler before it completes.</p>
			<p>Using <strong class="source-inline">devm_request_threaded()</strong> (or the non-managed variant), it is possible to request an exclusively threaded IRQ by omitting the hard-interrupt handler. In this case, it is mandatory to set the <strong class="source-inline">IRQF_ONESHOT</strong> flag, else the kernel will complain because the threaded handler would run with the interrupt unmasked at both device and controller levels.</p>
			<p>Here is an<a id="_idIndexMarker303"/> example of this:</p>
			<p class="source-code">static irqreturn_t data_event_handler(int irq,</p>
			<p class="source-code">                                      void *dev_id)</p>
			<p class="source-code">{</p>
			<p class="source-code">    struct big_structure *bs = dev_id;</p>
			<p class="source-code">    clear_device_interupt(bs);</p>
			<p class="source-code">    process_data(bs-&gt;buffer);</p>
			<p class="source-code">    return IRQ_HANDLED;</p>
			<p class="source-code">}</p>
			<p class="source-code">static int my_probe(struct i2c_client *client)</p>
			<p class="source-code">{</p>
			<p class="source-code">[...]</p>
			<p class="source-code">    if (client-&gt;irq &gt; 0) {</p>
			<p class="source-code">        ret = request_threaded_irq(client-&gt;irq, NULL, </p>
			<p class="source-code">                &amp;data_event_handler,</p>
			<p class="source-code">                IRQF_TRIGGER_LOW | IRQF_ONESHOT,</p>
			<p class="source-code">                id-&gt;name, private);</p>
			<p class="source-code">        if (ret)</p>
			<p class="source-code">            goto error_irq;</p>
			<p class="source-code">    }</p>
			<p class="source-code">...</p>
			<p class="source-code">    return 0;</p>
			<p class="source-code">error_irq:</p>
			<p class="source-code">    do_cleanup();</p>
			<p class="source-code">    return ret;</p>
			<p class="source-code">}</p>
			<p>In the preceding example, our device sits on an I2C bus, so accessing the device may put the underlying task to sleep. Such an operation must never be performed in the hard-interrupt handler. </p>
			<p>Here is an<a id="_idIndexMarker304"/> excerpt from the message in the link that introduced the <strong class="source-inline">IRQF_ONESHOT</strong> flag and which explains what it does (the whole message can be found via this link: <a href="http://lkml.iu.edu/hypermail/linux/kernel/0908.1/02114.html">http://lkml.iu.edu/hypermail/linux/kernel/0908.1/02114.html</a>):</p>
			<p>"<em class="italic">It allows drivers to request that the interrupt is not unmasked (at controller level) after the hard interrupt context handler has been executed and the thread has been woken. The interrupt line is unmasked after the thread handler function has been executed</em>."</p>
			<p>If one driver has set either <strong class="source-inline">IRQF_SHARED</strong> or <strong class="source-inline">IRQF_ONESHOT</strong> flags on a given IRQ, then the other driver sharing the IRQ must set the same flags. The <strong class="source-inline">/proc/interrupts</strong> file lists IRQs with their number of processing per CPU, the IRQ name as given during the requesting step, and a comma-separated list of drivers that registered a handler for that interrupt.</p>
			<p>Threading the IRQs is the best choice for interrupt processing that can hog too many CPU cycles (exceeding a jiffy, for example), such as bulk data processing. Threading IRQs allows the priority and CPU affinity of their associated threads to be managed individually. As this concept comes from the real-time kernel tree, it fulfills many requirements of a real-time system, such as allowing a fine-grained priority model and reducing interrupt latency in the kernel. You can have a look at <strong class="source-inline">/proc/irq/&lt;IRQ&gt;/smp_affinity</strong>, which can be used to get or set the corresponding <strong class="source-inline">&lt;IRQ&gt;</strong> affinity. This file returns and accepts a bitmask that represents which processors can handle ISRs registered for this IRQ. This way, you can—for example—decide to set the affinity of the hard-interrupt handler to one CPU, while setting the affinity of the threaded handler to another CPU.</p>
			<h3>Requesting a context-agnostic IRQ</h3>
			<p>A driver<a id="_idIndexMarker305"/> requesting an IRQ must know in advance the nature of the interrupt and decide whether its handler can run in a hard-IRQ context or not, which may influence the choice between <strong class="source-inline">devm_request_irq()</strong> and <strong class="source-inline">devm_request_threaded_irq()</strong>.</p>
			<p>The problem with those approaches is that sometimes, a driver requesting an IRQ does not know about the nature of the interrupt controller that provides this IRQ line, especially when the interrupt controller is a discrete chip (typically, a <strong class="bold">general-purpose I/O</strong> (<strong class="bold">GPIO</strong>) expander connected over SPI or I2C buses). Now comes the <strong class="source-inline">request_any_context_irq()</strong>, function with which drivers requesting an IRQ will know whether the handler will run in a thread context or not, and call <strong class="source-inline">request_threaded_irq()</strong> or <strong class="source-inline">request_irq()</strong> accordingly. This means that whether the IRQ associated with our device comes from an interrupt controller that may not sleep (a memory-mapped one) or from one that can sleep (behind an I2C/SPI bus), there will be no need to change the code. Its prototype looks like this:</p>
			<p class="source-code">int request_any_context_irq(unsigned int irq,</p>
			<p class="source-code">                irq_handler_t handler, unsigned long flags,</p>
			<p class="source-code">                const char *name, void *dev_id)</p>
			<p><strong class="source-inline">devm_request_any_context_irq()</strong> and <strong class="source-inline">devm_request_irq()</strong> have the same interface but different semantics. Depending on the underlying context (the hardware platform), <strong class="source-inline">devm_request_any_context_irq()</strong> selects either a hard-interrupt handling using <strong class="source-inline">request_irq()</strong> or a threaded handling method using <strong class="source-inline">request_threaded_irq()</strong>. It returns a negative error value on failure, while on success, it returns either <strong class="source-inline">IRQC_IS_HARDIRQ</strong> (meaning a hard-interrupt handling method is used) or <strong class="source-inline">IRQC_IS_NESTED</strong> (meaning a threaded one is used). With this function, the behavior of the interrupt handler is decided at runtime. For more information, you can have a look at the commit introducing it in the kernel by following this link: <a href="https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=ae731f8d0785">https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=ae731f8d0785</a>.</p>
			<p>The advantage of using <strong class="source-inline">devm_request_any_context_irq()</strong> is that the driver does not need to care about what can be done in the IRQ handler, as the context in which the handler will run depends on the interrupt controller that provides the IRQ line. For example, for a GPIO-IRQ based device driver, if the GPIO belongs to a controller that sits on an I2C or SPI bus (GPIO access may sleep), the handler will be threaded. Otherwise (that is, the GPIO access does not sleep and is memory-mapped as it is part of the SoC), the handler will run in a hard-IRQ handler.</p>
			<p>In the <a id="_idIndexMarker306"/>following example, the device expects an IRQ line mapped to a GPIO. The driver cannot assume that the given GPIO line will be memory-mapped, coming from the SoC. It may come from a discrete I2C or SPI GPIO controller as well. A good practice would be to use <strong class="source-inline">request_any_context_irq()</strong> here:</p>
			<p class="source-code">static irqreturn_t packt_btn_interrupt(int irq,</p>
			<p class="source-code">                                       void *dev_id)</p>
			<p class="source-code">{</p>
			<p class="source-code">    struct btn_data *priv = dev_id;</p>
			<p class="source-code">    input_report_key(priv-&gt;i_dev, BTN_0,</p>
			<p class="source-code">        gpiod_get_value(priv-&gt;btn_gpiod) &amp; 1);</p>
			<p class="source-code">    input_sync(priv-&gt;i_dev);</p>
			<p class="source-code">    return IRQ_HANDLED;</p>
			<p class="source-code">}</p>
			<p class="source-code">static int btn_probe(struct platform_device *pdev)</p>
			<p class="source-code">{</p>
			<p class="source-code">    struct gpio_desc *gpiod;</p>
			<p class="source-code">    int ret, irq;</p>
			<p class="source-code">    gpiod = gpiod_get(&amp;pdev-&gt;dev, "button", GPIOD_IN);</p>
			<p class="source-code">    if (IS_ERR(gpiod))</p>
			<p class="source-code">        return -ENODEV;</p>
			<p class="source-code">    priv-&gt;irq = gpiod_to_irq(priv-&gt;btn_gpiod);</p>
			<p class="source-code">    priv-&gt;btn_gpiod = gpiod;</p>
			<p class="source-code">[...]</p>
			<p class="source-code">    ret = request_any_context_irq(priv-&gt;irq,</p>
			<p class="source-code">              packt_btn_interrupt,</p>
			<p class="source-code">             (IRQF_TRIGGER_FALLING | IRQF_TRIGGER_RISING),</p>
			<p class="source-code">             "packt-input-button", priv);</p>
			<p class="source-code">    if (ret &lt; 0)</p>
			<p class="source-code">        goto err_btn;</p>
			<p class="source-code">    return 0;</p>
			<p class="source-code">err_btn:</p>
			<p class="source-code">    do_cleanup();</p>
			<p class="source-code">    return ret;</p>
			<p class="source-code">}</p>
			<p>The <a id="_idIndexMarker307"/>preceding code is simple enough but quite safe since <strong class="source-inline">devm_request_any_context_irq()</strong> does the job, which prevents it from mistaking the type of the underlying GPIO. The advantage of this approach is that you do not need to care about the nature of the interrupt controller that provides the IRQ line. In our example, if the GPIO belongs to a controller sitting on an I2C or SPI bus, the handler will be threaded. Otherwise (memory-mapped), the handler will run in a hard-IRQ context.</p>
			<h3>Using a workqueue to defer the bottom half</h3>
			<p>As we have<a id="_idIndexMarker308"/> already discussed the workqueue API in a dedicated section, it is now preferable to give an example here. This example is not error-free and has not been tested. It is just a demonstration to highlight the concept of bottom-half deferring by means of a workqueue.</p>
			<p>Let's start by defining a data structure that will hold the elements we need for further development, as follows:</p>
			<p class="source-code">struct private_struct {</p>
			<p class="source-code">    int counter;</p>
			<p class="source-code">    struct work_struct my_work;</p>
			<p class="source-code">    void __iomem *reg_base;</p>
			<p class="source-code">    spinlock_t lock;</p>
			<p class="source-code">    int irq;</p>
			<p class="source-code">    /* Other fields */</p>
			<p class="source-code">    [...]</p>
			<p class="source-code">};</p>
			<p>In the preceding data structure, our work structure is represented by the <strong class="source-inline">my_work</strong> element. We do not use a pointer here because we will need to use a <strong class="source-inline">container_of()</strong> macro in order to grab back the pointer to the initial data structure. Next, we can define a method<a id="_idIndexMarker309"/> that will be invoked in the worker thread, as follows:</p>
			<p class="source-code">static void work_handler(struct work_struct *work)</p>
			<p class="source-code">{</p>
			<p class="source-code">    int i;</p>
			<p class="source-code">    unsigned long flags;</p>
			<p class="source-code">    struct private_data *my_data =</p>
			<p class="source-code">          container_of(work, struct private_data, my_work);</p>
			<p class="source-code">    /*</p>
			<p class="source-code">     * Processing at least half of MIN_REQUIRED_FIFO_SIZE</p>
			<p class="source-code">     * prior to re-enabling the irq at device level,</p>
			<p class="source-code">     * so that buffer can receive further data</p>
			<p class="source-code">     */</p>
			<p class="source-code">    for (i = 0, i &lt; MIN_REQUIRED_FIFO_SIZE, i++) {</p>
			<p class="source-code">        device_pop_and_process_data_buffer();</p>
			<p class="source-code">        if (i == MIN_REQUIRED_FIFO_SIZE / 2)</p>
			<p class="source-code">            enable_irq_at_device_level(my_data);</p>
			<p class="source-code">    }</p>
			<p class="source-code">    spin_lock_irqsave(&amp;my_data-&gt;lock, flags);</p>
			<p class="source-code">    my_data-&gt;buf_counter -= MIN_REQUIRED_FIFO_SIZE;</p>
			<p class="source-code">    spin_unlock_irqrestore(&amp;my_data-&gt;lock, flags);</p>
			<p class="source-code">}</p>
			<p>In the preceding <a id="_idIndexMarker310"/>work structure, we start data processing when enough data has been buffered. We can now provide our IRQ handler, which is responsible for scheduling our work, as follows:</p>
			<p class="source-code">/* This is our hard-IRQ handler. */</p>
			<p class="source-code">static irqreturn_t my_interrupt_handler(int irq,</p>
			<p class="source-code">                                        void *dev_id)</p>
			<p class="source-code">{</p>
			<p class="source-code">    u32 status;</p>
			<p class="source-code">    unsigned long flags;</p>
			<p class="source-code">    struct private_struct *my_data = dev_id;</p>
			<p class="source-code">    /* we read the status register to know what to do */</p>
			<p class="source-code">    status = readl(my_data-&gt;reg_base + REG_STATUS_OFFSET);</p>
			<p class="source-code">    /*</p>
			<p class="source-code">     * Ack irq at device level. We are safe if another</p>
			<p class="source-code">     * irq pokes since it is disabled at controller</p>
			<p class="source-code">     * level while we are in this handler</p>
			<p class="source-code">     */</p>
			<p class="source-code">    writel(my_data-&gt;reg_base + REG_STATUS_OFFSET,</p>
			<p class="source-code">            status | MASK_IRQ_ACK);</p>
			<p class="source-code">    /*</p>
			<p class="source-code">     * Protecting the shared resource, since the worker</p>
			<p class="source-code">     * also accesses this counter</p>
			<p class="source-code">     */</p>
			<p class="source-code">    spin_lock_irqsave(&amp;my_data-&gt;lock, flags);</p>
			<p class="source-code">    my_data-&gt;buf_counter++;</p>
			<p class="source-code">    spin_unlock_irqrestore(&amp;my_data-&gt;lock, flags);</p>
			<p class="source-code">    /*</p>
			<p class="source-code">     * Our device raised an interrupt to inform it has</p>
			<p class="source-code">     * new data in its fifo. But is it enough for us</p>
			<p class="source-code">     * to be processed ?</p>
			<p class="source-code">     */</p>
			<p class="source-code">    if (my_data-&gt;buf_counter != MIN_REQUIRED_FIFO_SIZE)) {</p>
			<p class="source-code">       /* ack and re-enable this irq at controller level */</p>
			<p class="source-code">       return IRQ_HANDLED;</p>
			<p class="source-code">    } else {</p>
			<p class="source-code">        /* Right. prior to scheduling the worker and</p>
			<p class="source-code">         * returning from this handler, we need to</p>
			<p class="source-code">         * disable the irq at device level</p>
			<p class="source-code">         */</p>
			<p class="source-code">        writel(my_data-&gt;reg_base + REG_STATUS_OFFSET,</p>
			<p class="source-code">                MASK_IRQ_DISABLE);</p>
			<p class="source-code">        schedule_work(&amp;my_work);</p>
			<p class="source-code">    }</p>
			<p class="source-code">    /* This will re-enable the irq at controller level */</p>
			<p class="source-code">    return IRQ_HANDLED;</p>
			<p class="source-code">};</p>
			<p>The comments in the IRQ handler code are meaningful enough. <strong class="source-inline">schedule_work()</strong> is the function that schedules our work. Finally, we can write our probe method that will request our IRQ and<a id="_idIndexMarker311"/> register the previous handler, as follows:</p>
			<p class="source-code">static int foo_probe(struct platform_device *pdev)</p>
			<p class="source-code">{</p>
			<p class="source-code">    struct resource *mem;</p>
			<p class="source-code">    struct private_struct *my_data;</p>
			<p class="source-code">    my_data = alloc_some_memory(</p>
			<p class="source-code">                        sizeof(struct private_struct));</p>
			<p class="source-code">    mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);</p>
			<p class="source-code">    my_data-&gt;reg_base = ioremap(ioremap(mem-&gt;start,</p>
			<p class="source-code">                                resource_size(mem)););</p>
			<p class="source-code">    if (IS_ERR(my_data-&gt;reg_base))</p>
			<p class="source-code">        return PTR_ERR(my_data-&gt;reg_base);</p>
			<p class="source-code">    /*</p>
			<p class="source-code">     * workqueue initialization. "work_handler" is</p>
			<p class="source-code">     * the callback that will be executed when our work</p>
			<p class="source-code">     * is scheduled.</p>
			<p class="source-code">     */</p>
			<p class="source-code">    INIT_WORK(&amp;my_data-&gt;my_work, work_handler);</p>
			<p class="source-code">    spin_lock_init(&amp;my_data-&gt;lock);</p>
			<p class="source-code">    my_data-&gt;irq = platform_get_irq(pdev, 0);</p>
			<p class="source-code">    if (devm_request_irq(&amp;pdev-&gt;dev, my_data-&gt;irq,</p>
			<p class="source-code">                        my_interrupt_handler, 0,</p>
			<p class="source-code">                        pdev-&gt;name, my_data))</p>
			<p class="source-code">        handler_this_error()</p>
			<p class="source-code">    return 0;</p>
			<p class="source-code">}</p>
			<p>The structure of<a id="_idIndexMarker312"/> the preceding probe method shows without a doubt that we are facing a platform device driver. Generic IRQ and workqueue APIs have been used here for initializing our workqueue and registering our handler.</p>
			<h3>Locking from within an interrupt handler</h3>
			<p>It is <a id="_idIndexMarker313"/>common to use spinlocks on SMP systems, as this guarantees mutual exclusion at the CPU level. Therefore, if a resource is shared only with a threaded bottom half (that is, it is never accessed from the hard IRQ), it is better to use mutexes, as we see in the following example:</p>
			<p class="source-code">static int my_probe(struct platform_device *pdev)</p>
			<p class="source-code">{</p>
			<p class="source-code">    int irq;</p>
			<p class="source-code">    int ret;</p>
			<p class="source-code">    irq = platform_get_irq(pdev, i);</p>
			<p class="source-code">    ret = devm_request_threaded_irq(&amp;pdev-&gt;dev, irq, NULL,</p>
			<p class="source-code">                my_threaded_irq, IRQF_ONESHOT,</p>
			<p class="source-code">                dev_name(dev), my_data);</p>
			<p class="source-code">[...]</p>
			<p class="source-code">    return 0;</p>
			<p class="source-code">}</p>
			<p class="source-code">static irqreturn_t my_threaded_irq(int irq, void *dev_id)</p>
			<p class="source-code">{</p>
			<p class="source-code">    struct priv_struct *my_data = dev_id;</p>
			<p class="source-code">    /* Save FIFO Underrun &amp; Transfer Error status */</p>
			<p class="source-code">    mutex_lock(&amp;my_data-&gt;fifo_lock);</p>
			<p class="source-code">    /*</p>
			<p class="source-code">     * Accessing the device's buffer through i2c</p>
			<p class="source-code">     */</p>
			<p class="source-code">    device_get_i2c_buffer_and_push_to_fifo();</p>
			<p class="source-code">    mutex_unlock(&amp;ldev-&gt;fifo_lock);</p>
			<p class="source-code">    return IRQ_HANDLED;</p>
			<p class="source-code">}</p>
			<p>However, if the shared resource is accessed from within the hard-interrupt handler, you must <a id="_idIndexMarker314"/>use the <strong class="source-inline">_irqsave</strong> variant of the spinlock, as in the following example, starting with the probe method:</p>
			<p class="source-code">static int my_probe(struct platform_device *pdev)</p>
			<p class="source-code">{</p>
			<p class="source-code">    int irq;</p>
			<p class="source-code">    int ret;</p>
			<p class="source-code">    [...]</p>
			<p class="source-code">    irq = platform_get_irq(pdev, 0);</p>
			<p class="source-code">    if (irq &lt; 0)</p>
			<p class="source-code">        goto handle_get_irq_error;</p>
			<p class="source-code">    ret = devm_request_threaded_irq(&amp;pdev-&gt;dev, irq,</p>
			<p class="source-code">                    hard_handler, threaded_handler, </p>
			<p class="source-code">                    IRQF_ONESHOT, dev_name(dev), my_data);</p>
			<p class="source-code">    if (ret &lt; 0)</p>
			<p class="source-code">        goto err_cleanup_irq;</p>
			<p class="source-code">     [...]</p>
			<p class="source-code">    return 0;</p>
			<p class="source-code">}</p>
			<p>Now that the probe method has been implemented, let's implement the top half—that is, the<a id="_idIndexMarker315"/> hard-IRQ handler—as follows:</p>
			<p class="source-code">static irqreturn_t hard_handler(int irq, void *dev_id)</p>
			<p class="source-code">{</p>
			<p class="source-code">    struct priv_struct *my_data = dev_id;</p>
			<p class="source-code">    u32 status;</p>
			<p class="source-code">    unsigned long flags;</p>
			<p class="source-code">    /* Protecting the shared resource */</p>
			<p class="source-code">    spin_lock_irqsave(&amp;my_data-&gt;lock, flags);</p>
			<p class="source-code">    my_data-&gt;status = __raw_readl(</p>
			<p class="source-code">            my_data-&gt;mmio_base + my_data-&gt;foo.reg_offset);</p>
			<p class="source-code">    spin_unlock_irqrestore(&amp;my_data-&gt;lock, flags);</p>
			<p class="source-code">    /* Let us schedule the bottom-half */</p>
			<p class="source-code">    return IRQ_WAKE_THREAD;</p>
			<p class="source-code">}</p>
			<p>The return value of the top half will wake the threaded bottom half, which is implemented as follows:</p>
			<p class="source-code">static irqreturn_t threaded_handler(int irq, void *dev_id)</p>
			<p class="source-code">{</p>
			<p class="source-code">    struct priv_struct *my_data = dev_id;</p>
			<p class="source-code">    spin_lock_irqsave(&amp;my_data-&gt;lock, flags);</p>
			<p class="source-code">    /* doing sanity depending on the status */</p>
			<p class="source-code">    process_status(my_data-&gt;status);</p>
			<p class="source-code">    spin_unlock_irqrestore(&amp;my_data-&gt;lock, flags);</p>
			<p class="source-code">    /*</p>
			<p class="source-code">     * content of status not needed anymore, let's do</p>
			<p class="source-code">     * some other work</p>
			<p class="source-code">     */</p>
			<p class="source-code">     [...]</p>
			<p class="source-code">    return IRQ_HANDLED;</p>
			<p class="source-code">}</p>
			<p>There<a id="_idIndexMarker316"/> is a case where protection may not be necessary between the hard IRQ and its threaded counterpart when the <strong class="source-inline">IRQF_ONESHOT</strong> flag is set while requesting the IRQ line. This flag keeps the interrupt disabled after the hard-interrupt handler has finished. With this flag set, the IRQ line is disabled until the threaded handler has been run. This way, the hard handler and its threaded counterpart will never compete, and a lock for a resource shared between the two might not be necessary.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor059"/>Summary</h1>
			<p>In this chapter, we discussed the fundamental elements to start driver development, presenting all the mechanisms frequently used in drivers such as work scheduling and time management, interrupt handling, and locking primitives. This chapter is very important since it discusses topics other chapters in this book rely on. </p>
			<p>For instance, the next chapter, dealing with character devices, will use some of the elements discussed in this chapter.</p>
		</div>
	</body></html>