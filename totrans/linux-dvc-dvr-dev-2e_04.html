<html><head></head><body>
		<div><h1 id="_idParaDest-39"><em class="italic"><a id="_idTextAnchor039"/>Chapter 3</em>: Dealing with Kernel Core Helpers</h1>
			<p>The Linux kernel<a id="_idIndexMarker133"/> is a standalone piece of software—as you'll see in this chapter—that does not depend on any external library as it implements any functionalities it needs to use (from list management to compression algorithms, everything is implemented from scratch). It implements any mechanism you may encounter in modern libraries and even more, such as compression, string functions, and so on. We will walk step by step through the most important aspects of such capabilities.</p>
			<p> In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Linux kernel locking mechanisms and shared resources</li>
				<li>Dealing with kernel waiting, sleeping, and delay mechanisms</li>
				<li>Understanding Linux kernel time management</li>
				<li>Implementing work-deferring mechanisms</li>
				<li>Kernel interrupt handling</li>
			</ul>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor040"/>Linux kernel locking mechanisms and shared resources</h1>
			<p>A resource<a id="_idIndexMarker134"/> is said to be shared when it is accessible by several contenders, whether exclusively or not. When it is exclusive, access must be synchronized so that only the allowed contender(s) may own the resource. Such resources might be memory locations or<a id="_idIndexMarker135"/> peripheral devices, and the contenders might be processors, processes, or threads. The operating system performs mutual exclusion by <a id="_idIndexMarker136"/>atomically modifying a variable that holds the current state of the resource, making this visible to all contenders that might access the variable at the same time. Atomicity guarantees the modification to be entirely successful, or not successful at all. Modern operating systems nowadays rely on hardware (which should allow atomic operations) to implement synchronization, though a simple system may ensure atomicity by disabling interrupts (and avoiding scheduling) around the critical code section.</p>
			<p>We can enumerate two synchronization mechanisms, as follows:</p>
			<ul>
				<li><strong class="bold">Locks</strong>: Used <a id="_idIndexMarker137"/>for mutual exclusion. When one contender holds the lock, no other can hold it (others are excluded). The most known locks in the kernel are spinlocks and mutexes.</li>
				<li><strong class="bold">Conditional variables</strong>: For<a id="_idIndexMarker138"/> waiting for a change. These are implemented differently in the kernel, as we will see later.</li>
			</ul>
			<p>When it comes to locking, it is up to the hardware to allow such synchronizations by means of atomic operations, which the kernel uses to implement locking facilities. Synchronization <a id="_idIndexMarker139"/>primitives are data structures used for coordinating access to shared resources. Because only one contender can hold the lock (and thus access the shared resource), it might perform an arbitrary operation on the resource associated with the lock, which would appear to be atomic to others.</p>
			<p>Apart from dealing with the <a id="_idIndexMarker140"/>exclusive ownership of a given shared resource, there are situations where it is better to wait for the state of the resource to change—for example, waiting for a list to contain at least one object (its state then passes from empty to not empty) or for a task to complete (a <strong class="bold">direct memory access</strong> (<strong class="bold">DMA</strong>) transaction, for example). The Linux kernel does<a id="_idIndexMarker141"/> not implement conditional variables. From user space, we could think of conditional variables for both situations, but to achieve the same or even better, the kernel provides the following mechanisms:</p>
			<ul>
				<li><strong class="bold">Wait queue</strong>: To <a id="_idIndexMarker142"/>wait for a change—designed to work in concert with locks</li>
				<li><strong class="bold">Completion queue</strong>: To wait <a id="_idIndexMarker143"/>for the completion of a given computation, mostly used with DMAs</li>
			</ul>
			<p>All the aforementioned mechanisms are supported by the Linux kernel and exposed to drivers by means of a reduced set of <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>) (which significantly ease their use for developers), which we will discuss in the coming sections.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/>Spinlocks</h2>
			<p>A spinlock<a id="_idIndexMarker144"/> is a hardware-based locking primitive that depends on hardware capabilities to provide atomic operations (such as <code>test_and_set</code>, which in a non-atomic implementation would result in read, modify, and write operations). It is the simplest and the base locking primitive, working as described in the following scenario. </p>
			<p>When <em class="italic">CPUB</em> is running, and task <em class="italic">B</em> wants to acquire the spinlock (task <em class="italic">B</em> calls the spinlock's locking function), and this <a id="_idIndexMarker145"/>spinlock is already held by another <code>while</code> loop (thus blocking task <em class="italic">B</em>) until the other CPU releases the lock (task <em class="italic">A</em> calls the spinlock's release function). This spinning will only happen on multi-core machines (hence the use case described previously, involving more than one CPU) because, on a single-core machine, it cannot happen (the task either holds the spinlock and proceeds or never runs until the lock is released). A spinlock is said to be a lock held by a CPU, in contrast to a mutex (which we will discuss in the next section of this chapter), which is a lock held by a task.</p>
			<p>A spinlock operates by disabling the scheduler on the local CPU (that is, the CPU running the task that called the spinlock's locking API). This also means that a task currently running on that CPU cannot be preempted except <a id="_idIndexMarker146"/>by <strong class="bold">interrupt requests</strong> (<strong class="bold">IRQs</strong>) if they are not disabled on the local CPU (more on this later). In other words, spinlocks protect resources that only one CPU can take/access at a time. This makes spinlocks suitable<a id="_idIndexMarker147"/> for <strong class="bold">symmetrical multiprocessing</strong> (<strong class="bold">SMP</strong>) safety and for executing atomic tasks.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Not only do spinlocks take advantage of hardware atomic functions. In the Linux kernel, for example, preemption status depends on a per-CPU variable that, if it equals <code>0</code>, means preemption is enabled; if it is greater than 0, this means preemption is disabled (<code>schedule()</code> becomes inoperative). Thus, disabling preemption (<code>preempt_disable()</code>) consists of adding 1 to the current per-CPU variable (<code>preempt_count</code>, actually), while <code>preempt_enable()</code> subtracts <code>1</code> from the variable, checks whether the new value is <code>0</code>, and calls <code>schedule()</code>. Those addition/subtraction operations are atomic and thus rely on the CPU being able to provide atomic addition/subtraction functions.</p>
			<p>A <a id="_idIndexMarker148"/>spinlock is created either statically using a <code>DEFINE_SPINLOCK</code> macro, as illustrated here, or dynamically by calling <code>spin_lock_init()</code> on an uninitialized spinlock:</p>
			<pre>static DEFINE_SPINLOCK(my_spinlock);</pre>
			<p>To understand how this works, we just must look at the definition of this macro in <code>include/linux/spinlock_types.h</code>, as follows:</p>
			<pre>#define DEFINE_SPINLOCK(x) spinlock_t x = \
                                 __SPIN_LOCK_UNLOCKED(x)</pre>
			<p>This can be used as follows:</p>
			<pre>static DEFINE_SPINLOCK(foo_lock);</pre>
			<p>After this, the spinlock will be accessible through its name <code>foo_lock</code>, and its address would be <code>&amp;foo_lock</code>.</p>
			<p>However, for dynamic (runtime) allocation, it's better to embed the spinlock into a bigger structure, allocating memory for this structure and then calling <code>spin_lock_init()</code> on the spinlock element, as illustrated in the following code snippet:</p>
			<pre>struct bigger_struct {
    spinlock_t lock;
    unsigned int foo;
    [...]
};
static struct bigger_struct *fake_init_function()
{
    struct bigger_struct *bs;
    bs = kmalloc(sizeof(struct bigger_struct), GFP_KERNEL);
    if (!bs)
        return -ENOMEM;
    spin_lock_init(&amp;bs-&gt;lock);
    return bs;
}</pre>
			<p>It's<a id="_idIndexMarker149"/> better to use <code>DEFINE_SPINLOCK</code> whenever possible. This offers compile-time initialization and requires fewer lines of code, with no real drawback. In this step, we can lock/unlock the spinlock using <code>spin_lock()</code> and <code>spin_unlock()</code> inline functions, both defined in <code>include/linux/spinlock.h</code>, as follows:</p>
			<pre>static __always_inline void spin_unlock(spinlock_t *lock)
static __always_inline void spin_lock(spinlock_t *lock)</pre>
			<p>That said, there are some known limitations in using spinlocks this way. Though a spinlock prevents preemption on the local CPU, it does not prevent this CPU from being hogged by an interrupt (thus executing this interrupt's handler). Imagine a situation where the CPU holds a "spinlock" on behalf of task A in order to protect a given resource, and an interrupt occurs. The CPU will stop its current task and branch to this interrupt handler. So far, so good. Now, imagine if this IRQ handler needs to acquire this same spinlock (you probably already guessed that the resource is shared with the interrupt handler). It will infinitely spin in place, trying to acquire a lock already locked by a task that it has preempted. This situation will result in a deadlock, for sure.</p>
			<p>To address this issue, the Linux kernel provides <code>_irq</code> variant functions for spinlocks, which, in addition to disabling/enabling preemption, also disable/enable interrupts on the local CPU. These functions are <code>spin_lock_irq()</code> and <code>spin_unlock_irq()</code>, defined as follows:</p>
			<pre>static void spin_unlock_irq(spinlock_t *lock)
static void spin_lock_irq(spinlock_t *lock)</pre>
			<p>We might think that this solution is sufficient, but it isn't. The <code>_irq</code> variant partially solves the problem. Imagine interrupts are already disabled on the processor before your code starts locking; when you call <code>spin_unlock_irq()</code>, you will not just release the lock but enable interrupts also, but probably in an erroneous manner since there is no way for <code>spin_unlock_irq()</code> to know which interrupts were enabled before locking and which were not.</p>
			<p>Let's consider the following example:</p>
			<ul>
				<li>Let's say interrupt <code>x</code> and <code>y</code> were disabled before a spinlock was acquired, and <code>z</code> was not.</li>
				<li><code>spin_lock_irq()</code> will disable the interrupts (<code>x</code>, <code>y</code>, and <code>z</code> are now disabled) and take the lock.</li>
				<li><code>spin_unlock_irq()</code> will enable the interrupts. <code>x</code>, <code>y</code>, and <code>z</code> find themselves all enabled, which was not the case before acquiring the lock. This is where the problem lies.</li>
			</ul>
			<p>This <a id="_idIndexMarker150"/>makes <code>spin_lock_irq()</code> unsafe when called from IRQs off-context as its counterpart <code>spin_unlock_irq()</code> will dumbly enable IRQs, with the risk of enabling those that were not enabled while <code>spin_lock_irq()</code> was invoked. It makes sense to use <code>spin_lock_irq()</code> only when you know that interrupts are enabled—that is, you are sure nothing else might have disabled interrupts on the local CPU.</p>
			<p>Now, imagine if you save the interrupts' status in a variable before acquiring the lock and restore them exactly as they were while releasing—there would be no further issues at all. To achieve this, the kernel provides <code>_irqsave</code> variant functions that behave exactly like the <code>_irq</code> ones, with saving and restoring interrupts status features in addition. These are <code>spin_lock_irqsave()</code> and <code>spin_lock_irqrestore()</code>, defined as follows:</p>
			<pre>spin_lock_irqsave(spinlock_t *lock, unsigned long flags)
spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)</pre>
			<p class="callout-heading">Note</p>
			<p class="callout"><code>spin_lock()</code> and all its variants automatically call <code>preempt_disable()</code>, which disables preemption on the local CPU, while <code>spin_unlock()</code> and its variants call <code>preempt_enable()</code>, which tries to enable preemption (Yes—tries!!! It depends on whether other spinlocks are locked, which would affect the value of the preemption counter), and which internally calls <code>schedule()</code> if enabled (depending on the current value of the counter, whose current value should be <code>0</code>). <code>spin_unlock()</code> is then a preemption point and might re-enable preemption.</p>
			<h3>Disabling preemption versus disabling interrupts</h3>
			<p>Though disabling interrupts<a id="_idIndexMarker151"/> may prevent kernel preemption (scheduler tick disabled), nothing prevents the protected section from invoking the scheduler (<code>schedule()</code> function). A lot of kernel functions indirectly invoke the scheduler, such as those dealing with spinlocks. As a result, even a simple <code>printk()</code> function may invoke the scheduler since it deals with the spinlock that protects the kernel message buffer. The kernel disables or enables the scheduler (and, thus, preemption) by increasing or decreasing a kernel global and per-CPU variable (which defaults to <code>0</code>, meaning <em class="italic">enabled</em>) called <code>preempt_count</code>. When this variable is greater than <code>0</code> (which is checked by the <code>schedule()</code> function), the scheduler simply returns and does nothing. This variable is incremented at each invocation of a <code>spin_lock*</code> family function. On the other side, releasing a spinlock (any <code>spin_unlock*</code> family function) decrements it from <code>1</code>, and whenever it reaches <code>0</code>, the scheduler is invoked, meaning that your critical section would not be that atomic.</p>
			<p>Thus, only disabling interrupts protects you from kernel preemption only in cases where the protected code does not trigger preemption itself. That said, code that locked a spinlock may not sleep as there would be no way to wake it up (remember—timer interrupts and/or schedulers are disabled on the local CPU).</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor042"/>Mutexes</h2>
			<p>A mutex<a id="_idIndexMarker152"/> is the second and last locking primitive we will discuss in this chapter. It behaves exactly like a spinlock, with the only difference being that your code can sleep. If you try to lock a mutex that is already held by another task, your task will find itself suspended and woken up only when the mutex is released. No spinning this time, meaning that the CPU can do something else while your task waits in a sleeping state. As I mentioned previously, a spinlock is a lock held by a CPU. A mutex, on the other hand, is a lock held by a task.</p>
			<p>A mutex <a id="_idIndexMarker153"/>is a simple data structure that embeds a wait queue (to put contenders to sleep) and a spinlock to protect access to this wait queue, as illustrated in the following code snippet:</p>
			<pre>struct mutex {
    atomic_long_t owner;
    spinlock_t wait_lock;
#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
    struct optimistic_spin_queue osq; /* Spinner MCS lock */
#endif
    struct list_head wait_list;
[...]
};</pre>
			<p>In the preceding code snippet, elements used only in debugging mode have been removed for the sake of readability. However, as we can see, mutexes are built on top of spinlocks. <code>owner</code> represents the process that owns (holds) the lock. <code>wait_list</code> is the list in which the mutex's contenders are put to sleep. <code>wait_lock</code> is the spinlock that protects <code>wait_list</code> manipulation (removal or insertion of contenders to sleep in it). It helps to keep <code>wait_list</code> coherent on SMP systems.</p>
			<p>The mutex APIs can be found in the <code>include/linux/mutex.h</code> header file. Prior to acquiring and releasing a mutex, it must be initialized. As for other kernel core data structures, there is a static initialization, as shown here:</p>
			<pre>static DEFINE_MUTEX(my_mutex);</pre>
			<p>Here is a definition of the <code>DEFINE_MUTEX()</code> macro:</p>
			<pre>#define DEFINE_MUTEX(mutexname) \
struct mutex mutexname = __MUTEX_INITIALIZER(mutexname)</pre>
			<p>A second approach the kernel offers is dynamic initialization, possible thanks to a call to a <code>__mutex_init()</code> low-level function, which is actually wrapped by a much more user-friendly macro, <code>mutex_init()</code>. You can see this in action in the following code snippet:</p>
			<pre>struct fake_data {
    struct i2c_client *client;
    u16 reg_conf;
    struct mutex mutex;
};
static int fake_probe(struct i2c_client *client)
{
[...]
    mutex_init(&amp;data-&gt;mutex);
[...]
}</pre>
			<p>Acquiring (aka locking) a <a id="_idIndexMarker154"/>mutex is as simple as calling one of the following three functions:</p>
			<pre>void mutex_lock(struct mutex *lock);
int mutex_lock_interruptible(struct mutex *lock);
int mutex_lock_killable(struct mutex *lock);</pre>
			<p>If the mutex is free (unlocked), your task will immediately acquire it without going to sleep. Otherwise, your task will be put to sleep in a manner that depends on the locking function you use. With <code>mutex_lock()</code>, your task will be put in an uninterruptible sleep state (<code>TASK_UNINTERRUPTIBLE</code>) while waiting for the mutex to be released (if it is held by another task). <code>mutex_lock_interruptible()</code> will put your task in an interruptible sleep state, in which the sleep can be interrupted by any signal. <code>mutex_lock_killable()</code> will allow your sleeping task to be interrupted only by signals that actually kill the task. Each of these functions returns <code>0</code> if the lock has been acquired successfully. Moreover, interruptible variants return <code>-EINTR</code> when the locking attempt was interrupted by a signal.</p>
			<p>Whichever locking function is used, the mutex owner (and only the owner) should release the mutex using <code>mutex_unlock()</code>, defined as follows:</p>
			<pre>void mutex_unlock(struct mutex *lock);</pre>
			<p>If it is worth checking the status of the mutex, you can use <code>mutex_is_locked()</code>, as follows:</p>
			<pre>static bool mutex_is_locked(struct mutex *lock)</pre>
			<p>This function simply checks if the<a id="_idIndexMarker155"/> mutex owner is <code>NULL</code> and returns <code>true</code> if so or <code>false</code> otherwise.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">It is recommended to use <code>mutex_lock()</code> only when you can guarantee the mutex will not be held for a long time. If not, you should use an interruptible variant instead.</p>
			<p>There are specific rules while using mutexes. The most important ones are enumerated in the <code>include/linux/mutex.h</code> kernel mutex API header file, and some of these are outlined here:</p>
			<ul>
				<li>A mutex can be held by one and only one task at a time.</li>
				<li>Once held, the mutex can only be unlocked by the owner (that is, the task that locked it).</li>
				<li>Multiple, recursive, or nested locks/unlocks are not allowed.</li>
				<li>A mutex object must be initialized via the API. It must not be initialized by copying nor by using <code>memset</code>, just as held mutexes must not be reinitialized.</li>
				<li>A task that holds a mutex may not exit, just as memory areas where held locks reside must not be freed.</li>
				<li>Mutexes may not be used in hardware or software interrupt contexts such as tasklets and timers.</li>
			</ul>
			<p>All this makes mutexes suitable for the following cases:</p>
			<ul>
				<li>Locking only in the user context</li>
				<li>If the protected resource is not accessed from an IRQ handler and the operations need not be atomic</li>
			</ul>
			<p>However, it may be <a id="_idIndexMarker156"/>cheaper (in terms of CPU cycles) to use spinlocks for very small critical sections since the spinlock only suspends the scheduler and starts spinning, compared to the cost of using a mutex, which needs to suspend the current task and insert it into the mutex's wait queue, requiring the scheduler to switch to another task and rescheduling the sleeping task once the mutex is released.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor043"/>Trylock methods</h2>
			<p>There are cases where <a id="_idIndexMarker157"/>we may need to acquire the lock only if it is not already held by another contender elsewhere. Such methods try to acquire the lock and immediately (without spinning if we are using a spinlock, nor sleeping if we are using a mutex) return a status value, showing whether the lock has been successfully locked or not.</p>
			<p>Both spinlock and mutex APIs provide a trylock method. These are, respectively, <code>spin_trylock()</code> and <code>mutex_trylock()</code>, the latter of which you can see here. Both methods return 0 on failure (the lock is already locked) or 1 on success (lock acquired). Thus, it makes sense to use these functions along with an <code>if</code> statement:</p>
			<pre>int mutex_trylock(struct mutex *lock)</pre>
			<p><code>spin_trylock()</code> actually targets spinlocks. It will lock the spinlock if it is not already locked, just as the <code>spin_lock()</code> method does. However, it immediately returns <code>0</code> without spinning in cases where the spinlock is already locked. You can see it in action here:</p>
			<pre>static DEFINE_SPINLOCK(foo_lock);
[...]
static void foo(void)
{
    [...]
    if (!spin_trylock(&amp;foo_lock)) {
        /* Failure! the spinlock is already locked */
        [...]
        return;
    }
    /*
    * reaching this part of the code means that the
    * spinlock has been successfully locked
    */
    [...]
    spin_unlock(&amp;foo_lock);
    [...]
}</pre>
			<p>On the other<a id="_idIndexMarker158"/> hand, <code>mutex_trylock()</code> targets mutexes. It will lock the mutex if it is not already locked, just as the <code>mutex_lock()</code> method does. However, it immediately returns <code>0</code> without sleeping in cases where the mutex is already locked. You can see an example of this in the following code snippet:</p>
			<pre>static DEFINE_MUTEX(bar_mutex);
[...]
static void bar (void)
{
    [...]
    if (!mutex_trylock(&amp;bar_mutex)){
        /* Failure! the mutex is already locked */
        [...]
        return;
    }
    /*
     * reaching this part of the code means that the
     * mutex has been successfully acquired
     */
    [...]
    mutex_unlock(&amp;bar_mutex);
    [...]
}</pre>
			<p>In the preceding <a id="_idIndexMarker159"/>excerpt, <code>mutex_trylock()</code> is used along with an <code>if</code> statement so that the driver can adapt its behavior.</p>
			<p>Now that we are done with the trylock variant, let's switch to a totally different concept—learning how to explicitly delay execution. </p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor044"/>Dealing with kernel waiting, sleeping, and delay mechanisms</h1>
			<p>The term <em class="italic">sleeping</em> in<a id="_idIndexMarker160"/> this section refers to a mechanism by which a task (on behalf of the running kernel code) voluntarily relaxes the processor, with the possibility of another task being scheduled. While simple sleeping would consist of a task sleeping and being awakened after a given duration (to passively delay an operation, for example), there are sleeping mechanisms based on external events (such as data availability). Simple sleeps are implemented in the kernel using dedicated APIs; waking up from such sleeps is implicit (handled by the kernel itself) after the duration expires. The other sleeping mechanism is conditioned on an event and the waking-up is explicit (another task must explicitly wake us up based on a condition, else we sleep forever) unless a sleeping timeout is specified. This mechanism is implemented in the kernel using the concept of wait queues. That said, both sleep APIs and wait queues implement what we can call passive waiting. The difference between the two is how the waking-up process occurs.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>Wait queue</h2>
			<p>The kernel scheduler <a id="_idIndexMarker161"/>manages a list of tasks to run (tasks in a <code>TASK_RUNNING</code> state), known as a runqueue. On the other hand, sleeping tasks, whether interruptible or not (in a <code>TASK_INTERRUPTIBLE</code> or <code>TASK_UNINTERRUPTIBLE</code> state), have their own queues, known as wait queues.</p>
			<p>Wait queues are a higher-level mechanism essentially used to process blocking <code>true</code>, to wait for a given event to occur, or to sense data or resource availability. To understand how they work, let's have a look at the following structure in <code>include/linux/wait.h</code>:</p>
			<pre>struct wait_queue_head {
    spinlock_t lock;
    struct list_head head;
};</pre>
			<p>A wait queue is nothing but a list (with sleeping processes in it waiting to be awakened) and a spinlock to protect access to this list. We can use a wait queue when more than one process wants to sleep, waiting for one or more events to occur in order to be awakened. The head member is actually a list of processes waiting for the event(s). Each process that wants to sleep while waiting for the event to occur puts itself it this list before going to sleep. While a process is in the list, it is called wait queue entry. When an event occurs, one or more processes on the list are woken up and moved off the list.</p>
			<p>We can declare and initialize a wait queue in two ways. The first method is to statically use <code>DECLARE_WAIT_QUEUE_HEAD</code>, as follows:</p>
			<pre>DECLARE_WAIT_QUEUE_HEAD(my_event);</pre>
			<p>Alternatively, we can dynamically use <code>init_waitqueue_head()</code>, as follows:</p>
			<pre>wait_queue_head_t my_event;
init_waitqueue_head(&amp;my_event);</pre>
			<p>Any process that wants to sleep while waiting for <code>my_event</code> to occur can invoke either <code>wait_event_interruptible()</code> or <code>wait_event()</code>. Most of the time, the event is just the fact that a resource becomes available, thus it makes sense for a process to go to sleep only after a first check of the availability of that resource. To make things easy, these functions both take an expression in place of the second argument so that the process is put to sleep only if the expression evaluates <code>false</code>, as illustrated in the following code snippet:</p>
			<pre>wait_event(&amp;my_event, (event_occured == 1));
/* or */
wait_event_interruptible(&amp;my_event, (event_occured == 1));</pre>
			<p><code>wait_event()</code> or <code>wait_event_interruptible()</code> simply evaluates the condition when called. If the <a id="_idIndexMarker162"/>condition is <code>false</code>, the process is put into either a <code>TASK_UNINTERRUPTIBLE</code> or a <code>TASK_INTERRUPTIBLE</code> (for the <code>_interruptible</code> variant) state and removed from the runqueue.</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><code>wait_event()</code> puts the process into an exclusive wait, aka uninterruptible sleep, and can't thus be interrupted by the signal. It should be used only for critical tasks. Interruptible functions are recommended in most situations.</p>
			<p>There may be cases where you need not only the condition to be <code>true</code> but to time out after a certain waiting duration. You can address such cases using <code>wait_event_timeout()</code>, whose prototype is shown here:</p>
			<pre>wait_event_timeout(wq_head, condition, timeout)</pre>
			<p>This function has two behaviors, depending on the timeout having elapsed or not. These are outlined here:</p>
			<ul>
				<li><code>0</code> if the condition is evaluated to <code>false</code> or <code>1</code> if it is evaluated to <code>true</code>.</li>
				<li><code>true</code>.</li>
			</ul>
			<p>The time unit for timeout is a<a id="_idIndexMarker163"/> jiffy. There are convenient APIs to convert convenient time units such as milliseconds and microseconds to jiffies, defined as follows: </p>
			<pre>unsigned long msecs_to_jiffies(const unsigned int m)
unsigned long usecs_to_jiffies(const unsigned int u)</pre>
			<p>After a change on any variable that could affect the result of the wait condition, you must call the appropriate <code>wake_up*</code> family function. That being said, in order to wake up a process sleeping on a wait queue, you should call either <code>wake_up()</code>, <code>wake_up_all()</code>, <code>wake_up_interruptible()</code>, or <code>wake_up_interruptible_all()</code>. Whenever you call any of these functions, the condition is re-evaluated again. If the condition is <code>true</code> at that time, then a process (or all processes for the <code>_all()</code> variant) in the wait queue will be awakened, and its (their) state set to <code>TASK_RUNNING</code>; otherwise, (the condition is <code>false</code>), nothing happens. The following code snippet illustrates this concept:</p>
			<pre>wake_up(&amp;my_event);
wake_up_all(&amp;my_event);
wake_up_interruptible(&amp;my_event);
wake_up_interruptible_all(&amp;my_event);</pre>
			<p>In the preceding code snippet, <code>wake_up()</code> will wake only one process from the wait queue, while <code>wake_up_all()</code> will wake all processes from the wait queue. On the other hand, <code>wake_up_interruptible()</code> will wake only one process from the wait queue that is in interruptible sleep, and <code>wake_up_interruptible_all()</code> will wake all processes <a id="_idIndexMarker164"/>from the wait queue that are in interruptible sleep.</p>
			<p>Because they can be interrupted by signals, you should check the return value of the <code>_interruptible</code> variants. A nonzero value means your sleep has been interrupted by some sort of signal, and the driver should return <code>ERESTARTSYS</code>, as illustrated in the following code snippet:</p>
			<pre>#include &lt;linux/module.h&gt;
#include &lt;linux/init.h&gt;
#include &lt;linux/sched.h&gt;
#include &lt;linux/time.h&gt;
#include &lt;linux/delay.h&gt;
#include&lt;linux/workqueue.h&gt;
static DECLARE_WAIT_QUEUE_HEAD(my_wq);
static int condition = 0;
/* declare a work queue*/
static struct work_struct wrk;
static void work_handler(struct work_struct *work)
{
    pr_info("Waitqueue module handler %s\n", __FUNCTION__);
    msleep(5000);
    pr_info("Wake up the sleeping module\n");
    condition = 1;
    wake_up_interruptible(&amp;my_wq);
}
static int __init my_init(void)
{
    pr_info("Wait queue example\n");
    INIT_WORK(&amp;wrk, work_handler);
    schedule_work(&amp;wrk);
    pr_info("Going to sleep %s\n", __FUNCTION__);
    if (wait_event_interruptible(my_wq, condition != 0)) {
        pr_info("Our sleep has been interrupted\n");
        return -ERESTARTSYS;
    }
    pr_info("woken up by the work job\n");
    return 0;
}
void my_exit(void)
{
    pr_info("waitqueue example cleanup\n");
}
module_init(my_init)
module_exit(my_exit);
MODULE_AUTHOR("John Madieu &lt;john.madieu@gmail.com&gt;");
MODULE_LICENSE("GPL");</pre>
			<p>In the preceding example, we have<a id="_idIndexMarker165"/> used the <code>msleep()</code> API, which will be explained shortly. Back to the behavior of the code—the current process (actually, <code>insmod</code>) will be put to sleep in the wait queue for 5 seconds and woken up by the work handler. The <code>dmesg</code> output is shown here:</p>
			<pre>[342081.385491] Wait queue example
[342081.385505] Going to sleep my_init
[342081.385515] Waitqueue module handler work_handler
[342086.387017] Wake up the sleeping module
[342086.387096] woken up by the work job
[342092.912033] waitqueue example cleanup</pre>
			<p>Now that we are comfortable with the concept of wait queue, which allows us to put processes to sleep and wait for these to be awakened, let's learn another simple sleeping mechanism that simply consists of delaying the execution flow unconditionally.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor046"/>Simple sleeping in the kernel</h2>
			<p>This<a id="_idIndexMarker166"/> simple sleeping can also be referred to<a id="_idIndexMarker167"/> as <code>#include &lt;linux/delay&gt;</code>, which would make the following function available:</p>
			<pre>usleep_range(unsigned long min, unsigned long max)
msleep(unsigned long msecs)
msleep(unsigned long msecs)
msleep_interruptible(unsigned long msecs)</pre>
			<p>In the preceding APIs, <code>msecs</code> is the number of milliseconds of sleep. <code>min</code> and <code>max</code> are the minimum and upper bounds of sleeping in microseconds.</p>
			<p>The <code>usleep_range()</code> API relies<a id="_idIndexMarker168"/> on <code>µsecs</code> or small <code>msecs</code> (between 10 microseconds and 20 milliseconds), avoiding the busy-wait loop of <code>udelay()</code>.</p>
			<p><code>msleep*()</code> APIs <a id="_idIndexMarker169"/>are backed by <code>jiffies</code>/legacy timers. You should use this for larger milliseconds of sleep (10 milliseconds or more). This API sets the current task to <code>TASK_UNINTERRUPTIBLE</code>, whereas <code>msleep_interruptible()</code> sets the current task to <code>TASK_INTERRUPTIBLE</code> before scheduling the sleep. In short, the difference is whether the sleep can be ended early by a signal. It is recommended to use <code>msleep()</code> unless you have a need for the interruptible variant.</p>
			<p>APIs in this section must be used for inserting delays in a non-atomic context exclusively.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>Kernel delay or busy waiting</h2>
			<p>First, the term "delay" in this <a id="_idIndexMarker170"/>section can be considered as busy waiting as the task actively waits (corresponding to the <code>for()</code> or the <code>while()</code> loop), consuming CPU resources, in contrast to sleep, which is a passive delay as the task sleeps while waiting.</p>
			<p>Even for busy loop <a id="_idIndexMarker171"/>waiting, the driver must include <code>#include &lt;linux/delay&gt;</code>, which would make the following APIs available as well:</p>
			<pre>ndelay(unsigned long nsecs)
udelay(unsigned long usecs)
mdelay(unsigned long msecs)</pre>
			<p>The advantage of such APIs is that they can be used in both atomic and non-atomic contexts.</p>
			<p>The precision<a id="_idIndexMarker172"/> of <code>ndelay</code> depends on how accurate your timer is (not always the case on an embedded <code>ndelay</code>-level precision may not actually exist on many non-PC devices. Instead, you are more likely to come across the following:</p>
			<ul>
				<li><code>udelay</code>: This API is <a id="_idIndexMarker173"/>busy-wait loop-based. It will busy wait for enough loop cycles to achieve the desired delay. You should use this function if you need to sleep for a few <code>µsecs</code> (&lt; ~10 microseconds). It is recommended to use this API even for sleeping less than 10 us because, on slower systems (some embedded SoCs), the overhead of setting up hrtimers for <code>usleep</code> may not be worth it. Such an evaluation will obviously depend on your specific situation, but it is something to be aware of.</li>
				<li><code>mdelay</code>: This is<a id="_idIndexMarker174"/> a macro wrapper around <code>udelay</code> to account for possible overflow when passing large arguments to <code>udelay</code>. In general, the use of <code>mdelay</code> is discouraged, and code should be refactored to allow for the use of <code>msleep</code>.</li>
			</ul>
			<p>At this step, we are done with Linux kernel sleeping or delay mechanisms. We should be able to design and implement a time-slice-managed execution flow. We can now get deeper into the way the Linux kernel manages time.</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor048"/>Understanding Linux kernel time management</h1>
			<p>Time is one of the most<a id="_idIndexMarker175"/> used resources in computer systems, right after memory. It is used to do almost everything: timer, sleep, scheduling, and many other tasks.</p>
			<p>The Linux kernel includes software timer concepts to enable kernel functions to be invoked at a later time. </p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>The concepts of clocksource, clockevent, and tick device</h2>
			<p>In the original Linux timer implementation, the main hardware timer was mainly used for timekeeping. It was also programmed to fire interrupts periodically at HZ frequency, whose corresponding period is<a id="_idIndexMarker176"/> called a <strong class="bold">jiffy</strong> (both are explained later in this chapter, in the <em class="italic">Jiffies and HZ</em> section). Each of these interrupts generated every 1/HZ second was (and still is) referred to as a tick. Throughout this <a id="_idIndexMarker177"/>section, the term <em class="italic">tick</em> will refer to the interrupt generated at a 1/HZ period.</p>
			<p>The whole system time management (either from the kernel or user space) was bound to jiffies, which is also a global variable in the kernel, incremented at each tick. In addition to incrementing the value of <code>jiffies</code> (on top of which a timer wheel was implemented), the tick handler was also responsible for processes scheduling, statistic updating, and profiling.</p>
			<p>Starting from kernel version 2.6.21, as a first improvement, hrtimers implementation was merged (and is now available through <code>CONFIG_HIGH_RES_TIMERS</code>). This feature was (and still is) transparent and has come with <code>hrtimer</code> timers as a functionality of its<a id="_idIndexMarker178"/> own, with a new data type, <code>ktime_t</code>, which is used to keep time value on a nanosecond basis. The former legacy (tick-based and low-res) timer implementation remained, however. The improvement converted the <code>nanosleep()</code>, <code>nanosleep()</code> and <code>clock_nanosleep()</code> was made possible by the conversion of <code>nanosleep()</code> and POSIX timers. Without this improvement, the best accuracy that could be obtained for timer events would be 1 jiffy, whose duration depends on the value of <code>HZ</code> in the kernel.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">That said, high-resolution timer implementation is an independent feature. hrtimer timers can be enabled whatever the platform, but whether they work in high-resolution mode or not depends on the underlying hardware timer. Otherwise, the system is said to be <a id="_idIndexMarker180"/>in <strong class="bold">low-resolution</strong> (<strong class="bold">low-res</strong>) mode.</p>
			<p>Improvements have been done all along the kernel evolutions until the generic clockevent interface came in, with the concepts of clock source, clock event, and tick devices. This completely changed time management in the kernel and influenced CPU power management.</p>
			<h3>The clocksource framework and clock source devices</h3>
			<p>A clock source<a id="_idIndexMarker181"/> is a monotonic, atomic, and free-running counter. This can be considered<a id="_idIndexMarker182"/> as a timer that acts as a free-running counter that provides time stamping and read access to the monotonically increasing time value. The common operation performed on clock source devices is reading the counter's value.</p>
			<p>In the kernel, there is a <code>clocksource_list</code> global list that tracks the clock source devices registered with the system, enqueued ordered by rating. This allows the Linux kernel to know about all registered clock source devices and switch to a clock source with a better rating and features. For instance, the <code>__clocksource_select()</code> function is invoked after each registration of a new clock source, which ensures that the best clock source is always selected. Clock sources are registered with either <code>clocksource_mmio_init()</code> or <code>clocksource_register_hz()</code> (you can <code>grep</code> these words). However, clock source device drivers are in <code>drivers/clocksource/</code> in kernel sources.</p>
			<p>On a running Linux system, the most intuitive way to list clock source devices that are registered with the framework is by looking for the word <code>clocksource</code> in the kernel log message buffer, as shown here:</p>
			<div><div><img src="img/B17934_03_001.jpg" alt="Figure 3.1 – System clocksource list&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – System clocksource list</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In the preceding output logs (from a Pi 4), the <code>jiffies</code> clock source is a jiffy granularity-based and always provided clock source registered by the kernel as <code>clocksource_jiffies</code> in <code>kernel/time/jiffies.c</code>, with the lowest valid rating value (that is, used as the last resort). On the x86 platform, this clock source is refined and renamed into <code>refined-jiffies</code>—see the <code>register_refined_jiffies()</code> function call in <code>arch/x86/kernel/setup.c</code>.</p>
			<p>However, the preferred way (especially where the <code>dmesg</code> buffer has rotated or has been cleared) to enumerate the available <a id="_idIndexMarker183"/>clock source on a running Linux system is by reading the content of the <code>available_clocksource</code> file in <code>/sys/devices/system/clocksource/clocksource0/</code>, as shown in the following code snippet (on a Pi 4):</p>
			<pre>root@raspberrypi4-64-d0:~# cat  /sys/devices/system/clocksource/clocksource0/available_clocksource 
arch_sys_counter 
root@raspberrypi4-64-d0:~#</pre>
			<p>On an i.MX6 board, we have<a id="_idIndexMarker184"/> the following:</p>
			<pre>root@udoo-labcsmart:~# cat  /sys/devices/system/clocksource/clocksource0/available_clocksource 
mxc_timer1 
root@udoo-labcsmart:~#</pre>
			<p>To check the currently used clock source, you can use the following code:</p>
			<pre>root@raspberrypi4-64-d0:~# cat  /sys/devices/system/clocksource/clocksource0/current_clocksource 
arch_sys_counter
root@raspberrypi4-64-d0:~#</pre>
			<p>On my x86 machine, we have the following for both available clock sources and the currently used one:</p>
			<pre>jma@labcsmart:~$ cat /sys/devices/system/clocksource/clocksource0/available_clocksource
tsc hpet acpi_pm 
jma@labcsmart:~$ cat /sys/devices/system/clocksource/clocksource0/current_clocksource 
tsc
jma@labcsmart:~$</pre>
			<p>To change the <a id="_idIndexMarker185"/>current clock source, you can echo the name of one of the available<a id="_idIndexMarker186"/> clock sources into the <code>current_clocksource</code> file, like this:</p>
			<pre>jma@labcsmart:~$ echo acpi_pm &gt;  /sys/devices/system/clocksource/clocksource0/current_clocksource 
jma@labcsmart:~$</pre>
			<p>Changing the current clock source must be done with caution since the current clock source selected by the kernel during the boot is always the best one.</p>
			<h4>Linux kernel timekeeping</h4>
			<p>One of the main goals of the<a id="_idIndexMarker187"/> clock source device is feeding the timekeeper. There can be multiple clock sources in a system, but the timekeeper will choose the one with the highest precision to use. The timekeeper needs to obtain the value of the clock source periodically to update the system time, which is usually updated during the tick processing, as illustrated in the following diagram:</p>
			<div><div><img src="img/B17934_03_002.jpg" alt="Figure 3.2 – Linux kernel timekeeper implementation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Linux kernel timekeeper implementation</p>
			<p>The <a id="_idIndexMarker188"/>timekeeper provides several types of time: <strong class="bold">xtime</strong>, <strong class="bold">monotonic time</strong>, <strong class="bold">raw monotonic time</strong>, and <strong class="bold">boot time</strong>, which are outlined in more detail here:</p>
			<ul>
				<li><strong class="bold">xtime</strong>: This is<a id="_idIndexMarker189"/> wall (real) time, which represents the current time as given by <a id="_idIndexMarker190"/>the <strong class="bold">Real-Time Clock</strong> (<strong class="bold">RTC</strong>) chip.</li>
				<li><strong class="bold">Monotonic time</strong>: The <a id="_idIndexMarker191"/>cumulative time since the system is turned on, but does not count the time the system sleeps.</li>
				<li><strong class="bold">Raw monotonic time</strong>: This <a id="_idIndexMarker192"/>has the same meaning as monotonic time, but it is purer and will not be affected by <strong class="bold">Network Time Protocol</strong> (<strong class="bold">NTP</strong>) time <a id="_idIndexMarker193"/>adjustment.</li>
				<li><strong class="bold">Boot time</strong>: This adds <a id="_idIndexMarker194"/>the time the system spent sleeping to the monotonic time, which gives the total time after the system is powered on.</li>
			</ul>
			<p>The following table shows the <a id="_idIndexMarker195"/>different types of time and their kernel getter functions:</p>
			<div><div><img src="img/Table_01.jpg" alt="Table 3.1 – Linux kernel timekeeping functions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 3.1 – Linux kernel timekeeping functions</p>
			<p>Now that we are familiar with the Linux kernel timekeeping mechanisms and APIs, we are free to learn another concept involved in this time management—the clockevent framework.</p>
			<h3>The clockevent framework and clock event devices</h3>
			<p>Before the concept of clockevent was introduced, the locality of hardware timers was not considered. The clock source/event hardware was programmed to periodically generate <code>HZ</code> ticks (interrupts) per second, the interval between each tick being a jiffy. With the introduction of <a id="_idIndexMarker196"/>clockevent/source in the kernel, the interruption of the clock became abstracted as an event. The main function of the clockevent framework is to distribute the clock interrupts (events) and set the next trigger condition. It is a generic framework for next-event interrupt programming.</p>
			<p>A clock event device<a id="_idIndexMarker197"/> is a device that can fire interrupts and allow us to program when the next interrupt (an event) will poke in the future. Each clock event device driver must provide a <code>set_next_event</code> function (or <code>set_next_ktime</code> in the case of an hrtimer-backed clock event device), which is used by the framework when it comes to using the underlying clock event device to program the next interrupt.</p>
			<p>Clock event devices are orthogonal to clock source devices. This is probably why their drivers are in the same place (and, sometimes, in the same compilation unit) as clock source device drivers—that is, in <code>drivers/clocksource</code>. On most platforms, the same hardware and register range may be used for the clock event and for the clock source, but they are essentially different things. This is the case, for example, with the BCM2835 System Timer, which is a memory-mapped peripheral found on the BCM2835 used in the Raspberry Pi. It has a 64-bit free-running counter that runs at 1 <strong class="bold">megahertz</strong> (<strong class="bold">MHz</strong>), as well as four distinct "output compare registers" that can be used to schedule interrupts. In such cases, the driver usually registers the clock source and the clock event device in the same compilation unit.</p>
			<p>On a running Linux system, the available clock event devices can by listed from the <code>/sys/devices/system/clockevents/</code> directory. Here is an example on a Pi 4:</p>
			<pre>root@raspberrypi4-64-d0:~# ls /sys/devices/system/clockevents/         
broadcast    clockevent1  clockevent3  uevent
clockevent0  clockevent2  power
root@raspberrypi4-64-d0:~#</pre>
			<p>On a dual-core i.MX6 running system, we have the following:</p>
			<pre>root@udoo-labcsmart:~# ls /sys/devices/system/clockevents/
broadcast    clockevent0  clockevent1  consumers    power        suppliers    uevent
root@empair-labcsmart:~#</pre>
			<p>And finally, on my height core machine, we have the following:</p>
			<pre>jma@labcsmart:~$ ls /sys/devices/system/clockevents/
broadcast  clockevent0  clockevent1  clockevent2  clockevent3  clockevent4  clockevent5  clockevent6  clockevent7  power  uevent
jma@labcsmart:~$</pre>
			<p>From the preceding listings of available<a id="_idIndexMarker198"/> clock event devices on the system, we can say the following:</p>
			<ul>
				<li>There are as many clock event devices as CPUs on the system (allowing per-CPU clock devices, thus involving timer locality).</li>
				<li>There is always a strange directory, <code>broadcast</code>. We will discuss this particular timer in the next sections.</li>
			</ul>
			<p>To know the underlying timer of a given clock event device, you can read the content of <code>current_device</code> in the clock event directory. We'll now look at some examples on three different machines.</p>
			<p>On the i.MX 6 platform, we have the following:</p>
			<pre>root@udoo-labcsmart:~# cat /sys/devices/system/clockevents/clockevent0/current_device 
local_timer
root@udoo-labcsmart:~# cat /sys/devices/system/clockevents/clockevent1/current_device 
local_timer</pre>
			<p>On the Pi 4, we have the following:</p>
			<pre>root@raspberrypi4-64-d0:~# cat /sys/devices/system/clockevents/clockevent2/current_device
arch_sys_timer
root@raspberrypi4-64-d0:~# cat /sys/devices/system/clockevents/clockevent3/current_device
arch_sys_timer</pre>
			<p>On my x86 running machine, we have the following:</p>
			<pre>jma@labcsmart:~$ cat /sys/devices/system/clockevents/clockevent0/current_device
lapic-deadline
jma@labcsmart:~$ cat /sys/devices/system/clockevents/clockevent1/current_device
lapic-deadline</pre>
			<p>For the sake of readability, the choice has been made to read two entries only, and from what we have read, we can conclude the following:</p>
			<ul>
				<li>Clock event devices<a id="_idIndexMarker199"/> are backed by the same hardware timer, which is different from the hardware timer backing the clock source device.</li>
				<li>At least two hardware timers are needed to support the high-resolution timer interface, one playing the clock source role and one (ideally per-CPU) baking clock event devices. </li>
			</ul>
			<p>A clock event device can be configured to work in either one-shot mode or in periodic mode, as outlined here:</p>
			<ul>
				<li>In periodic mode, it is configured to generate a tick every 1/HZ second and does all the things the legacy (low-resolution) timer-based tick did, such as updating jiffies, accounting CPU time, and so on. In other words, in periodic mode, it is used the same way as the legacy low-resolution timer was, but it is run out of the new infrastructure.</li>
				<li>One-shot mode makes the hardware generate a tick after a specific number of cycles from the current time. It is mostly used to program the next interrupt that will wake the CPU before it goes idle.</li>
			</ul>
			<p>To track the operating mode of a clock event device, the concept of a tick device was introduced. This is further explained in the next section.</p>
			<h3>Tick devices</h3>
			<p>Tick devices <a id="_idIndexMarker200"/>are software extensions of clock event devices to provide a continuous stream of tick events that happen at regular time intervals. A tick device is automatically created by the kernel when a new clock event device is registered and always selects the best clock event device. It then goes without saying that a tick device is bound to a clock event device and that a tick device is backed by a clock event device.</p>
			<p>The definition of a tick-device data structure is shown in the following code snippet:</p>
			<pre>struct tick_device {
    struct clock_event_device *evtdev;
    enum tick_device_mode mode;
};</pre>
			<p>In this data structure, <code>evtdev</code> is the clock event device that is abstracted by the tick device. <code>mode</code> is used to track the working mode of the underlying clock event. Therefore, when a tick device is said to be in periodic mode, it also means that the underlying clock event device is configured to work in this mode. The following diagram illustrates this:</p>
			<div><div><img src="img/B17934_03_003.jpg" alt="Figure 3.3 – Clockevent and tick-device correlation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – Clockevent and tick-device correlation</p>
			<p>A tick device can either be global to the system or local (per-CPU tick device). Whether a tick device must be global or not is decided by the framework, by selecting one local tick device based on the features of the underlying clock event device of this tick device. The descriptions of each type of tick device is as follows:</p>
			<ul>
				<li>A per-CPU tick device<a id="_idIndexMarker201"/> is used to provide local CPU functionality <a id="_idIndexMarker202"/>such as process accounting, profiling, and—obviously—CPU local periodic tick (in periodic mode) and CPU local next event interrupt (non-periodic mode), for CPU local hrtimers management (see the <code>update_process_times()</code> function to learn how all that is handled). In the timer core code, there is a <code>tick_cpu_device</code> per-CPU variable that represents the instance of the tick device for each CPU in the system.</li>
				<li>A global tick device<a id="_idIndexMarker203"/> is responsible for providing the period ticks that <a id="_idIndexMarker204"/>mainly run the <code>do_timer()</code> and <code>update_wall_time()</code> functions. Thus, the first one updates the global <code>jiffies</code> value and updates the system load average, while the latter updates the wall time (which is stored in <code>xtime</code>, which records the time difference from January 1, 1970, to now), and runs any dynamic timers that have expired (for instance, running local process timers). In the timer core code, there is the <code>tick_do_timer_cpu</code> global variable, which holds the CPU number whose tick device has the role of the global tick device—the one that executes <code>do_timer()</code>. There is another global variable, <code>tick_next_period</code>, which keeps track of the next time the global tick device will fire.<p class="callout-heading">Note</p><p class="callout">This also means that the <code>jiffies</code> variable is always managed from one core at a time, but its function management affinity can jump from one core to the another as and when <code>tick_do_timer_cpu</code> changes.</p></li>
			</ul>
			<p>From its interrupt routine, the driver of the underlying clock event device must invoke <code>evtdev-&gt;event_handler()</code>, which is the default handler of the clock device installed by the framework. While it is transparent for the device driver, this handler is set by the framework depending on other parameters, as follows: two kernel configuration options (<code>CONFIG_HIGH_RES_TIMERS</code> and <code>CONFIG_NO_HZ</code>), the underlying hardware timer resolution, and whether the tick device is operating in dynamic mode or one-shot mode.</p>
			<p><code>NO_HZ</code> is the kernel option<a id="_idIndexMarker205"/> enabling dynamic tick support, and <code>HIGH_RES_TIMERS</code> allows the use of hrtimer APIs. With hrtimers enabled, the base code is still tick-driven, but the periodic tick interrupt is replaced by timers under hrtimers (<code>softirq</code> is called in the timer softirq context). However, whether the hrtimers will work in high-resolution mode depends on the underlying hardware timer being high-resolution or not. If not, the hrtimers will be fed by the old low-resolution tick-based timer.</p>
			<p>A tick device can operate in either one-shot mode or periodic mode. In periodic mode, the framework uses a per-CPU hrtimer via a control structure to emulate the ticks so that the base code is still tick-driven, but the periodic tick interrupt is replaced by timers under hrtimers embedded in the control structure. This control structure is a <code>tick_sched</code> struct, defined as follows:</p>
			<pre>struct tick_sched {
    struct hrtimer               sched_timer;
    enum tick_nohz_mode          nohz_mode;
[...]
};</pre>
			<p>Then, a per-CPU instance of this control structure is declared, as follows:</p>
			<pre>static DEFINE_PER_CPU(struct tick_sched, tick_cpu_sched);</pre>
			<p>This per-CPU instance allows a per-CPU tick emulation, which will drive the low-res timer processing via the <code>sched_timer</code> element, periodically reprogrammed to the next-low-res-timer-expires interval. This seems, however, obvious since each CPU has its own runqueue and ready processes list to manage.</p>
			<p>A <code>tick_sched</code> element can be configured by the framework to work in three different modes, described as follows:</p>
			<ul>
				<li><code>NOHZ_MODE_INACTIVE</code>: This mode means no dynamic tick and no hrtimers support. It is the state the system is in during initialization. In this mode, the local per-CPU tick-device event handler is <code>tick_handle_periodic()</code>, and entering <code>idle</code> will be interrupted by the tick timer interrupt.</li>
				<li><code>NOHZ_MODE_LOWRES</code>: This is also the <code>lowres</code> mode, which means a dynamic tick enabled in low-resolution mode. It means no high-resolution hardware timer has been found on the<a id="_idIndexMarker206"/> system to allow hrtimers to work in high-resolution mode and that they work in low-precision mode, which has the same precision as the low-precision timer (<code>tick_nohz_handler()</code>, and entering <code>idle</code> will not be interrupted by the tick timer interrupt.</li>
				<li><code>NOHZ_MODE_HIGHRES</code>: This is also the <code>highres</code> mode. In this mode, both dynamic tick and hrtimer "high-resolution" modes are enabled. The local per-CPU tick-device event handler is <code>hrtimer_interrupt()</code>. Here, hrtimers work in high-precision mode, which has the same accuracy as the hardware timer (<code>tick_device</code>, and the conventional tick timer is converted into a sub-timer of hrtimer.</li>
			</ul>
			<p>Tick core-related source code in the kernel is in the <code>kernel/time/</code> directory, implemented in <code>tick-*.c</code> files. These are <code>tick-broadcast.c</code>, <code>tick-common.c</code>, <code>tick-broadcast-hrtimer.c</code>, <code>tick-legacy.c</code>, <code>tick-oneshot.c</code>, and <code>tick-sched.c</code>.</p>
			<h3>Broadcast tick device</h3>
			<p>On platforms implementing <a id="_idIndexMarker207"/>CPU power management, most (if not all) of the hardware timers backing clock event devices will be turned off in some <code>CPUidle</code> states. To keep the software timers functional, the kernel relies on an always-on clock event device (that is, backed by an always-on timer) to be present in the platform to relay the interrupt signaling when the timer expires. This always-on timer is called a <code>broadcast</code> directory in <code>/sys/devices/system/clockevents/</code>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">A broadcast tick device is able to wake up any CPU by issuing an <code>wake_up_nohz_cpu()</code>, which is used for this purpose.</p>
			<p>To see the underlying <a id="_idIndexMarker209"/>timer backing the broadcast device, you can read the <code>current_device</code> variable in its directory.</p>
			<p>On the x86 platform, we have the following output:</p>
			<pre>jma@labcsmart:~$ cat /sys/devices/system/clockevents/broadcast/current_device
hpet</pre>
			<p>The Pi 4 output is shown here:</p>
			<pre>root@raspberrypi4-64-d0:~# cat /sys/devices/system/clockevents/broadcast/current_device
bc_hrtimer</pre>
			<p>Finally, the i.MX 6 broadcast device is backed by the following timer:</p>
			<pre>root@udoo-labcsmart:~# cat /sys/devices/system/clockevents/broadcast/current_device 
mxc_timer1</pre>
			<p>From the preceding output showing the timer backing the broadcast device, we can conclude that clock source, clock event, and broadcast device timers are all different.</p>
			<p>Whether a tick device can be used as a broadcast device or not is decided by the <code>tick_install_broadcast_device()</code> core function, invoked at each tick-device registration. This function will exclude tick devices with <code>CLOCK_EVT_FEAT_C3STOP</code> flags set (which means the underlying clock event device's timer stops in the <code>C3</code> idle state) and rely on other criteria (such as supporting one-shot mode—that is, having the <code>CLOCK_EVT_FEAT_ONESHOT</code> flag set). Finally, the <code>tick_broadcast_device</code> global variable defined in <code>kernel/time/tick-broadcast.c</code> contains the tick device that has the role of a broadcast device. When a tick device is selected as a broadcast device, its next event handler is set to <code>tick_handle_periodic_broadcast()</code>, instead of to <code>tick_handle_periodic()</code>.</p>
			<p>There are, however, platforms implementing CPU core gating that do not have an always-on hardware timer. For such platforms, the kernel provides a kernel hrtimer-based clock event device that is unconditionally registered upon boot (and can be chosen as a tick broadcast device) with the lowest possible rating value so that any broadcast-capable hardware clock event device present on the system will be chosen in preference to the tick broadcast device.</p>
			<p>This hrtimer-backed clock event device<a id="_idIndexMarker210"/> relies on a dynamically chosen CPU (such that if there is a CPU about to enter into deep sleep with its wake-up time earlier than the hrtimer expiration time, this CPU becomes the new broadcast CPU) to be always powered up. This CPU will then relay the timer interrupt to CPUs in deep-idle states through its hardware local timer device. It is implemented in <code>kernel/time/tick-broadcast-hrtimer.c</code>, registered as <code>ce_broadcast_hrtimer</code> with the <code>name</code> field set to <code>bc_hrtimer</code>. It is, for instance, as you can see, the broadcast tick device used by the Pi 4 platform.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">It goes without saying that having an always-on CPU has implications for power management platform capabilities and makes <code>CPUidle</code> suboptimal since at least a CPU is kept always in a shallow idle state by the kernel to relay timer interrupts. It is a trade-off between CPU power management and a high-resolution timer interface, which at least leaves the kernel with a functional system with some working power management capabilities.</p>
			<h3>Understanding the sched_clock function</h3>
			<p><code>sched_clock()</code> is a kernel <a id="_idIndexMarker211"/>timekeeping and timestamping function that returns the number of nanoseconds since the system started. It is weakly defined (to allow its overriding by architecture or platform code) in <code>kernel/sched/clock.c</code>, as follows:</p>
			<pre>unsigned long long __weak sched_clock(void)</pre>
			<p>It is, for instance, the function that provides a timestamp to <code>printk()</code> or is invoked when using <code>ktime_get_boottime()</code> or related kernel APIs. It defaults to a jiffy-backed implementation (which could affect scheduling accuracy). If overridden, the new implementation must return a 64-bit monotonic timestamp in nanoseconds that represents the number of nanoseconds since the last reboot. Most platforms achieve this by directly reading the timer registers. On platforms that lack timers, this feature is implemented with the same timer as the one used to back the main clock source device. This is the case on Raspberry Pi, for example. When this is the case, the registers to read the main clock source device value and the registers from where the <code>sched_clock()</code> value comes are the same: see <code>drivers/clocksource/bcm2835_timer.c</code>.</p>
			<p>The timer driving <code>sched_clock()</code> has to be very fast compared with the clock source timers. As its name <a id="_idIndexMarker212"/>states, it is mainly used by the scheduler, which means it is called much more often. If you have to do trade-offs between accuracy compared to the clock source, you may sacrifice accuracy for speed in <code>sched_clock()</code>.</p>
			<p>When <code>sched_clock()</code> is not overridden directly, the kernel time core provides a <code>sched_clock_register()</code> helper to supply a platform-dependent timer reading function as well as a rating value. Anyway, this timer reading function will end up in the <code>cd</code> kernel time framework variable, which is of type <code>struct clock_data</code> (assuming that the rate of the new underlying timer is greater than the rate of the timer driving the current function).</p>
			<h3>Dynamic tick/tickless kernel</h3>
			<p>Dynamic ticks<a id="_idIndexMarker213"/> are the logical consequence of migration to a high-resolution timer interface. Before they were introduced, periodic ticks periodically issued interrupts (<code>HZ</code> times per second) to drive the operating system. This kept the system awake even when there were no tasks or timer handlers to run. With this approach, long rests were impossible for the CPUs, which had to wake for no real purpose.</p>
			<p>The dynamic tick mechanism came with a solution, allowing periodic ticks to be stopped during certain time intervals to save power. With this new approach, periodic ticks are enabled back only when some tasks need to be performed; otherwise, they are disabled.</p>
			<p>How does it work? When a CPU has no more tasks to run (that is, the idle task is scheduled on this CPU), only two events can create new work to do after the CPU goes idle: the expiration of one of the internal kernel timers, which is predictable, or the completion of an I/O operation. When a CPU enters an idle state, the timer framework examines the next scheduled timer event and, if it is later than the next periodic tick, it reprograms the per-CPU clock event device to this later event. This will allow the idle CPU to enter longer idle sleeps without being interrupted unnecessarily by a periodic tick.</p>
			<p>There are, however, systems with low power states where even the per-CPU clock event device would stop. On such platforms, it is the broadcast tick device that is programmed with the next future event.</p>
			<p>Just before a CPU <a id="_idIndexMarker214"/>enters such an idle state (the <code>do_idle()</code> function), it calls into the tick broadcast framework (<code>tick_nohz_idle_enter()</code>), and the periodic tick of its <code>tick_device</code> variable is disabled (see <code>tick_nohz_idle_stop_tick()</code>). This CPU is then added to a list of CPUs to be woken up by setting the bit corresponding to this CPU in the <code>tick_broadcast_mask</code> "broadcast map" variable, which is a bitmap that represents a list of processors that are in a sleeping mode. Then, the framework calculates the time at which this CPU must be woken up (its next event time); if this time is earlier than the time at which <code>tick_broadcast_device</code> is currently programmed, the time at which <code>tick_broadcast_device</code> should interrupt is updated to reflect the new value, and this new value is programmed into the clock event device backing the broadcast tick device. The <code>tick_cpu_device</code> variable of the CPU that is about to enter a deep idle state is now put in shutdown mode, which means that it is no longer functional.</p>
			<p>The foregoing procedures are repeated each<a id="_idIndexMarker215"/> time a CPU enters a deep idle state, and the <code>tick_broadcast_device</code> variable is programmed to fire at the earliest of the wake-up times of the CPUs in deep idle states.</p>
			<p>When the tick broadcast device next event pokes, it will look into the bitmask of sleeping CPUs, looking for the CPU(s) owning the timer(s) that might have expired and will send an IPI to any remote CPU in this bitmask that might host an expired timer.</p>
			<p>If, however, a CPU leaves the idle state upon an interrupt (the architecture code calls <code>handle_IRQ()</code>, which indirectly calls <code>tick_irq_enter()</code>), this CPU tick device is enabled (first in one-shot mode), and before it performs any task, the <code>tick_nohz_irq_enter()</code> function is called to ensure that <code>jiffies</code> are up to date so that the interrupt handler does not have to deal with a stale jiffy value, and then it resumes the periodic tick, which is kept active until the next call to <code>tick_nohz_idle_stop_tick()</code> (which is essentially called from <code>do_idle()</code>).</p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/>Using standard kernel low-precision (low-res) timers</h2>
			<p>Standard (now legacy and also referred to as low-resolution) timers are kernel timers operating on <a id="_idIndexMarker216"/>the granularity of <code>jiffies</code>. The resolution of these timers is bound to the resolution of the regular system tick, which depends on the architecture and configuration (that is, <code>CONFIG_HZ</code> or, simply, <code>HZ</code>) used by the kernel to control the scheduling and execution of a function at a certain point in the future (based on jiffies).</p>
			<h3>Jiffies and HZ</h3>
			<p>A jiffy<a id="_idIndexMarker217"/> is a kernel unit of time whose duration depends on the value of <code>HZ</code>, which represents the incrementation frequency of the <code>jiffies</code> variable in the kernel. Each increment event is called a tick. The clock source based on <code>jiffies</code> is the lowest common denominator clock source that should function on any system.</p>
			<p>Since the <code>jiffies</code> variable is incremented <code>HZ</code> times every second, if <code>HZ = 1,000</code>, then it is incremented 1,000 times per second (that is, one tick every 1/1,000 seconds, or 1 millisecond). On most <code>HZ</code> defaults <a id="_idIndexMarker218"/>to <code>100</code>, while it defaults to <code>250</code> on x86, which would result in a resolution of 10 milliseconds or 4 milliseconds.</p>
			<p>Here are different <code>HZ</code> values on two running systems:</p>
			<pre>jma@labcsmart:~$ grep ‘CONFIG_HZ=' /boot/config-$(uname -r)
CONFIG_HZ=250
jma@labcsmart:~$</pre>
			<p>The preceding code has been executed on a running x86 machine. On an ARM running machine, we have the following:</p>
			<pre>root@udoo-labcsmart:~# zcat /proc/config.gz |grep CONFIG_HZ
CONFIG_HZ_100=y
root@udoo-labcsmart:~#</pre>
			<p>The preceding code says the current <code>HZ</code> value is <code>100</code>.</p>
			<h3>Kernel timer APIs</h3>
			<p>A timer<a id="_idIndexMarker219"/> is <a id="_idIndexMarker220"/>represented in the kernel as an instance of <code>struct timer_list</code>, defined as follows:</p>
			<pre>struct timer_list {
    struct hlist_node entry;
    unsigned long expires;
    void (*function)(struct timer_list *);
    u32 flags;
);</pre>
			<p>In the preceding data structure, <code>expires</code> is an absolute value in jiffies that defines when this timer will expire in the future. <code>entry</code> is internally used by the kernel to track this timer in a per-CPU global list of timers. <code>flags</code> are OR'ed bitmasks that represent the timer flags, such as the way the timer is managed and the CPU on which the callback will be scheduled, and <code>function</code> is the callback to be executed when this timer expires.</p>
			<p>You can dynamically define a timer using <code>timer_setup()</code> or statically create one with <code>DEFINE_TIMER()</code>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In Linux kernel versions prior to 4.15, <code>setup_timer()</code> was used as the dynamic variant.</p>
			<p>Here are the definitions of both macros:</p>
			<pre>void timer_setup( struct timer_list *timer,        \
           void (*function)( struct timer_list *), \
           unsigned int flags);
#define DEFINE_TIMER(_name, _function) [...]</pre>
			<p>After the timer has been initialized, you must set its expiration delay before starting it using one of the following APIs:</p>
			<pre>int mod_timer(struct timer_list *timer,
               unsigned long expires);
void add_timer(struct timer_list *timer)</pre>
			<p>The <code>mod_timer()</code> function<a id="_idIndexMarker221"/> is used to either set an initial expiration delay or to update its value on an active timer, which means that calling this function on an inactive timer will activate this timer.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Activating a timer here means arming and queueing this timer. That said, when a timer is just armed, queued, and counting down, waiting for its expiration before running the callback function, it is said to be pending.</p>
			<p>You should prefer this function over <code>add_timer()</code>, which is another function to start inactive timers exclusively. Before calling <code>add_timer()</code>, you must have set the timer expiration delay and the callback as follows:</p>
			<pre>my_timer.expires = jiffies + ((12 * HZ) / 10); /* 1.2s */
add_timer(&amp;my_timer);</pre>
			<p>The value <code>mod_timer()</code> returns depends on the state of the timer prior to it being invoked. Calling <code>mod_timer()</code> on an inactive timer returns <code>0</code> on success, while it returns <code>1</code> when successfully invoked on a pending timer or a timer whose callback function is being executed. This means it is totally safe to execute <code>mod_timer()</code> from the timer callback. When invoked on an active timer, it is equivalent to <code>del_timer(timer); timer-&gt;expires = expires; add_timer(timer);</code>.</p>
			<p>Once done with the timer, it can be released or cancelled using one of the following functions: </p>
			<pre>int del_timer(struct timer_list *timer);
int del_timer_sync(struct timer_list *timer);</pre>
			<p><code>del_timer()</code> removes (dequeues) the <code>timer</code> object from the timer management queue. On success, it returns a different value depending on whether it is invoked on an inactive timer or on an active timer. In the first case, it returns <code>0</code>, while it returns <code>1</code> in the latter case, even if the function callback of this timer is currently being executed. </p>
			<p>Let's consider the following execution flow, where a timer is being deleted on a CPU while its callback is being executed on another CPU:</p>
			<pre>mainline (CPUx)                  handler(CPUy)
==============                   =============
                                 enter xxx_timer()
del_timer()
kfree(some_resource)
                                 access(some_resource)</pre>
			<p>In the preceding code <a id="_idIndexMarker222"/>snippet, using <code>del_timer()</code> does not guarantee that the callback is not running anymore. Here is another example:</p>
			<pre>mainline (CPUx)            handler(CPUy)
==============             =============
                           enter xxx_timer()
 del_timer()
 kfree(timer)
                           mod_timer(timer)</pre>
			<p>When <code>del_timer()</code> returns, it only guarantees that the timer is deactivated and unqueued, ensuring that it will not be executed in the future. However, on a multiprocessing machine, the timer function might already be executing on another processor. <code>del_timer_sync()</code> should be used in such cases, which will deactivate the timer and wait for any executing handler to exit before returning. This function will check each processor to make sure that the given timer is not currently running there. By using <code>del_timer_sync()</code> in the preceding race condition examples, <code>kfree()</code> could be invoked without worrying about the resource being used in the callback or not. You should almost always use <code>del_timer_sync()</code> instead of <code>del_timer()</code>. The driver must not hold a lock preventing the handler's completion; otherwise, it will result in a deadlock. This makes the <code>del_timer()</code> context agnostic as it is asynchronous, while <code>del_timer_sync()</code> is to be used in a non-atomic context exclusively.</p>
			<p>Moreover, for sanity purposes, we can independently check whether the timer is pending or not using the following API:</p>
			<pre>int timer_pending(const struct timer_list *timer);</pre>
			<p>This function checks whether this timer is armed and pending.</p>
			<p>The following code snippet <a id="_idIndexMarker223"/>shows a basic usage of standard kernel timers:</p>
			<pre>#include &lt;linux/init.h&gt;
#include &lt;linux/kernel.h&gt;
#include &lt;linux/module.h&gt;
#include &lt;linux/timer.h&gt;
static struct timer_list my_timer;
void my_timer_callback(struct timer_list *t)
{
    pr_info("Timer callback&amp;; called\n");
}
 
static int __init my_init(void)
{
    int retval;
    pr_info("Timer module loaded\n");
    timer_setup(&amp;my_timer, my_timer_callback, 0);
    pr_info("Setup timer to fire in 500ms (%ld)\n",
              jiffies);
    retval = mod_timer(&amp;my_timer,
                        jiffies + msecs_to_jiffies(500));
    if (retval)
        pr_info("Timer firing failed\n");
 
    return 0;
}
 
static void my_exit(void)
{
    int retval;
    retval = del_timer(&amp;my_timer);
    /* Is timer still active (1) or no (0) */
    if (retval)
        pr_info("The timer is still in use...\n");
    pr_info("Timer module unloaded\n");
}
module_init(my_init);
module_exit(my_exit);
MODULE_AUTHOR("John Madieu &lt;john.madieu@gmail.com&gt;");
MODULE_DESCRIPTION("Standard timer example");
MODULE_LICENSE("GPL");</pre>
			<p>In the preceding example, we demonstrated a basic usage of standard timers. We request 500 milliseconds of timeout. That said, the unit of time of this kind of timer is a jiffy. So, in order to pass a timeout value in a human format (seconds or milliseconds), you must use conversion helpers. You can see some here:</p>
			<pre>unsigned long msecs_to_jiffies(const unsigned int m)
unsigned long usecs_to_jiffies(const unsigned int u)
unsigned long timespec64_to_jiffies(
                  const struct timespec64 *value);</pre>
			<p>With the preceding helper <a id="_idIndexMarker224"/>functions, you should not expect any accuracy better than a jiffy. For example, using <code>usecs_to_jiffies(100)</code> will return a jiffy. The returned value is rounded up to the closest jiffy value.</p>
			<p>In order to pass additional arguments to the timer callback, the preferred way is to embed them as elements into a structure together with the timer and use the <code>from_timer()</code> macro on the element to retrieve the bigger structure, from which you can access each element. This macro is defined as follows:</p>
			<pre>#define from_timer(var, callback_timer, timer_fieldname) \
    container_of(callback_timer, typeof(*var), timer_fieldname) </pre>
			<p>As an example, let's consider we need to pass two elements to the timer callback: the first one is of type <code>struct sometype</code> and the second one is an integer. In order to pass arguments, we define an additional structure, as follows:</p>
			<pre>struct fake_data {
    struct timer_list timer;
    struct sometype foo;
    int bar;
};</pre>
			<p>After this, we pass the embedded timer to the setup function, as follows:</p>
			<pre>struct fake_data *fd = alloc_init_fake_data();
timer_setup(&amp;fd-&gt;timer, timer_callback, 0);</pre>
			<p>Later in the callback, you must use the <code>from_timer</code> variable to retrieve the bigger structure from which you can access arguments. Here is an example of this in use:</p>
			<pre>void timer_callback(struct timer_list *t)
{
    struct fake_data *fd = from_timer(fd, t, timer);
    sometype data = fd-&gt;data;
    int var = fd-&gt;bar;
[...]
}</pre>
			<p>In the preceding code <a id="_idIndexMarker225"/>snippet, we described how to pass data to the timer callback and how to get this data, using a container structure. By default, a pointer to <code>timer_list</code> is passed to the callback function, instead of the <code>unsigned long</code> data type in versions prior to 4.15.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>High-resolution timers (hrtimers) </h2>
			<p>While the legacy<a id="_idIndexMarker226"/> timer implementation is bound to ticks, high-precision timers provide us with nanosecond-level and tick-agnostic timing accuracy to meet the urgent need for precise time applications or kernel drivers. This has been introduced in kernel v2.6.16 and can be enabled in the build using the <code>CONFIG_HIGH_RES_TIMERS</code> option in the kernel configuration.</p>
			<p>While the standard timer interface keeps/represents time values in <code>jiffies</code>, the high-resolution timer interface came with a new data type, allowing us to keep the time value: <code>ktime_t</code>, which is a simple 64-bit scalar.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Prior to kernel 3.17, the <code>ktime_t</code> type was represented differently on 32- or 64-bit machines. On 64-bit CPUs, it was represented as a plain 64-bit nanosecond value as it is nowadays all over the kernel, while it was represented as a two-32-bit-fields data structure ([<code>seconds – nanoseconds</code>] pair) on 32-bit CPUs.</p>
			<p>Hrtimer APIs require a <code>#include &lt;linux/hrtimer.h&gt;</code> header. That said, in the header file, the structure that characterizes a high-resolution timer is defined as follows:</p>
			<pre>struct hrtimer {
    ktime_t                 _softexpires;
    enum hrtimer_restart    (*function)(struct hrtimer *);
    struct hrtimer_clock_base    *base;
    u8                     state;
[...]
};</pre>
			<p>The elements in the data structure have been shortened to the strict minimum to cover the needs of the book. For the rest, we'll now look at their meaning.</p>
			<p>Before using the<a id="_idIndexMarker227"/> hrtimer, it must be initialized with <code>hrtimer_init()</code>, defined as follows:</p>
			<pre>void hrtimer_init(struct hrtimer *timer,
                  clockid_t which_clock,
                  enum hrtimer_mode mode);</pre>
			<p>In the preceding function, <code>timer</code> is a pointer to the hrtimer to initialize. <code>clock_id</code> tells which type of clock must be used to feed this hrtimer. The following are common options:</p>
			<ul>
				<li><code>CLOCK_REALTIME</code>: This selects the real-time time—that is, the wall time. If the system time changes, it can affect this timer.</li>
				<li><code>CLOCK_MONOTONIC</code>: This is an incremental time, not affected by system changes. However, it stops incrementing when the system goes to sleep or suspends.</li>
				<li><code>CLOCK_BOOTTIME</code>: The running time of the system. Similar to <code>CLOCK_MONOTONIC</code>, the difference is that it includes sleep time. When suspended, it will still increase.</li>
			</ul>
			<p>In the preceding code snippet, the <code>mode</code> parameter tells how the hrtimer should be working. Here are some possible options:</p>
			<ul>
				<li> <code>HRTIMER_MODE_ABS</code>: This means that this timer expires after an absolute specified time.</li>
				<li><code>HRTIMER_MODE_REL</code>: This timer expires after a specified relative from now.</li>
				<li><code>HRTIMER_MODE_PINNED</code>: This hrtimer is bound to a CPU. This is only considered when starting the hrtimer so that the hrtimer fires and executes the callback on the same CPU on which it is queued.</li>
				<li><code>HRTIMER_MODE_ABS_PINNED</code>: This is a combination of the first and the third flags.</li>
				<li><code>HRTIMER_MODE_REL_PINNED</code>: This is a combination of the second and third flags.</li>
			</ul>
			<p>After the hrtimer<a id="_idIndexMarker228"/> has been initialized, it must be assigned a callback function that will be executed upon the timer expiration. The following code snippet shows the expected prototype:</p>
			<pre>enum hrtimer_restart callback(struct hrtimer *h);</pre>
			<p><code>hrtimer_restart</code> is the type to be returned by the callback. It must be either <code>HRTIMER_NORESTART</code> to indicate that the timer must not be restarted (used to perform a one-shot operation) or <code>HRTIMER_RESTART</code> to indicate that the timer must be restarted (to simulate periodical mode). In the first case, when returning <code>HRTIMER_NORESTART</code>, the driver will have to explicitly restart the timer (using <code>hrtimer_start()</code>, for example) if need be. When returning <code>HRTIMER_RESTART</code>, the timer restart is implicit as it will be handled by the kernel. However, the driver needs to reset the timeout before returning from the callback. In order to do so, the driver can use <code>hrtimer_forward()</code>, defined as follows:</p>
			<pre>u64 hrtimer_forward(struct hrtimer *timer,
                    ktime_t now, ktime_t interval)</pre>
			<p>In the preceding code snippet, <code>timer</code> is the hrtimer to forward, <code>now</code> is the point from where the timer must be forwarded, and <code>interval</code> is how long in the future the timer must be forwarded. Do, however, note that this only updates the timer expiry value and does not requeue the timer.</p>
			<p>The <code>now</code> parameter can be obtained in different ways, either by using <code>ktime_get()</code>, which would return the current monotonic clock time or with <code>hrtimer_get_expires()</code>, which would return the time when the timer is supposed to expire before forwarding. This is illustrated in the following code snippet:</p>
			<pre>hrtimer_forward(hrtimer, ktime_get(), ms_to_ktime(500));
/* or */
hrtimer_forward(handle, hrtimer_get_expires(handle),
                 ns_to_ktime(450));</pre>
			<p>In the first line of the<a id="_idIndexMarker229"/> preceding example, the hrtimer is forwarded 500 milliseconds from the current time, while in the second line, it is forwarded 450 nanoseconds from the time when it was supposed to expire. The first line in the example is equivalent to <code>hrtimer_forward_now()</code>, which forwards the hrtimer to a specified time from the current time (from now). It is declared as follows:</p>
			<pre>u64 hrtimer_forward_now(struct hrtimer *timer,
                          ktime_t interval)</pre>
			<p>Now that the timer has been set up and its callback defined, it can be armed (started) using <code>hrtimer_start()</code>, which has the following prototype:</p>
			<pre>int hrtimer_start(struct hrtimer *timer, ktime_t time,
                    const enum hrtimer_mode mode);</pre>
			<p>In the preceding code snippet, <code>mode</code> represents the timer expiry mode, and it should be either <code>HRTIMER_MODE_ABS</code> for an absolute time value or <code>HRTIMER_MODE_REL</code> for a time value relative to now. This parameter must be consistent with the initialization mode parameter. The <code>timer</code> parameter is a pointer to the initialized hrtimer. Finally, <code>time</code> is the expiry time of the hrtimer. Since it is of type <code>ktime_t</code>, various helper functions allow us to generate a <code>ktime_t</code> element from various input time units. These are shown here:</p>
			<pre>ktime_t ktime_set(const s64 secs,
                  const unsigned long nsecs);
ktime_t ns_to_ktime(u64 ns);
ktime_t ms_to_ktime(u64 ms);</pre>
			<p>In the preceding list, <code>ktime_set()</code> generates a <code>ktime_t</code> element from a given number of seconds and nanoseconds. <code>ns_to_ktime()</code> or <code>ms_to_ktime()</code> generate a <code>ktime_t</code> element from a given number of nanoseconds or milliseconds, respectively.</p>
			<p>You may also be interested in returning the number of nano-/microseconds, given a <code>ktime_t</code> input element using the following functions: </p>
			<pre>s64 ktime_to_ns(const ktime_t kt)
s64 ktime_to_us(const ktime_t kt)</pre>
			<p>Moreover, given one or two <code>ktime_t</code> elements, you can perform some arithmetical operations using the following helpers:</p>
			<pre>ktime_t ktime_sub(const ktime_t lhs, const ktime_t rhs);
ktime_t ktime_sub(const ktime_t lhs, const ktime_t rhs);
ktime_t ktime_add(const ktime_t add1, const ktime_t add2);
ktime_t ktime_add_ns(const ktime_t kt, u64 nsec);</pre>
			<p>To subtract or<a id="_idIndexMarker230"/> add <code>ktime</code> objects, you can use <code>ktime_sub()</code> and <code>ktime_add()</code>, respectively. <code>ktime_add_ns()</code> increments a <code>ktime_t</code> element by a specified number of nanoseconds. <code>ktime_add_us()</code> is another variant for microseconds. For subtraction, <code>ktime_sub_ns()</code> and <code>ktime_sub_us()</code> can be used.</p>
			<p>After calling <code>hrtimer_start()</code>, the hrtimer will be armed (activated) and enqueued in a (time-ordered) per-CPU bucket, waiting for its expiration. This bucket will be local to the CPU on which <code>hrtimer_start()</code> has been invoked, but it is not guaranteed that the callback will run on this CPU (migration might happen). For a CPU-bound hrtimer, you should use a <code>*_PINNED</code> mode variant when you initialize the hrtimer.</p>
			<p>An enqueued hrtimer is always started. Once the timer expires, its callback is invoked, and depending on the return value, the hrtimer can be requeued or not. In order to cancel a timer, drivers can use <code>hrtimer_cancel()</code> or <code>hrtimer_try_to_cancel()</code>, declared as follows:</p>
			<pre>int hrtimer_cancel(struct hrtimer *timer);
int hrtimer_try_to_cancel(struct hrtimer *timer);</pre>
			<p>Both functions return <code>0</code> when the timer is not active during the call. <code>hrtimer_try_to_cancel()</code> will return <code>1</code> if the timer is active (running but not executing the callback function) and has been successfully canceled or will fail, returning <code>-1</code> if the callback function is being executed. On the other hand, <code>hrtimer_cancel()</code> will cancel the timer if the callback function is not running yet or will wait for it to finish if it is being executed. When <code>hrtimer_cancel()</code> returns, the caller can be guaranteed that the timer is no <a id="_idIndexMarker231"/>longer active and that its expiration function is not running.</p>
			<p>Drivers can, however, independently check whether the hrtimer callback is still running with the following code:</p>
			<pre>int hrtimer_callback_running(struct hrtimer *timer);</pre>
			<p>For instance, <code>hrtimer_try_to_cancel()</code> internally calls <code>hrtimer_callback_running()</code> and returns <code>-1</code> if the callback is running.</p>
			<p>Let's write a module example to put our hrtimer knowledge into practice. We first start by writing the callback function, as follows:</p>
			<pre>#include &lt;linux/module.h&gt;
#include &lt;linux/kernel.h&gt;
#include &lt;linux/hrtimer.h&gt;
#include &lt;linux/ktime.h&gt;
static struct hrtimer hr_timer;
static enum hrtimer_restart timer_callback(struct hrtimer *timer)
{
    pr_info("Hello from timer!\n");
#ifdef PERIODIC_MS_500
    hrtimer_forward_now(timer, ms_to_ktime(500));
    return HRTIMER_RESTART;
#else
    return HRTIMER_NORESTART;
#endif
}</pre>
			<p>In the preceding hrtimer callback function, we can decide to run in one-shot mode or periodic mode. For periodic mode, the user must define <code>PERIODIC_MS_500</code>, in which case the timer will be forwarded 500 milliseconds in the future from the current hrtimer clock base time before being requeued.</p>
			<p>Then, the rest of the <a id="_idIndexMarker232"/>module implementation looks like this:</p>
			<pre>static int __init hrtimer_module_init(void)
{;
    ktime_t init_time;
    init_time = ktime_set(1, 1000);
    hrtimer_init(&amp;hr_timer, CLOCK_MONOTONIC,
                   HRTIMER_MODE_REL);
    hr_timer.function = &amp;timer_callback;
    hrtimer_start(&amp;hr_timer, init_time, HRTIMER_MODE_REL);
    return 0;
}
static void __exit hrtimer_module_exit(void) {
    int ret;
    ret = hrtimer_cancel(&amp;hr_timer);
    if (ret)
        pr_info("Our timer is still in use...\n");
     pr_info("Uninstalling hrtimer module\n");
}
module_init(hrtimer_module_init);
module_exit(hrtimer_module_exit);</pre>
			<p>In the preceding implementation, we generated an initial <code>ktime_t</code> element of 1 second and 1,000 nanoseconds—that is, 1 second and 1 millisecond, which is used as initial expiration duration. When the <a id="_idIndexMarker233"/>hrtimer expires for the first time, our callback is invoked. If <code>PERIODIC_MS_500</code> is defined, the hrtimer will be forwarded to 500 milliseconds later, and the callback will be periodically invoked (every 500 milliseconds) after the initial invocation; otherwise, it is a one-shot invocation.</p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor052"/>Implementing work-deferring mechanisms</h1>
			<p>Deferring<a id="_idIndexMarker234"/> is a method by which <a id="_idIndexMarker235"/>you schedule a piece of work to be executed in the future. It's a way to report an action later. Obviously, the kernel provides facilities to implement such a mechanism; it allows you to defer functions, whatever their type, to be called and executed later. There are three of them in the kernel, as outlined here:</p>
			<ul>
				<li><strong class="bold">Softirqs</strong>: Executed<a id="_idIndexMarker236"/> in an atomic context</li>
				<li><strong class="bold">Tasklets</strong>: Executed <a id="_idIndexMarker237"/>in an atomic context</li>
				<li><strong class="bold">Workqueues</strong>: Executed <a id="_idIndexMarker238"/>in a process context</li>
			</ul>
			<p>In the next three sections, we will learn in detail the implementation of each of them.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor053"/>Softirqs</h2>
			<p>As the name <a id="_idIndexMarker239"/>suggests, <code>kernel/softirq.c</code> in the kernel source tree, and drivers that wish to use this API need to include <code>&lt;linux/interrupt.h&gt;</code>.</p>
			<p>Softirqs are represented by <code>struct softirq_action</code> structures and are defined as follows:</p>
			<pre>struct softirq_action {
    void (*action)(struct softirq_action *);
};</pre>
			<p>This structure embeds a pointer to the function to run when the softirq is raised. Thus, the prototype of your softirq handler should look like this:</p>
			<pre>void softirq_handler(struct softirq_action *h)</pre>
			<p>Running a<a id="_idIndexMarker240"/> softirq handler results in executing this action function, which has only one parameter: a pointer to the corresponding <code>softirq_action</code> structure. You can register the softirq handler at runtime by means of the <code>open_softirq()</code> function, as illustrated here:</p>
			<pre>void open_softirq(int nr,
                  void (*action)(struct softirq_action *))</pre>
			<p><code>nr</code> represents the softirq index, which is also considered as the softirq priority (where 0 is the highest). <code>action</code> is a pointer to the softirq handler. Possible indexes are enumerated in the following code snippet: </p>
			<pre>enum
{
    HI_SOFTIRQ=0,   /* High-priority tasklets */
    TIMER_SOFTIRQ,  /* Timers */
    NET_TX_SOFTIRQ, /* Send network packets */
    NET_RX_SOFTIRQ, /* Receive network packets */
    BLOCK_SOFTIRQ,  /* Block devices */
    BLOCK_IOPOLL_SOFTIRQ, /* Block devices with I/O polling
                           * blocked on other CPUs */
    TASKLET_SOFTIRQ,/* Normal Priority tasklets */
    SCHED_SOFTIRQ,  /* Scheduler */
    HRTIMER_SOFTIRQ,/* High-resolution timers */
    RCU_SOFTIRQ,    /* RCU locking */
    NR_SOFTIRQS     /* This only represent the number
                     * of softirqs type, 10 actually */
};</pre>
			<p>Softirqs <a id="_idIndexMarker241"/>with lower indexes (highest priority) run before those with higher indexes (lowest priority). The name of all the available softirqs in the kernel are listed in the following array:</p>
			<pre>const char * const softirq_to_name[NR_SOFTIRQS] = {
    "HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "BLOCK_IOPOLL", 
    "TASKLET", "SCHED", "HRTIMER", "RCU"
};</pre>
			<p>It's easy to check some in the output of the <code>/proc/softirqs</code> virtual file, as follows:</p>
			<pre>root@udoo-labcsmart:~# cat /proc/softirqs
                    CPU0       CPU1       
          HI:       3535          1
       TIMER:    4211589    4748893
      NET_TX:    1277827         39
      NET_RX:    1665450          0
       BLOCK:       1978        201
    IRQ_POLL:          0          0
     TASKLET:     455761         33
       SCHED:    4212802    4750408
     HRTIMER:          3          0
         RCU:     438826     286874
root@udoo-labcsmart:~#</pre>
			<p>A <code>NR_SOFTIRQS</code>-entry array of <code>struct softirq_action</code> is declared in <code>kernel/softirq.c</code>, as follows:</p>
			<pre>static struct softirq_action softirq_vec[NR_SOFTIRQS] ;</pre>
			<p>Each entry in this array may contain one—and only one—softirq. Consequently, there can be a maximum of <code>NR_SOFTIRQS</code> (actually, 10 in v5.10, the last stable version at the time of writing) registered<a id="_idIndexMarker242"/> softirqs. The following code snippet from <code>kernel/softirq.c</code> illustrates this:</p>
			<pre>void open_softirq(int nr,
                  void (*action)(struct softirq_action *))
{
    softirq_vec[nr].action = action;
}</pre>
			<p>A concrete example is the network subsystem, which registers the softirqs it needs (in <code>net/core/dev.c</code>), as follows:</p>
			<pre>open_softirq(NET_TX_SOFTIRQ, net_tx_action);
open_softirq(NET_RX_SOFTIRQ, net_rx_action);</pre>
			<p>Before a registered softirq can execute, it should be activated/scheduled. In order to do so, you have to call <code>raise_softirq()</code> or <code>raise_softirq_irqoff()</code> (if interrupts are already off), as illustrated in the following code snippet:</p>
			<pre>void __raise_softirq_irqoff(unsigned int nr)
void raise_softirq_irqoff(unsigned int nr)
void raise_softirq(unsigned int nr)</pre>
			<p>The first function simply sets the appropriate bit in the per-CPU softirq bitmap (the <code>__softirq_pending</code> field in the <code>struct irq_cpustat_t</code> data structure allocated per CPU in <code>kernel/softirq.c</code>), as follows:</p>
			<pre>irq_cpustat_t irq_stat[NR_CPUS] ____cacheline_aligned;
EXPORT_SYMBOL(irq_stat);</pre>
			<p>When the flag is checked, this allows it to run. This function is described here for study purposes and should not be used directly.</p>
			<p><code>raise_softirq_irqoff</code> needs to be called with interrupts disabled. First, it internally calls <code>__raise_softirq_irqoff()</code>, described previously, to activate the softirq. Afterward, it <a id="_idIndexMarker243"/>checks whether it has been called from within an interrupt (either hardirq or softirq) context by mean of an <code>in_interrupt()</code> macro (which simply returns the value of <code>current_thread_info( )-&gt;preempt_count</code>, where 0 means preemption-enabled, stating that we are not in an interrupt context, and a &gt; 0 value means we are in an interrupt context). If <code>in_interrupt() &gt; 0</code>, this does nothing when we are in an interrupt context because softirq flags are checked on the exit path of any I/O IRQ handler (see <code>asm_do_IRQ()</code> for ARM or <code>do_IRQ()</code> for x86 platforms, which makes a call to <code>irq_exit()</code>). Here, softirqs run in an interrupt context. However, if <code>in_interrupt() == 0</code>, it invokes <code>wakeup_softirqd()</code>, responsible for waking the local CPU <code>ksoftirqd</code> thread up (it schedules it, actually) in order to ensure the softirq runs soon but in a process context this time.</p>
			<p><code>raise_softirq</code>, on the other hand, first calls <code>local_irq_save()</code> (which disables interrupts on the local processor after saving its current interrupt flags). It then calls <code>raise_softirq_irqoff()</code>, described previously, in order to schedule the softirq on the local CPU (remember—this function must be invoked with IRQs disabled on the local CPU). Finally, it calls <code>local_irq_restore()</code> in order to restore the previously saved interrupt flags.</p>
			<p>Here are a few things to remember about softirqs:</p>
			<ul>
				<li>A softirq can never preempt another softirq. Only hardware interrupts can. Softirqs are executed at a high priority, with scheduler preemption disabled but IRQs enabled. This makes softirqs suitable for the most time-critical and important deferred processing on the system.</li>
				<li>While a handler runs on a CPU, other softirqs on this CPU are disabled. Softirqs run concurrently, however. While a softirq is running, another softirq (even the same one) can run on another processor. This is one of the main advantages of softirqs over hardirqs and is the reason why they are used in the networking subsystem, which may require heavy CPU power.</li>
				<li>Softirqs are <a id="_idIndexMarker244"/>mostly scheduled in the return path of hardware interrupt handlers. If any is scheduled out of the interrupt context, it will run in a process context if it is still pending when the local <code>ksoftirqd</code> thread is given to the CPU. Their execution may be triggered in the following cases:<ul><li>By the local per-CPU timer interrupt (on SMP system only, with <code>CONFIG_SMP</code> enabled). See <code>timer_tick()</code>, <code>update_process_times()</code>, and <code>run_local_timers()</code>.</li><li>By a call to the <code>local_bh_enable()</code> function (mostly invoked by the network subsystem for handling packet-receiving/-transmitting softirqs).</li><li>On the exit path of any I/O IRQ handler (see <code>do_IRQ</code>, which makes a call to <code>irq_exit()</code>, in turn invoking <code>invoke_softirq()</code>.</li><li>When the local <code>ksoftirqd</code> thread is given to the CPU (aka awakened).</li></ul></li>
			</ul>
			<p>The actual kernel function responsible for walking through the softirqs' pending bitmap and running them is <code>__do_softirq()</code>, defined in <code>kernel/softirq.c</code>. This function is always invoked with interrupts disabled on the local CPU. It does the following tasks:</p>
			<ul>
				<li>Once invoked, the function first saves the current per-CPU pending softirqs' bitmap in a variable named <code>pending</code>, and locally disables softirqs by means of <code>__local_bh_disable_ip</code>.</li>
				<li>It then resets the current per-CPU pending bitmask (which has already been saved) and then re-enables interrupts (softirqs run with interrupts enabled).</li>
				<li>After this, it enters a <code>while</code> loop, checking for pending softirqs in the saved bitmap. If there is no softirq pending, it will execute the handler of each pending softirq, taking care to increment their execution statistics.</li>
				<li>After all pending IRQ handlers have been executed (we are out of the <code>while </code>loop), <code>__do_softirq()</code> again reads the per-CPU pending bitmask in order to check if any softirqs were scheduled again when it was in the <code>while</code> loop. If there are any pending softirqs, the whole process will restart (based on a <code>goto</code> loop), starting from <em class="italic">Step 2</em>. This helps in handling, for example, softirqs that rescheduled themselves.</li>
			</ul>
			<p>However, <code>__do_softirq()</code> will not repeat if one of the following conditions occurs:</p>
			<ul>
				<li>It has already repeated up to <code>MAX_SOFTIRQ_RESTART</code> times, which is set to 10 in <code>kernel/softirq.c</code>. This is the limit of the softirqs' processing loop, not the upper bound of the previously described <code>while</code> loop.</li>
				<li>It has hogged<a id="_idIndexMarker245"/> the CPU more than <code>MAX_SOFTIRQ_TIME</code>, which is set to 2 milliseconds (<code>msecs_to_jiffies(2)</code>) in <code>kernel/softirq.c</code>, since this would prevent the scheduler from being enabled.</li>
			</ul>
			<p>If one of the two aforementioned situations occurs, <code>__do_softirq()</code> will break its loop and call <code>wakeup_softirqd()</code> in order to wake the local <code>ksoftirqd</code> thread, which will later execute the pending softirqs in a process context. Since <code>do_softirq</code> is called at many points in the kernel, it is likely that another invocation of <code>__do_softirqs</code> handles pending softirqs before the <code>ksoftirqd</code> thread has a chance to run.</p>
			<h3>Some words about ksoftirqd</h3>
			<p><code>ksoftirqd</code> is a <a id="_idIndexMarker246"/>per-CPU kernel thread raised to handle unserviced software interrupts. It is spawned early during the kernel boot process, as stated in <code>kernel/softirq.c</code> and shown here:</p>
			<pre>static __init int spawn_ksoftirqd(void)
{
    cpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, "softirq:dead",
                          NULL, takeover_tasklets);
    BUG_ON(smpboot_register_percpu_thread(&amp;softirq_threads));
    return 0;
}
early_initcall(spawn_ksoftirqd);</pre>
			<p>After running the top command <a id="_idIndexMarker247"/>in the preceding code snippet, we can see some <code>ksoftirqd/&lt;n&gt;</code> entries, where <code>&lt;n&gt;</code> is the logical CPU index of the CPU running the <code>ksoftirqd</code> thread. As <code>ksoftirqd</code> threads run in a process context, they are equal to classic processes/threads, so they compete for the CPU. <code>ksoftirqd</code> threads hogging CPUs for a long time may indicate a system under heavy load.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor054"/>Tasklets</h2>
			<p>Before starting to discuss<a id="_idIndexMarker248"/> about <strong class="bold">tasklets</strong>, you must notice that these are scheduled for removal in the Linux kernel, thus the purpose of this section is purely for pedagogic reasons, to help you understanding their use in older kernel modules. Consequently, you must not use these in your developments.</p>
			<p>Tasklets are bottom halves that are built on top of <code>HI_SOFTIRQ</code> and <code>TASKLET_SOFTIRQ</code> <code>HI_SOFTIRQ</code>-based tasklets run before <code>TASKLET_SOFTIRQ</code>-based ones. Simply put, tasklets are softirqs and obey the same rules. Unlike softirqs, however, two of the same tasklets never run concurrently. The tasklet API is quite basic and intuitive.</p>
			<p>Tasklets are represented by a <code>struct tasklet_struct</code> structure defined in <code>&lt;linux/interrupt.h&gt;</code>. Each instance of this structure represents a unique tasklet, as illustrated in the following code snippet:</p>
			<pre>struct tasklet_struct
{
	struct tasklet_struct *next;
	unsigned long state;
	atomic_t count;
	bool use_callback;
	union {
		void (*func)(unsigned long data);
		void (*callback)(struct tasklet_struct *t);
	};
	unsigned long data;
};</pre>
			<p>Though this API is scheduled for removal, it has been slightly modernized as compared to its legacy implementation. The callback function is stored in the <code>callback()</code> field rather than <code>func()</code>, which is kept for compatibility with the old implementation. This new callback simply takes a pointer to the <code>tasklet_struct</code> structure as its one argument. The handler will be executed by the underlying softirq. It is the equivalent of <code>action</code> to a softirq, with the same prototype and the same argument meaning. <code>data</code> will be passed as its sole argument.</p>
			<p>Whether <code>callback()</code> handler or <code>func()</code> handler is executed depends on the way the tasklet is initialized. A tasklet can be statically initialized using either <code>DECLARE_TASKLET()</code> macro or <code>DECLARE_TASKLET_OLD()</code> macro. These macros are defined as follows:</p>
			<pre>#define DECLARE_TASKLET_OLD(name, _func)       \
    struct tasklet_struct name = {             \
    .count = ATOMIC_INIT(0),            	   \
    .func = _func,                    	        \
}
#define DECLARE_TASKLET(name, _callback)       \
     struct tasklet_struct name = {            \
     .count = ATOMIC_INIT(0),                  \
     .callback = _callback,                    \
     .use_callback = true,                     \
}</pre>
			<p>From what we can see, by using <code>DECLARE_TASKLET_OLD()</code>, the legacy<a id="_idIndexMarker249"/> implementation is kept and <code>func()</code> is used as the callback. Therefore, the prototype of the provided handler must be as follows:</p>
			<pre>void foo(unsigned long data);</pre>
			<p>By using <code>DECLARE_TASKLET()</code>, the <code>callback</code> field is used as the handler and <code>use_callback</code> filed is set to <code>true</code> (this is because the tasklet core checks this value to determine the handler that must be invoked). In this case, the protype of the callback is as follows:</p>
			<pre>void foo(struct tasklet_struct *t)</pre>
			<p>In the previous snipped, <code>t</code> pointer is passed by the tasklet core while invoking the handler. It will point to your tasklet. Since a pointer to the tasklet is passed as argument to the callback, it is common to embed the tasklet object within a larger, user-specific structure, the pointer to which can be obtained with the <code>container_of()</code> macro. In order to do so, you should rather use the dynamic initialization, which can be achieved thanks to <code>tasklet_setup()</code> function, defined as follows:</p>
			<pre>void tasklet_setup(struct tasklet_struct *t,
     void (*callback)(struct tasklet_struct *));</pre>
			<p>According to the previous prototype, we can guess that by using the dynamic initialization, we have no choice but to use the new implementation where <code>callback</code> field is used as the tasklet handler.</p>
			<p>Using static or dynamic method depends <a id="_idIndexMarker250"/>on what you need to achieve, for example, if you want the tasklet to be unique for the whole module or to be private per probed device, or even more, if you need to have a direct or indirect reference to the tasklet.</p>
			<p>By default, an initialized tasklet is runnable when it is scheduled: <em class="italic">it is said to be enabled</em>. <code>DECLARE_TASKLET_DISABLED</code> is an alternative to statically initialize default-disabled tasklets. There is no such alternative for a dynamically initialized tasklet, unless you invoke <code>tasklet_disable()</code> on this tasklet after it has been dynamically initialized. A disabled tasklet will require the <code>tasklet_enable()</code> function to be invoked to make this tasklet runnable. Tasklets are scheduled (similar to raising the softirq) via the <code>tasklet_schedule()</code> and <code>tasklet_hi_schedule()</code> functions. You can use <code>tasklet_disable()</code> API to disable a scheduled or running tasklet. This function disables the tasklet and returns only when the tasklet has terminated its execution (assuming it was running). After this, the tasklet can still be scheduled, but it will not run on the CPU until it is enabled again. The asynchronous variant <code>tasklet_disable_nosync()</code> can be used too, which returns immediately, even if the termination has not occurred. Moreover, a tasklet that has been disabled several times should be enabled the same number of times (this is enforced and verified by the kernel thanks to the <code>count</code> field in the <code>struct tasklet</code> object). The definition of the previously mentioned tasklet APIs is illustrated in the following snippet:</p>
			<pre>DECLARE_TASKLET(name, _callback)
DECLARE_TASKLET_DISABLED(name, _callback);
DECLARE_TASKLET_OLD(name, func);
void tasklet_setup(struct tasklet_struct *t,
     void (*callback)(struct tasklet_struct *));
void tasklet_enable(struct tasklet_struct *t);
void tasklet_disable(struct tasklet_struct *t);
void tasklet_schedule(struct tasklet_struct *t);
void tasklet_hi_schedule(struct tasklet_struct *t);</pre>
			<p>The kernel maintains normal-priority and high-priority tasklets in two per-CPU queues (each CPU maintains its low- and high-priority queue pair). <code>tasklet_schedule()</code> adds the tasklet into the normal priority list of the CPU on which it is invoked, scheduling the associated softirq with a <code>TASKLET_SOFTIRQ</code> flag. With <code>tasklet_hi_schedule()</code>, the tasklet is added into the high-priority list (still of the list on which it is invoked), scheduling the associated softirq with a <code>HI_SOFTIRQ</code> flag. When the tasklet is scheduled, its <code>TASKLET_STATE_SCHED</code> flag is set, and the tasklet is queued for execution. At the time of execution, a <code>TASKLET_STATE_RUN</code> flag is set, and the <code>TASKLET_STATE_SCHED</code> state is removed, thus making the tasklet re-schedulable during its execution, either by the tasklet itself or from within an interrupt handler.</p>
			<p><code>tasklet_schedule()</code> on a tasklet already scheduled and whose execution has not started yet will do nothing, resulting in the tasklet being executed only once. A tasklet can reschedule itself, and you can safely call <code>tasklet_schedule()</code> in a tasklet. High priority tasklets are always executed before normal ones and should then be used carefully, else you may increase system latency. Stopping a tasklet is as simple as calling <code>tasklet_kill()</code>, as illustrated in the following code snippet, which will prevent the tasklet from running again, or waiting for its completion before killing it if the tasklet is currently scheduled to run. If a tasklet re-schedules itself, you should first prevent the tasklet from re-scheduling itself prior to calling this function:</p>
			<pre>void tasklet_kill(struct tasklet_struct *t);</pre>
			<h3>Writing your tasklet handler</h3>
			<p>All that being said, let's <a id="_idIndexMarker251"/> see some use cases, as follows:</p>
			<pre># #include &lt;linux/init.h&gt;
#include &lt;linux/module.h&gt;
#include &lt;linux/kernel.h&gt;
#include &lt;linux/interrupt.h&gt;    /* for tasklets api */
/* Tasklet handler, that just prints the handler name */
void tasklet_function(struct tasklet_struct *t)
{
    pr_info("running %s\n", __func__);
}
DECLARE_TASKLET(my_tasklet, tasklet_function);
static int __init my_init(void)
{
    /* Schedule the handler */
    tasklet_schedule(&amp;my_tasklet);
    pr_info("tasklet example\n");
    return 0;
}
void my_exit( void )
{
    /* Stop the tasklet before we exit */
    tasklet_kill(&amp;my_tasklet);
    pr_info("tasklet example cleanup\n");
    return;
}
module_init(my_init);
module_exit(my_exit);
MODULE_AUTHOR("John Madieu &lt;john.madieu@gmail.com&gt;");
MODULE_LICENSE("GPL");</pre>
			<p>In the preceding  <a id="_idIndexMarker252"/>code snippet, we statically declare our <code>my_tasklet</code> tasklet and the function supposed to be invoked when this tasklet is scheduled. Since we did not used the <code>_OLD</code> variant, we defined the handler prototype the same as <code>callback</code> field in the tasklet object.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Tasklet API is deprecated, and you should consider using threaded IRQs instead.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor055"/>Workqueues</h2>
			<p>Added since Linux kernel 2.6, the <a id="_idIndexMarker253"/>most used and simple deferring mechanism is a workqueue. As a deferring mechanism, it takes an opposite approach to the others we've seen, running only in a preemptible context. It is the only choice when sleeping is required unless you implicitly create a kernel thread or unless you are using a threaded interrupt. That said, workqueues are built on top of kernel threads, and for this simple reason, we will not implicitly cover kernel threads in this book. </p>
			<p>At the core of the workqueue subsystem, there are two data structures that fairly well explain the concept behind this, as follows:</p>
			<ul>
				<li>The work to be deferred (referred to as a work item), represented in the kernel by instances of <code>struct work_struct</code>, which indicates the handler function to run. If you need a delay before the work runs after it has been submitted to the workqueue, the kernel provides a <code>struct delayed_work</code> instance instead. A work item is a simple structure that only contains a pointer to the function that is to be scheduled for asynchronous execution. To summarize, we can enumerate two types of work item structures, as follows:<ul><li>A <code>work_struct</code> structure, which schedules a task to run as soon as possible when the system allows it</li><li>A <code>delayed_work</code> structure, which schedules a task to run after at least a given time interval</li></ul></li>
				<li>The workqueue itself, which is represented by a <code>struct workqueue_struct</code> instance and is the structure onto which work is placed. It is a queue of work items.</li>
			</ul>
			<p>Apart from these data structures, there are two generic terms you should be familiar with, as follows:</p>
			<ul>
				<li><strong class="bold">Worker threads</strong>, which <a id="_idIndexMarker254"/>are dedicated threads that execute and pull the functions off the queue, one by one, one after the other.</li>
				<li><strong class="bold">Worker pools</strong>: This is <a id="_idIndexMarker255"/>a collection of worker threads (a thread pool) that are used to better manage the worker threads.</li>
			</ul>
			<p>The first step in using workqueues consists of creating a work item, represented by <code>struct work_struct</code> or <code>struct delayed_work</code> for the delayed variant, and defined in <code>linux/workqueue.h</code>. The kernel provides either a <code>DECLARE_WORK</code> macro to statically declare and initialize a work structure or dynamically uses an <code>INIT_WORK</code> macro. If you need delayed work, you can use the <code>INIT_DELAYED_WORK</code> macro for dynamic allocation and initialization or <code>DECLARE_DELAYED_WORK</code> for a static one. You can see the macros in action in the following code snippet:</p>
			<pre>DECLARE_WORK(name, function)
DECLARE_DELAYED_WORK(name, function)
INIT_WORK(work, func );
INIT_DELAYED_WORK( work, func);</pre>
			<p>Here is what our <a id="_idIndexMarker256"/>work item structure looks like:</p>
			<pre>struct work_struct {
    atomic_long_t data;
    struct list_head entry;
    work_func_t func;
};
struct delayed_work {
    struct work_struct work;
    struct timer_list timer;
    struct workqueue_struct *wq;
    int cpu;
};</pre>
			<p>The <code>func</code> field, which is of type <code>work_func_t</code>, tells us a bit more about the header of a <code>work</code> function, as illustrated here:</p>
			<pre>typedef void (*work_func_t)(struct work_struct *work);</pre>
			<p><code>work</code> is an input parameter that corresponds to the work structure to be scheduled. If you submitted delayed work, it would correspond to the <code>delayed_work.work</code> field. It would then be necessary to use the <code>to_delayed_work()</code> function in order to get the underlying delayed work structure, as illustrated in the following code snippet:</p>
			<pre>struct delayed_work *to_delayed_work(
                struct work_struct *work)</pre>
			<p>Workqueue infrastructure<a id="_idIndexMarker257"/> allows drivers to create a dedicated kernel thread (a workqueue) called a worker thread to run work functions. A new workqueue can be created with the following functions:</p>
			<pre>struct workqueue_struct *create_workqueue(const char *name)
struct workqueue_struct *create_singlethread_workqueue(
                                          const char *name)</pre>
			<p><code>create_workqueue()</code> creates a dedicated thread (a worker thread) per CPU on the system. For example, on an 8-core system, it will result in 8 kernel threads created to run the works submitted to your workqueue. Unless you have strong reasons to create a thread per CPU, you should prefer the single thread variant. In most cases, a single system kernel thread should be enough. In this case, you should use <code>create_singlethread_workqueue()</code> instead, which creates—as its name states—a single-threaded workqueue. Either normal or delayed works can be enqueued onto the same queue. In order to schedule works on your created workqueue, you can use either <code>queue_work()</code> or <code>queue_delayed_work()</code>, depending on the nature of the work. These functions are defined as follows:</p>
			<pre>bool queue_work(struct workqueue_struct *wq,
                 struct work_struct *work)
bool queue_delayed_work(struct workqueue_struct *wq,
                        struct delayed_work *dwork,
                        unsigned long delay)</pre>
			<p>These functions return <code>false</code> if the work was already on a queue and <code>true</code> otherwise. <code>queue_dalayed_work()</code> is to be used to schedule a work item (a delayed one) for later execution after a given delay. The time unit for the delay is a jiffy. There are, however, APIs to convert milliseconds and microseconds to jiffies, defined as follows:</p>
			<pre>unsigned long msecs_to_jiffies(const unsigned int m)
unsigned long usecs_to_jiffies(const unsigned int u)</pre>
			<p>The following example uses 200 milliseconds as a delay:</p>
			<pre>queue_delayed_work(my_wq, &amp;drvdata-&gt;tx_work,
                  usecs_to_jiffies(200));</pre>
			<p>You should not expect this<a id="_idIndexMarker258"/> delay to be accurate as the delay will be rounded up to the closest jiffy value. Thus, even when requesting 200 us, you should expect a jiffy. Submitted work items can be canceled by calling either <code>cancel_delayed_work()</code>, <code>cancel_delayed_work_sync()</code>, or <code>cancel_work_sync()</code>. These cancelation functions are defined as follows:</p>
			<pre>bool cancel_work_sync(struct work_struct *work)
bool cancel_delayed_work(struct delayed_work *dwork)
bool cancel_delayed_work_sync(struct delayed_work *dwork)</pre>
			<p><code>cancel_work_sync()</code> synchronously cancels the given work—in other words, it cancels work and waits for its execution to finish. The kernel guarantees <code>work</code> not to be pending or executing on any CPU on return from this function, even if the work migrates to another workqueue or requeues itself. It returns <code>true</code> if <code>work</code> was pending and <code>false</code> otherwise.</p>
			<p><code>cancel_delayed_work()</code> asynchronously cancels a delayed entry. It returns <code>true</code> (actually, a nonzero value) if <code>dwork</code> was pending and canceled and <code>false</code> if it wasn't pending, probably because it is actually running, and thus might still be running after <code>cancel_delayed_work()</code> has returned. To make sure the work really ran to its end, you may want to use <code>flush_workqueue()</code>, which flushes every work item in the given queue, or <code>cancel_delayed_work_sync()</code>, which is the synchronous version of <code>cancel_delayed_work()</code>.</p>
			<p>When you are done with a workqueue, you should destroy it with <code>destroy_workqueue()</code>, as illustrated here:</p>
			<pre>void flush_workqueue(struct worksqueue_struct * queue); 
void destroy_workqueue(structure workqueque_struct *queue);</pre>
			<p>While waiting for any pending work to execute, the <code>_sync</code> variant functions sleep and thus can be called only from a process context.</p>
			<h3>Kernel-global workqueue – the shared queue</h3>
			<p>In most situations, your <a id="_idIndexMarker259"/>code does not necessarily need the performance of its own dedicated set of threads, and because <code>create_workqueue()</code> creates one worker thread for each CPU, it may be a bad idea to use it on very large multi-CPU systems. You may then want to use the kernel shared queue, which already has its set of kernel threads pre-allocated (early during the boot, via the <code>workqueue_init_early()</code> function) for running works.</p>
			<p>This kernel-global workqueue is the so-called <code>system_wq</code> workqueue, defined in <code>kernel/workqueue.c</code>. There is actually one instance per CPU, each backed by a dedicated thread named <code>events/n</code>, where <code>n</code> is the processor number (or index) to which the thread is bound.</p>
			<p>You can queue a work item in the default system workqueue using one of the following functions:</p>
			<pre>int schedule_work(struct work_struct *work);
int schedule_delayed_work(struct delayed_work *dwork,
                          unsigned long delay);
int schedule_work_on(int cpu,
               struct work_struct *work);
int schedule_delayed_work_on(int cpu,
                struct delayed_work *dwork,
                unsigned long delay);</pre>
			<p><code>schedule_work()</code> immediately schedules the work that will be executed as soon as possible after the worker thread on the current processor wakes up. With <code>schedule_delayed_work()</code>, the work will be put in the queue in the future, after the delay timer ticks. <code>_on</code> variants are used to schedule the work on a specific CPU (which does not absolutely need to be the current one). Each of these functions queue works on the system's shared workqueue, <code>system_wq</code>, defined in <code>kernel/workqueue.c</code> as follows:</p>
			<pre>struct workqueue_struct *system_wq __read_mostly;
EXPORT_SYMBOL(system_wq);</pre>
			<p>You should also note that since this system workqueue is shared, you should not queue works which can run for too long, otherwise it may slow down other contender works, which could then wait for longer than they should before being executed.</p>
			<p>In order to flush the kernel-global workqueue—that is, ensure a given batch of work is completed—we can use <code>flush_scheduled_work()</code>, as follows:</p>
			<pre>void flush_scheduled_work(void);</pre>
			<p><code>flush_scheduled_work()</code> is a<a id="_idIndexMarker260"/> wrapper that calls <code>flush_workqueue()</code> on <code>system_wq</code>. Note that there may be works in the <code>system_wq</code> workqueue that you have not submitted and have no control over. Flushing this workqueue entirely is thus overkill, and it is recommended to run <code>cancel_delayed_work_sync()</code> or <code>cancel_work_sync()</code> instead.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Unless you have a strong reason for creating a dedicated thread, the default (kernel-global) thread is preferred.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor056"/>Workqueues' new generation</h2>
			<p>The original (now legacy) workqueue <a id="_idIndexMarker261"/>implementation used two kinds of workqueues: those with a single thread system-wide, and those with a thread per CPU. However, for a growing number of CPUs, this led to some limitations, as outlined here:</p>
			<ul>
				<li>On very large systems, the kernel could run out of <code>init</code> process started.</li>
				<li>Multithreaded workqueues provided poor concurrency management as their threads compete for the CPU with each other threads on the system. As there were more CPU contenders, this introduced some overhead—that is, more context switches than necessary.</li>
				<li>The consumption of far more resources than were really needed.</li>
			</ul>
			<p>Moreover, subsystems that needed a dynamic or fine-grained level of concurrency had to implement their own thread pools. As a result of this, a new workqueue API has been designed, and the legacy workqueue API (<code>create_workqueue()</code>,  <code>create_singlethread_workqueue()</code>, and <code>create_freezable_workqueue()</code>) is scheduled for removal, though they are actually wrappers around the new one, the so-called <strong class="bold">Concurrency Managed Workqueue</strong> (<strong class="bold">cmwq</strong>), using per-CPU<a id="_idIndexMarker262"/> worker pools shared by all workqueues in order to automatically provide a dynamic and flexible level of concurrency, abstracting such details for the API users.</p>
			<h3>cmwq</h3>
			<p>cmwq<a id="_idIndexMarker263"/> is an upgrade of workqueue APIs. Using this new API implies you are choosing between the <code>alloc_workqueue()</code> function and the <code>alloc_ordered_workqueue()</code> macro to create a workqueue. They both allocate a workqueue and return a pointer to it on success, and <code>NULL</code> on failure. The returned workqueue can be freed using the <code>destroy_workqueue()</code> function. You can see an illustration of the code in the following snippet:</p>
			<pre>struct workqueue_struct *alloc_workqueue(const char *fmt,
                             unsigned int flags,
                             int max_active, ...);
#define alloc_ordered_workqueue(fmt, flags, args...) [...]
void destroy_workqueue(struct workqueue_struct *wq)</pre>
			<p><code>fmt</code> is the <code>printf</code> format for the name of the workqueue, and <code>args...</code> are arguments for <code>fmt</code>.</p>
			<p><code>destroy_workqueue()</code> is to be called on the workqueue once you are done with it. All works currently pending will be done first before the kernel truly destroys the workqueue. <code>alloc_workqueue()</code> creates a workqueue based on <code>max_active</code>, which defines the concurrency level by limiting the number of works (tasks, actually) that can be executing (workers in a runnable state) simultaneously from this workqueue on any given CPU. For example, a <code>max_active</code> value of 5 would mean at most 5 work items of this workqueue can be executing at the same time per CPU. On the other hand, <code>alloc_ordered_workqueue()</code> creates a <a id="_idIndexMarker264"/>workqueue that processes each work item one by one in queued order (that is, <strong class="bold">first-in, first-out</strong> (<strong class="bold">FIFO</strong>) order).</p>
			<p><code>flags</code> controls how and when work items are queued, assigned execution resources, scheduled, and executed. There are various flags used in this new API on which we should spend some time, as follows:</p>
			<ul>
				<li><code>WQ_UNBOUND</code>: Legacy <a id="_idIndexMarker265"/>workqueues had a worker thread per CPU and were designed to run tasks on the CPU where they were submitted. The kernel scheduler had no choice but to always schedule a worker on the CPU on which it was defined. With this approach, even a single workqueue was able to prevent a CPU from idling and being turned off, which leads to increased power consumption or poor scheduling policies. <code>WQ_UNBOUND</code> turns off the previously-described behavior. Work items are <a id="_idIndexMarker266"/>not bound to the CPU anymore, hence the name <strong class="bold">unbound workqueues</strong>. There is no more locality, and the scheduler can reschedule workers on any CPU as it sees fit. The scheduler has the last word now and can balance CPU load, especially for long and sometimes CPU-intensive works.</li>
				<li><code>WQ_MEM_RECLAIM</code>: This<a id="_idIndexMarker267"/> flag is to be set for workqueues that need to guarantee forward progress during the memory reclaim path (when free memory is running dangerously low; the system is said to be under memory pressure). In this case, <code>GFP_KERNEL</code> allocations may block and deadlock the entire workqueue. The workqueue is then guaranteed to have at least a ready-to-use worker thread—a so-called rescuer thread—reserved for it, regardless of memory pressure, so that it can progress forward. There's one rescuer thread allocated for each workqueue that has this flag set.</li>
			</ul>
			<p>Let's consider a situation where we have three work items (<code>w1</code>, <code>w2</code>, and <code>w3</code>) in our workqueue <code>W</code>. <code>w1</code> does some work and then waits for <code>w3</code> to complete (let's say it depends on the computation result of <code>w3</code>). Afterward, <code>w2</code> (which is independent of the others) does some <code>kmalloc()</code> allocation (<code>GFP_KERNEL</code>) and, oops—there's not enough memory. While <code>w2</code> is blocked, it still occupies <code>W</code>'s workqueue. This results in <code>w3</code> not being able to run, even though there is no dependency between <code>w2</code> and <code>w3</code>. As there is not enough memory available, there would be no way of allocating a new thread to run <code>w3</code>. A pre-allocated thread would for sure solve this problem, not by magically allocating the memory for <code>w2</code>, but by running <code>w3</code> so that <code>w1</code> can continue its job, and so on. <code>w2</code> will continue its progression as soon as possible when there is enough available memory to allocate. This pre-allocated thread is a so-called <code>WQ_MEM_RECLAIM</code> flag if you think the workqueue is likely to be used in the memory reclaim path. This flag replaces the legacy <code>WQ_RESCUER</code> flag as of this commit: <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=493008a8e475771a2126e0ce95a73e35b371d277">https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=493008a8e475771a2126e0ce95a73e35b371d277</a>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Memory reclaim<a id="_idIndexMarker269"/> is a Linux mechanism on a memory allocation path that consists of allocating memory only after throwing the current content of that memory somewhere else.</p>
			<ul>
				<li><code>WQ_FREEZABLE</code>: This<a id="_idIndexMarker270"/> flag is used for power-management purposes. A workqueue with this flag set will be frozen when the system is suspended or hibernates. On the freezing path, all current work items of the worker(s) will be processed. When the freeze is complete, no new work item will be executed until the system is unfrozen. Filesystem-related workqueue(s) may use this flag (that is, to ensure that modifications made on files are pushed to the disk or create a hibernation image on the freezing path and that no modification is made on-disk after the hibernation image has been created. In this situation, non-freezable items or doing things differently could lead to filesystem corruption. As an example, all <code>fs/xfs/xfs_super.c</code>) to ensure no further changes are made on-disk once the freezer infrastructure freezes kernel threads and creates a hibernation image. You should definitely not have this flag set if your workqueue can run tasks as part of the hibernation/suspend/resume process of the system. More information on this topic can be found in <code>Documentation/power/freezing-of-tasks.txt</code> and by having a look at the <code>freeze_workqueues_begin()</code> and <code>thaw_workqueues()</code> internal kernel functions.</li>
				<li><code>WQ_HIGHPRI</code>: Tasks<a id="_idIndexMarker272"/> with this flag set run immediately and do not wait for the CPU to become available. This flag is used for workqueues that queue work items requiring a high priority for execution. Such workqueues have worker threads with a high priority level (lower nice value). In the earlier days of the cmwq, high-priority work items were just queued at the head of a global normal priority worklist so that they could immediately run. Nowadays, there are no interactions between normal-priority and high-priority workqueues as each has its own worklist and its own worker pool. Work items of high-priority workqueues are queued to the high-priority worker pool of the target CPU. Tasks in this workqueue should not block much. Use this flag if you do not want your work item competing for CPU with normal- or lower-priority tasks. Crypto and block devices subsystems use this, for example.</li>
				<li><code>WQ_CPU_INTENSIVE</code>: Work<a id="_idIndexMarker273"/> items of CPU-intensive workqueues may burn a lot of CPU cycles and do not participate in workqueue concurrency management. Instead, as with any other task, their execution is regulated by the system scheduler, which makes this flag handy for bound work items that may consume a lot of CPU time. Though the system scheduler controls their execution, concurrency management controls the start of their execution, and runnable non-CPU-intensive work items might cause CPU-intensive work items to be delayed. The <code>crypto</code> and <code>dm-crypt</code> subsystems use such workqueues. To prevent such tasks from delaying the execution of other non-CPU-intensive work items, they will not be considered when the workqueue code determines whether the CPU is available or not.</li>
			</ul>
			<p>To be compliant and feature-compatible with the old workqueue API, the following mappings are done to keep this API compatible with the original one:</p>
			<ul>
				<li><code>create_workqueue(name)</code> is mapped onto <code>alloc_workqueue(name,WQ_MEM_RECLAIM, 1)</code></li>
				<li><code>create_singlethread_workqueue(name)</code> is mapped onto <code>alloc_ordered_workqueue(name, WQ_MEM_RECLAIM)</code></li>
				<li><code>create_freezable_workqueue(name)</code> is mapped onto <code>alloc_workqueue(name,WQ_FREEZABLE | WQ_UNBOUND|WQ_MEM_RECLAIM, 1)</code></li>
			</ul>
			<p>To summarize, <code>alloc_ordered_workqueue()</code> actually replaces <code>create_freezable_workqueue()</code> and <code>create_singlethread_workqueue()</code> (as per this commit: <a href="https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=81dcaf6516d8">https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=81dcaf6516d8</a>). Workqueues allocated with <code>alloc_ordered_workqueue()</code> are unbound and have <code>max_active</code> set to <code>1</code>.</p>
			<p>When it comes to scheduling items in a workqueue, the work items that have been queued to a specific CPU using <code>queue_work_on()</code> will execute on that CPU. Work items queued via <code>queue_work()</code> will prefer the queueing CPU, but locality is not guaranteed.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Note that <code>schedule_work()</code> is a wrapper that calls <code>queue_work()</code> on the system workqueue (<code>system_wq</code>), while <code>schedule_work_on()</code> is a wrapper around <code>queue_work_on()</code>. Also, keep in mind the following: <code>system_wq = alloc_workqueue("events", 0, 0);</code>. You can have a look at the <code>workqueue_init_early()</code> function in <code>kernel/workqueue.c</code> in kernel sources to see how other system-wide workqueues are created.</p>
			<p>We are done with the new Linux kernel workqueue management implementation—that is, cmwq. Because workqueues can be used to defer works from interrupt handlers, we can move on to the next section and learn how to handle interrupts from the Linux kernel.</p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor057"/>Kernel interrupt handling</h1>
			<p>Apart from servicing <a id="_idIndexMarker274"/>processes and user requests, another job of the Linux kernel is managing and speaking with hardware. This is either from the CPU to the device or from the device to the CPU and is achieved by means of interrupts. An interrupt is a signal sent to the processor by an external hardware device requesting immediate attention. Prior to an interrupt being visible to the CPU, this interrupt should be enabled by the interrupt controller, which is a device on its own whose main job consists of routing interrupts to CPUs.</p>
			<p>The Linux kernel allows the provision of handlers for interrupts we are interested in so that when those interrupts are triggered, our handlers are executed. </p>
			<p>An interrupt is how a device halts the kernel, telling it that something interesting or important has happened. These are called IRQs on Linux systems. The main advantage interrupts offer is to avoid device polling. It is up to the device to tell if there is a change in its state; it is not up to us to poll it.</p>
			<p>To be notified when an interrupt occurs, you need to register with that IRQ, providing a function called an interrupt handler that will be called every time that interrupt is raised.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor058"/>Designing and registering an interrupt handler</h2>
			<p>When an interrupt handler is executed, it runs with interrupts disabled on the local CPU. This involves respecting <a id="_idIndexMarker275"/>certain constraints while designing an <strong class="bold">interrupt service routine</strong> (<strong class="bold">ISR</strong>), as outlined here:</p>
			<ul>
				<li><strong class="bold">Execution time</strong>: As <a id="_idIndexMarker276"/>IRQ handlers run with interrupts disabled on the local CPU, the code must be as short and small as possible and fast enough to assure a fast re-enabling of the previously disabled CPU-local interrupts in order not to miss any further occurring IRQs. Time-consuming IRQ handlers may considerably alter the real-time properties of the system and slow it down.</li>
				<li><strong class="bold">Execution context</strong>: Since <a id="_idIndexMarker277"/>interrupt handlers are executed in an atomic context, sleeping (or other mechanisms that may sleep—such as mutexes—copying data from kernel to user space or vice versa, and so on) is forbidden. Any part of the code requiring or involving sleeping must be deferred into another, safer context (that is, a process context).</li>
			</ul>
			<p>An IRQ handler <a id="_idIndexMarker278"/>needs to be given two arguments: the interrupt line to<a id="_idIndexMarker279"/> install the handler for, and a <strong class="bold">unique device ID</strong> (<strong class="bold">UDI</strong>) of the peripheral (mostly used as a context data structure—that is, a pointer to the per-device or private structure of the associated hardware device)—as illustrated here:</p>
			<pre>typedef irqreturn_t (*irq_handler_t)(int, void *);</pre>
			<p>The device driver wishing to register an interrupt handler for a given IRQ should call <code>devm_request_irq()</code>, defined in <code>&lt;linux/interrupt.h&gt;</code> as follows:</p>
			<pre>devm_request_irq(struct device *dev, unsigned int irq,
                  irq_handler_t handler,
                  unsigned long irqflags, 
                  onst char *devname, void *dev_id)</pre>
			<p>The preceding function argument list, <code>dev</code>, is the device responsible for the IRQ line, <code>irq</code> represents the interrupt line (that is, the interrupt number of the issuing device) to register <code>handler</code> for. Prior to validating the request, the kernel will make sure the requested interrupt is valid and that it is not already assigned to another device unless both devices request this <code>irq</code> line to be shared (with help of <code>flags</code>). <code>handler</code> is the function pointer to the interrupt handler, and <code>flags</code> represent the interrupt flags. <code>name</code> is an <code>dev</code> should be unique to each registered handler and cannot be <code>NULL</code> for shared IRQs since it is used by the kernel IRQ core to identify the device. A common way of using it is to provide a pointer to the device structure or a pointer to any per-device (and potentially useful to the handler) data structure, since when an interrupt occurs, both the interrupt line (<code>irq</code>) and this parameter will be passed to the registered handler, which can use this data as context data for further processing.</p>
			<p><code>flags</code> mangles the <a id="_idIndexMarker280"/>state or the behavior of the IRQ line or its handler by means of the following masks, which can be OR'ed to form a final desired bitmask according to your needs:</p>
			<pre>#define IRQF_SHARED 0x00000080
#define IRQF_PROBE_SHARED 0x00000100
#define IRQF_NOBALANCING 0x00000800
#define IRQF_IRQPOLL 0x00001000
#define IRQF_ONESHOT 0x00002000
#define IRQF_NO_SUSPEND 0x00004000
#define IRQF_FORCE_RESUME 0x00008000
#define IRQF_NO_THREAD 0x00010000
#define IRQF_EARLY_RESUME 0x000200002
#define IRQF_COND_SUSPEND 0x00040000</pre>
			<p>Note that <code>flags</code> can be <code>0</code> as well. Let's now explain some important flags—we leave the rest for the user<a id="_idIndexMarker281"/> to explore in <code>include/linux/interrupt.h</code>, but these are the ones we'll look at in more detail:</p>
			<ul>
				<li><code>IRQF_NOBALANCING</code> excludes <a id="_idIndexMarker282"/>the interrupt from IRQ balancing, which is a mechanism that consists of distributing/relocating interrupts across CPUs, with a goal of increasing performance. It is kind of preventing the CPU affinity of that IRQ from being changed. This flag may be useful to provide a flexible setup for clock source or clock event devices, to prevent misattribution of the event to the wrong core. This flag is meaningful on multi-core systems only.</li>
				<li><code>IRQF_IRQPOLL</code>: This <a id="_idIndexMarker283"/>flag allows the implementation of an <code>irqpoll</code> mechanism, intended to fix interrupt problems, meaning this handler should be added to the list of known interrupt handlers that can be looked for when a given interrupt is not handled.</li>
				<li><code>IRQF_ONESHOT</code>: Normally, the<a id="_idIndexMarker284"/> actual interrupt line being serviced is re-enabled after its hardirq handler completes, whether it awakes a threaded handler or not. This flag keeps the interrupt line disabled after the hardirq handler finishes. It must be set on threaded interrupts (we will discuss this later) for which the interrupt line must remain disabled until the threaded handler has completed, after which it will be re-enabled.</li>
				<li><code>IRQF_NO_SUSPEND</code> does<a id="_idIndexMarker285"/> not disable the IRQ during system hibernation/suspension. It does mean the interrupt is able to wake up the system from a suspended state. Such IRQs may be timer interrupts that may trigger and need to be handled even during system suspension. The whole IRQ line is affected by this flag such that if the IRQ is shared, every registered handler for this shared line will be executed, not only the one that installed this flag. You should avoid as much as possible using <code>IRQF_NO_SUSPEND</code> and <code>IRQF_SHARED</code> at the same time.</li>
				<li><code>IRQF_FORCE_RESUME</code> enables<a id="_idIndexMarker286"/> the IRQ in the system resume path even if <code>IRQF_NO_SUSPEND</code> is set.</li>
				<li><code>IRQF_NO_THREAD</code> prevents<a id="_idIndexMarker287"/> the interrupt handler from being threaded. This flag overrides the kernel <code>threadirqs</code> command-line option that forces every interrupt to be threaded. This flag has been introduced to address the non-threadability of some interrupts (for example, timers, which cannot be threaded even when all interrupt handlers are forced to be threaded).</li>
				<li><code>IRQF_TIMER</code> marks<a id="_idIndexMarker288"/> this handler as being specific to system timer<a id="_idIndexMarker289"/> interrupts. It helps not to disable the timer IRQ during system suspension to ensure normal resumption and not thread them when full preemption (that is, <code>PREEMPT_RT</code>) is enabled. It is just an alias for <code>IRQF_NO_SUSPEND | IRQF_NO_THREAD</code>.</li>
				<li><code>IRQF_EARLY_RESUME</code> resumes<a id="_idIndexMarker290"/> IRQ early at resume time of <strong class="bold">system core</strong> (<strong class="bold">syscore</strong>) operations instead of at device resume time. The following link points to the message of the commit introducing its support: <a href="https://lkml.org/lkml/2013/11/20/89">https://lkml.org/lkml/2013/11/20/89</a>.</li>
				<li><code>IRQF_SHARED</code> allows <a id="_idIndexMarker291"/>for the sharing of the interrupt line among several devices. However, each device driver that needs to share the given interrupt line must set with this flag; otherwise, the handler registration will fail.</li>
			</ul>
			<p>We must also consider the <code>irqreturn_t</code> return type of interrupt handlers since it may involve further actions after the return of the handler. Possible return values are listed here:</p>
			<ul>
				<li><code>IRQ_NONE</code>: On a shared interrupt line, once the interrupt occurs, the kernel IRQ core successively walks through handlers registered for this line and executes them in the order they have been registered. The driver then has the responsibility to check whether it is its device that issued the interrupt. If the interrupt does not come<a id="_idIndexMarker292"/> from its device, it must return <code>IRQ_NONE</code> to instruct the kernel to call the next registered interrupt handler. This return value is mostly used on shared interrupt lines since it informs the kernel that the interrupt does not come from our device. However, if 99,900 of the previous 100,000 interrupts of a given IRQ line have not been handled, the kernel then assumes that this IRQ is stuck in some manner, drops a diagnostic, and tries to turn the IRQ off. For more information on this, you can have a look at the <code>__report_bad_irq()</code> function in the kernel source tree.</li>
				<li><code>IRQ_HANDLED</code>: This value should be returned if the interrupt has been handled successfully. On a threaded IRQ, this value does acknowledge the interrupt (at a controller level) without waking the thread handler up.</li>
				<li><code>IRQ_WAKE_THREAD</code>: On a thread IRQ handler, this value must be returned by the hard-IRQ handler to wake the handler thread. In this case, <code>IRQ_HANDLED</code> must only be returned by that threaded handler, previously registered with <code>devm_request_threaded_irq()</code>. We will discuss this later in the chapter.<p class="callout-heading">Note</p><p class="callout">You should never re-enable IRQs from within your IRQ handler as this would involve allowing "interrupt reentrancy".</p></li>
			</ul>
			<p><code>devm_request_irq()</code> is the managed version of <code>request_irq()</code>, defined as follows:</p>
			<pre>int request_irq(unsigned int irq, irq_handler_t handler,
                unsigned long flags, const char *name,
                void *dev) </pre>
			<p>They both <a id="_idIndexMarker293"/>have the same variable meanings. If the driver used the managed version, the IRQ core will take care of releasing the resources. In other cases, such as at the unloading path or when the device leaves, the driver will have to release the IRQ resources by unregistering the interrupt handler using <code>free_irq()</code>, declared as follows:</p>
			<pre>void free_irq(unsigned int irq, void *dev_id)</pre>
			<p><code>free_irq()</code> removes the handler (identified by <code>dev_id</code> when it comes to shared interrupts) and disables the line. If the interrupt line is shared, the handler is just removed from the list of handlers for this <code>irq</code>, and the interrupt line is disabled in the future when the last handler is removed. Moreover, if possible, your code must make sure the interrupt is really disabled on the card it drives before calling this function, since omitting this may lead to spurious IRQ.</p>
			<p>There are a few things worth mentioning here about interrupts that you should never forget, as follows:</p>
			<ul>
				<li>On Linux systems, when the handler of an IRQ is being executed by a CPU, all interrupts are disabled on that CPU and the interrupt being serviced is masked on all the other cores. This means interrupt handlers need not be reentrant because the same interrupt will never be received until the current handler has completed. However, all other interrupts but the serviced one remain enabled (or, should we say, unchanged) on other cores, so other interrupts keep being serviced, though the current line is always disabled, as well as further interrupts on the local CPU. As a result, the same interrupt handler is never invoked concurrently to service a nested interrupt. This makes writing your interrupt handler a lot easier.</li>
				<li>Critical regions that need to run with interrupts disabled should be limited as much as possible. To remember this, tell yourself that your interrupt handler has interrupted other code and needs to give the CPU back.</li>
				<li>The interrupt context has its own (fixed and quite low) stack size. It thus totally makes sense to disable IRQs while running an ISR as reentrancy could cause stack overflow if too many preemptions happen.</li>
				<li>Interrupt handlers <a id="_idIndexMarker294"/>cannot block; they do not run in a process context. Thus, you may not perform the following operations from within an interrupt handler:<ul><li>You cannot transfer data to/from user space since this may block.</li><li>You cannot sleep or rely on code that may lead to sleep, such as invoking <code>wait_event()</code>, memory allocation with any flag other than <code>GFP_ATOMIC</code>, or using a mutex/semaphore. The threaded handler can handle this.</li><li>You cannot trigger or call <code>schedule()</code>.<p class="callout-heading">Note</p><p class="callout">If a device issues an IRQ while this IRQ is disabled (or masked) at a controller level, it will not be processed at all (masked in the flow handler), but an interrupt will instantaneously occur if it is still pending (at a device level) when the IRQ is enabled (or unmasked).</p><p class="callout">The concept of non-reentrancy for an interrupt means that, if an interrupt is already in an active state, it cannot enter it again until the active status is cleared.</p></li></ul></li>
			</ul>
			<h3>Understanding the concept of top and bottom halves</h3>
			<p>External devices send interrupt requests to the CPU either to signal a particular event or request a service. As stated in the previous section, bad interrupt management may considerably increase a system's latency and decrease its real-time properties. We also stated that interrupt processing—that is, the hard-IRQ handler at least—must be very fast not only to keep the system responsive but also not to miss other interrupt events.</p>
			<p>The idea here is to split the interrupt handler into two parts. The first part (a function, actually) will run in a so-called hard-IRQ context with interrupts disabled, and will perform the minimum required work (such as doing some quick sanity checks—essentially, time-sensitive tasks, read/write hardware registers, and fast processing of this data and acknowledging interrupts to the device that raised it). This first part is the so-called <strong class="bold">top half</strong> on <a id="_idIndexMarker295"/>Linux systems. The top-half would then schedule a thread handler, which would run a <a id="_idIndexMarker296"/>so-called <strong class="bold">bottom-half</strong> function, with interrupts re-enabled, and which is the second part of the interrupt. The bottom half could then perform time-consuming operations (such as buffer processing) and tasks that may sleep, as it runs in a thread.</p>
			<p>This splitting would considerably increase system responsiveness as the time spent with IRQs disabled is reduced to its minimum, and since bottom halves are run in kernel threads, they compete for the CPU with other processes on the runqueue. Moreover, they may have their real-time properties set. The top half is actually the handler registered using <code>devm_request_irq()</code>. When using <code>devm_request_threaded_irq()</code>, as we will see in the next section, the top half is the first handler given to the function. </p>
			<p>As described previously in the <em class="italic">Implementing work-deferring mechanisms</em> section, a bottom half represents nowadays any task (or work) scheduled from within an interrupt handler. Bottom halves are designed using work-deferring mechanisms, which we have seen previously.</p>
			<p>Depending on which one you choose, it may run in a (software) interrupt context or in a process context. These are softirqs, tasklets, workqueues, and threaded IRQs.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Tasklets and softirqs have nothing to do with the "thread interrupt" mechanism since they run in their own special (atomic) contexts.</p>
			<p>Since softirq handlers run at a high priority with the scheduler preemption disabled, not relinquishing CPU to processes/threads until they complete, care must be taken while using them for bottom-half delegation. As nowadays the quantum allocated for a particular process may vary, there is no strict rule for how long the softirq handler should take to complete in order not to slow the system down as the kernel would not be able to give CPU time to other processes. I would say no longer than half a jiffy.</p>
			<p>The hard IRQ (top half, actually) must be as fast as possible, and most of the time, just reading and writing in I/O memory. Any other computation should be deferred in the bottom half, whose main goal is to perform any time-consuming and not interrupt-related work not performed by the top half. There is no clear guideline on the repartition of work between the top and bottom halves. Here is some advice:</p>
			<ul>
				<li>Hardware-related or time-sensitive work can be performed in the top half.</li>
				<li>If the work really need not be interrupted, it can be performed in the top half.</li>
				<li>From my point of view, everything else can be deferred and thus performed in the bottom half, which will run with interrupts enabled and when the system is less busy.</li>
				<li>If the hard IRQ is fast enough to process and acknowledge interrupts within a few microseconds consistently, then there is absolutely no need to use bottom-half delegations.</li>
			</ul>
			<h3>Working with threaded IRQ handlers</h3>
			<p>Threaded interrupt handlers<a id="_idIndexMarker297"/> were introduced to reduce the time spent in interrupt handlers and defer the rest of the work (that is, processing) out into kernel threads. So, the top half (hard IRQ) would consist of quick sanity checks such as ensuring whether the interrupt comes from its device and waking the bottom half accordingly. A threaded interrupt handler runs in its own thread, either in the thread of their parent (if they have one) or in a separate kernel thread. Moreover, the dedicated kernel thread can have its real-time priority set, though it runs at normal real-time priority (that is, <code>MAX_USER_RT_PRIO/2</code>, as you can see in the <code>setup_irq_thread()</code> function in <code>kernel/irq/manage.c</code>).</p>
			<p>The general rule behind threaded interrupts is simple: keep the hard-IRQ handler as minimal as possible and defer as much work to the kernel thread as possible (preferably, all work). You should use <code>devm_request_threaded_irq()</code> if you wish to request a threaded interrupt handling. Here is its prototype:</p>
			<pre>devm_request_threaded_irq(struct device *dev, unsigned int irq,
                  irq_handler_t handler, irq_handler_t thread_fn,
                  unsigned long irqflags, const char *devname,
                  void *dev_id);</pre>
			<p>This function accepts two special parameters on which we should spend some time, <code>handler</code>, and <code>thread_fn</code>. They are outlined in more detail here:</p>
			<ul>
				<li><code>handler</code> immediately<a id="_idIndexMarker298"/> runs when the interrupt occurs, in an interrupt context, and acts as a hard-IRQ handler. Its job usually consists of reading the interrupt cause (in the device's status register) to determine whether or how to handle the interrupt (this is frequent <a id="_idIndexMarker299"/>on <code>IRQ_NONE</code>. This return value usually only makes sense on shared interrupt lines.</li>
			</ul>
			<p>If this hard-IRQ handler can finish interrupt processing fast enough (this is not a universal rule, but let's say no longer than a half of jiffy—that is, not longer than 500 µs if <code>CONFIG_HZ</code>, which defines the value of a jiffy, is set to <code>1000</code>) for some set of interrupt causes, it should return <code>IRQ_HANDLED</code> after processing in order to acknowledge the interrupts. Interrupt processing that does not fall in this time lapse should be deferred in the thread IRQ handlers. In this case, the hard-IRQ handler should return <code>IRQ_WAKE_THREAD</code> to awake the threaded handler. Returning <code>IRQ_WAKE_THREAD</code> makes sense only when the <code>thread_fn</code> handler is also provided.</p>
			<ul>
				<li><code>thread_fn</code> is the threaded handler added to the scheduler runqueue when the hard-IRQ handler function returns <code>IRQ_WAKE_THREAD</code>. If <code>thread_fn</code> is <code>NULL</code> while the handler is set and returns <code>IRQ_WAKE_THREAD</code>, nothing happens at the return path of the hard-IRQ handler but a simple warning message (we can see that in the <code>__irq_wake_thread()</code> function in the kernel sources). As <code>thread_fn</code> competes for the CPU with other processes on the runqueue, it may be executed immediately or later in the future when the system has less load. This function should return <code>IRQ_HANDLED</code> when it has completed the interrupt handling. After that, the associated kernel thread will be taken off the runqueue and put in a blocked state until woken up again by the hard-IRQ function.</li>
			</ul>
			<p>A default hard-IRQ handler will<a id="_idIndexMarker300"/> be installed by the kernel if <code>handler</code> is <code>NULL</code> and <code>thread_fn != NULL</code>. This is the default primary handler. It does nothing but return <code>IRQ_WAKE_THREAD</code> to wake up the associated kernel thread that will execute the <code>thread_fn</code> handler.</p>
			<p>It is implemented as follows:</p>
			<pre>/* Default primary interrupt handler for threaded
 * interrupts. Assigned as primary handler when
 * request_threaded_irq is called with handler == NULL.
 * Useful for oneshot interrupts.
 */
static irqreturn_t irq_default_primary_handler(int irq,
                                         void *dev_id)
{
    return IRQ_WAKE_THREAD;
}
int request_threaded_irq(unsigned int irq,
          irq_handler_t handler, irq_handler_t thread_fn,
          unsigned long irqflags, const char *devname,
          void *dev_id)
{
[...]
    if (!handler) {
        if (!thread_fn)
            return -EINVAL;
        handler = irq_default_primary_handler;
    }
[...]
}
EXPORT_SYMBOL(request_threaded_irq);</pre>
			<p>This makes it possible to move the execution of interrupt handlers entirely to the process context, thus preventing buggy drivers (buggy IRQ handlers, actually) from breaking the whole system and reducing interrupt latency.</p>
			<p>In new kernel releases, <code>request_irq()</code> simply wraps <code>request_threaded_irq()</code> with the <code>thread_fn</code> parameter set to <code>NULL</code> (the same goes for the <code>devm_</code> variant).</p>
			<p>Note that the interrupt<a id="_idIndexMarker301"/> is acknowledged at an interrupt controller level when you return from the hard-IRQ handler (whatever the return value is), thus allowing you to take other interrupts into account. In such a situation, if the interrupt hasn't been acknowledged at the device level, the interrupt will fire again and again, resulting in stack overflows (or being stuck in the hard-IRQ handler forever) for level-triggered interrupts since the issuing device will still have the interrupt line asserted.</p>
			<p>For threaded interrupt implementation, when drivers needed to run the bottom half in a thread, they had to mask the interrupt at device level from the hard-interrupt handler. This required accessing the issuing device, which is not, however, always possible for devices sitting on slow buses (such I2C <a id="_idIndexMarker302"/>or <code>IRQF_ONESHOT</code>, this operation is not mandatory anymore as it helps keep the IRQ disabled at a controller level even when the threaded handler runs. Drivers must, however, clear the device interrupt in the threaded handler before it completes.</p>
			<p>Using <code>devm_request_threaded()</code> (or the non-managed variant), it is possible to request an exclusively threaded IRQ by omitting the hard-interrupt handler. In this case, it is mandatory to set the <code>IRQF_ONESHOT</code> flag, else the kernel will complain because the threaded handler would run with the interrupt unmasked at both device and controller levels.</p>
			<p>Here is an<a id="_idIndexMarker303"/> example of this:</p>
			<pre>static irqreturn_t data_event_handler(int irq,
                                      void *dev_id)
{
    struct big_structure *bs = dev_id;
    clear_device_interupt(bs);
    process_data(bs-&gt;buffer);
    return IRQ_HANDLED;
}
static int my_probe(struct i2c_client *client)
{
[...]
    if (client-&gt;irq &gt; 0) {
        ret = request_threaded_irq(client-&gt;irq, NULL, 
                &amp;data_event_handler,
                IRQF_TRIGGER_LOW | IRQF_ONESHOT,
                id-&gt;name, private);
        if (ret)
            goto error_irq;
    }
...
    return 0;
error_irq:
    do_cleanup();
    return ret;
}</pre>
			<p>In the preceding example, our device sits on an I2C bus, so accessing the device may put the underlying task to sleep. Such an operation must never be performed in the hard-interrupt handler. </p>
			<p>Here is an<a id="_idIndexMarker304"/> excerpt from the message in the link that introduced the <code>IRQF_ONESHOT</code> flag and which explains what it does (the whole message can be found via this link: <a href="http://lkml.iu.edu/hypermail/linux/kernel/0908.1/02114.html">http://lkml.iu.edu/hypermail/linux/kernel/0908.1/02114.html</a>):</p>
			<p>"<em class="italic">It allows drivers to request that the interrupt is not unmasked (at controller level) after the hard interrupt context handler has been executed and the thread has been woken. The interrupt line is unmasked after the thread handler function has been executed</em>."</p>
			<p>If one driver has set either <code>IRQF_SHARED</code> or <code>IRQF_ONESHOT</code> flags on a given IRQ, then the other driver sharing the IRQ must set the same flags. The <code>/proc/interrupts</code> file lists IRQs with their number of processing per CPU, the IRQ name as given during the requesting step, and a comma-separated list of drivers that registered a handler for that interrupt.</p>
			<p>Threading the IRQs is the best choice for interrupt processing that can hog too many CPU cycles (exceeding a jiffy, for example), such as bulk data processing. Threading IRQs allows the priority and CPU affinity of their associated threads to be managed individually. As this concept comes from the real-time kernel tree, it fulfills many requirements of a real-time system, such as allowing a fine-grained priority model and reducing interrupt latency in the kernel. You can have a look at <code>/proc/irq/&lt;IRQ&gt;/smp_affinity</code>, which can be used to get or set the corresponding <code>&lt;IRQ&gt;</code> affinity. This file returns and accepts a bitmask that represents which processors can handle ISRs registered for this IRQ. This way, you can—for example—decide to set the affinity of the hard-interrupt handler to one CPU, while setting the affinity of the threaded handler to another CPU.</p>
			<h3>Requesting a context-agnostic IRQ</h3>
			<p>A driver<a id="_idIndexMarker305"/> requesting an IRQ must know in advance the nature of the interrupt and decide whether its handler can run in a hard-IRQ context or not, which may influence the choice between <code>devm_request_irq()</code> and <code>devm_request_threaded_irq()</code>.</p>
			<p>The problem with those approaches is that sometimes, a driver requesting an IRQ does not know about the nature of the interrupt controller that provides this IRQ line, especially when the interrupt controller is a discrete chip (typically, a <code>request_any_context_irq()</code>, function with which drivers requesting an IRQ will know whether the handler will run in a thread context or not, and call <code>request_threaded_irq()</code> or <code>request_irq()</code> accordingly. This means that whether the IRQ associated with our device comes from an interrupt controller that may not sleep (a memory-mapped one) or from one that can sleep (behind an I2C/SPI bus), there will be no need to change the code. Its prototype looks like this:</p>
			<pre>int request_any_context_irq(unsigned int irq,
                irq_handler_t handler, unsigned long flags,
                const char *name, void *dev_id)</pre>
			<p><code>devm_request_any_context_irq()</code> and <code>devm_request_irq()</code> have the same interface but different semantics. Depending on the underlying context (the hardware platform), <code>devm_request_any_context_irq()</code> selects either a hard-interrupt handling using <code>request_irq()</code> or a threaded handling method using <code>request_threaded_irq()</code>. It returns a negative error value on failure, while on success, it returns either <code>IRQC_IS_HARDIRQ</code> (meaning a hard-interrupt handling method is used) or <code>IRQC_IS_NESTED</code> (meaning a threaded one is used). With this function, the behavior of the interrupt handler is decided at runtime. For more information, you can have a look at the commit introducing it in the kernel by following this link: <a href="https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=ae731f8d0785">https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=ae731f8d0785</a>.</p>
			<p>The advantage of using <code>devm_request_any_context_irq()</code> is that the driver does not need to care about what can be done in the IRQ handler, as the context in which the handler will run depends on the interrupt controller that provides the IRQ line. For example, for a GPIO-IRQ based device driver, if the GPIO belongs to a controller that sits on an I2C or SPI bus (GPIO access may sleep), the handler will be threaded. Otherwise (that is, the GPIO access does not sleep and is memory-mapped as it is part of the SoC), the handler will run in a hard-IRQ handler.</p>
			<p>In the <a id="_idIndexMarker306"/>following example, the device expects an IRQ line mapped to a GPIO. The driver cannot assume that the given GPIO line will be memory-mapped, coming from the SoC. It may come from a discrete I2C or SPI GPIO controller as well. A good practice would be to use <code>request_any_context_irq()</code> here:</p>
			<pre>static irqreturn_t packt_btn_interrupt(int irq,
                                       void *dev_id)
{
    struct btn_data *priv = dev_id;
    input_report_key(priv-&gt;i_dev, BTN_0,
        gpiod_get_value(priv-&gt;btn_gpiod) &amp; 1);
    input_sync(priv-&gt;i_dev);
    return IRQ_HANDLED;
}
static int btn_probe(struct platform_device *pdev)
{
    struct gpio_desc *gpiod;
    int ret, irq;
    gpiod = gpiod_get(&amp;pdev-&gt;dev, "button", GPIOD_IN);
    if (IS_ERR(gpiod))
        return -ENODEV;
    priv-&gt;irq = gpiod_to_irq(priv-&gt;btn_gpiod);
    priv-&gt;btn_gpiod = gpiod;
[...]
    ret = request_any_context_irq(priv-&gt;irq,
              packt_btn_interrupt,
             (IRQF_TRIGGER_FALLING | IRQF_TRIGGER_RISING),
             "packt-input-button", priv);
    if (ret &lt; 0)
        goto err_btn;
    return 0;
err_btn:
    do_cleanup();
    return ret;
}</pre>
			<p>The <a id="_idIndexMarker307"/>preceding code is simple enough but quite safe since <code>devm_request_any_context_irq()</code> does the job, which prevents it from mistaking the type of the underlying GPIO. The advantage of this approach is that you do not need to care about the nature of the interrupt controller that provides the IRQ line. In our example, if the GPIO belongs to a controller sitting on an I2C or SPI bus, the handler will be threaded. Otherwise (memory-mapped), the handler will run in a hard-IRQ context.</p>
			<h3>Using a workqueue to defer the bottom half</h3>
			<p>As we have<a id="_idIndexMarker308"/> already discussed the workqueue API in a dedicated section, it is now preferable to give an example here. This example is not error-free and has not been tested. It is just a demonstration to highlight the concept of bottom-half deferring by means of a workqueue.</p>
			<p>Let's start by defining a data structure that will hold the elements we need for further development, as follows:</p>
			<pre>struct private_struct {
    int counter;
    struct work_struct my_work;
    void __iomem *reg_base;
    spinlock_t lock;
    int irq;
    /* Other fields */
    [...]
};</pre>
			<p>In the preceding data structure, our work structure is represented by the <code>my_work</code> element. We do not use a pointer here because we will need to use a <code>container_of()</code> macro in order to grab back the pointer to the initial data structure. Next, we can define a method<a id="_idIndexMarker309"/> that will be invoked in the worker thread, as follows:</p>
			<pre>static void work_handler(struct work_struct *work)
{
    int i;
    unsigned long flags;
    struct private_data *my_data =
          container_of(work, struct private_data, my_work);
    /*
     * Processing at least half of MIN_REQUIRED_FIFO_SIZE
     * prior to re-enabling the irq at device level,
     * so that buffer can receive further data
     */
    for (i = 0, i &lt; MIN_REQUIRED_FIFO_SIZE, i++) {
        device_pop_and_process_data_buffer();
        if (i == MIN_REQUIRED_FIFO_SIZE / 2)
            enable_irq_at_device_level(my_data);
    }
    spin_lock_irqsave(&amp;my_data-&gt;lock, flags);
    my_data-&gt;buf_counter -= MIN_REQUIRED_FIFO_SIZE;
    spin_unlock_irqrestore(&amp;my_data-&gt;lock, flags);
}</pre>
			<p>In the preceding <a id="_idIndexMarker310"/>work structure, we start data processing when enough data has been buffered. We can now provide our IRQ handler, which is responsible for scheduling our work, as follows:</p>
			<pre>/* This is our hard-IRQ handler. */
static irqreturn_t my_interrupt_handler(int irq,
                                        void *dev_id)
{
    u32 status;
    unsigned long flags;
    struct private_struct *my_data = dev_id;
    /* we read the status register to know what to do */
    status = readl(my_data-&gt;reg_base + REG_STATUS_OFFSET);
    /*
     * Ack irq at device level. We are safe if another
     * irq pokes since it is disabled at controller
     * level while we are in this handler
     */
    writel(my_data-&gt;reg_base + REG_STATUS_OFFSET,
            status | MASK_IRQ_ACK);
    /*
     * Protecting the shared resource, since the worker
     * also accesses this counter
     */
    spin_lock_irqsave(&amp;my_data-&gt;lock, flags);
    my_data-&gt;buf_counter++;
    spin_unlock_irqrestore(&amp;my_data-&gt;lock, flags);
    /*
     * Our device raised an interrupt to inform it has
     * new data in its fifo. But is it enough for us
     * to be processed ?
     */
    if (my_data-&gt;buf_counter != MIN_REQUIRED_FIFO_SIZE)) {
       /* ack and re-enable this irq at controller level */
       return IRQ_HANDLED;
    } else {
        /* Right. prior to scheduling the worker and
         * returning from this handler, we need to
         * disable the irq at device level
         */
        writel(my_data-&gt;reg_base + REG_STATUS_OFFSET,
                MASK_IRQ_DISABLE);
        schedule_work(&amp;my_work);
    }
    /* This will re-enable the irq at controller level */
    return IRQ_HANDLED;
};</pre>
			<p>The comments in the IRQ handler code are meaningful enough. <code>schedule_work()</code> is the function that schedules our work. Finally, we can write our probe method that will request our IRQ and<a id="_idIndexMarker311"/> register the previous handler, as follows:</p>
			<pre>static int foo_probe(struct platform_device *pdev)
{
    struct resource *mem;
    struct private_struct *my_data;
    my_data = alloc_some_memory(
                        sizeof(struct private_struct));
    mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
    my_data-&gt;reg_base = ioremap(ioremap(mem-&gt;start,
                                resource_size(mem)););
    if (IS_ERR(my_data-&gt;reg_base))
        return PTR_ERR(my_data-&gt;reg_base);
    /*
     * workqueue initialization. "work_handler" is
     * the callback that will be executed when our work
     * is scheduled.
     */
    INIT_WORK(&amp;my_data-&gt;my_work, work_handler);
    spin_lock_init(&amp;my_data-&gt;lock);
    my_data-&gt;irq = platform_get_irq(pdev, 0);
    if (devm_request_irq(&amp;pdev-&gt;dev, my_data-&gt;irq,
                        my_interrupt_handler, 0,
                        pdev-&gt;name, my_data))
        handler_this_error()
    return 0;
}</pre>
			<p>The structure of<a id="_idIndexMarker312"/> the preceding probe method shows without a doubt that we are facing a platform device driver. Generic IRQ and workqueue APIs have been used here for initializing our workqueue and registering our handler.</p>
			<h3>Locking from within an interrupt handler</h3>
			<p>It is <a id="_idIndexMarker313"/>common to use spinlocks on SMP systems, as this guarantees mutual exclusion at the CPU level. Therefore, if a resource is shared only with a threaded bottom half (that is, it is never accessed from the hard IRQ), it is better to use mutexes, as we see in the following example:</p>
			<pre>static int my_probe(struct platform_device *pdev)
{
    int irq;
    int ret;
    irq = platform_get_irq(pdev, i);
    ret = devm_request_threaded_irq(&amp;pdev-&gt;dev, irq, NULL,
                my_threaded_irq, IRQF_ONESHOT,
                dev_name(dev), my_data);
[...]
    return 0;
}
static irqreturn_t my_threaded_irq(int irq, void *dev_id)
{
    struct priv_struct *my_data = dev_id;
    /* Save FIFO Underrun &amp; Transfer Error status */
    mutex_lock(&amp;my_data-&gt;fifo_lock);
    /*
     * Accessing the device's buffer through i2c
     */
    device_get_i2c_buffer_and_push_to_fifo();
    mutex_unlock(&amp;ldev-&gt;fifo_lock);
    return IRQ_HANDLED;
}</pre>
			<p>However, if the shared resource is accessed from within the hard-interrupt handler, you must <a id="_idIndexMarker314"/>use the <code>_irqsave</code> variant of the spinlock, as in the following example, starting with the probe method:</p>
			<pre>static int my_probe(struct platform_device *pdev)
{
    int irq;
    int ret;
    [...]
    irq = platform_get_irq(pdev, 0);
    if (irq &lt; 0)
        goto handle_get_irq_error;
    ret = devm_request_threaded_irq(&amp;pdev-&gt;dev, irq,
                    hard_handler, threaded_handler, 
                    IRQF_ONESHOT, dev_name(dev), my_data);
    if (ret &lt; 0)
        goto err_cleanup_irq;
     [...]
    return 0;
}</pre>
			<p>Now that the probe method has been implemented, let's implement the top half—that is, the<a id="_idIndexMarker315"/> hard-IRQ handler—as follows:</p>
			<pre>static irqreturn_t hard_handler(int irq, void *dev_id)
{
    struct priv_struct *my_data = dev_id;
    u32 status;
    unsigned long flags;
    /* Protecting the shared resource */
    spin_lock_irqsave(&amp;my_data-&gt;lock, flags);
    my_data-&gt;status = __raw_readl(
            my_data-&gt;mmio_base + my_data-&gt;foo.reg_offset);
    spin_unlock_irqrestore(&amp;my_data-&gt;lock, flags);
    /* Let us schedule the bottom-half */
    return IRQ_WAKE_THREAD;
}</pre>
			<p>The return value of the top half will wake the threaded bottom half, which is implemented as follows:</p>
			<pre>static irqreturn_t threaded_handler(int irq, void *dev_id)
{
    struct priv_struct *my_data = dev_id;
    spin_lock_irqsave(&amp;my_data-&gt;lock, flags);
    /* doing sanity depending on the status */
    process_status(my_data-&gt;status);
    spin_unlock_irqrestore(&amp;my_data-&gt;lock, flags);
    /*
     * content of status not needed anymore, let's do
     * some other work
     */
     [...]
    return IRQ_HANDLED;
}</pre>
			<p>There<a id="_idIndexMarker316"/> is a case where protection may not be necessary between the hard IRQ and its threaded counterpart when the <code>IRQF_ONESHOT</code> flag is set while requesting the IRQ line. This flag keeps the interrupt disabled after the hard-interrupt handler has finished. With this flag set, the IRQ line is disabled until the threaded handler has been run. This way, the hard handler and its threaded counterpart will never compete, and a lock for a resource shared between the two might not be necessary.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor059"/>Summary</h1>
			<p>In this chapter, we discussed the fundamental elements to start driver development, presenting all the mechanisms frequently used in drivers such as work scheduling and time management, interrupt handling, and locking primitives. This chapter is very important since it discusses topics other chapters in this book rely on. </p>
			<p>For instance, the next chapter, dealing with character devices, will use some of the elements discussed in this chapter.</p>
		</div>
	</body></html>