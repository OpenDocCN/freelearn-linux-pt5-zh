<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Sizing the Storage</h1></div></div></div><p>The storage layer is perhaps one of the most critical components in a VMware View design. For many VDI professionals, this is likely to be the major issue when called in for a performance troubleshooting exercise. Commonly, the storage layer is the root cause of the performance issue. Why is storage so critical? To answer this question, we first need to understand how Intel®-based desktops work and interact with Windows operating systems before diving into the world of storage for VDI.<a id="id359" class="indexterm"/>
</p><p>Physical desktops have always had dedicated hard disks to rely upon and only a single Windows kernel had access to the disk causing a single I/O stream. Despite being dedicated to the desktop, that device also faced disk contention. This contention could have been generated due to an excessive amount of disk I/O operations or I/O block sizes. The important thing is that no matter what type of operation causes the contention, the end result is high response time, also known as<strong> latency</strong>.<a id="id360" class="indexterm"/>
</p><p>Recent disk technology advancements such as<strong> Solid State Drives (SSD)</strong> drastically reduced the latency implications, and thus improved the end user experience. The most advanced SSD can ingest and deliver an enormous amount of I/O and throughput.<a id="id361" class="indexterm"/>
</p><p>With the ability to use faster disks, users became spoilt with the given enhanced performance. However, microprocessors and RAM technology also evolved in such a fast fashion that technologies like Intel Core i7 and DDR3 quickly made even the fastest SSD become the performance bottleneck once again.</p><p>With a few exceptions, VDI implementations do not utilize a single dedicated disk. Instead, VDI uses a pool of disks to provide storage capacity, I/O, and throughput to vDesktops.</p><p>Most VDI implementations require shared storage to provide shared datastores to multiple servers. Shared storage is also a key enabler for VMware vSphere features such as vMotion, DRS, and Fault Tolerance. Implementations that utilize a combination of floating pools and roaming profiles may opt for storage appliance solutions, where no persistent data is stored on those appliances.</p><p>Local disks,<strong> Dedicated Attached Storage (DAS)</strong>, and even diskless solutions can be employed in VDI deployments. It is important to understand the use cases and implications behind each of the approaches.<a id="id362" class="indexterm"/>
</p><p>Storage architecture decisions made during the VDI design phase will have a deep impact on how the infrastructure will perform and operate. The type of storage and transport protocol chosen will determine how VMware View and vSphere will operate. Yet, the type of storage and protocol will also dictate how datastores should be designed or how many desktops per datastore should be used.</p><div><div><div><div><h1 class="title"><a id="ch08lvl1sec01"/>VMware View Composer</h1></div></div></div><p>The VMware View infrastructure includes VMware View Composer as an optional component. View Composer runs as a Windows service on the vCenter Server(s) and enables View Manager to rapidly clone and deploy multiple virtual desktops from a single centralized standard base image. View Composer was originally designed to reduce the total storage required in VDI deployments; however, today View Composer also provides essential management features such as the Refresh and Recompose operations.<a id="id363" class="indexterm"/>
</p><p>View Composer uses linked clone technology. Unlike a traditional virtual machine model wherein each VM exists as an independent entity with dedicated virtual disks, View Composer creates dependent VMs all linked to a master VM. This master VM is called the<strong> Parent VM</strong> in VMware terminology.<a id="id364" class="indexterm"/>
</p><p>The Parent VM is used as a base image, and a snapshot and copy are taken from the Parent VM to create the replica image, which will serve as the master VM disk for all linked clones in a desktop pool.</p><div><img src="img/1124EN_08_01.jpg" alt="VMware View Composer"/></div><p>The replica disk is created as a read-only thin provisioned entity from the Parent VM to ensure that any subsequent changes to the Parent VM do not impact the linked clone desktops. As mentioned previously, the replica is thin provisioned which means that only the data contained within the Parent VM is copied to the replica. As an example, if the Parent VM was created with a 40 GB disk but only 20 GB appears on the guest's Windows NTFS volume, then the replica will be 20 GB.</p><p>The replica image is a protected entity in vCenter Server via a<strong> VM LockStatus</strong> parameter added to the VM annotations, as seen in the following screenshot. If a replica needs to be deleted for any reason, the process outlined in KB1008704 must be followed. It is given at the following URL:<a id="id365" class="indexterm"/>
</p><p>
<a class="ulink" href="http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&amp;cmd=displayKC&amp;externalId=1008704">http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&amp;cmd=displayKC&amp;externalId=1008704</a>.</p><p>The following screenshot shows the<strong> VM LockStatus</strong> parameter:
<a id="id366" class="indexterm"/>
</p><div><img src="img/1124EN_08_02.jpg" alt="VMware View Composer"/></div><p>A Parent VM may contain several snapshots that represent changes introduced to the base image. These differences may be due to fixes, patches, and upgrades required by the Windows Guest OS. The deployment of new applications to the base image, application upgrades, or even Windows configuration changes can be added to the snapshots.<a id="id367" class="indexterm"/>
</p><p>After each modification, the Parent VM must be shutdown by the administrator and a new snapshot is taken.</p><p>The snapshot to be used in a desktop pool is selected during the desktop pool configuration. A single snapshot is assigned to the entire desktop pool. However, it is possible to individually recompose virtual desktops using different snapshots from the same or different Parent VM.</p><p>The following screenshot demonstrates VMware vCenter Server Snapshot Manager with a few snapshots already taken. It is recommended practice to annotate the changes made to the Parent VM in the description field:</p><div><img src="img/1124EN_08_03.jpg" alt="VMware View Composer"/></div><p>The VMware View snapshot selection during the desktop pool configuration process can be seen in the following screenshot, which shows snapshot selection in the View Admin console:<a id="id368" class="indexterm"/>
</p><div><img src="img/1124EN_08_04.jpg" alt="VMware View Composer"/></div><p>In releases prior to VMware View 4.5, a unique replica disk was created in each datastore hosting virtual desktops for a desktop pool. Additionally, for each different snapshot in use by a desktop pool, a new replica disk used to be created for each datastore in use.</p><p>Only a single snapshot can be assigned to the desktop pool at any one time. However, after selecting a different snapshot for the desktop pool and triggering Recompose action, a second replica disk representing the new snapshot is created in each datastore in use by the virtual desktops using the replica disk. In this case, each datastore may contain two replicas for the desktop pool.</p><p>In a Recompose operation, the original replica image is only deleted after all desktops in the datastore are recomposed with the new base replica disk and the old replica is not required anymore. Therefore, it is important to ensure that there is ample space available on the datastore(s) dedicated to storing replicas.</p><p>This scenario is still applicable with VMware View 5.0 when the administrator does not select the optional Dedicated Replica Datastore feature during the datastore selection.<a id="id369" class="indexterm"/>
</p><p>VMware View 4.5 and later implemented the ability to specify a unique datastore to host replica disks for an entire desktop pool. This piece is part of the VMWare View Tiered Storage feature.</p><p>The following screenshot shows datastore selection in the View Admin console:<a id="id370" class="indexterm"/>
</p><div><img src="img/1124EN_08_05.jpg" alt="VMware View Composer"/></div><p>If the desktop pool is large and eventually uses the entire vSphere cluster resources, it is possible to end up with a single replica disk for the entire cluster. For VMware View 5.0, the maximum number of virtual desktops supported in a single desktop pool is 1,000. While it is possible to go further than 1,000 desktops, it is not recommended nor supported.</p><p>In some cases, where multiple snapshots are in use, multiple replicas will be created in the single datastore selected during the pool configuration. The following diagram demonstrates the differences between not using and using the Dedicated Replica Datastore option in VMware View 4.5 and above:</p><div><img src="img/1124EN_08_06.jpg" alt="VMware View Composer"/></div><p>In the following sample scenario, VMware View is running 256 virtual desktops across 2 desktop pools with 2 snapshots in use for each pool. If the replica disk size is 20 GB, the total storage allocation for the replica disks would be 320 GB, being 80 GB per datastore.<a id="id371" class="indexterm"/>
</p><p>Number of desktop pools * replicas in use * number of datastores * replica size = 2 * 2 * 4 * 20 GB = 160 GB.</p><p>Using the same sample scenario with Dedicated Replica Datastore, the total storage allocation for the replica disks would be 80 GB in a single datastore.</p><p>Number of desktop pools * replicas in use * replica size = 2 * 2 * 20 GB = 80 GB</p><p>The following diagram demonstrates both scenarios. It is an illustration showing the difference between the replica disk placement when using multiple snapshots and a Dedicated Replica Disk Datastore in View 4.5 and above:</p><div><img src="img/1124EN_08_07.jpg" alt="VMware View Composer"/></div><p>Running the same calculations for the preceding scenario with 2,000 virtual desktops, the storage savings provided by the View Composer technology could be as large as 6.3 TB. The more virtual machines and desktop pools in the environment, the bigger the number of replicas per datastore when the Dedicated Replica Datastore option is not selected.<a id="id372" class="indexterm"/>
</p><p>It is possible to break down the amount of replica disks and storage consumption if all datastores are not selected for each desktop pool, therefore, limiting the placement of virtual desktops across datastores.</p><p>The number of datastores and datastore sizes must provide the capacity and performance requirements for the provisioning of the required number of desktops. This concept assumes that the administrator will carefully manage and select the datastores in use by the desktop pool, not allowing all datastores to be used by all desktop pools.</p><p>The following screenshot shows a solution not using a Dedicated Replica Datastore option:</p><div><img src="img/1124EN_08_08.jpg" alt="VMware View Composer"/></div><p>A linked clone disk is also called a<strong> delta disk</strong> because it accumulates delta changes. After the replica disk is created, View Composer starts to create the linked clone virtual desktops. Each linked clone has a unique delta disk and is linked to the replica disk.<a id="id373" class="indexterm"/>
</p><p>Delta disks contain only the differences from the original read-only replica disk that are unique to the cloned virtual desktop, resulting in significant storage savings. Linked Clone disks will grow over time according to block write changes requested by the Guest OS, and may grow up to the maximum size of the Parent VM.</p><p>As an example, if the Parent VM was originally configured by the administrator with a 30 GB flat disk, this will be the maximum size of the delta disk.</p><p>View Composer allows for great storage savings; however, there will be dozens or hundreds of linked clone virtual desktops using the same datastore to read that single existing replica disk. If the Dedicated Replica Datastore option is not in use, the replica is only used by the desktops hosted in the same datastore.</p><p>All virtual desktops accessing the replica disk will cause I/O stress on LUNs, RAID group, and disks, and may create I/O contention. The I/O contention is caused by multiple virtual desktops and users accessing the same datastore, all at the same time.</p><p>Each datastore is normally backed by a LUN, if<strong> Fibre Channel Protocol (FCP)</strong> is in use or by an Export if NFS is in use. Both LUN and Export are backed by a RAID Group configuration that encompasses a pool of disks configured to support the workload. Those disks and LUNS together must be able to meet the required performance specifications for the replica disk. These specifications are<strong> Input/Output Operations Per Second (IOPS)</strong> and throughput.<a id="id374" class="indexterm"/>
</p><p>If a decision to use a dedicated replica datastore is made during the design phase, it is recommended to allocate Tier 1 storage, for example, SDD to host the replica disk.</p><p>The following diagram is an illustration showing a typical virtual disk to drive type associations:</p><div><img src="img/1124EN_08_09.jpg" alt="VMware View Composer"/></div><p>Storage vendors have different solutions and architectures to solve response time and latency issues. Some of the solutions available are automated storage tiering, storage pools, diskless environments, inline I/O de-duplication, and local host caching.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec02"/>VMware vSphere files</h1></div></div></div><p>Mentioned in the following table are the files and disks created for the virtual desktop provisioned through VMware View and are the standard files for any virtual machine created on a vSphere hypervisor:<a id="id375" class="indexterm"/>
</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>File type</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">.vmx</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The<code class="literal"> .vmx</code> file is the primary configuration file for a virtual machine. It contains information such as the operating system, disk sizes, networking, and so on.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">.vmsd</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Information and metadata about snapshots.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">.vmxf</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Supplemental configurations file for virtual machines that are in a team. Note that the<code class="literal"> .vmxf</code> file remains if a virtual machine is removed from the team.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">.vswp</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The<code class="literal"> .vswp</code> file is a swap file created for each virtual machine to allow for VM memory over-commitment on an ESXi host. This file is created when a VM is powered on and will be equal in size to the unreserved memory configured for the VM. When VMs are created, the default memory reservation is 0 MB, so the size of the<code class="literal"> .vswp</code> file is equal to the amount of memory allocated to the VM. If a VM is configured with a 1024 MB memory reservation, the size of the<code class="literal"> .vswp</code> file will be equal to the amount of memory allocated to the VM minus the 1024 MB reservation.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">.vmss</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The<code class="literal"> .vmss</code> file is created when a VM is suspended and is used to save the suspended state. In essence, this is a copy of the VM's memory and will always have the size of the total amount of allocated RAM. One thing to note is that a<code class="literal"> .vmss</code> file is created when the machine enters the suspended state, however it's not removed when the VM is removed from the suspended state. The<code class="literal"> .vmss</code> file is only removed when the VM is powered off. If a VM is configured with 2048 MB memory, the size of the<code class="literal"> .vmss</code> file will be 2048 MB.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">.nvram</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the file that stores the state of the virtual machine's BIOS.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">.vmsn</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Snapshot state file, which stores the running state of a virtual machine at the time you take the snapshot.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-flat.vmdk</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The<code class="literal"> -flat.vmdk</code> is a raw disk file that is created for each virtual disk allocated to a given VM and will be of the same size as the virtual disks added to the VM at the time of creation. It's a pre-allocated disk file only available with full clone VMs.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">.log</code>
</p>
</td><td style="text-align: left" valign="top">
<p>VM log files are relatively small.</p>
</td></tr></tbody></table></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec03"/>VMware View specific files</h1></div></div></div><p>Mentioned in the following table are the files and disks created for virtual desktops provisioned through VMware View. Some of the disks are only created by View Composer or when assigning a persistent or disposable disk to the desktop pool:<a id="id376" class="indexterm"/>
</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>File type</p>
</th><th style="text-align: left" valign="bottom">
<p>Composer</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">replica-GUID.vmdk</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Replica VM is used to spin-up linked clone VMs.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-internal.vmdk</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Data configuration for Quick Prep/Sysprep.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">VM-s000[n].vmdk</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Created when a virtual machine has snapshot(s). This file stores changes made to a virtual disk while the virtual machine is running. There may be more than one such file. The 3-digit number after the letter (000 after s in this case) indicates a unique suffix added automatically to avoid duplicate filenames.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">VDM-disposable-GUID.vmdk</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Redirected Windows OS pagefile and temporary files.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">.log</code>
</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>VM log files.</p>
</td></tr></tbody></table></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec04"/>Tiered storage</h1></div></div></div><p>VMware View 4.5 and above allow administrators to select different datastores to host different types of virtual disks (replica, linked clone, and persistent). Data performance classification is an important part of storage tiering implementation and allows administrators to select datastores that provide the most appropriate storage tier in regards to performance, cost, and capacity for each type of disk in use.</p><div><h3 class="title"><a id="note22"/>Note</h3><p>Important: Do not confuse the storage tiering feature provided by VMware View with auto storage tiering offered by storage vendors. The solution provided by VMware View is static and does not automatically move data around to achieve best performance. The solutions are complementary to each other.</p></div><p>With the introduction of storage tiering and the ability to segment workloads across datastores and types of disks, it is important to understand what type of disks and data are created for each virtual desktop. The type of disks created may differ for each implementation. Desktop pools that utilize linked clone technology may have additional virtual disks that are not created when using traditional full clone provisioning.</p><p>The following diagram is an illustration showing the multiple types of virtual disks in use by VMware View:<a id="id377" class="indexterm"/>
</p><div><img src="img/1124EN_08_10.jpg" alt="Tiered storage"/></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec01"/>Replica disk</h2></div></div></div><p>The<code class="literal"> replica-GUID.vmdk</code> folder contains all files required to run the virtual machine, however, it will be exclusively used as read-only and as the base for linked clone virtual desktops. The following screenshot demonstrates the folder and files created to host a replica disk.<a id="id378" class="indexterm"/>
</p><p>Despite the fact that the provisioned size of the disk is set to 30 GB (31,457,280 KB) in the following example, only 8 GB is in fact used. The reason for this is that View Composer makes use of vSphere VMFS thin provisioning technology to create replica disks. This is an automatic setting that cannot be changed via View Manager UI and will work independently of the Parent VM being thin or thick provisioned. For NFS deployments, thin is also the only provisioning mechanism.<a id="id379" class="indexterm"/>
</p><div><img src="img/1124EN_08_11.jpg" alt="Replica disk"/></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec02"/>Internal disk</h2></div></div></div><p>The<code class="literal"> internal.vmdk</code> disk is a small disk and contains the configuration data for Quick Prep/Sysprep. In previous VMware View releases, operations, for example, Refresh would incur a full desktop deletion, followed by provision and customization of a new virtual desktop. This process used to take a long time to complete and would normally draw a high number of Compute and Storage resources.<a id="id380" class="indexterm"/>
</p><p>VMware View 4.5 and later started implementing a different technique to refresh virtual desktops that make use of the vSphere snapshot technology.</p><p>A Refresh operation is simply a snapshot revert-back operation. The internal disk is created to store the Active Directory computer account password changes that Windows performs every so often as per default AD policy setting. The computer account password is encrypted before being stored on the internal disk.</p><p>Whenever the domain computer account password is changed, VMware View Agent stores another encrypted copy of the password in the disk. This ensures that domain connectivity is maintained when a desktop is refreshed.</p><p>The internal disk is connected to the desktop; however, it does not get a drive letter assigned to it.</p><p>The following screenshot shows the internal disk screenshot from within a guest vDesktop showing the internal disk:</p><div><img src="img/1124EN_08_12.jpg" alt="Internal disk"/></div><p>The internal disk is the only disk created by VMware View that is not thin provisioned. Its size is so small that being thick provisioned doesn't change the capacity requirements in the solution.<a id="id381" class="indexterm"/>
</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec03"/>Delta/differential disk</h2></div></div></div><p>The following screenshot demonstrates folders and files created in the VM folder to host a linked clone desktop. In the following example, the delta disk is<code class="literal"> VMView-7-D-01-000001.vmdk</code>.</p><p>After the customization process is complete, the VM is shutdown and View Composer takes a snapshot of the linked clone.<a id="id382" class="indexterm"/>
</p><p>The following screenshot shows the folders and files created in the respective VM's folder:</p><div><img src="img/1124EN_08_13.jpg" alt="Delta/differential disk"/></div><p>After the snapshot is taken, data is no longer written to the base<code class="literal"> .vmdk</code> file. Instead, changes are written to the delta disk. A delta disk will be created every time a snapshot is taken. It is important that any requirements to use snapshots are considered when defining the datastore size requirements.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec04"/>Disposable disk</h2></div></div></div><p>VMware View allows for the creation of an optional fixed-size non-persistent disk for each virtual desktop. When disposable disks are assigned to a desktop pool, VMware View redirects Windows temporary system files and folders to a disposable disk.<a id="id383" class="indexterm"/>
</p><p>Disposable disks are automatically deleted when the virtual desktop is powered off, refreshed or recomposed, meaning that temporary files are also deleted during these operations.</p><p>Disposable disks are also thin provisioned and will grow over time to the maximum size set during the desktop pool configuration.<a id="id384" class="indexterm"/>
</p><p>The following screenshot shows the<strong> Disposable File Redirection</strong> configuration within the View Admin console:</p><div><img src="img/1124EN_08_14.jpg" alt="Disposable disk"/></div><p>The disposable disk is hardcoded to register itself as the first available drive on the Windows desktop. This behavior may cause some implications while trying to map network drives in Windows virtual desktops. Even when the CD-ROM is not in use, the first available drive letter would be<code class="literal"> E:</code> because<code class="literal"> C:</code> is taken by the OS and<code class="literal"> D:</code> by the disposable disk.<a id="id385" class="indexterm"/>
</p><p>The files that are commonly offloaded to disposable disks are Windows paging files, VMware log files, and temporary internet files.</p><div><div><div><div><h3 class="title"><a id="ch08lvl3sec01"/>Windows paging files</h3></div></div></div><p>When Windows is constantly running out of physical memory, it will start to page memory to disk. This paging process will incur in block writes that will increase the size of the disposable disk. As a recommended approach, administrators should make sure that virtual desktops have enough virtual memory available to avoid disk paging. Disk paging has a negative impact on performance.<a id="id386" class="indexterm"/>
</p></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec02"/>Temporary internet files</h3></div></div></div><p>User temporary files are kept on the system or persistent data disk and this may include files written to<code class="literal"> %USERPROFILE%\AppData\Local\Temp</code> and<code class="literal"> Temporary Internet Files</code>. These are the temporary files that can grow very fast and consume disk space.<a id="id387" class="indexterm"/>
</p><p>As the temporary files are offloaded to the disposable disk, the steady growth of the delta file is reduced. Instead of utilizing delta disks that will grow according to block changes, View Composer utilizes the disposable disk. However, it is important to size the disposable disks according to the virtual desktop and workload requirements. Once assigned to a virtual desktop pool, this setting cannot be changed through the View Manager.</p><p>The following screenshot shows a disposable disk with Windows temporary files:</p><div><img src="img/1124EN_08_15.jpg" alt="Temporary internet files"/></div><div><h3 class="title"><a id="note23"/>Note</h3><p>When configuring a linked clone pool, make sure that the disposable disk is larger than the Windows paging file size plus overhead for temporary files.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec05"/>Persistent disk</h2></div></div></div><p>Persistent data disk is the new name for what used to be called<strong> User Data Disk (UDD)</strong> in previous releases of VMware View and maintains the similar characteristics to the UDD. Persistent disks were created to maintain a one-to-one relationship between users and virtual desktops.<a id="id388" class="indexterm"/>
</p><p>When selected during the desktop pool configuration, the persistent disk is created and VMware View Agent makes modifications to the Windows Guest OS to allow the user profile to be redirected to this disk.</p><p>In VMware View 4.5 and later, persistent disks can be managed. The disk can be detached and reattached to virtual desktops. Note that this will only work if the disks are created in VMware View 4.5 or later. If a VMware View environment has been upgraded from earlier releases, these operations will not be available.</p><p>Just like disposable disks, persistent disks cannot be disabled or enabled once they are configured for the desktop pool. It is also not possible to change the size through the View Manager graphical user interface after the initial configuration. However, it is possible to change the persistent disk's size for virtual desktops being newly provisioned in the desktop pool if the administrator changes the pool settings.</p><p>When configuring persistent disks, you should make sure that the size of the disk is adequate for your users. If Active Directory Folder Redirection or VMware View Persona Management is not in use, the user profile size could be several gigabytes in size.</p><p>As a general rule, if you are using persona management or Windows roaming profiles, make sure the disk is large enough to cater to the user's roaming profiles, or apply quotas to roaming profiles.</p><p>The following screenshot demonstrates the persistent disk selection screen during the desktop pool configuration. It shows the configuration of persistent disks within the View Admin console:<a id="id389" class="indexterm"/>
</p><div><img src="img/1124EN_08_16.jpg" alt="Persistent disk"/></div><p>In the Windows OS, the persistent disk and the disposable disk can be seen in Windows Explorer. Important folders will be locked and users cannot delete them. However, through the use of Windows Group Policy, it is possible to hide the drives while still making them available for use.</p><p>The following screenshot shows the various disks, as seen within the guest vDesktop:</p><div><img src="img/1124EN_08_17.jpg" alt="Persistent disk"/></div><p>The persistent disk's drive letter is selected during the desktop pool configuration process and the content is similar to the following screenshot. In the<strong> Users</strong> folder, the profile folders and settings are found:<a id="id390" class="indexterm"/>
</p><div><img src="img/1124EN_08_18.jpg" alt="Persistent disk"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec05"/>Storage overcommit</h1></div></div></div><p>With the introduction of linked clone technology and the ability to specify when each virtual desktop is refreshed or recomposed, there is an opportunity to specify how much storage should be overcommitted to help reduce storage consumption.<a id="id391" class="indexterm"/>
</p><p>Let's assume that not every virtual desktop will utilize the full provisioned storage at the same time, thereby leaving a gap for storage utilization and overallocation (overcommit).<a id="id392" class="indexterm"/>
</p><p>During the desktop pool provisioning process, the administrator has the option to select "Refresh OS Disk after log off" with one of the following options:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Never:</strong> If the<strong> Never</strong> option is selected, then virtual desktops will never execute the delta disk Refresh operation. The delta disk will grow with every block change up to the limit of the disk itself. If the disk size defined for the virtual desktop is 40 GB, this is the limit. When 40 GB is reached, then vSphere VMFS starts reutilizing the blocks just like it does with full clones. You will not run out of disk space in this case.<a id="id393" class="indexterm"/></li><li class="listitem" style="list-style-type: disc"><strong>Always:</strong> If the<strong> Always</strong> option is in use, then virtual desktops will be refreshed every time a user logs off from the desktop. Assuming that only a few gigabytes have been added to the delta disk during use, they will then be recuperated when the virtual desktop is refreshed.<a id="id394" class="indexterm"/></li><li class="listitem" style="list-style-type: disc"><strong>Every x number of days:</strong> If the<strong> Every x number of days</strong> option is selected, then virtual desktops will be refreshed on the number of days defined, independent of the utilization of the delta disk. Delta files grow over time based on a number of factors that include Windows and application utilization. Therefore, while selecting this option, it is important to understand how big the delta can get during that period, so you are able to size datastores accordingly.<a id="id395" class="indexterm"/></li><li class="listitem" style="list-style-type: disc"><strong>At y percent of disk utilization:</strong> If the<strong> At y percentage disk utilization</strong> option is selected and<em> y</em> is set to 50 percent, then the virtual desktop will be refreshed when half of the total provisioned storage is utilized by the delta. This calculation does not include additional disks such as persistent or disposable. If a virtual desktop has been created with a total disk size of 40 GB, the Refresh operation would happen when the user logs off and the delta disk utilization is more than or equal to 20 GB.<a id="id396" class="indexterm"/></li></ul></div><p>An important aspect to remember is that linked clones start at a fraction of its full provisioned size. The storage capacity savings provided by VMware View Composer through the Refresh operation allow administrators to decide how the available storage capacity should be utilized until the storage is fully occupied. As an example, the administrator who selected the<strong> Always</strong> option knows that delta files, on average, will grow up to 300 MB while the desktop is in use during business hours.</p><p>Based on the storage utilization, it is possible to enforce the placement of more virtual desktops per datastore than would be possible with full clone virtual desktops. This is called the<strong> storage overcommit level.</strong>
<a id="id397" class="indexterm"/>
</p><div><h3 class="title"><a id="note24"/>Note</h3><p>VMware View does not allow administrators to configure the maximum number of linked clones per datastore and the limitation on the number of desktops comes from the datastore size. It is critical to size datastores appropriately to support the required number of desktops, yet be compliant with VMware View and View Composer maximums and limits.</p></div><p>Typically, the overcommit level is defined based on how virtual desktops are used. If a desktop pool with floating assignment has desktops that have<strong> Always</strong> Refresh option after logoff, storage consumption will be low and you may set the overcommit to<strong> Aggressive</strong>. However, if virtual desktops are not frequently refreshed, you may prefer to set it to<strong> Conservative</strong>.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec06"/>Storage overcommit level options</h2></div></div></div><p>It is possible to define different overcommit levels among different types of datastores to address different levels of capacity, performance, or availability provided. For example, a NAS datastore may have a different overcommit level than a SAN datastore; in the same way an SSD datastore can have a different overcommit level than an FC datastore.<a id="id398" class="indexterm"/>
</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Option</p>
</th><th style="text-align: left" valign="bottom">
<p>Storage overcommit level</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>Storage is not overcommitted.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Conservative</p>
</td><td style="text-align: left" valign="top">
<p>Four times the size of the datastore. This is the default level.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Moderate</p>
</td><td style="text-align: left" valign="top">
<p>Seven times the size of the datastore.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Aggressive</p>
</td><td style="text-align: left" valign="top">
<p>Fifteen times the size of the datastore.</p>
</td></tr></tbody></table></div><div><h3 class="title"><a id="note25"/>Note</h3><p>It is recommended practice to always match vSphere datastores with LUNS or Exports on a one-by-one basis. Administrators should avoid using large storage LUNS backed by multiple datastores.</p></div><p>The number of linked clones per datastore defined by the storage overcommit level is based on the size of the Parent VM. Based on a 30 GB VM and a 200 GB datastore, VMware View would be able to fit approximately 6 full clone virtual desktops. However, if using overcommit level 7 (Moderate), VMware View would be able to fit approximately 42 desktops.</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>VM size (GB)</p>
</th><th style="text-align: left" valign="bottom">
<p>Datastore size (GB)</p>
</th><th style="text-align: left" valign="bottom">
<p>Overcommit level</p>
</th><th style="text-align: left" valign="bottom">
<p>Number of full clone VMs</p>
</th><th style="text-align: left" valign="bottom">
<p>Number of linked clone VMs</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>30</p>
</td><td style="text-align: left" valign="top">
<p>200</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>24</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>30</p>
</td><td style="text-align: left" valign="top">
<p>200</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>42</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>30</p>
</td><td style="text-align: left" valign="top">
<p>200</p>
</td><td style="text-align: left" valign="top">
<p>15</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>90</p>
</td></tr></tbody></table></div><div><h3 class="title"><a id="note26"/>Note</h3><p>It's possible to run out of storage capacity. When the storage available in a datastore is not sufficient, VMware View will not provision new desktops, however, the existing linked clone desktops will keep growing and eventually fill up the datastore. This situation is more common with overcommit level set to<strong> Aggressive.</strong>
</p></div><p>To make sure that linked clones do not run out of disk space, administrators should periodically refresh or rebalance desktop pools to reduce the linked clone footprint to its original size.</p><p>The following screenshot demonstrates storage overcommit selection during the desktop pool provisioning or configuration process:</p><div><img src="img/1124EN_08_19.jpg" alt="Storage overcommit level options"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec06"/>Storage protocols</h1></div></div></div><p>VMware View is supported by the VMware vSphere, and therefore supports multiple storage protocols for storing data. VMware vSphere is capable of using Fiber Channel, iSCSI,<strong> Fiber Channel over Ethernet (FCoE)</strong>, and NFS.<a id="id399" class="indexterm"/>
</p><p>The main considerations for protocol choice for VMware View are maximum throughput, VMDK behavior, and the cost of reusing existing versus acquiring new storage infrastructure. These considerations affect network design and performance.<a id="id400" class="indexterm"/>
</p><p>The intention of this section is not to cover each protocol or how they perform in a VDI environment. The numbers in the following table are based on VMware View 5 and vSphere 5 and are intended to help with the decision on the storage protocol to be used:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="bottom">
<p>Fiber Channel</p>
</td><td style="text-align: left" valign="bottom">
<p>iSCSI</p>
</td><td style="text-align: left" valign="bottom">
<p>FCoE</p>
</td><td style="text-align: left" valign="bottom">
<p>NFS</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p>Type</p>
</td><td style="text-align: left" valign="top">
<p>Block</p>
</td><td style="text-align: left" valign="top">
<p>Block</p>
</td><td style="text-align: left" valign="top">
<p>Block</p>
</td><td style="text-align: left" valign="top">
<p>File</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p>VAAI</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p>Transmission rate</p>
</td><td style="text-align: left" valign="top">
<p>4 Gbps or 8 Gbps</p>
</td><td style="text-align: left" valign="top">
<p>Multiple 10 Gbps</p>
</td><td style="text-align: left" valign="top">
<p>Multiple 10 Gbps</p>
</td><td style="text-align: left" valign="top">
<p>Multiple 10 Gbps</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p>Maximum number of hosts</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p>LUNs/Exports per host</p>
</td><td style="text-align: left" valign="top">
<p>256</p>
</td><td style="text-align: left" valign="top">
<p>256</p>
</td><td style="text-align: left" valign="top">
<p>256</p>
</td><td style="text-align: left" valign="top">
<p>256</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p>Clones per datastore</p>
</td><td style="text-align: left" valign="top">
<p>64 to 140</p>
</td><td style="text-align: left" valign="top">
<p>64 to 140</p>
</td><td style="text-align: left" valign="top">
<p>64 to 140</p>
</td><td style="text-align: left" valign="top">
<p>Not validated</p>
</td></tr></tbody></table></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec07"/>Maximums and limits</h1></div></div></div><p>Designing a large-scale VMware View solution is a complex task. The same challenges faced in large deployments may be faced in small deployments if VMware validated maximums and limits are not observed.</p><div><h3 class="title"><a id="note27"/>Note</h3><p>It is recommended to use a conservative approach when sizing the VDI environment.</p></div><p>There are tools to help administrators to understand requirements and constraints from graphics, CPU, memory, and storage perspectives. Other tools help to calculate the infrastructure size based on the number of virtual desktops, average IOPS, memory size, percentage of shared memory, percentage of used memory, percentage of read/write IOPS, and so on.</p><p>No matter what results these tools provide, the VDI architect should always ensure that the numbers are within VMware vSphere and VMware View maximums and limits.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec07"/>64 — to 140 linked clones per datastore (VMFS)</h2></div></div></div><p>For FC arrays with support for<strong> vStorage APIs for Array Integration(VAAI)</strong> , the maximum number of linked clones per datastore is 140. The VAAI primitives that augment the number of virtual desktops per datastore is called<strong> hardware assisted locking</strong> or<strong> Atomic Test and Set (ATS)</strong>.<a id="id401" class="indexterm"/>
</p><p>The vSphere VMkernel has to update VMFS metadata for operations involving the virtual desktops stored in the VMFS. Updates to metadata occur as a result of powering on/off virtual desktops, suspending/resuming virtual desktops, and various other operations.<a id="id402" class="indexterm"/>
</p><p>In a VDI environment, that would mean the VMkernel may have to update metadata for many hundreds of virtual desktops, and would therefore have to lock the entire VMFS in order to update its metadata just to power on a single virtual desktop. That operation takes no more than a few milliseconds, but does become problematic when powering on many virtual desktops simultaneously.</p><p>Hardware assisted locking, available with vSphere 4.1 and compatible vendor array code, allows the VMkernel to lock metadata at the block level within the VMFS stored on the array and allow multiple operations to occur simultaneously within the VMFS, which in turn allows many desktop VMs to be powered on at the same time.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec08"/>250 linked clones per datastore (NFS)</h2></div></div></div><p>For datastores backed by<strong> Network File System (NFS)</strong>, there is no limitation on the number of virtual desktops per datastore because NFS doesn't present the same SCSI reservation problems. However, to date there is no official validation from VMware on the maximum number of virtual desktops that can be hosted in a single datastore backed by NFS. The recommendation thus far is to keep the number of virtual desktops per NFS datastore to below 250.<a id="id403" class="indexterm"/>
</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec09"/>32 full — clones desktops per datastore (VMFS)</h2></div></div></div><p>It would be good if it was possible to answer the "Why?" question for every maximum and limit dictated by the documentation. Some of the widely known limits are either based on quality assurance tests or field experience. Some other limits are based on best practices that have been set for prior technology, but due to the lack of additional tests they remain valid.<a id="id404" class="indexterm"/>
</p><p>We tried to understand where the 32 virtual desktops limit when using full clone came from, and the answer we received from VMware was that there was no good answer, except that the limitation is based on server workloads, SCSI reservations, and storage administrators not doing a good job at sizing the infrastructure.</p><p>If this limit was set for virtual server workloads, it could have been set years ago when storage architectures didn't have features such as advanced caching and VAAI support, among others.</p><div><div><div><div><h3 class="title"><a id="ch08lvl3sec03"/>8 hosts per vSphere cluster with View Composer</h3></div></div></div><p>VMware View will stop the provision of new virtual desktop if the number of hosts in a cluster with View Composer surpasses 8. The behavior is hard-coded into View Composer. However, the source of the limitation lies in the VMFS layer.<a id="id405" class="indexterm"/>
</p><p>VMFS structure only allows for a maximum of 8 hosts to access to read or write a single VMDK file. In a linked clone implementation, all hosts in a cluster may have virtual desktops reading storage blocks from the same replica disk.</p><div><h3 class="title"><a id="note28"/>Note</h3><p>At the time of writing, VMware was working to validate up to 16 hosts per vSphere cluster with View Composer.</p></div></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec04"/>1,000 clones per replica</h3></div></div></div><p>The number of clones per replica also determines the number of linked clones that may coexist in a single desktop pool. This number is resulting from VMware's QA validation labs, however this is a soft limit and despite not being recommended, it can be increased.</p><p>In previous releases, the VMware View validated limit was 512 virtual desktops per replica or desktop pool. This limit was a result from a maximum of 64 linked clones per datastore multiplied by 8 hosts per vSphere cluster. (64 * 8 = 512).<a id="id406" class="indexterm"/>
</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec08"/>Storage I/O profile</h1></div></div></div><p>The I/O storage profile produced by each virtual desktop is entirely dependent on which type of Windows OS is in use, the applications deployed, and even how each user individually interacts with the environment.<a id="id407" class="indexterm"/>
</p><p>IOPS (pronounced as<strong> eye-ops)</strong> is a common performance measurement used to benchmark computer storage devices such as<strong> hard disk drives (HDDs)</strong>,<strong> solid state drives (SSDs)</strong>, and<strong> storage area networks (SANs)</strong>. As with any benchmark, IOPS numbers published by storage device manufacturers do not guarantee real-world application performance.<a id="id408" class="indexterm"/>
</p><p>According to Wikipedia, predictions of what the average virtual desktop I/O profile will likely be is one of the most difficult tasks when designing a VDI solution. The reason for that is the lack of information about the workload that will be running in each one of the virtual desktops at the design time.<a id="id409" class="indexterm"/>
</p><p>It is possible to use pre-trended numbers as a baseline; however, despite the indication of what the workload would likely be, it could be a point out of the curve in some cases.</p><p>In a nirvana scenario, a VDI pilot project has been operational for a little while and data can be collected and trended appropriately.</p><p>A few metrics must be taken into consideration to size storage correctly. They are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Storage size:</strong> How much storage capacity is required?<a id="id410" class="indexterm"/></li><li class="listitem" style="list-style-type: disc"><strong>LUN size:</strong> How many LUNs and/or datastores are required?</li><li class="listitem" style="list-style-type: disc"><strong>Tier type:</strong> What type of disk is required and what is the disk placement?</li><li class="listitem" style="list-style-type: disc"><strong>IOPS (cmd/s):</strong> What is the number of I/O commands per second?</li><li class="listitem" style="list-style-type: disc"><strong>Read/write ratio:</strong> What is the read and write ratio?</li></ul></div><p>The first three items in the list may be calculated without major understanding of the I/O workload; however, it would require knowledge about the virtual desktop storage capacity utilization.</p><p>The real problem lies with the I/O per second and the read/write I/O pattern. Without those values, storage architects/administrators will probably not be able to provision storage with the performance that the virtual desktop infrastructure requires.</p><p>IOPS, also known as the<strong> disk I/O profile</strong>, will differ for each type of Windows OS. The profile is also dependent upon the type and number of applications deployed, including services running in the OS. The I/O profile is also dependent on how users interact with their virtual desktops. VMware and partners have validated I/O profiles that can be used as a baseline. It is highly recommended that you find out the correct I/O profile for your particular VDI environment.</p><p>VMware View documentation establishes some I/O baselines:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Light (5 IOPS):</strong> Light users typically use e-mail (Outlook), Excel, Word, and a web browser (Internet Explorer or Firefox) during the normal workday. These workers are usually data entry operators or clerical staff.<a id="id411" class="indexterm"/></li><li class="listitem" style="list-style-type: disc"><strong>Heavy (15 IOPS):</strong> Heavy users are full knowledge workers using all the tools of the light worker (Outlook, Excel, Word, Internet Explorer, and Firefox) and also working with large PowerPoint presentations and performing other large file manipulations. These workers include business managers, executives, and members of the marketing staff.</li></ul></div><p>Another study performed by PQR Consultants (Herco van Brug) demonstrates I/O profiles as follows:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="bottom">
<p>Windows XP</p>
</td><td style="text-align: left" valign="bottom">
<p>Windows 7</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p>Light</p>
</td><td style="text-align: left" valign="top">
<p>3 to 4</p>
</td><td style="text-align: left" valign="top">
<p>4 to 5</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p>Medium</p>
</td><td style="text-align: left" valign="top">
<p>6 to 8</p>
</td><td style="text-align: left" valign="top">
<p>8 to 10</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p>Heavy</p>
</td><td style="text-align: left" valign="top">
<p>12 to 16</p>
</td><td style="text-align: left" valign="top">
<p>14 to 20</p>
</td></tr></tbody></table></div><p>The<em> VDI &amp; Storage Deep Impact v1-25 hot!</em> article can be found at<a class="ulink" href="http://www.virtuall.nl/whitepapers/solutions"> http://www.virtuall.nl/whitepapers/solutions</a>.</p><p>It is common to talk about average IOPS per virtual desktop; however, when sizing the VDI solutions, it is crucial that the peaks are also catered for. Otherwise the storage infrastructure will be under heavy stress and will not be able to deliver the required IOPS and throughput.</p><p>Common scenarios where high performance and high throughput are required are during boot and login storms. As an example, a Windows 7 desktop can generate up to 700 IOPS during boot time. Another good example is if the "Refresh on logoff" option is used in conjunction with floating pools, it is common to see utilization peaks at the end of the work shift when business users start to log off.</p><p>Now, we have an existing paradigm where, from a cost perspective, storage infrastructure should be sized for the average performance requirements over time, but from a performance perspective it should be sized for those peaks. For that reason, storage vendors have implemented their own proprietary caching solutions to optimize storage arrays to deal with the high peaks yet volatile VDI I/O requirements.<a id="id412" class="indexterm"/>
</p><p>Most architecture documents or white papers published will demonstrate some divergences on these numbers. If you want to run your own I/O benchmarking during the pilot phase, use storage array admin tools, VMware vCenter client, or tools, for example, vscsiStats that will provide you with a much more granular overview.</p></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec09"/>Read/write I/O ratio</h1></div></div></div><p>The read/write I/O ratio will determine how many disks are required to support the VDI workload in a RAID configuration. Now we will learn how critical it is to understand the read and write I/O ratio to allow administrators to properly size storage arrays from a frontend (storage processors) and backend (disks) standpoint.<a id="id413" class="indexterm"/>
</p><p>In the last topic, we talked about the total number of IOPS that a virtual desktop produces during boot, log in, and steady state utilization. IOPS may be read or written. Every time a disk block is read, we have a read I/O and every time a blog is written, we have a write I/O.</p><p>The Windows operating systems are by nature very I/O intensive, and most of those I/Os are write I/Os. This is actually a very interesting subject. During workload simulations, it is possible to identify that Windows is constantly issuing more write than read I/Os. However, the most interesting fact is that even when Windows is idle, it still produces more write than read I/Os.</p><p>The same study performed by PQR Consultants (Herco van Brug ) says that:</p><div><blockquote class="blockquote"><p>"The amount of IOPS a client produces is very much dependent on the users and their applications. But on average, the IOPS required amount to eight to ten per client in a read/write ratio of between 40/60 percent and 20/80 percent. For XP the average is closer to eight, for Windows 7 it is closer to ten, assuming the base image is optimized to do as little as possible by itself and all I/Os come from the applications, not the OS."</p></blockquote></div><p>During an experiment conducted by Andre Leibovici and published in his blog at<a class="ulink" href="http://myvirtualcloud.net/?p=2138"> http://myvirtualcloud.net/?p=2138</a>, it was possible to clearly identify the intensive write I/O pattern in contrast to the read I/Os during a Login VSI workload generation.</p><p>The following screenshot shows I/O testing performed at<a class="ulink" href="http://myvirtualcloud.net/"> http://myvirtualcloud.net/:</a>
<a id="id414" class="indexterm"/>
</p><div><img src="img/1124EN_08_20.jpg" alt="Read/write I/O ratio"/></div><p>VMware View Reference Architecture documentation points us to read and write ratios with the following magnitudes: 70/30, 60/40, and 50/50. However, it's not uncommon to see VDI workloads with 10 percent reads and 90 percent writes.<a id="id415" class="indexterm"/>
</p><p>The following diagram shows an illustration of the read/write ratio from real production data:<a id="id416" class="indexterm"/>
</p><div><img src="img/1124EN_08_21.jpg" alt="Read/write I/O ratio"/></div><p>The reason we are focusing so much on the I/O pattern is because it determines how many drives are required to support the VDI workload. There are numerous proprietary technologies that reduce the impact of the I/Os on the physical drives. However the methodology to calculate the number of Input/Outputs Per Second required will not change.</p><p>In many cases, architects will be asked to design a solution without knowing what the I/O profile will look like. For most of those cases there is an ongoing pilot. It's very common for organizations to try to understand costs before actually going to a pilot and that's what makes it a difficult task to guess IOPS. If all organizations went first for a small pilot and then decided to buy the whole infrastructure to support the VDI solution, we would be living in an ideal world.</p><p>If you are designing VDI architecture without knowledge of the I/O profile, you should be extremely conservative to avoid undersizing the storage solution. If this is the case, you should use the Heavy I/O profile for all virtual desktops with read and write ratios of 20 reads and 80 writes. Hopefully, you will not be in this situation.<a id="id417" class="indexterm"/>
</p><p>The RAID type selected will determine the performance and number of hard drives required to support the workload based on the amount of IOPS and read/write ratio. When sizing the storage infrastructure, the RAID group selected will add a write performance penalty due to the requirements to stripe the data and record the parity across disk drives. The read I/Os do not suffer a penalty for different types of RAID groups.<a id="id418" class="indexterm"/>
</p><p>The most common RAID types utilized with VDI workloads are RAID 5, 6, and 10:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">RAID 10 adds a write penalty of 2</li><li class="listitem" style="list-style-type: disc">RAID 5 adds a write penalty of 4</li><li class="listitem" style="list-style-type: disc">RAID 6 adds a write penalty of 6</li></ul></div><p>The preceding numbers can be tabulated as follows:<a id="id419" class="indexterm"/>
</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="bottom">
<p><strong>I/O impact</strong></p>
</td><td class="auto-generated"> </td></tr><tr><td style="text-align: left" valign="bottom">
<p><strong>RAID level</strong></p>
</td><td style="text-align: left" valign="bottom">
<p><strong>Read</strong></p>
</td><td style="text-align: left" valign="bottom">
<p><strong>Write</strong></p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p><strong>RAID 0</strong></p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p><strong>RAID 1 (and 10)</strong></p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p><strong>RAID 5</strong></p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p><strong>RAID 6</strong></p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td></tr></tbody></table></div><div><h3 class="title"><a id="note29"/>Note</h3><p>It could be argued that for RAID 10, the read impact is 0.5 as the Windows operating system is able to read the same block off two disks at the same time, or read half of one disk and half of the other. Therefore, we get twice the read performance.</p></div><p>The formula commonly used to calculate these penalties is as follows:</p><p>VM I/O = VM Read I/O + (VM Write I/O * RAID Penalty)</p><p>Other important information for architecting a VDI solution is that Windows operating systems predominantly have small random I/Os. Jim Moyle, in his paper<em> Windows 7 IOPS for VDI: Deep Dive</em> at<a class="ulink" href="http://jimmoyle.com/wordpress/wp-content/uploads/downloads/2011/05/Windows_7_IOPS_for_VDI_a_Deep_Dive_1_0.pdf"> http://jimmoyle.com/wordpress/wp-content/uploads/downloads/2011/05/Windows_7_IOPS_for_VDI_a_Deep_Dive_1_0.pdf</a>, defines the Windows nature to generate small I/Os:</p><div><blockquote class="blockquote"><p>"This is due to how Windows Memory works, memory pages are 4 K in size, as such windows will load files into memory in 4 K blocks, this means that most of the read and write activity has a 4 K block size. Windows 7 does try and aggregate sequential writes to a larger block size to make writing a more efficient process. It will try and aggregate the writes to up to 1 MB in size. The reason for this is that again Windows is expecting a local, dedicated spindle and spinning disks are very good at writing large blocks."</p></blockquote></div><p>Windows operating systems constantly read and write information in blocks with different disk placement, and the native user interaction is another reason for the behavior. Random access with small blocks is a time consuming task for mechanic disk drives and the number of operations per second (IOPS) for each drive is very limited. For this reason, it is important to utilize RAID groups to achieve the required I/O throughput.</p><p>SSDs provide excellent read performance, but also have limitation for small, random write I/Os. Nonetheless, they provide better performance over disk drives, at a much higher cost.</p><p>The following diagram shows an illustration showing the difference between sequential access and random access:<a id="id420" class="indexterm"/>
</p><div><img src="img/1124EN_08_22.jpg" alt="Read/write I/O ratio"/></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec10"/>Storage tiering and I/O distribution</h1></div></div></div><p>Previously in this chapter, we discussed VMware View tiered storage and the ability to assign different datastores or exports to different types of disks. Now, we will discuss how those tiers interact with the storage infrastructure from an I/O perspective.</p><p>VMware View 4.5 introduced the ability to select a dedicated datastore, where replica disks are stored. The VMware View Architecture Guide recommends that this datastore should be served by a pool of SSDs. SSDs generally provide a larger amount of Input/Outputs Per Second and throughput.<a id="id421" class="indexterm"/>
</p><p>As mentioned earlier, common scenarios where high performance and high throughput are required are during boot and login storms, and during large scale application deployment to users or AV updates. As an example, Windows 7 can generate up to 700 IOPS during boot time. Another good example is if the "Refresh on logoff" option is used in conjunction with floating pools, it is common to see utilization peaks at the end of the work shift, when business users start to logoff.</p><p>The total amount of IOPS generated by a virtual desktop is a combination of the number of read I/Os in the replica disk plus read and write I/Os on all other disks. During the different utilization phases, the virtual desktop performs differently and requires a different number of I/Os and a different read/write I/O pattern for each individual tier.</p><p>The best way to understand how many I/Os are required for power on, customization, and first boot is to find out the averaged maximum I/O per datastore. The reason for this is that each storage tier will have different performance requirements.</p><p>The following diagram is an illustration showing the IOPS breakdown. It demonstrates the number of IOPS generated by a virtual desktop from the first power on operation to its first boot. The operations involved are as follows:</p><div><ol class="orderedlist"><li class="listitem">Power on.</li><li class="listitem">Customization.</li><li class="listitem">First boot.<div><img src="img/1124EN_08_23.jpg" alt="Storage tiering and I/O distribution"/></div></li></ol></div><p>The same numbers from the preceding diagram can be demonstrated in a percent style per storage tier. The following diagram shows a table that provides great visibility of what is happening with the virtual desktop during its creation process:</p><div><img src="img/1124EN_08_32.jpg" alt="Storage tiering and I/O distribution"/></div><div><h3 class="title"><a id="note30"/>Note</h3><p>Important: Please remember that those numbers may be completely different in your VDI environment.</p></div><p>Understanding how many virtual desktops will be booting, logging on, or working simultaneously is critical to correctly design the tier supporting replica disks. If the Dedicated Replica Datastore option is selected, it is even more critical that this single disk tier supporting the replica disks can efficiently deliver the performance required, considering that up to 1,000 virtual desktops may be using that single replica disk simultaneously.<a id="id422" class="indexterm"/>
</p><p>The following diagram is an illustration showing the use of a Dedicated Replica Datastore option:<a id="id423" class="indexterm"/>
</p><div><img src="img/1124EN_08_24.jpg" alt="Storage tiering and I/O distribution"/></div><p>Getting to the exact number of IOPS required for each tier of the storage solutions can be a tireless task. To provide you with an insight into how complex this can get, imagine a linked clone virtual desktop making use of a disposable disk and persistent disk. Assume that the replica disk is hosted in the dedicated replica datastore, the linked clone and the disposable disk on a different datastore, and the persistent disk on yet another datastore. The following diagram demonstrates this scenario. It is an illustration showing the breakdown of I/O when using the various virtual disks of VMware View:</p><div><img src="img/1124EN_08_25.jpg" alt="Storage tiering and I/O distribution"/></div><p>As can be observed in the preceding diagram, in this scenario the virtual desktop is generating 30 percent read I/O and 70 percent write I/O. Let's assume that the total I/O for the virtual desktop is 20; then we have 6 read I/Os and 14 write I/Os.<a id="id424" class="indexterm"/>
</p><p>We know replicas are 100 percent read; however, unless we scrutinize the replica disk it's not possible to know how many of the 14 read I/Os are actually being issued against the disk. Read I/Os could also be issued to the linked clone, disposable or persistent disk.</p><p>For the tier supporting linked clones and disposable disks, and for the tier supporting the persistent disk, we will also have a different number of read and write I/Os that are a small percentage from the 6 read I/Os and 14 write I/Os produced by the virtual desktop.</p><p>As you can see, the total number of operations produced by a virtual desktop will often be split across multiple tiers and datastores. The best time to gather those numbers is during the VDI pilot. VMware has the ideal tools for the job esxtop and vscsiStats.</p><p>The truth is that there is no magic formula to help you to get to the exact I/O profile other than analyzing an existing environment's usage patterns. Plan your storage for performance and whenever possible utilize real workload data to calculate the environment. Pre-trended data from white papers and reference architecture guides may give you a baseline; however, they may not apply to your workload and you may end up with an undersized or oversized infrastructure.</p><p>The most common formulae for IOPS calculation are as follows:<a id="id425" class="indexterm"/>
</p><p>Replica tier (read I/O only) = (Concurrent Boot VMs * Peak Boot IOPS) + (Concurrent VMs - Concurrent Boot VMs) * (Replica Steady State IOPS)</p><p>All other tiers = (VM Read I/O + (VM Write I/O * RAID Penalty) * concurrent VMs</p><p>Paul Wilson from Citrix has created an attention-grabbing complex model based on peak IOPS, steady state IOPS, and estimated boot IOPS that takes into consideration the launch rate and desktop login time. His article can be found at<a class="ulink" href="http://blogs.citrix.com/2010/10/31/finding-a-better-way-to-estimate-iops-for-vdi"> http://blogs.citrix.com/2010/10/31/finding-a-better-way-to-estimate-iops-for-vdi</a>.</p></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec11"/>Disk types</h1></div></div></div><p>A common question is related to the type of disk that should be utilized for each storage tier. That's a complex discussion that you will need to have with your storage administrator or vendor. Most intelligent storage arrays provide some type of acceleration or caching mechanism that will potentially reduce the storage backend requirements. With the reduction of the requirements, disks with higher capacity and lower performance may be used.<a id="id426" class="indexterm"/>
</p><p>The following diagram is an illustration showing the virtual disk-to-disk type relationship typically used by storage providers:<a id="id427" class="indexterm"/>
</p><div><img src="img/1124EN_08_26.jpg" alt="Disk types"/></div><p>The most common type of disks used for VMware View deployments are as follows:<a id="id428" class="indexterm"/>
</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>SSDs:</strong> They provide the best throughput performance and may be leveraged for the replica tier requiring bursting capabilities during boot and login storms.<a id="id429" class="indexterm"/></li><li class="listitem" style="list-style-type: disc"><strong>Fibre Channel and SAS:</strong> They provide the best relationship between costs versus performance. Nowadays, Fibre Channel and SAS disks are probably the most common type of disks in enterprise environment and may be leveraged to host Linked Clone disks.<a id="id430" class="indexterm"/></li><li class="listitem" style="list-style-type: disc"><strong>Serial Advanced Technology Attachment (SATA)</strong>: They provide the highest capacity and lowest cost, but with lowest performance. A common use for SATA disks is for persistent or user profile disk placement.<a id="id431" class="indexterm"/></li></ul></div><p>It's important to note that these are just conventional recommendations and your environment may pose different challenges or features. As an example, some scale-out NAS appliances may use very large pools of SATA disks and perform as well as Fibre Channel disks when the matter is throughput.</p></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec12"/>Capacity sizing exercises</h1></div></div></div><p>Sizing the storage infrastructure correctly might be the difference between succeeding or failing a VDI rollout. Many deployments that have excellent performance during the pilot and initial production quickly start to run into storage contention issues because of the lack of understanding of the storage layer.</p><p>In the next section, we demonstrate a few sizing exercises for different VMware View implementations.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec10"/>Sizing full clones</h2></div></div></div><p>Let's see two scenarios for sizing full clones.<a id="id432" class="indexterm"/>
</p><div><div><div><div><h3 class="title"><a id="ch08lvl3sec05"/>Scenario 1</h3></div></div></div><p>The following are the parameters:<a id="id433" class="indexterm"/>
</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Desktops:</strong> 1,000</li><li class="listitem" style="list-style-type: disc"><strong>Pool type:</strong> Dedicated (full clones)</li><li class="listitem" style="list-style-type: disc"><strong>Guest OS:</strong> Windows 7</li><li class="listitem" style="list-style-type: disc"><strong>RAM:</strong> 2 GB</li><li class="listitem" style="list-style-type: disc"><strong>Disk size:</strong> 40 GB</li><li class="listitem" style="list-style-type: disc"><strong>Disk consumed:</strong> 22 GB</li><li class="listitem" style="list-style-type: disc"><strong>Overhead:</strong> 10 percent</li></ul></div><div><div><div><div><h4 class="title"><a id="ch08lvl4sec01"/>Parent VM</h4></div></div></div><p>The Parent VM may be thin or thick provisioned and is usually powered off. If the Parent VM is thick provisioned, its size is similar to the creation of the disk plus log files. As an example, if the Parent VM disk size is set to 40 GB, this will be the approximate size of the Parent VM.</p><p>If the Parent VM is created using thin provisioning, its size is equal to the amount of storage utilized by the Windows operating system at the NTFS plus log files. As an example, if the Parent VM disk size is set to 40 GB but only 10 GB is used, the total size of the Parent VM will be approximately 10 GB plus log files.<a id="id434" class="indexterm"/>
</p><p>There is no considerable performance improvement using thick provisioning over thin provisioning for the Parent VM, given that these are master images and won't be used unless a new replica disk is required.</p></div><div><div><div><div><h4 class="title"><a id="ch08lvl4sec02"/>Overhead</h4></div></div></div><p>The VMware recommendation on storage overhead per datastore is at least 10 percent.</p><p>The following table explains the features and requirements:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Feature</p>
</th><th style="text-align: left" valign="bottom">
<p>Requirement</p>
</th><th colspan="2" style="text-align: left" valign="bottom">
<p>Reasoning</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>VMs per datastore</p>
</td><td style="text-align: left" valign="top">
<p>32 VMFS</p>
</td><td colspan="2" style="text-align: left" valign="top">
<p>Recommended limit of full clones per datastore</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>VM datastore size</p>
</td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: left" valign="top">
<p>Size based on the following calculations:</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Raw file size</p>
</td><td style="text-align: left" valign="top">
<p>40,960 MB</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Log file size</p>
</td><td style="text-align: left" valign="top">
<p>100 MB</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Swap file size</p>
</td><td style="text-align: left" valign="top">
<p>2,048 MB</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Free space allocation</p>
</td><td style="text-align: left" valign="top">
<p>10 percent overhead</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: left" valign="top">
<p>Minimum allocated datastore size = (VMs * (raw + swap + log) + overhead) = (32 * (40,960 MB + 2,048 MB + 100 MB) + 137 GB) = 1.44 TB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Number of datastores</p>
</td><td style="text-align: left" valign="top">
<p>1 per 32 virtual desktops</p>
</td><td colspan="2" style="text-align: left" valign="top">
<p>Number of VMs/VMs per datastore = 1,000/32 = 32</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Total storage</p>
</td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: left" valign="top">
<p>Number of datastores * datastore size = 32 * 1.44 TB = 46 TB</p>
</td></tr></tbody></table></div></div><div><div><div><div><h4 class="title"><a id="ch08lvl4sec03"/>Comments</h4></div></div></div><p>The following is a list of comments:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">As a safety number, it's an assumption that all full clone raw files will eventually achieve the full size (40 GB). Some administrators may prefer to use a fraction of the full utilization size to cut storage costs during calculation.</li><li class="listitem" style="list-style-type: disc">In addition to the storage allocation required to support all full clone virtual desktops, it is important to set aside at least another one datastore per VMware View cluster to host the Parent VM and ISO images.<a id="id435" class="indexterm"/></li></ul></div></div></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec06"/>Scenario 2</h3></div></div></div><p>Here are the parameters:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Desktops:</strong> 2,000</li><li class="listitem" style="list-style-type: disc"><strong>Pool type:</strong> Dedicated (full clones)</li><li class="listitem" style="list-style-type: disc"><strong>Guest OS:</strong> Windows 7</li><li class="listitem" style="list-style-type: disc"><strong>RAM:</strong> 2 GB</li><li class="listitem" style="list-style-type: disc"><strong>Disk size:</strong> 32 GB</li><li class="listitem" style="list-style-type: disc"><strong>Disk consumed:</strong> 22 GB</li><li class="listitem" style="list-style-type: disc"><strong>VM memory reservation:</strong> 50 percent (1,024 MB)</li><li class="listitem" style="list-style-type: disc"><strong>Overhead:</strong> 10 percent</li></ul></div><p>The following table shows the features and requirements:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Feature</p>
</th><th style="text-align: left" valign="bottom">
<p>Requirement</p>
</th><th colspan="2" style="text-align: left" valign="bottom">
<p>Reasoning</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>VMs per datastore</p>
</td><td style="text-align: left" valign="top">
<p>32 VMFS</p>
</td><td colspan="2" style="text-align: left" valign="top">
<p>Recommended limit of full clones per datastore</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>VM datastore size</p>
</td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: left" valign="top">
<p>Size based on the following calculations:</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Raw file size</p>
</td><td style="text-align: left" valign="top">
<p>32,768 MB</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Log file size</p>
</td><td style="text-align: left" valign="top">
<p>100 MB</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Swap file size</p>
</td><td style="text-align: left" valign="top">
<p>1,024 MB</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Free space allocation</p>
</td><td style="text-align: left" valign="top">
<p>10 percent overhead</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: left" valign="top">
<p>Minimum allocated datastore size = (VMs * (raw + swap + log) + overhead) = (32 * (40,960 MB + 1,024 MB + 100 MB) + 108 GB) = 1.13 TB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Number of datastores</p>
</td><td style="text-align: left" valign="top">
<p>1 per 32 virtual desktops</p>
</td><td colspan="2" style="text-align: left" valign="top">
<p>Number of VMs/VMs per datastore = 2,000/32 = 33</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Total storage</p>
</td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: left" valign="top">
<p>Number of datastores * datastore size = 32 * 1.12 TB = 36 TB</p>
</td></tr></tbody></table></div><div><div><div><div><h4 class="title"><a id="ch08lvl4sec04"/>Comments</h4></div></div></div><p>The following is a list of comments:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The disk size for this scenario has been changed to 32 GB, reducing the overall storage footprint required. It's important to size virtual desktops appropriately for what the users require, not adding extra fat to the infrastructure.</li><li class="listitem" style="list-style-type: disc">This scenario introduces 50 percent VM memory reservation, reducing the size of the<code class="literal"> .vswp</code> file to half of the virtual desktop memory. In this scenario, the<code class="literal"> .vswp</code> file is 1,024 MB.</li></ul></div></div></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec11"/>Sizing linked clones</h2></div></div></div><p>Sizing linked clone virtual desktops is to a certain extent more complex than sizing full clones due to the number of variables involved in the calculation. As mentioned earlier in this chapter, linked clone virtual desktops introduce new files and may work differently when the Dedicated Replica Datastore option is selected.<a id="id436" class="indexterm"/>
</p><div><div><div><div><h3 class="title"><a id="ch08lvl3sec07"/>Parent VM</h3></div></div></div><p>The Parent VM used with linked clones is similar to the one used with full clones, however, it includes VM snapshots that are used by View Composer to determine what baseline image is used to create replica disks.<a id="id437" class="indexterm"/>
</p></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec08"/>Replica</h3></div></div></div><p>Replica disks are created as thin provisioned clones from the Parent VM. If the Parent VM is set to a 40 GB disk size, the replica is equal to the amount of storage utilized by the Windows operating system at the NTFS plus the snapshot selected. As an example, if the Parent VM disk size is set to 40 GB but only 10 GB is used, the total size of the replica is approximately 10 GB.<a id="id438" class="indexterm"/>
</p><p>Without making use of the Dedicated Replica Datastore option, for any desktop pool, a unique replica is created in each datastore assigned to the pool. If multiple snapshots are in use in a desktop pool, multiple replicas per datastore may be created if VMware View decides to use the datastore. It is common to have two or more snapshots in use at the same time in a single datastore, especially during Recompose operations.</p><p>Desktop pools * snapshots * datastores = number of replicas</p><p>2 * 1 * 32 = 64 replicas (2 per datastore)</p><p>2 * 2 * 32 = 128 replicas (4 per datastore)</p><p>If the Dedicated Replica Datastore option is in use, VMware View uses a single datastore to create all replicas for the desktop pool. The calculation of the number of replicas is also subject to the number of snapshots concurrently in use.</p><p>Desktop pools * snapshots * datastores = number of replicas</p><p>2 * 1 * 1 = 2 replicas</p><p>2 * 2 * 1 = 4 replicas</p></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec09"/>Scenario 1</h3></div></div></div><p>Here are the parameters:<a id="id439" class="indexterm"/>
</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Desktops:</strong> 5,000</li><li class="listitem" style="list-style-type: disc"><strong>Pool type:</strong> Floating (linked clones)</li><li class="listitem" style="list-style-type: disc"><strong>Guest OS:</strong> Windows 7</li><li class="listitem" style="list-style-type: disc"><strong>RAM:</strong> 2 GB</li><li class="listitem" style="list-style-type: disc"><strong>Disk size:</strong> 32 GB</li><li class="listitem" style="list-style-type: disc"><strong>Disk consumed:</strong> 22 GB</li><li class="listitem" style="list-style-type: disc"><strong>Refresh on logoff:</strong> 10 percent</li><li class="listitem" style="list-style-type: disc"><strong>Overhead:</strong> 10 percent</li><li class="listitem" style="list-style-type: disc"><strong>VAAI:</strong> Enabled</li></ul></div><p>The following table explains the features and requirements:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Feature</p>
</th><th style="text-align: left" valign="bottom">
<p>Requirement</p>
</th><th colspan="2" style="text-align: left" valign="bottom">
<p>Reasoning</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>VMs per datastore</p>
</td><td style="text-align: left" valign="top">
<p>140 VMFS (VAAI)</p>
</td><td colspan="2" style="text-align: left" valign="top">
<p>Recommended limit of full clones per datastore</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>VM datastore size</p>
</td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: left" valign="top">
<p>Size based on the following calculations:</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Raw file size</p>
</td><td style="text-align: left" valign="top">
<p>3,277 MB</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Log file size</p>
</td><td style="text-align: left" valign="top">
<p>100 MB</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Swap file size</p>
</td><td style="text-align: left" valign="top">
<p>1,024 MB</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Free space allocation</p>
</td><td style="text-align: left" valign="top">
<p>10 percent overhead</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: left" valign="top">
<p>Minimum allocated datastore size = (VMs * (raw + swap + log) + overhead) = (140 * (3,277 MB + 1,024 MB + 100 MB) + 60 GB) = 661 GB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Number of datastores</p>
</td><td style="text-align: left" valign="top">
<p>1 per 140 virtual desktops</p>
</td><td colspan="2" style="text-align: left" valign="top">
<p>Number of VMs/VMs per datastore = 5,000/140 = 36</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Total storage</p>
</td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: left" valign="top">
<p>Number of datastores * datastore size = 36 * 661 GB = 23 TB</p>
</td></tr></tbody></table></div><div><div><div><div><h4 class="title"><a id="ch08lvl4sec05"/>Comments</h4></div></div></div><p>The following is a list of comments:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The desktop pool type is floating and that means that whenever the user has logged off, the virtual desktop will be refreshed. The important information here is to know for how long, on average, the users will remain connected to the virtual desktop and how much data they will generate on the delta disk during their usage.<p>If the desktops are designated to classes that will last 45 minutes, the chances are that the delta disk will present marginal growth. However, if the virtual desktop is used for a whole day, the chances are that the delta will grow a few hundred megabytes.
</p><p>For this exercise, we are assuming that the delta disk will grow to a maximum size of 10 percent of the Parent VM, that is, 3,277 MB.
</p></li><li class="listitem" style="list-style-type: disc">VAAI is enabled in this scenario, enabling higher virtual desktop consolidation per datastore. The maximum number of virtual desktops per datastore supported with VAAI is 140.</li></ul></div></div></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec10"/>Scenario 2</h3></div></div></div><p>The following are the parameters:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Desktops:</strong> 10,000</li><li class="listitem" style="list-style-type: disc"><strong>Pool type:</strong> Persistent (linked clones)</li><li class="listitem" style="list-style-type: disc"><strong>Guest OS:</strong> Windows 7 (64 bit)</li><li class="listitem" style="list-style-type: disc"><strong>RAM:</strong> 4 GB</li><li class="listitem" style="list-style-type: disc"><strong>Disk size:</strong> 32 GB</li><li class="listitem" style="list-style-type: disc"><strong>Disk consumed:</strong> 22 GB</li><li class="listitem" style="list-style-type: disc"><strong>Refresh:</strong> Never</li><li class="listitem" style="list-style-type: disc"><strong>Overhead:</strong> 10 percent</li></ul></div><p>The following table explains the features and requirements:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Feature</p>
</th><th style="text-align: left" valign="bottom">
<p>Requirement</p>
</th><th colspan="2" style="text-align: left" valign="bottom">
<p>Reasoning</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>VMs per datastore</p>
</td><td style="text-align: left" valign="top">
<p>100 VMFS</p>
</td><td colspan="2" style="text-align: left" valign="top">
<p>Recommended limit of full clones per datastore</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: left" valign="top">
<p>Size based on the following calculations:</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Raw file size</p>
</td><td style="text-align: left" valign="top">
<p>32,768 MB</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Log file size</p>
</td><td style="text-align: left" valign="top">
<p>100 MB</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Swap file size</p>
</td><td style="text-align: left" valign="top">
<p>4,096 MB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>VM datastore size</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Free space allocation</p>
</td><td style="text-align: left" valign="top">
<p>10 percent overhead</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: left" valign="top">
<p>Minimum allocated datastore size = (VMs * (raw + swap + log) + overhead) = (100 * (32,768 MB + 4,096 MB + 100 MB) + 360 GB) = 3.9 TB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Number of datastores</p>
</td><td style="text-align: left" valign="top">
<p>1 per 100 virtual desktops</p>
</td><td colspan="2" style="text-align: left" valign="top">
<p>Number of VMs/VMs per datastore = 10,000/100 = 100</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Total storage</p>
</td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: left" valign="top">
<p>Number of datastores * datastore size = 100 * 3.9 TB = 390 TB</p>
</td></tr></tbody></table></div><div><div><div><div><h4 class="title"><a id="ch08lvl4sec06"/>Comments</h4></div></div></div><p>This scenario explores the idea of using linked clone but not having an internal policy to refresh virtual desktops so often. The result is similar to implementing full clones as the delta disks will grow to its full capacity, 32 GB in this case.</p><p>With 64-bit Windows 7 and 4 GB RAM without any VM memory reservation, the swap file is responsible for consuming 4 GB of storage capacity per virtual desktop. In total, the<code class="literal"> .vswap</code> file will be consuming 40 TB; however, with only 20 percent memory reservation, this total would go down to approximately 31 TB of used storage space.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec13"/>vSphere 5.0 video swap</h1></div></div></div><p>VMware View has always automatically calculated video RAM based on resolution and color depth. Up to VMware View 4.6, only 24-bit color depth was supported and VMware published the vRAM overhead that each resolution type would require. vRAM overhead is part of the VM memory overhead in virtual machines running on ESXi. The other part of the overhead comes from the number of vCPUs and amount of RAM.</p><p>VMware View 5.0 introduces a 32-bit color depth and makes it the default option. On top of that, to allow 3D support, VMware introduced a new feature in View 5.0 that allows administrators to select how much video RAM should be assigned to virtual desktops.<a id="id440" class="indexterm"/>
</p><p>The following screenshot shows the configuration of vRAM for vDesktops requiring 3D capabilities:<a id="id441" class="indexterm"/>
</p><div><img src="img/1124EN_08_27.jpg" alt="vSphere 5.0 video swap"/></div><p>The VMware View explanation of how to configure vRAM for 3D support is not very helpful and essentially says: the more vRAM, the more 3D performance will be available to vDesktop(s).<a id="id442" class="indexterm"/>
</p><p>To support the new 3D option, vSphere 5.0 implements a second<code class="literal"> .vswp</code> file for every virtual desktop created either using hardware version 7 or 8. This second<code class="literal"> .vswp</code> file is dedicated to video memory overhead and will be used when the virtual desktop is under video resource constraint.</p><p>The following screenshot shows the second<code class="literal"> .vswp</code> file; this is used for video memory:</p><div><img src="img/1124EN_08_28.jpg" alt="vSphere 5.0 video swap"/></div><p>The total memory overhead is defined by the following factors:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Number of virtual CPUs</li><li class="listitem" style="list-style-type: disc">Amount of RAM</li><li class="listitem" style="list-style-type: disc">Amount of vRAM (defined by number of displays, screen resolution, and color depth)</li><li class="listitem" style="list-style-type: disc">3D support</li></ul></div><p>Memory overhead is nothing new to VMware administrators, as they used to calculate overhead based on vCPU, RAM, and vRAM. However, with the introduction of a video memory calculator, vSphere Client 5.0 provides an easy way to define the amount of vRAM required for a given video configuration.</p><p>The following screenshot shows advanced<strong> Video Memory Calculator:</strong>
<a id="id443" class="indexterm"/>
</p><div><img src="img/1124EN_08_29.jpg" alt="vSphere 5.0 video swap"/></div><p>The new video overhead<code class="literal"> .vswp</code> file will affect storage footprint and datastore sizing. In order to understand the real impact, we have reverse-engineered the new video support option. The<strong> swap (MB)</strong> column demonstrates the total storage utilized by the video swap file, and the<strong> overhead (MB)</strong> column demonstrates the amount of RAM overhead utilized for each combination of vCPU, vRAM, and color depth:</p><div><img src="img/1124EN_08_30.jpg" alt="vSphere 5.0 video swap"/></div><div><h3 class="title"><a id="note31"/>Note</h3><p>When 3D support is enabled, a 256 MB overhead is added to the secondary<code class="literal"> .vswp</code> file. Therefore, if you are planning to use 3D, you should size datastores appropriately to accommodate this difference. This additional 256 MB will help virtual desktops not to run into video performance issues when executing 3D display operations. The 256 MB overhead is independent of how much vRAM you assigned to the virtual desktop in VMware View 5.0.</p></div><p>A datastore with 100 desktops will require additional 25 GB with 3D support enabled:</p><p>100 VMs * 256 MB = 25 GB</p><p>The following screenshot shows a table that demonstrates the<code class="literal"> .vswp</code> file (swap) resulting from 3D support enabled:</p><div><img src="img/1124EN_08_31.jpg" alt="vSphere 5.0 video swap"/></div><p>When sizing for 3D support, you will need to ensure that datastores are appropriately sized for the amount of virtual desktops that will reside in the datastore, plus any additional 3D swap overhead.<a id="id444" class="indexterm"/>
</p></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec14"/>Summary</h1></div></div></div><p>Storage for virtualized environments already offers significant complexity from a design perspective. By adding VDI on top of a classic server virtualization solution, the additional storage technologies that were (for example, View Composer) potentially utilized, makes the storage design exponentially more complex. This chapter covered both the high-level aspects of storage design for VMware View solutions as well as the subtle intricacies that can often make or break a solution. Storage design, especially for solutions that are intended to scale over time, can require significant effort. It is important to not only understand fundamental storage principles before embarking on a VMware View storage design, but also understand the various types of virtual disks, as well as how the underlying guest uses its disk.</p><p>Now that all of the major design concepts have been covered, the next chapter will focus on backup and recovery. While a robust VMware View solution should be able to mitigate most outage scenarios, there may be times where a recovery action needs to be taken; as such, understanding the points of interest from a backup perspective as well as the recovery process is important as a design is implemented and handed over to an operations team.</p></div></body></html>