- en: Chapter 6. Sizing the VDI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Sizing** is the process of determining how much horsepower any given component
    of the VDI solution requires, which is ideally based on metrics collected during
    the assessment phase. In most situations, the challenge will not be handling the
    average daily VDI workloads, but it will be handling the peaks. Peak loads in
    a VDI environment are often short in duration and may not be able to be mitigated
    through conventional techniques such as **VMware Distributed Resource Scheduler
    (DRS)** or manual vMotion balancing.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: The components discussed in earlier chapters of this book, for example, VMware
    View Connection Server, require minimal sizing considerations when compared to
    the hardware components that must be sized. The reason being that the software
    components are primarily performing relatively lightweight work and merely brokering
    connections or performing provisioning tasks, which likely aren't happening constantly.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the sizing layers of a VDI solution:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '![Sizing the VDI](img/1124EN_06_01.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
- en: For example, having a properly sized and performing database infrastructure
    is important, as slow database response times can impact both View Composer tasks
    as well as tasks within vCenter. Also, it is important to ensure that the View
    Connection Server has adequate virtual or physical resources such as CPU and memory.
    However, the primary focus of this chapter is on sizing the physical components
    of the VDI.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'To properly understand how to size a VDI, it''s important to gather proper
    metrics during the assessment phase, which was covered in [Chapter 2](ch02.html
    "Chapter 2. Solution Methodology"), *Solution Methodology*. Such metrics include
    the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Number of concurrent users
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User classes and number of vCPUs, memory, and so on, per user class
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network requirements
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: USB redirection frequency
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This chapter will focus on the following components from a sizing perspective,
    not necessarily from a redundancy perspective. This chapter is the *n* in *n*
    + 1\. These components include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: VMware View Connection Server
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware vCenter Server
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server hardware
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network infrastructure
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Storage sizing is covered in [Chapter 8](ch08.html "Chapter 8. Sizing the Storage"),
    *Sizing the Storage*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'An improperly sized VDI could experience any of the following problems:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Slow logons
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poor PCoIP performance
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inability to power on vDesktops due to reaching vCenter maximums
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inability to log in to the VDI
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication errors
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random failures
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network considerations
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While understanding the networking connectivity between the end users and the
    VDI is fairly obvious in a remote scenario, where the end user is removed geographically
    (for example, working from home) from the VDI, it's less obvious in a local scenario.
    While a local scenario may not blatantly cause a VDI architect to think about
    network sizing, it is still imperative to analyze and size the network component
    of a VDI solution even when all components reside on a **Local Area Network (LAN)**.
    This is the only way to truly confirm that the end user's experience should be
    as positive as possible.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在远程场景下，理解最终用户与VDI之间的网络连接相当显而易见（例如，用户在家工作，地理位置远离VDI），但在本地场景下则不太明显。虽然本地场景可能不会明显导致VDI架构师考虑网络规模问题，但即使所有组件都位于**局域网（LAN）**上，分析和调整VDI解决方案的网络组件仍然至关重要。这是确保最终用户体验尽可能积极的唯一途径。
- en: Sizing the network
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络规模调整
- en: 'As a general rule of thumb, a typical task worker requires approximately 250
    Kbps of network throughput for a positive end user experience. By generally accepted
    industry terms, a task worker is a user that has the following characteristics:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，一个典型的任务工作者需要大约250 Kbps的网络吞吐量才能获得良好的最终用户体验。根据行业普遍接受的术语，任务工作者是指具有以下特点的用户：
- en: He uses typical office applications or terminal windows
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他使用典型的办公应用程序或终端窗口
- en: He does not require multimedia
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他不需要多媒体
- en: He does not require 3D graphics
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他不需要3D图形
- en: He does not require bidirectional audio
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他不需要双向音频
- en: However, where a task worker can potentially generate significant network bandwidth
    is with the use of USB peripherals. If a task worker requires USB peripherals
    to perform his job, then it is imperative to perform a network analysis of the
    specific USB peripherals in action prior to full-scale implementation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，任务工作者在使用USB外设时，可能会产生显著的网络带宽需求。如果任务工作者需要USB外设来完成工作，那么在全面实施之前，必须对具体USB外设的使用进行网络分析。
- en: 'The list of the consumables (Kbps) is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是消耗品（Kbps）列表：
- en: PCoIP baseline = 250 Kbps
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCoIP基线 = 250 Kbps
- en: PCoIP burst headroom = 500 Kbps
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCoIP突发余量 = 500 Kbps
- en: Multimedia video = 1,024 Kbps
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多媒体视频 = 1,024 Kbps
- en: 3D graphics = 10,240 Kbps
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3D图形 = 10,240 Kbps
- en: 480p video = 1,024 Kbps
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 480p视频 = 1,024 Kbps
- en: 720p video = 4,096 Kbps
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 720p视频 = 4,096 Kbps
- en: 1080p video = 6,144 Kbps
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1080p视频 = 6,144 Kbps
- en: Bidirectional audio = 500 Kbps
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双向音频 = 500 Kbps
- en: USB peripherals = 500 Kbps
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: USB外设 = 500 Kbps
- en: Stereo audio = 500 Kbps
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 立体声音频 = 500 Kbps
- en: CD quality audio = 2,048 Kbps
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CD质量音频 = 2,048 Kbps
- en: 'The network checklist is given at [http://techsupport.teradici.com/ics/support/default.asp?deptID=15164](http://techsupport.teradici.com/ics/support/default.asp?deptID=15164).
    But before that, you will be required to create an account at this site: [http://techsupport.teradici.com](http://techsupport.teradici.com).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 网络清单请参考[http://techsupport.teradici.com/ics/support/default.asp?deptID=15164](http://techsupport.teradici.com/ics/support/default.asp?deptID=15164)。但在此之前，您需要在此网站创建一个账户：[http://techsupport.teradici.com](http://techsupport.teradici.com)。
- en: 'The other weights are as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其他权重如下：
- en: Buffer = 80 percent
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓冲区 = 80%
- en: Bandwidth offset = 105 percent
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带宽偏移 = 105%
- en: 'The minimum bandwidth to deliver acceptable performance is determined by the
    activity and requirements of the user''s session. Some baseline numbers for the
    minimum bandwidth needed for a respective user type are as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 提供可接受性能所需的最小带宽由用户会话的活动和要求决定。以下是不同用户类型所需最小带宽的一些基准数字：
- en: '| Description | Kbps |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | Kbps |'
- en: '| --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Office worker low without multimedia | 250 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 不带多媒体的低端办公人员 | 250 |'
- en: '| Office worker high without multimedia | 315 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 不带多媒体的高端办公人员 | 315 |'
- en: '| Office worker low with multimedia | 340 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 带多媒体的低端办公人员 | 340 |'
- en: '| Office worker high with multimedia | 375 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 带多媒体的高端办公人员 | 375 |'
- en: 'The following diagram is an illustration showing bandwidth provisioning of
    a given network connection:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了给定网络连接的带宽配置示意图：
- en: '![Sizing the network](img/1124EN_06_09.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![Sizing the network](img/1124EN_06_09.jpg)'
- en: In most environments, the only network traffic that should have a higher network
    priority than PCoIP is network traffic related to **Voice over IP (VoIP)** communications.
    Giving PCoIP a higher priority than VoIP could cause poor quality or loss of connections
    in certain environments with an improperly sized network. Therefore, it is recommended
    to give VoIP a higher priority than PCoIP (approximately up to 20 percent of the
    overall connection), give PCoIP traffic the second highest priority, and classify
    the remaining traffic appropriately.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Network connection characteristics
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Teradici has made significant improvements in the ability of the PCoIP protocol
    to handle high-latency and/or low-bandwidth scenarios. Teradici's PCoIP protocol
    is a purpose-built protocol for delivering a native desktop experience. In order
    to deliver the best possible end user experience, PCoIP will consume as much bandwidth
    as is available at a given time, up to the point where it can deliver a favorable
    end user experience. PCoIP is dynamic in nature, and as the available bandwidth
    changes, so does the amount of bandwidth that PCoIP attempts to consume. PCoIP
    initially uses **Transmission Control Protocol (TCP)** to establish the connection
    and then uses **User Datagram Protocol (UDP)** to transmit the desktop experience.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: PCoIP also has two primary settings that should be understood, the PCoIP maximum
    bandwidth and the PCoIP bandwidth floor.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The PCoIP maximum bandwidth is the maximum amount of bandwidth a given PCoIP
    session is allowed to consume. Configuring this setting can ensure that end users
    never exceed a certain amount of bandwidth themselves. In addition, properly configuring
    the PCoIP maximum bandwidth provides a sense of insurance in a solution. Without
    limiting consumptions per session (even if the maximum is configured to be very
    generous) it is possible to have a runaway PCoIP session consuming a disproportionate
    amount of the available bandwidth. This disproportionate consumption could negatively
    impact the other users sharing the same network connection.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of the bandwidth floor and the bandwidth
    maximums:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![Network connection characteristics](img/1124EN_06_10.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: 'The PCoIP bandwidth floor is the minimum threshold of bandwidth that must be
    available for PCoIP to throttle the stream. Following is an example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: An organization has 500 task workers and is looking to understand how large
    a network pipe they need to provide for their VMware View solution. The VDI users
    only use basic office applications and require no other capabilities.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Average Bandwidth Consumption = (Total Users * 250 Kbps) + (Special Need * Bandwidth
    Penalty) * Bandwidth Offsite * Buffer
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'So, substituting the values given in the preceding example gives us the following
    output:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Average Bandwidth Consumption = (500 * 250 Kbps) + 0 * 80 percent = 100,000
    KBps (approximately 97 Mbps)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: DHCP considerations
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While it is possible to cobble together a VDI solution that uses static **Internet
    Protocol (IP)** addresses, it is highly not recommended. Due to the potential
    volatility of a VDI and for ease of management, **Dynamic Host Configuration Protocol
    (DHCP)** is the preferred method for managing issuing the IP addresses of the
    vDesktops. When using DHCP, vDesktops do not own a specific IP address, but rather
    it leases it from a DHCP server.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'A single DHCP scope consists of a pool of IP addresses on a particular subnet.
    A DHCP superscope allows a DHCP server to distribute IP addresses from more than
    one scope to devices on a single physical network. Proper subnetting can ensure
    that enough IP leases exist in a particular scope to serve the number of end devices
    requiring IP addresses. The following diagram shows a DHCP workflow:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![DHCP considerations](img/1124EN_06_11.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: 'The workflow of a DHCP lease allocation is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The client broadcasts a DHCPDISCOVER message on its physical subnet.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Available DHCP servers on the subnet respond with an IP address by sending a
    DHCPOFFER packet back to the client.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A client replies with a DHCPREQUEST message to signal which DHCP server he/she
    accepted the DHCPOFFER packet from. The other DHCP servers withdraw their offer
    for a DHCP lease.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DHCP server in the DHCPREQUEST message from the client replies with a DHCPACK
    packet to acknowledge the completion of the lease transaction.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DHCP reallocation occurs when a client that already has an address within a
    valid lease expiration window reboots or starts up after being shut down. When
    it starts back up, it will contact the DHCP server previously confirmed via a
    DHCPACK packet to verify the lease and obtain any necessary parameters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: After a set period of time (T1) has elapsed since the original lease allocation,
    the client will attempt to renew the lease. If the client is unable to successfully
    renew the lease, it will enter the rebinding phase (starts at T2). During the
    rebinding phase, it will attempt to obtain a lease from any available DHCP server.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![DHCP considerations](img/1124EN_06_12.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, **T[1]** is defined as 50 percent of the lease duration
    and **T[2]** is defined as 90 percent of the lease duration.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: For this example, assume a lease duration of 120 minutes (two hours).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: A vDesktop boots and is successfully allocated a DHCP lease for a duration of
    120 minutes from DHCP_SERVER_01\. At T1 (50 percent of 120 minutes, that is, 60
    minutes), the vDesktop attempts to renew its lease from DHCP_SERVER_01\. During
    the renewal period, the vDesktop successfully renews its DHCP lease. The lease
    clock is now reset back to a full 120 minute lease since the renew was successful.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: This time the vDesktop is unsuccessful during the renewal period and enters
    the rebinding period. The vDesktop successfully obtains a new DHCP lease from
    DHCP_SERVER_03 with a lease of a fresh 120 minutes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: In most VDI scenarios, a DHCP lease time of one hour is sufficient. Typically,
    this is considerably less than the average DHCP lease time in default scopes used
    by most organizations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数 VDI 场景中，1 小时的 DHCP 租约时间已足够。通常，这比大多数组织默认范围内的 DHCP 租约时间要短得多。
- en: If a desktop pool is set to delete a vDesktop after a user logs off, this could
    generate significant DHCP lease thrashing and a very short DHCP lease time should
    be considered (depending on the frequency of vDesktop deletions).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果桌面池设置为在用户注销后删除虚拟桌面，则可能会导致显著的 DHCP 租约波动，并应考虑设置一个非常短的 DHCP 租约时间（具体取决于虚拟桌面删除的频率）。
- en: VMware View Composer tasks such as Recompose and Refresh should maintain the
    same MAC address throughout the process as the VMX settings related to the vNIC
    should not be altered. Therefore, the original lease would attempt to be reallocated
    during the boot process.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: VMware View Composer 任务（如重新组合和刷新）应在整个过程中保持相同的 MAC 地址，因为与 vNIC 相关的 VMX 设置不应被更改。因此，在启动过程中，原始的租约将尝试重新分配。
- en: Virtual switch considerations
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟交换机注意事项
- en: Virtual switch design for VDI environments is another component that may prove
    challenging for those unfamiliar with large-scale virtual infrastructure, or those
    accustomed to designing solutions with potentially high virtual machine volatility.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 设计 VDI 环境中的虚拟交换机是另一个可能对不熟悉大规模虚拟基础设施或习惯于设计具有高虚拟机波动性的解决方案的人来说具有挑战性的组成部分。
- en: '![Virtual switch considerations](img/1124EN_06_13.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![虚拟交换机注意事项](img/1124EN_06_13.jpg)'
- en: The preceding diagram shows, at a high level, the network components of a VDI
    environment. Not shown is the abstraction (that would reside in-between the physical
    switch and the virtual switch) done by the hypervisor.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图示以高层次展示了 VDI 环境中的网络组件。未显示的是由虚拟化管理程序（hypervisor）执行的抽象（位于物理交换机和虚拟交换机之间）。
- en: When a standard vSwitch is created it has, by default, 120 ports. This parameter
    is defined at a kernel (hypervisor) layer, and any changes to the number of ports
    in a standard switch requires a reboot of the physical host.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 创建标准 vSwitch 时，默认有 120 个端口。此参数是在内核（虚拟化管理程序）层定义的，任何更改标准交换机端口数量的操作都需要重启物理主机。
- en: When a distributed vSwitch, also known as a **dvSwitch**, is created, it has,
    by default, 128 ports. This parameter can be changed dynamically and does not
    require a reboot of the physical host for changing the number of ports from its
    original value of 128.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个分布式 vSwitch（也称为**dvSwitch**）时，默认有 128 个端口。此参数可以动态更改，并且无需重启物理主机即可更改端口数量（从默认的
    128 开始）。
- en: Standard versus distributed switches
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准交换机与分布式交换机
- en: Standard vSwitches are not impacted by the loss of a VMware vCenter Server,
    and are best used by functions such as Service console, vMotion, and storage connectivity
    as they can all be easily managed from the command line. However, in large VDI
    solutions leveraging multiple **Virtual Local Area Networks (VLANs)**, dozens
    or hundreds of physical hosts, dvSwitches help to greatly streamline the virtual
    network management across the virtual infrastructure.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 标准 vSwitch 不受 VMware vCenter Server 丧失的影响，最好用于像服务控制台、vMotion 和存储连接等功能，因为这些都可以通过命令行轻松管理。然而，在利用多个**虚拟局域网（VLAN）**的大型
    VDI 解决方案中，数十台或数百台物理主机的 dvSwitch 帮助大大简化了虚拟基础设施中的虚拟网络管理。
- en: VMware vSphere hosts keep a local cache of dvSwitch, dvPortGroup, and dvPort
    information to use when the VMware vCenter Server is unavailable. The local cache
    configuration copies are read-only and cannot be manipulated by the administrator.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: VMware vSphere 主机会在 VMware vCenter Server 无法使用时，保持一个 dvSwitch、dvPortGroup 和
    dvPort 信息的本地缓存。该本地缓存配置副本是只读的，管理员无法对其进行操作。
- en: Port binding
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端口绑定
- en: '**Port binding** is the process of assigning a specific port, also known as
    a **dvPort**, to a specific **network interface controller (NIC)** on a specific
    virtual machine. Think of this assignment as analogous to taking a patch cable
    and plugging one end into the NIC on a physical desktop and the other end into
    an available switch. dvPorts decide how a virtual machine''s network traffic is
    mapped to a specific distributed port group or **dvPortGroup.**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**端口绑定**是将特定端口（也称为**dvPort**）分配给特定虚拟机上的特定**网络接口控制器（NIC）**的过程。可以将这种分配类比为将一根补丁电缆的一端插入物理桌面上的
    NIC，另一端插入可用的交换机。dvPorts 决定了虚拟机的网络流量如何映射到特定的分布式端口组或**dvPortGroup**。'
- en: 'There are three types of port bindings used by dvSwitches; they are as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: dvSwitch 使用三种类型的端口绑定，具体如下：
- en: Static binding
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态绑定
- en: Dynamic binding
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态绑定
- en: Ephemeral binding
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 临时绑定
- en: Static binding
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 静态绑定
- en: Static binding assigns an available port on the dvPortGroup of the dvSwitch
    when a vNIC is added to a virtual machine. For example, if VM009 is a powered
    off Windows 2008 virtual machine and the administrator goes into Edit Settings
    and adds an NIC on dvPortGroup VLAN 71, a dvPort from the VLAN 71 dvPortGroup
    is assigned to the NIC, assuming one is available. It does not matter if the virtual
    machine VM009 is powered on or powered off, it is still assigned a dvPort and
    the dvPort will be unavailable to other virtual machines.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当向虚拟机添加 vNIC 时，静态绑定会在 dvSwitch 的 dvPortGroup 上分配一个可用端口。例如，如果 VM009 是已关机的 Windows
    2008 虚拟机，并且管理员进入“编辑设置”并在 dvPortGroup VLAN 71 上添加一个 NIC，则会分配 VLAN 71 dvPortGroup
    上的一个 dvPort 给该 NIC，前提是有可用的 dvPort。无论虚拟机 VM009 是否已开启，它仍会被分配一个 dvPort，并且该 dvPort
    对其他虚拟机不可用。
- en: The assigned dvPort is released only when the virtual machine has been removed
    from the dvPortGroup. Virtual machines using static binding can only be connected
    to a dvPortGroup through the vCenter Server.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 仅当虚拟机已从 dvPortGroup 中移除时，分配的 dvPort 才会释放。只能通过 vCenter Server 将使用静态绑定的虚拟机连接到
    dvPortGroup。
- en: '**Advantage:** The advantage of static binding is that a virtual machine can
    be powered on even if the vCenter Server is unavailable. In addition, network
    statistics are maintained after a vMotion event and after a power cycle of the
    virtual machine.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**优势：** 静态绑定的优势在于，即使 vCenter Server 不可用，虚拟机也可以启动。此外，在 vMotion 事件和虚拟机电源周期后仍然保留网络统计信息。'
- en: '**Disadvantage:** The disadvantage of static binding is that the dvPortGroup
    cannot be overcommitted. In volatile VDI using non-persistent desktops that are
    deleted at the time of logoff, it is possible that the solution could run out
    of available dvPorts on the dvPortGroup. Static binding is strongly discouraged
    in environments leveraging VMware View Composer.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**劣势：** 静态绑定的劣势在于，dvPortGroup 无法过度使用。在使用非持久桌面的易失性 VDI 环境中，这可能会导致 dvPortGroup
    上的可用 dvPort 不足。在利用 VMware View Composer 的环境中，强烈不建议使用静态绑定。'
- en: Dynamic binding
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态绑定
- en: Dynamic binding assigns an available dvPort on the dvPortGroup when a virtual
    machine is powered on and its NIC is in the connected state. For example, if VM009
    is a Windows 2008 virtual machine and the administrator goes into Edit Settings
    and adds an NIC on dvPortGroup VLAN 71, a dvPort from VLAN 71 dvPortGroup is not
    yet assigned. Once virtual machine VM009 is powered on, it is assigned a dvPort
    on the dvPortGroup and that specific dvPort will be unavailable to other virtual
    machines.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟机开启并且其 NIC 处于连接状态时，动态绑定会在 dvPortGroup 上分配一个可用的 dvPort。例如，如果 VM009 是 Windows
    2008 虚拟机，并且管理员进入“编辑设置”并在 dvPortGroup VLAN 71 上添加一个 NIC，则 VLAN 71 dvPortGroup 上的
    dvPort 尚未分配。一旦虚拟机 VM009 开启，它将在 dvPortGroup 上分配一个 dvPort，该特定 dvPort 将对其他虚拟机不可用。
- en: The assigned dvPort is released when the virtual machine has been powered down
    or the NIC is in the disconnected state. Virtual machines using dynamic binding
    can only be connected to a dvPortGroup through the vCenter Server.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟机已经关机或者 NIC 处于断开状态时，分配的 dvPort 会被释放。只能通过 vCenter Server 将使用动态绑定的虚拟机连接到 dvPortGroup。
- en: Dynamic binding is useful in environments where there are more virtual machines
    than available dvPorts on a given dvPortGroup; however, the number of powered
    on virtual machines will not exceed the number of available dvPorts on a given
    dvPortGroup.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 动态绑定适用于虚拟机数量超过给定 dvPortGroup 上可用 dvPort 的环境；但是，已开启的虚拟机数量不会超过给定 dvPortGroup 上的可用
    dvPort 数量。
- en: '**Advantage:** The advantage of dynamic binding is that as a virtual machine
    doesn''t occupy a dvPort until it is powered on, it is possible to overcommit
    the port on a given dvPortGroup. In addition, network statistics are maintained
    after a vMotion event.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**优势：** 动态绑定的优势在于，由于虚拟机在开启之前不占用 dvPort，因此可以在给定的 dvPortGroup 上过度使用端口。此外，在 vMotion
    事件后仍然保留网络统计信息。'
- en: '**Disadvantage:** The disadvantage of dynamic binding is that as a virtual
    machine isn''t assigned a dvPort until it is powered on, it must be powered on
    by the vCenter Server. Therefore, if the vCenter Server is unavailable, the virtual
    machine will not be able to be powered on. Network statistics are not maintained
    after the power cycle of a virtual machine as the dvPort is assigned at the time
    of boot.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点：** 动态绑定的缺点在于，虚拟机直到启动后才会分配dvPort，因此它必须由vCenter Server启动。如果vCenter Server不可用，虚拟机将无法启动。由于dvPort是在启动时分配的，因此在虚拟机电源循环后，网络统计信息不会被保留。'
- en: Ephemeral binding
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 短暂绑定
- en: Ephemeral binding creates and assigns a dvPort on the dvPortGroup when a virtual
    machine is powered on and its NIC is in the connected state. For example, if VM009
    is a Windows 2008 virtual machine and the administrator goes into Edit Settings
    and adds an NIC on dvPortGroup VLAN 71, a dvPort from VLAN71 dvPortGroup is not
    yet assigned. Once virtual machine VM009 is powered on, a dvPort is first created
    and then it is assigned a dvPort on the dvPortGroup and that specific dvPort will
    be unavailable to other virtual machines.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 短暂绑定在虚拟机启动并且其NIC处于连接状态时，会在dvPortGroup上创建并分配一个dvPort。例如，如果VM009是一个Windows 2008虚拟机，管理员进入编辑设置并添加一个在dvPortGroup
    VLAN 71上的NIC，那么VLAN71 dvPortGroup尚未分配dvPort。一旦虚拟机VM009启动，首先会创建一个dvPort，并将其分配到dvPortGroup上，这个特定的dvPort将无法被其他虚拟机使用。
- en: The assigned dvPort is released when the virtual machine has been powered down
    or the NIC is in the disconnected state. Virtual machines using ephemeral binding
    can be connected to a dvPortGroup through the vCenter Server or from ESX/ESXi.
    Therefore, if the vCenter Server is unavailable, the virtual machine network connections
    can still be managed.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦虚拟机关闭或其NIC处于断开连接状态，分配的dvPort将被释放。使用短暂绑定的虚拟机可以通过vCenter Server或从ESX/ESXi连接到dvPortGroup。因此，即使vCenter
    Server不可用，虚拟机的网络连接仍然可以被管理。
- en: When a virtual machine is vMotion'd, the original dvPort is deleted from the
    source dvPortGroup and a new dvPort is created on the destination dvPortGroup.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟机进行vMotion时，原有的dvPort会从源dvPortGroup中删除，并且在目标dvPortGroup上创建一个新的dvPort。
- en: Ephemeral binding is useful in environments of high volatility, for example,
    a non-persistent VDI solution, where virtual machines are created and deleted
    often. The number of ports on a dvPortGroup is defined and limited by the number
    of ports available of the dvSwitch.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 短暂绑定在高波动性的环境中非常有用，例如非持久的VDI解决方案，在这些环境中，虚拟机经常被创建和删除。dvPortGroup上的端口数量由dvSwitch上可用的端口数量定义和限制。
- en: '**Advantage:** The advantage of ephemeral binding is that as a virtual machine
    doesn''t occupy a dvPort until it is powered on, it is possible to overcommit
    the port on a given dvPortGroup.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点：** 短暂绑定的优点在于，虚拟机在启动之前不会占用dvPort，因此可以在给定的dvPortGroup上进行端口过度分配。'
- en: '**Disadvantage:** Network statistics are not maintained after the power cycle
    of a virtual machine or after a vMotion event as the dvPort is created and assigned
    at the time of boot or vMotion.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点：** 网络统计信息在虚拟机电源循环后或vMotion事件发生后不会被保留，因为dvPort是在启动时或vMotion时创建并分配的。'
- en: Port binding and VMware View Composer
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端口绑定与VMware View Composer
- en: For VDI solutions leveraging View Composer, it is important to recognize that
    tasks such as Recompose, Rebalance, and Refresh will attempt to use the same port
    that has been assigned to the replica image.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于利用View Composer的VDI解决方案，重要的是要认识到，诸如重组（Recompose）、负载均衡（Rebalance）和刷新（Refresh）等任务会尝试使用已经分配给副本镜像的端口。
- en: Therefore, it is recommended to use dynamic or ephemeral (preferred) binding
    if VMware View Composer will be leveraged.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果要使用VMware View Composer，建议使用动态绑定或短暂绑定（推荐）。
- en: Compute considerations
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算考虑事项
- en: 'Compute is typically not an area of failure in most VDI projects, but it is
    still important to understand the computing requirements of an organization before
    implementing a final design. Programs that can cause unforeseen failure from a
    compute perspective are as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数VDI项目中，计算通常不是失败的领域，但在实施最终设计之前，了解组织的计算需求仍然非常重要。可能会导致计算方面预料之外故障的程序如下：
- en: Dragon Medical/Dragon Naturally Speaking
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dragon Medical/Dragon Naturally Speaking
- en: Defense Connect Online
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Defense Connect Online
- en: AutoCAD
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoCAD
- en: Eclipse IDE
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eclipse IDE
- en: For VDI solutions that will be based on Windows XP, one vCPU can likely be used
    to address most basic computing needs. However, for VDI solutions leveraging Windows
    Vista, or more importantly Windows 7, two vCPUs may be necessary to ensure a favorable
    end user experience.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: For the most accurate calculation of CPU requirements, a proper assessment of
    the environment should be performed. This will help identify potential pitfalls
    such as some of the applications listed previously, prior to rollout.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: While both AMD and Intel-based x86 servers will suffice for VDI solutions, in
    large-scale and/or demanding environments, Intel-based solutions have consistently
    outperformed their AMD counterparts from a density (number of vDesktops per core)
    perspective.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: In VDI solutions, there is also the potential for unnecessary CPU load due to
    tasks such as antivirus scanning, poorly-tuned applications, single-threaded processes,
    added visual effects, and impacts from video or audio processing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Compute considerations](img/1124EN_06_14.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram illustrates a single processor with 6 cores. As a safe
    baseline, 10 vDesktops per core is used for design purposes. For basic task workers,
    this number could be significantly higher, and there are multiple reference architectures
    that validate 15 to 18 vDesktops per core. The use of the Teradici APEX offload
    card could also increase users per core density.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Continuing to use 10 vDesktops per core as a baseline, and assuming that the
    server has 2 processors (of 6 cores each) that nets a total of 12 cores per physical
    server. With 12 cores per server and 10 users per core, that yields 120 users
    per physical server (6 cores per processor * 2 processors per server * 10 users
    per core). Using 1.5 GB of RAM for each vDesktop (the minimum recommendation for
    64-bit Windows 7), the same physical server needs 180 GB of RAM (1.5 GB * 120
    users). That's a relative sweet spot for memory, as most servers are configurable
    with 256 GB of RAM from the factory.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'The following two tables have been extracted from the *Configuration Maximums,
    VMware vSphere 5.0* guide at [http://www.vmware.com/pdf/vsphere5/r50/vsphere-50-configuration-maximums.pdf](http://www.vmware.com/pdf/vsphere5/r50/vsphere-50-configuration-maximums.pdf).
    The tables explain compute maximums:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '| Host CPU maximums | Maximum |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| Logical CPUs per host | 160 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| Virtual machine maximums | Maximum |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| Virtual machines per host | 512 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| Virtual CPUs per host | 2048 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| Virtual CPUs per core | 25 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: Given the preceding information, we know that selected processors with significantly
    more cores per processor (for example, 24 cores per processor or 32 cores per
    processor) will not help vDesktop density on a given physical server.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table explains about memory maximums:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '| Host memory maximums | Maximum |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| RAM per host | 2 TB |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| Maximum RAM allocated to service console | 800 MB |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| Minimum RAM allocated to service console | 272 MB |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: The reason why increased density will not be realized (or more accurately, maximized),
    is partly due to memory limitations and also due to existing VMware vSphere limitations.
    Let's assume, for the sake of argument, that a 32-core physical server was selected
    as the standard for a given VDI solution and it was shipped with a maximum supported
    2 TB of RAM.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Using the conservative baseline of 10 vDesktops per core, that would yield 320
    vDesktops per host, requiring 640 GB of RAM.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table explains about cluster maximums:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '| Cluster maximums | Maximum |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| Hosts per cluster | 32 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| Virtual machines per cluster | 3,000 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| Virtual machines per host | 512 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: Comparing 320 vDesktops per host with the cluster maximums as defined by VMware,
    the maximum number of virtual machines per host would be reached.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Furthering the analysis from *Configuration Maximums, VMware vSphere* guide
    describes, "If more than one configuration option (such as, number of virtual
    machines, number of LUNs, number of vDS ports, and so on) are used at their maximum
    limit, some of the processes running on the host might run out of memory." Therefore,
    it is advised to avoid reaching the configuration maximums when possible.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: As with all portions of a VDI design, it is important to leverage real-world
    metrics, when possible, to understand how vDesktops will be used, and how they
    will impact the underlying physical infrastructure.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Given the preceding calculations, it is advisable to conserve capital expenditure
    on high core count processors and instead focus the funding elsewhere. In most
    environments, six, eight, or twelve core processors will be more than sufficient
    in terms of performance as well as ensuring that vSphere maximums are not reached.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Working with VMware vSphere maximums
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A strong case can be made that while VMware vSphere is by far the industry-leading
    hypervisor platform for server virtualization, its current maximums could be limiting
    in terms of mega-scale VDI environments. The following is a list of vCenter maximums
    taken from the *Configuration Maximums, VMware vSphere* guide that are most relevant
    to a VMware View solution:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '| vCenter Server scalability | Maximum |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| Hosts per vCenter Server | 1,000 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| Powered on virtual machines per vCenter Server | 10,000 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| Registered virtual machines per vCenter Server | 15,000 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| Linked vCenter Servers (pod) | 10 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| Hosts in linked vCenter Servers | 3,000 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| Powered on virtual machines in linked vCenter Servers | 30,000 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| Registered virtual machines in linked vCenter Servers | 50,000 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: The preceding limitations will be analyzed with a solution example in the next
    section.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Solution example — 25,000 seats of VMware View
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A VDI architect has been hired by Company, Inc. to design a solution for 25,000
    task workers in a single building. In this scenario, the networking and storage
    will be provided and will meet the necessary requirements of the VDI solution;
    therefore, the focus is on the physical server specification and the logical design
    of the VMware vSphere and VMware View environments.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Company, Inc. is looking for the following information:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '**Bill of materials (BOM)** for physical servers'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical design of the vSphere infrastructure
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical design of the View infrastructure
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With a quick look at the requirements, the architect has determined the following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Powered on virtual machines per vCenter Server will be exceeded (limit 10,000)
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registered virtual machines per vCenter Server will be exceeded (limit 15,000)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powered on virtual machines in linked vCenter Servers will not be exceeded (limit
    30,000)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registered virtual machines in linked vCenter Servers will not be exceeded (limit
    50,000)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum hosts per vCenter Server will not be exceeded (limit 1,000)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum virtual machines per host will not be exceeded (limit 320)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution design — physical server requirements
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To support 25,000 task workers running Windows 7 vDesktops, the physical server
    sizing must be determined. Through initial testing, 10 vDesktops per core was
    a conservative estimate. As 4-core processors are being phased out, 6-core processors
    were chosen for their price and availability. Therefore, with 2 6-core processors
    per physical host, that yields 12 cores per host. Using 10 vDesktops per core
    and 12 cores per host yields 120 vDesktops per host. With 1.5 GB per vDesktop
    used for the environment, 180 GB of RAM is required for vDesktops. By allocating
    the maximum supported, 800 MB of RAM to the service console, that yields 181 GB
    of RAM required. Therefore, a server with 192 GB of RAM will support the environment
    nicely. In addition, the following vNetwork maximums exist:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '| vNetwork Standard & Distributed Switch | Maximum |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| Total virtual network ports per host | 4,096 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| Maximum active ports per host | 1,016 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| Distributed switches per vCenter | 32 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: 'Given the preceding maximums, the following physical host design was leveraged:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '| Description | Value |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| Cores per processor | 6 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| Processors per host | 2 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| NICs per host | 8 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| Memory per host (GB) | 192 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| Approximate vDesktops per core | 10 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| Approximate vDesktops per host | 120 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| Standard vSwitches | 2 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| Distributed vSwitches | 1 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: 'The networking configuration is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![Solution design — physical server requirements](img/1124EN_06_15.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram represents two vNetwork standard switches and one vNetwork
    distributed switch. The first standard vSwitch, vS1, is used for service console
    and vMotion.The second standard vSwitch, vS2, is used for network-based storage.
    The only distributed vSwitch, vD1, is used for virtual machines.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Solution design — the pod concept
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concept of the pod is to give architects a method of creating building blocks
    to ease the design scalability for large environments. It also provides a conceptual
    framework for the solution architecture.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![Solution design — the pod concept](img/1124EN_06_16.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: 'The main components of a VMware View pod are as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '**Physical network:** This includes necessary switches, VLANs, network policies,
    and other network infrastructure required to support the VDI'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vCenter blocks:** This includes hosts, vCenter cluster design, vCenter Linked
    Mode configuration, and so on'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**View Connection Server pools:** This includes View Connection Servers and
    (if applicable) View Security Servers'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This concept of a pod can be carried through with the following architecture
    types:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Traditional
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional in modular form
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converged virtualization appliances
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **traditional** architecture type involves using servers (rackmount or
    blade), network switches, storage network switches (if applicable), and storage
    arrays. A traditional architecture approach is normally sufficient for an initial
    build-out but may not offer the scale-out capabilities of other approaches. The
    following diagram shows an illustration of a typical traditional architecture
    approach where disproportionate resources exist:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![Solution design — the pod concept](img/1124EN_06_17.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: For example, using the preceding diagram, sufficient compute, network, and storage
    resources may exist for the initial rollout of 400 VMware View users. In this
    example, an overabundance of storage capacity exists.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an illustration of a typical traditional architecture
    scale-out challenge:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![Solution design — the pod concept](img/1124EN_06_18.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: When the organization decides to add an additional 500 VMware View users, it
    runs into a problem. In the phase 1 rollout, an overabundance of storage capacity
    existed. However, to add capacity in a modular fashion, compute and network will
    still require an additional block of storage. Therefore, every addition will have
    some level of excess, which drives the price per vDesktop up due to architectural
    inefficiencies.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: An organization likely would not want to accept these inefficiencies so it would
    redesign its requirements every step of the way. Designing the scale out for every
    additional phase of a VDI solution also drives up cost through added complexity
    and man hours.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: In addition, every time a scale-out phase is re-architected, the chance of error
    becomes greater.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: The **traditional in modular form** architecture type involves using servers
    (rackmount or blade), network switches, storage network switches (if applicable),
    and storage arrays. Whereas, a traditional architecture is normally not able to
    scale proportionately, a traditional in modular form is designed to scale in building
    blocks. This approach does not need re-engineering for each scale-out phase, and
    instead an organization relies on the traditional yet modular architecture for
    predictable scale-out design.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an illustration of a typical traditional in modular
    form architecture approach, where proportionate resources exist:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![Solution design — the pod concept](img/1124EN_06_19.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: There are typically two ways to implement a traditional in modular form architecture.
    The first is by spending the time to architect and test a customer design, where
    compute (for example, Dell blade) is combined with network switches (for example,
    Cisco) and a storage array (for example, NetApp). The danger with this approach
    is that if the person or team designing the solution has never designed a VDI
    solution before, they are likely to have a few lessons learned through the process
    that will yield a less than optimal solution. This is not to say that this approach
    is not suitable and should not be taken, but special considerations should be
    taken to ensure the architecture is sound and scalable. A seasoned VDI architect
    can take any off-the-shelf hardware and build a sustainable VDI architecture.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The second way to implement a traditional in modular form architecture is by
    implementing a branded solution such as the VCE Vblock (Cisco servers + Cisco
    switches + EMC storage) or FlexPod (Cisco servers + Cisco switches + NetApp storage),
    for example. These solutions are proven, scalable in a predictive manner, and
    they offer a known architecture for VDI. The drawback of these solutions is that
    they often have a high barrier to entry in terms of cost and scale out in large
    modular blocks (for example, 1,000 users at a time).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: The third type of architecture uses **converged virtualization appliances**.
    Converged virtualization appliances are typically 2U to 6U appliances that comprise
    of one to many ESXi servers with local storage that is often shared among the
    ESXi servers in the appliance. The storage is typically shared through a virtual
    storage appliance model, where local storage is represented as either iSCSI or
    NFS storage to one or more ESXi servers in the appliance. The converged virtualization
    appliance model is relatively new to the VDI market.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Linked vCenter Servers
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As the number of virtual machines per vCenter Server will be exceeded, more
    than one vCenter Server will be required for this solution.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table illustrates the vCenter maximums:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '| vCenter Server scalability | Maximum |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| Powered on virtual machines per vCenter Server | 10,000 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| Registered virtual machines per vCenter Server | 15,000 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| Linked vCenter Servers | 10 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| Powered on virtual machines in linked vCenter Servers | 30,000 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| Registered virtual machines in linked vCenter Servers | 50,000 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: 'vCenter Linked Mode has a few basic prerequisites. They are as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Both vCenter Servers must reside in a functional DNS environment, where **fully
    qualified domain names (FQDNs)** of each vCenter Server can be resolved properly
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any vCenter Server participating in Linked Mode must reside in an Active Directory
    domain
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the vCenter Servers are in separate Active Directory domains, the respective
    domains must have a two-way trust
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both vCenter Servers must reside in a functional **Network Time Protocol (NTP)**
    environment, where time synchronization of the vCenter Servers is no more than
    5 minutes adrift of one another
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windows RPC port mapper must be allowed to open the **Remote Procedure Call
    (RPC)** ports for replication; this is covered in detail at [http://support.microsoft.com/kb/154596](http://support.microsoft.com/kb/154596)
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both VMware vCenter Servers have the VMware vCenter Standard Edition license
    (versus foundation, for example)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separate databases for each VMware vCenter Server
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Linked vCenter Servers](img/1124EN_06_20.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: VMware vCenter Linked Mode connects two or more vCenter Servers together via
    ADAM database replication to store information regarding user roles as well as
    VMware licensing. VMware vCenter Linked Mode does not do any form of database
    replication. If VMware vCenter Linked Mode would fail for any reason, the two
    (or more) vCenter Servers would still be viable as standalone instances.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Linked vCenter Servers](img/1124EN_06_21.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram, where there are two separate vCenter Server
    instances (vCenter1 and vCenter2), the virtual data centers, clusters, resource
    pools, and virtual machines are unique to their respective instance of vCenter.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Joining multiple vCenters together with vCenter Linked Mode forms what is known
    as a pod. A pod can consist of up to 10 vCenter Servers in Linked Mode.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: vCenter Servers
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using calculations from preceding sections, this solution is expected to have
    approximately 120 vDesktops per host; this means that 209 physical hosts are needed
    to support the vDesktop portion of this solution (not taking into account a virtualized
    vCenter, database, and so on).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Due to the nature of the end user population, the time they log in, the conservative
    nature of the original assessment (for example, 10 vDesktops per core), it has
    been decided that there will be no HA requirements for the vSphere Servers supporting
    vDesktops.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: It has also been determined that the management infrastructure including the
    View Connection Servers, vCenter Servers, database server, and a few other components
    require three physical hosts. In order to provide a level of protection, it has
    been determined to use an *n* + 1 solution and utilize 4 physical hosts.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: It was determined previously that any given vCenter can have a maximum of 10,000
    powered on virtual machines at any given time. This solution will need to support
    more than 25,000 powered on virtual machines; therefore, this solution will require
    3 vCenter Servers.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: To balance the load across the vCenter Servers, the clusters have been as equitably
    divided as possible.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the naming conventions used for the clusters in this example are:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'vCenter Server: vc-{letter}, for example, vc-b'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clusters: cl-{letter of vCenter}-{number}, for example, cl-c-6'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The vCenter Servers are named as vc-a, vc-b, vc-c, respectively. Their details
    along with the diagrams are as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![vCenter Servers](img/1124EN_06_22.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram explains about vCenter Server **vc-a**. The following
    list gives the details about vc-a:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 9 clusters of 8 hosts each (cl-a-1, cl-a-2, ..., cl-a-9)
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total of 72 hosts
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total of 8,640 vDesktops (120 vDesktops per host multiplied by 72 hosts)
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![vCenter Servers](img/1124EN_06_23.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram explains about vCenter Server **vc-b**. The following
    list gives the details about vc-b:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 9 clusters of 8 hosts each (cl-b-1, cl-b-2, ..., cl-b-9)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total of 72 hosts
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total of 8,640 vDesktops (120 vDesktops per host multiplied by 72 hosts)
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![vCenter Servers](img/1124EN_06_24.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram explains about vCenter Server **vc-c**. The following
    list gives the details about vc-c:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 7 clusters each having 8 hosts
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 cluster of 5 hosts
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 cluster of 4 hosts
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 cluster of 4 hosts dedicated to management
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total of 69 hosts
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total of 7,800 vDesktops and approximately 30 vServers (View Connection Server,
    database server, vCenter server, and so on)
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'vCenter vc-c has a cluster (cl-c-10) dedicated for hosting the infrastructure
    virtual machines. These virtual machines include:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 3 VMware vCenter Servers (vc-a, vc-b, vc-c)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 15 View Connection Servers
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supporting infrastructure (if needed) such as database servers, Liquidware Labs
    TM, and so on![vCenter Servers](img/1124EN_06_25.jpg)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware Update Manager Servers
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: VMware Update Manager is a solution that automated the application of patches
    to both vSphere Servers and virtual machines. It's most often used to patch vSphere
    Servers in large environments as it handles the task of placing a host in maintenance
    mode, migrating virtual machines, patch application, reboots, and normalization
    with a minimal amount of user interaction.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: VMware Update Manager Servers can only be paired with one VMware vCenter Server
    instance at a time. Therefore, in this solution three VMware Update Manager Servers
    will be required (one per vCenter Server instance).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '![VMware Update Manager Servers](img/1124EN_06_26.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: VMware vCenter Server Heartbeat
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this section, we have added a note about VMware vCenter Server Heartbeat.
    In most VMware View solutions, one or more highly available VMware vCenter Servers
    are required. vCenter Server is of paramount importance because if vCenter is
    unavailable, the following problems would be faced:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: New vDesktops cannot be provisioned
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vDesktops cannot be recomposed, refreshed, or rebalanced
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vDesktops cannot be deleted from the View Admin console
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, vCenter Server Heartbeat is often an affordable insurance policy
    for the vCenter Servers in a VDI solution.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: As noted previously, VMware Update Manager can only be linked to one instance
    of the VMware vCenter Server. However, it's important to note that a pair of vCenter
    Servers joined by VMware vCenter Server Heartbeat is considered to be only one
    instance.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the solution does not require additional VMware Update Manager Servers
    just because VMware vCenter Server Heartbeat is being leveraged.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Solution design — pools
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we will cover View Connection Servers.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: View Connection Servers
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As illustrated next, the VMware View infrastructure introduces its own maximums
    in addition to those already imposed by the VMware vSphere infrastructure. The
    View Connection maximums are given in the following table:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '| Connection Servers per deployment | Maximum |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| 1 Connection Server supporting direct RDP or PCoIP | 2,000 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| 7 Connection Servers (5 hot + 2 spare) supporting direct RDP or PCoIP | 10,000
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| Maximum hosts in a cluster when not using View Composer | 32 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| Maximum hosts in a cluster when using View Composer | 8 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: If a solution like Unidesk TM was used in lieu of View Composer, the end design
    could support more hosts per cluster.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: For the solution example, whereby 25,000 vDesktops must be supported, it's important
    to understand how many end users will be logging in at any given time. A VMware
    View Connection Server can support 2,000 direct PCoIP connections at any given
    time. In this example, all 25,000 end users could potentially log in at the same
    time. Therefore, a minimum of 13 View Connection Servers are required (2,000 *
    13 = 26,000 simultaneous direct PCoIP connections supported).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '![View Connection Servers](img/1124EN_06_27.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: In order to provide a level of redundancy in case of a View Connection Server
    outage, it is advised to add in *n* + 2 (or more) solutions. For example, increasing
    the required number of View Connection Servers, that is, 13 to a total of 15 View
    Connection Servers, provides the ability to support a maximum of 30,000 simultaneous
    PCoIP connections. Therefore, even if two View Connection Servers fail, all 25,000
    users would be able to log in to the VDI without incident.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The 15 View Connection Servers should be placed behind a redundant load balancing
    solution and should be configured to check that the View Connection Server is
    online via a simple ping (if **Internet Control Message Protocol (ICMP)** is allowed)
    and HTTP GET on the View Connection Server's URL. The entire pool of View Connection
    Servers should be accessible by a single name, such as `view.customer.com`, whereby
    end users would use `https://view.customer.com` to access the View environment.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the HTTP GET to verify functionality of a View Connection Server,
    a server whose applicable services have stopped will not successfully reply to
    the GET command and, therefore, will be removed from the load balancing pool.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Solution design — the formulae
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are some formulae to calculate the minimum number of vCenter
    Servers, Connection Servers, and Pods:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of vCenter Servers = Number of Desktops / 10,000
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimum number of View Connection Servers = Number of Simultaneous Connections
    / 2,000
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimum number of vCenter Pods = Number of vCenter Servers / 10
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As detailed in this chapter, there are many design considerations to make such
    as DHCP lease time, the minimum number of vCenters, and the number of cores to
    buy in a server platform. For large environments of thousands of vDesktops, it
    may be easiest to start with the vSphere maximums and work down. For small environments
    or PoCs that don't require a massive virtual infrastructure, the concepts covered
    in this chapter are still relevant as a successful PoC can grow rapidly in adoption.
    Finally, the concept of a pod architecture, or a collection of vCenter Servers,
    is typically new to those familiar only with designing virtual server solutions
    on the VMware vSphere platform. They can take some time to understand the new
    concepts and working up against the vSphere and vCenter maximums.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
