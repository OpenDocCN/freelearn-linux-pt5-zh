- en: Chapter 6. Sizing the VDI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Sizing** is the process of determining how much horsepower any given component
    of the VDI solution requires, which is ideally based on metrics collected during
    the assessment phase. In most situations, the challenge will not be handling the
    average daily VDI workloads, but it will be handling the peaks. Peak loads in
    a VDI environment are often short in duration and may not be able to be mitigated
    through conventional techniques such as **VMware Distributed Resource Scheduler
    (DRS)** or manual vMotion balancing.'
  prefs: []
  type: TYPE_NORMAL
- en: The components discussed in earlier chapters of this book, for example, VMware
    View Connection Server, require minimal sizing considerations when compared to
    the hardware components that must be sized. The reason being that the software
    components are primarily performing relatively lightweight work and merely brokering
    connections or performing provisioning tasks, which likely aren't happening constantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the sizing layers of a VDI solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sizing the VDI](img/1124EN_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For example, having a properly sized and performing database infrastructure
    is important, as slow database response times can impact both View Composer tasks
    as well as tasks within vCenter. Also, it is important to ensure that the View
    Connection Server has adequate virtual or physical resources such as CPU and memory.
    However, the primary focus of this chapter is on sizing the physical components
    of the VDI.
  prefs: []
  type: TYPE_NORMAL
- en: 'To properly understand how to size a VDI, it''s important to gather proper
    metrics during the assessment phase, which was covered in [Chapter 2](ch02.html
    "Chapter 2. Solution Methodology"), *Solution Methodology*. Such metrics include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of concurrent users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User classes and number of vCPUs, memory, and so on, per user class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: USB redirection frequency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This chapter will focus on the following components from a sizing perspective,
    not necessarily from a redundancy perspective. This chapter is the *n* in *n*
    + 1\. These components include:'
  prefs: []
  type: TYPE_NORMAL
- en: VMware View Connection Server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware vCenter Server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Storage sizing is covered in [Chapter 8](ch08.html "Chapter 8. Sizing the Storage"),
    *Sizing the Storage*.
  prefs: []
  type: TYPE_NORMAL
- en: 'An improperly sized VDI could experience any of the following problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Slow logons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poor PCoIP performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inability to power on vDesktops due to reaching vCenter maximums
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inability to log in to the VDI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While understanding the networking connectivity between the end users and the
    VDI is fairly obvious in a remote scenario, where the end user is removed geographically
    (for example, working from home) from the VDI, it's less obvious in a local scenario.
    While a local scenario may not blatantly cause a VDI architect to think about
    network sizing, it is still imperative to analyze and size the network component
    of a VDI solution even when all components reside on a **Local Area Network (LAN)**.
    This is the only way to truly confirm that the end user's experience should be
    as positive as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Sizing the network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a general rule of thumb, a typical task worker requires approximately 250
    Kbps of network throughput for a positive end user experience. By generally accepted
    industry terms, a task worker is a user that has the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: He uses typical office applications or terminal windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He does not require multimedia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He does not require 3D graphics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He does not require bidirectional audio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, where a task worker can potentially generate significant network bandwidth
    is with the use of USB peripherals. If a task worker requires USB peripherals
    to perform his job, then it is imperative to perform a network analysis of the
    specific USB peripherals in action prior to full-scale implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of the consumables (Kbps) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: PCoIP baseline = 250 Kbps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCoIP burst headroom = 500 Kbps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimedia video = 1,024 Kbps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3D graphics = 10,240 Kbps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 480p video = 1,024 Kbps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 720p video = 4,096 Kbps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1080p video = 6,144 Kbps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectional audio = 500 Kbps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: USB peripherals = 500 Kbps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stereo audio = 500 Kbps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CD quality audio = 2,048 Kbps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The network checklist is given at [http://techsupport.teradici.com/ics/support/default.asp?deptID=15164](http://techsupport.teradici.com/ics/support/default.asp?deptID=15164).
    But before that, you will be required to create an account at this site: [http://techsupport.teradici.com](http://techsupport.teradici.com).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other weights are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Buffer = 80 percent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bandwidth offset = 105 percent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The minimum bandwidth to deliver acceptable performance is determined by the
    activity and requirements of the user''s session. Some baseline numbers for the
    minimum bandwidth needed for a respective user type are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Description | Kbps |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Office worker low without multimedia | 250 |'
  prefs: []
  type: TYPE_TB
- en: '| Office worker high without multimedia | 315 |'
  prefs: []
  type: TYPE_TB
- en: '| Office worker low with multimedia | 340 |'
  prefs: []
  type: TYPE_TB
- en: '| Office worker high with multimedia | 375 |'
  prefs: []
  type: TYPE_TB
- en: 'The following diagram is an illustration showing bandwidth provisioning of
    a given network connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sizing the network](img/1124EN_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In most environments, the only network traffic that should have a higher network
    priority than PCoIP is network traffic related to **Voice over IP (VoIP)** communications.
    Giving PCoIP a higher priority than VoIP could cause poor quality or loss of connections
    in certain environments with an improperly sized network. Therefore, it is recommended
    to give VoIP a higher priority than PCoIP (approximately up to 20 percent of the
    overall connection), give PCoIP traffic the second highest priority, and classify
    the remaining traffic appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Network connection characteristics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Teradici has made significant improvements in the ability of the PCoIP protocol
    to handle high-latency and/or low-bandwidth scenarios. Teradici's PCoIP protocol
    is a purpose-built protocol for delivering a native desktop experience. In order
    to deliver the best possible end user experience, PCoIP will consume as much bandwidth
    as is available at a given time, up to the point where it can deliver a favorable
    end user experience. PCoIP is dynamic in nature, and as the available bandwidth
    changes, so does the amount of bandwidth that PCoIP attempts to consume. PCoIP
    initially uses **Transmission Control Protocol (TCP)** to establish the connection
    and then uses **User Datagram Protocol (UDP)** to transmit the desktop experience.
  prefs: []
  type: TYPE_NORMAL
- en: PCoIP also has two primary settings that should be understood, the PCoIP maximum
    bandwidth and the PCoIP bandwidth floor.
  prefs: []
  type: TYPE_NORMAL
- en: The PCoIP maximum bandwidth is the maximum amount of bandwidth a given PCoIP
    session is allowed to consume. Configuring this setting can ensure that end users
    never exceed a certain amount of bandwidth themselves. In addition, properly configuring
    the PCoIP maximum bandwidth provides a sense of insurance in a solution. Without
    limiting consumptions per session (even if the maximum is configured to be very
    generous) it is possible to have a runaway PCoIP session consuming a disproportionate
    amount of the available bandwidth. This disproportionate consumption could negatively
    impact the other users sharing the same network connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of the bandwidth floor and the bandwidth
    maximums:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network connection characteristics](img/1124EN_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The PCoIP bandwidth floor is the minimum threshold of bandwidth that must be
    available for PCoIP to throttle the stream. Following is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: An organization has 500 task workers and is looking to understand how large
    a network pipe they need to provide for their VMware View solution. The VDI users
    only use basic office applications and require no other capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Average Bandwidth Consumption = (Total Users * 250 Kbps) + (Special Need * Bandwidth
    Penalty) * Bandwidth Offsite * Buffer
  prefs: []
  type: TYPE_NORMAL
- en: 'So, substituting the values given in the preceding example gives us the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: Average Bandwidth Consumption = (500 * 250 Kbps) + 0 * 80 percent = 100,000
    KBps (approximately 97 Mbps)
  prefs: []
  type: TYPE_NORMAL
- en: DHCP considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While it is possible to cobble together a VDI solution that uses static **Internet
    Protocol (IP)** addresses, it is highly not recommended. Due to the potential
    volatility of a VDI and for ease of management, **Dynamic Host Configuration Protocol
    (DHCP)** is the preferred method for managing issuing the IP addresses of the
    vDesktops. When using DHCP, vDesktops do not own a specific IP address, but rather
    it leases it from a DHCP server.
  prefs: []
  type: TYPE_NORMAL
- en: 'A single DHCP scope consists of a pool of IP addresses on a particular subnet.
    A DHCP superscope allows a DHCP server to distribute IP addresses from more than
    one scope to devices on a single physical network. Proper subnetting can ensure
    that enough IP leases exist in a particular scope to serve the number of end devices
    requiring IP addresses. The following diagram shows a DHCP workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![DHCP considerations](img/1124EN_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The workflow of a DHCP lease allocation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The client broadcasts a DHCPDISCOVER message on its physical subnet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Available DHCP servers on the subnet respond with an IP address by sending a
    DHCPOFFER packet back to the client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A client replies with a DHCPREQUEST message to signal which DHCP server he/she
    accepted the DHCPOFFER packet from. The other DHCP servers withdraw their offer
    for a DHCP lease.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DHCP server in the DHCPREQUEST message from the client replies with a DHCPACK
    packet to acknowledge the completion of the lease transaction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DHCP reallocation occurs when a client that already has an address within a
    valid lease expiration window reboots or starts up after being shut down. When
    it starts back up, it will contact the DHCP server previously confirmed via a
    DHCPACK packet to verify the lease and obtain any necessary parameters.
  prefs: []
  type: TYPE_NORMAL
- en: After a set period of time (T1) has elapsed since the original lease allocation,
    the client will attempt to renew the lease. If the client is unable to successfully
    renew the lease, it will enter the rebinding phase (starts at T2). During the
    rebinding phase, it will attempt to obtain a lease from any available DHCP server.
  prefs: []
  type: TYPE_NORMAL
- en: '![DHCP considerations](img/1124EN_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, **T[1]** is defined as 50 percent of the lease duration
    and **T[2]** is defined as 90 percent of the lease duration.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, assume a lease duration of 120 minutes (two hours).
  prefs: []
  type: TYPE_NORMAL
- en: A vDesktop boots and is successfully allocated a DHCP lease for a duration of
    120 minutes from DHCP_SERVER_01\. At T1 (50 percent of 120 minutes, that is, 60
    minutes), the vDesktop attempts to renew its lease from DHCP_SERVER_01\. During
    the renewal period, the vDesktop successfully renews its DHCP lease. The lease
    clock is now reset back to a full 120 minute lease since the renew was successful.
  prefs: []
  type: TYPE_NORMAL
- en: This time the vDesktop is unsuccessful during the renewal period and enters
    the rebinding period. The vDesktop successfully obtains a new DHCP lease from
    DHCP_SERVER_03 with a lease of a fresh 120 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: In most VDI scenarios, a DHCP lease time of one hour is sufficient. Typically,
    this is considerably less than the average DHCP lease time in default scopes used
    by most organizations.
  prefs: []
  type: TYPE_NORMAL
- en: If a desktop pool is set to delete a vDesktop after a user logs off, this could
    generate significant DHCP lease thrashing and a very short DHCP lease time should
    be considered (depending on the frequency of vDesktop deletions).
  prefs: []
  type: TYPE_NORMAL
- en: VMware View Composer tasks such as Recompose and Refresh should maintain the
    same MAC address throughout the process as the VMX settings related to the vNIC
    should not be altered. Therefore, the original lease would attempt to be reallocated
    during the boot process.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual switch considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Virtual switch design for VDI environments is another component that may prove
    challenging for those unfamiliar with large-scale virtual infrastructure, or those
    accustomed to designing solutions with potentially high virtual machine volatility.
  prefs: []
  type: TYPE_NORMAL
- en: '![Virtual switch considerations](img/1124EN_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows, at a high level, the network components of a VDI
    environment. Not shown is the abstraction (that would reside in-between the physical
    switch and the virtual switch) done by the hypervisor.
  prefs: []
  type: TYPE_NORMAL
- en: When a standard vSwitch is created it has, by default, 120 ports. This parameter
    is defined at a kernel (hypervisor) layer, and any changes to the number of ports
    in a standard switch requires a reboot of the physical host.
  prefs: []
  type: TYPE_NORMAL
- en: When a distributed vSwitch, also known as a **dvSwitch**, is created, it has,
    by default, 128 ports. This parameter can be changed dynamically and does not
    require a reboot of the physical host for changing the number of ports from its
    original value of 128.
  prefs: []
  type: TYPE_NORMAL
- en: Standard versus distributed switches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Standard vSwitches are not impacted by the loss of a VMware vCenter Server,
    and are best used by functions such as Service console, vMotion, and storage connectivity
    as they can all be easily managed from the command line. However, in large VDI
    solutions leveraging multiple **Virtual Local Area Networks (VLANs)**, dozens
    or hundreds of physical hosts, dvSwitches help to greatly streamline the virtual
    network management across the virtual infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: VMware vSphere hosts keep a local cache of dvSwitch, dvPortGroup, and dvPort
    information to use when the VMware vCenter Server is unavailable. The local cache
    configuration copies are read-only and cannot be manipulated by the administrator.
  prefs: []
  type: TYPE_NORMAL
- en: Port binding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Port binding** is the process of assigning a specific port, also known as
    a **dvPort**, to a specific **network interface controller (NIC)** on a specific
    virtual machine. Think of this assignment as analogous to taking a patch cable
    and plugging one end into the NIC on a physical desktop and the other end into
    an available switch. dvPorts decide how a virtual machine''s network traffic is
    mapped to a specific distributed port group or **dvPortGroup.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three types of port bindings used by dvSwitches; they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Static binding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic binding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ephemeral binding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static binding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Static binding assigns an available port on the dvPortGroup of the dvSwitch
    when a vNIC is added to a virtual machine. For example, if VM009 is a powered
    off Windows 2008 virtual machine and the administrator goes into Edit Settings
    and adds an NIC on dvPortGroup VLAN 71, a dvPort from the VLAN 71 dvPortGroup
    is assigned to the NIC, assuming one is available. It does not matter if the virtual
    machine VM009 is powered on or powered off, it is still assigned a dvPort and
    the dvPort will be unavailable to other virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: The assigned dvPort is released only when the virtual machine has been removed
    from the dvPortGroup. Virtual machines using static binding can only be connected
    to a dvPortGroup through the vCenter Server.
  prefs: []
  type: TYPE_NORMAL
- en: '**Advantage:** The advantage of static binding is that a virtual machine can
    be powered on even if the vCenter Server is unavailable. In addition, network
    statistics are maintained after a vMotion event and after a power cycle of the
    virtual machine.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Disadvantage:** The disadvantage of static binding is that the dvPortGroup
    cannot be overcommitted. In volatile VDI using non-persistent desktops that are
    deleted at the time of logoff, it is possible that the solution could run out
    of available dvPorts on the dvPortGroup. Static binding is strongly discouraged
    in environments leveraging VMware View Composer.'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic binding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Dynamic binding assigns an available dvPort on the dvPortGroup when a virtual
    machine is powered on and its NIC is in the connected state. For example, if VM009
    is a Windows 2008 virtual machine and the administrator goes into Edit Settings
    and adds an NIC on dvPortGroup VLAN 71, a dvPort from VLAN 71 dvPortGroup is not
    yet assigned. Once virtual machine VM009 is powered on, it is assigned a dvPort
    on the dvPortGroup and that specific dvPort will be unavailable to other virtual
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: The assigned dvPort is released when the virtual machine has been powered down
    or the NIC is in the disconnected state. Virtual machines using dynamic binding
    can only be connected to a dvPortGroup through the vCenter Server.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic binding is useful in environments where there are more virtual machines
    than available dvPorts on a given dvPortGroup; however, the number of powered
    on virtual machines will not exceed the number of available dvPorts on a given
    dvPortGroup.
  prefs: []
  type: TYPE_NORMAL
- en: '**Advantage:** The advantage of dynamic binding is that as a virtual machine
    doesn''t occupy a dvPort until it is powered on, it is possible to overcommit
    the port on a given dvPortGroup. In addition, network statistics are maintained
    after a vMotion event.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Disadvantage:** The disadvantage of dynamic binding is that as a virtual
    machine isn''t assigned a dvPort until it is powered on, it must be powered on
    by the vCenter Server. Therefore, if the vCenter Server is unavailable, the virtual
    machine will not be able to be powered on. Network statistics are not maintained
    after the power cycle of a virtual machine as the dvPort is assigned at the time
    of boot.'
  prefs: []
  type: TYPE_NORMAL
- en: Ephemeral binding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ephemeral binding creates and assigns a dvPort on the dvPortGroup when a virtual
    machine is powered on and its NIC is in the connected state. For example, if VM009
    is a Windows 2008 virtual machine and the administrator goes into Edit Settings
    and adds an NIC on dvPortGroup VLAN 71, a dvPort from VLAN71 dvPortGroup is not
    yet assigned. Once virtual machine VM009 is powered on, a dvPort is first created
    and then it is assigned a dvPort on the dvPortGroup and that specific dvPort will
    be unavailable to other virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: The assigned dvPort is released when the virtual machine has been powered down
    or the NIC is in the disconnected state. Virtual machines using ephemeral binding
    can be connected to a dvPortGroup through the vCenter Server or from ESX/ESXi.
    Therefore, if the vCenter Server is unavailable, the virtual machine network connections
    can still be managed.
  prefs: []
  type: TYPE_NORMAL
- en: When a virtual machine is vMotion'd, the original dvPort is deleted from the
    source dvPortGroup and a new dvPort is created on the destination dvPortGroup.
  prefs: []
  type: TYPE_NORMAL
- en: Ephemeral binding is useful in environments of high volatility, for example,
    a non-persistent VDI solution, where virtual machines are created and deleted
    often. The number of ports on a dvPortGroup is defined and limited by the number
    of ports available of the dvSwitch.
  prefs: []
  type: TYPE_NORMAL
- en: '**Advantage:** The advantage of ephemeral binding is that as a virtual machine
    doesn''t occupy a dvPort until it is powered on, it is possible to overcommit
    the port on a given dvPortGroup.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Disadvantage:** Network statistics are not maintained after the power cycle
    of a virtual machine or after a vMotion event as the dvPort is created and assigned
    at the time of boot or vMotion.'
  prefs: []
  type: TYPE_NORMAL
- en: Port binding and VMware View Composer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For VDI solutions leveraging View Composer, it is important to recognize that
    tasks such as Recompose, Rebalance, and Refresh will attempt to use the same port
    that has been assigned to the replica image.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is recommended to use dynamic or ephemeral (preferred) binding
    if VMware View Composer will be leveraged.
  prefs: []
  type: TYPE_NORMAL
- en: Compute considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Compute is typically not an area of failure in most VDI projects, but it is
    still important to understand the computing requirements of an organization before
    implementing a final design. Programs that can cause unforeseen failure from a
    compute perspective are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Dragon Medical/Dragon Naturally Speaking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defense Connect Online
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoCAD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eclipse IDE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For VDI solutions that will be based on Windows XP, one vCPU can likely be used
    to address most basic computing needs. However, for VDI solutions leveraging Windows
    Vista, or more importantly Windows 7, two vCPUs may be necessary to ensure a favorable
    end user experience.
  prefs: []
  type: TYPE_NORMAL
- en: For the most accurate calculation of CPU requirements, a proper assessment of
    the environment should be performed. This will help identify potential pitfalls
    such as some of the applications listed previously, prior to rollout.
  prefs: []
  type: TYPE_NORMAL
- en: While both AMD and Intel-based x86 servers will suffice for VDI solutions, in
    large-scale and/or demanding environments, Intel-based solutions have consistently
    outperformed their AMD counterparts from a density (number of vDesktops per core)
    perspective.
  prefs: []
  type: TYPE_NORMAL
- en: In VDI solutions, there is also the potential for unnecessary CPU load due to
    tasks such as antivirus scanning, poorly-tuned applications, single-threaded processes,
    added visual effects, and impacts from video or audio processing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Compute considerations](img/1124EN_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram illustrates a single processor with 6 cores. As a safe
    baseline, 10 vDesktops per core is used for design purposes. For basic task workers,
    this number could be significantly higher, and there are multiple reference architectures
    that validate 15 to 18 vDesktops per core. The use of the Teradici APEX offload
    card could also increase users per core density.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing to use 10 vDesktops per core as a baseline, and assuming that the
    server has 2 processors (of 6 cores each) that nets a total of 12 cores per physical
    server. With 12 cores per server and 10 users per core, that yields 120 users
    per physical server (6 cores per processor * 2 processors per server * 10 users
    per core). Using 1.5 GB of RAM for each vDesktop (the minimum recommendation for
    64-bit Windows 7), the same physical server needs 180 GB of RAM (1.5 GB * 120
    users). That's a relative sweet spot for memory, as most servers are configurable
    with 256 GB of RAM from the factory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following two tables have been extracted from the *Configuration Maximums,
    VMware vSphere 5.0* guide at [http://www.vmware.com/pdf/vsphere5/r50/vsphere-50-configuration-maximums.pdf](http://www.vmware.com/pdf/vsphere5/r50/vsphere-50-configuration-maximums.pdf).
    The tables explain compute maximums:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Host CPU maximums | Maximum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Logical CPUs per host | 160 |'
  prefs: []
  type: TYPE_TB
- en: '| Virtual machine maximums | Maximum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Virtual machines per host | 512 |'
  prefs: []
  type: TYPE_TB
- en: '| Virtual CPUs per host | 2048 |'
  prefs: []
  type: TYPE_TB
- en: '| Virtual CPUs per core | 25 |'
  prefs: []
  type: TYPE_TB
- en: Given the preceding information, we know that selected processors with significantly
    more cores per processor (for example, 24 cores per processor or 32 cores per
    processor) will not help vDesktop density on a given physical server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table explains about memory maximums:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Host memory maximums | Maximum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RAM per host | 2 TB |'
  prefs: []
  type: TYPE_TB
- en: '| Maximum RAM allocated to service console | 800 MB |'
  prefs: []
  type: TYPE_TB
- en: '| Minimum RAM allocated to service console | 272 MB |'
  prefs: []
  type: TYPE_TB
- en: The reason why increased density will not be realized (or more accurately, maximized),
    is partly due to memory limitations and also due to existing VMware vSphere limitations.
    Let's assume, for the sake of argument, that a 32-core physical server was selected
    as the standard for a given VDI solution and it was shipped with a maximum supported
    2 TB of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Using the conservative baseline of 10 vDesktops per core, that would yield 320
    vDesktops per host, requiring 640 GB of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table explains about cluster maximums:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Cluster maximums | Maximum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Hosts per cluster | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Virtual machines per cluster | 3,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Virtual machines per host | 512 |'
  prefs: []
  type: TYPE_TB
- en: Comparing 320 vDesktops per host with the cluster maximums as defined by VMware,
    the maximum number of virtual machines per host would be reached.
  prefs: []
  type: TYPE_NORMAL
- en: Furthering the analysis from *Configuration Maximums, VMware vSphere* guide
    describes, "If more than one configuration option (such as, number of virtual
    machines, number of LUNs, number of vDS ports, and so on) are used at their maximum
    limit, some of the processes running on the host might run out of memory." Therefore,
    it is advised to avoid reaching the configuration maximums when possible.
  prefs: []
  type: TYPE_NORMAL
- en: As with all portions of a VDI design, it is important to leverage real-world
    metrics, when possible, to understand how vDesktops will be used, and how they
    will impact the underlying physical infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Given the preceding calculations, it is advisable to conserve capital expenditure
    on high core count processors and instead focus the funding elsewhere. In most
    environments, six, eight, or twelve core processors will be more than sufficient
    in terms of performance as well as ensuring that vSphere maximums are not reached.
  prefs: []
  type: TYPE_NORMAL
- en: Working with VMware vSphere maximums
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A strong case can be made that while VMware vSphere is by far the industry-leading
    hypervisor platform for server virtualization, its current maximums could be limiting
    in terms of mega-scale VDI environments. The following is a list of vCenter maximums
    taken from the *Configuration Maximums, VMware vSphere* guide that are most relevant
    to a VMware View solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '| vCenter Server scalability | Maximum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Hosts per vCenter Server | 1,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Powered on virtual machines per vCenter Server | 10,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Registered virtual machines per vCenter Server | 15,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Linked vCenter Servers (pod) | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Hosts in linked vCenter Servers | 3,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Powered on virtual machines in linked vCenter Servers | 30,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Registered virtual machines in linked vCenter Servers | 50,000 |'
  prefs: []
  type: TYPE_TB
- en: The preceding limitations will be analyzed with a solution example in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Solution example — 25,000 seats of VMware View
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A VDI architect has been hired by Company, Inc. to design a solution for 25,000
    task workers in a single building. In this scenario, the networking and storage
    will be provided and will meet the necessary requirements of the VDI solution;
    therefore, the focus is on the physical server specification and the logical design
    of the VMware vSphere and VMware View environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Company, Inc. is looking for the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bill of materials (BOM)** for physical servers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical design of the vSphere infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical design of the View infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With a quick look at the requirements, the architect has determined the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Powered on virtual machines per vCenter Server will be exceeded (limit 10,000)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registered virtual machines per vCenter Server will be exceeded (limit 15,000)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powered on virtual machines in linked vCenter Servers will not be exceeded (limit
    30,000)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registered virtual machines in linked vCenter Servers will not be exceeded (limit
    50,000)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum hosts per vCenter Server will not be exceeded (limit 1,000)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum virtual machines per host will not be exceeded (limit 320)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution design — physical server requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To support 25,000 task workers running Windows 7 vDesktops, the physical server
    sizing must be determined. Through initial testing, 10 vDesktops per core was
    a conservative estimate. As 4-core processors are being phased out, 6-core processors
    were chosen for their price and availability. Therefore, with 2 6-core processors
    per physical host, that yields 12 cores per host. Using 10 vDesktops per core
    and 12 cores per host yields 120 vDesktops per host. With 1.5 GB per vDesktop
    used for the environment, 180 GB of RAM is required for vDesktops. By allocating
    the maximum supported, 800 MB of RAM to the service console, that yields 181 GB
    of RAM required. Therefore, a server with 192 GB of RAM will support the environment
    nicely. In addition, the following vNetwork maximums exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '| vNetwork Standard & Distributed Switch | Maximum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Total virtual network ports per host | 4,096 |'
  prefs: []
  type: TYPE_TB
- en: '| Maximum active ports per host | 1,016 |'
  prefs: []
  type: TYPE_TB
- en: '| Distributed switches per vCenter | 32 |'
  prefs: []
  type: TYPE_TB
- en: 'Given the preceding maximums, the following physical host design was leveraged:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Description | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Cores per processor | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| Processors per host | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| NICs per host | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Memory per host (GB) | 192 |'
  prefs: []
  type: TYPE_TB
- en: '| Approximate vDesktops per core | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Approximate vDesktops per host | 120 |'
  prefs: []
  type: TYPE_TB
- en: '| Standard vSwitches | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Distributed vSwitches | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'The networking configuration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solution design — physical server requirements](img/1124EN_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram represents two vNetwork standard switches and one vNetwork
    distributed switch. The first standard vSwitch, vS1, is used for service console
    and vMotion.The second standard vSwitch, vS2, is used for network-based storage.
    The only distributed vSwitch, vD1, is used for virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: Solution design — the pod concept
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concept of the pod is to give architects a method of creating building blocks
    to ease the design scalability for large environments. It also provides a conceptual
    framework for the solution architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![Solution design — the pod concept](img/1124EN_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The main components of a VMware View pod are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Physical network:** This includes necessary switches, VLANs, network policies,
    and other network infrastructure required to support the VDI'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vCenter blocks:** This includes hosts, vCenter cluster design, vCenter Linked
    Mode configuration, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**View Connection Server pools:** This includes View Connection Servers and
    (if applicable) View Security Servers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This concept of a pod can be carried through with the following architecture
    types:'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional in modular form
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converged virtualization appliances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **traditional** architecture type involves using servers (rackmount or
    blade), network switches, storage network switches (if applicable), and storage
    arrays. A traditional architecture approach is normally sufficient for an initial
    build-out but may not offer the scale-out capabilities of other approaches. The
    following diagram shows an illustration of a typical traditional architecture
    approach where disproportionate resources exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solution design — the pod concept](img/1124EN_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For example, using the preceding diagram, sufficient compute, network, and storage
    resources may exist for the initial rollout of 400 VMware View users. In this
    example, an overabundance of storage capacity exists.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an illustration of a typical traditional architecture
    scale-out challenge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solution design — the pod concept](img/1124EN_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When the organization decides to add an additional 500 VMware View users, it
    runs into a problem. In the phase 1 rollout, an overabundance of storage capacity
    existed. However, to add capacity in a modular fashion, compute and network will
    still require an additional block of storage. Therefore, every addition will have
    some level of excess, which drives the price per vDesktop up due to architectural
    inefficiencies.
  prefs: []
  type: TYPE_NORMAL
- en: An organization likely would not want to accept these inefficiencies so it would
    redesign its requirements every step of the way. Designing the scale out for every
    additional phase of a VDI solution also drives up cost through added complexity
    and man hours.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, every time a scale-out phase is re-architected, the chance of error
    becomes greater.
  prefs: []
  type: TYPE_NORMAL
- en: The **traditional in modular form** architecture type involves using servers
    (rackmount or blade), network switches, storage network switches (if applicable),
    and storage arrays. Whereas, a traditional architecture is normally not able to
    scale proportionately, a traditional in modular form is designed to scale in building
    blocks. This approach does not need re-engineering for each scale-out phase, and
    instead an organization relies on the traditional yet modular architecture for
    predictable scale-out design.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an illustration of a typical traditional in modular
    form architecture approach, where proportionate resources exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solution design — the pod concept](img/1124EN_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are typically two ways to implement a traditional in modular form architecture.
    The first is by spending the time to architect and test a customer design, where
    compute (for example, Dell blade) is combined with network switches (for example,
    Cisco) and a storage array (for example, NetApp). The danger with this approach
    is that if the person or team designing the solution has never designed a VDI
    solution before, they are likely to have a few lessons learned through the process
    that will yield a less than optimal solution. This is not to say that this approach
    is not suitable and should not be taken, but special considerations should be
    taken to ensure the architecture is sound and scalable. A seasoned VDI architect
    can take any off-the-shelf hardware and build a sustainable VDI architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The second way to implement a traditional in modular form architecture is by
    implementing a branded solution such as the VCE Vblock (Cisco servers + Cisco
    switches + EMC storage) or FlexPod (Cisco servers + Cisco switches + NetApp storage),
    for example. These solutions are proven, scalable in a predictive manner, and
    they offer a known architecture for VDI. The drawback of these solutions is that
    they often have a high barrier to entry in terms of cost and scale out in large
    modular blocks (for example, 1,000 users at a time).
  prefs: []
  type: TYPE_NORMAL
- en: The third type of architecture uses **converged virtualization appliances**.
    Converged virtualization appliances are typically 2U to 6U appliances that comprise
    of one to many ESXi servers with local storage that is often shared among the
    ESXi servers in the appliance. The storage is typically shared through a virtual
    storage appliance model, where local storage is represented as either iSCSI or
    NFS storage to one or more ESXi servers in the appliance. The converged virtualization
    appliance model is relatively new to the VDI market.
  prefs: []
  type: TYPE_NORMAL
- en: Linked vCenter Servers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As the number of virtual machines per vCenter Server will be exceeded, more
    than one vCenter Server will be required for this solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table illustrates the vCenter maximums:'
  prefs: []
  type: TYPE_NORMAL
- en: '| vCenter Server scalability | Maximum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Powered on virtual machines per vCenter Server | 10,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Registered virtual machines per vCenter Server | 15,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Linked vCenter Servers | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Powered on virtual machines in linked vCenter Servers | 30,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Registered virtual machines in linked vCenter Servers | 50,000 |'
  prefs: []
  type: TYPE_TB
- en: 'vCenter Linked Mode has a few basic prerequisites. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Both vCenter Servers must reside in a functional DNS environment, where **fully
    qualified domain names (FQDNs)** of each vCenter Server can be resolved properly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any vCenter Server participating in Linked Mode must reside in an Active Directory
    domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the vCenter Servers are in separate Active Directory domains, the respective
    domains must have a two-way trust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both vCenter Servers must reside in a functional **Network Time Protocol (NTP)**
    environment, where time synchronization of the vCenter Servers is no more than
    5 minutes adrift of one another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windows RPC port mapper must be allowed to open the **Remote Procedure Call
    (RPC)** ports for replication; this is covered in detail at [http://support.microsoft.com/kb/154596](http://support.microsoft.com/kb/154596)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both VMware vCenter Servers have the VMware vCenter Standard Edition license
    (versus foundation, for example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separate databases for each VMware vCenter Server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Linked vCenter Servers](img/1124EN_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: VMware vCenter Linked Mode connects two or more vCenter Servers together via
    ADAM database replication to store information regarding user roles as well as
    VMware licensing. VMware vCenter Linked Mode does not do any form of database
    replication. If VMware vCenter Linked Mode would fail for any reason, the two
    (or more) vCenter Servers would still be viable as standalone instances.
  prefs: []
  type: TYPE_NORMAL
- en: '![Linked vCenter Servers](img/1124EN_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram, where there are two separate vCenter Server
    instances (vCenter1 and vCenter2), the virtual data centers, clusters, resource
    pools, and virtual machines are unique to their respective instance of vCenter.
  prefs: []
  type: TYPE_NORMAL
- en: Joining multiple vCenters together with vCenter Linked Mode forms what is known
    as a pod. A pod can consist of up to 10 vCenter Servers in Linked Mode.
  prefs: []
  type: TYPE_NORMAL
- en: vCenter Servers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using calculations from preceding sections, this solution is expected to have
    approximately 120 vDesktops per host; this means that 209 physical hosts are needed
    to support the vDesktop portion of this solution (not taking into account a virtualized
    vCenter, database, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Due to the nature of the end user population, the time they log in, the conservative
    nature of the original assessment (for example, 10 vDesktops per core), it has
    been decided that there will be no HA requirements for the vSphere Servers supporting
    vDesktops.
  prefs: []
  type: TYPE_NORMAL
- en: It has also been determined that the management infrastructure including the
    View Connection Servers, vCenter Servers, database server, and a few other components
    require three physical hosts. In order to provide a level of protection, it has
    been determined to use an *n* + 1 solution and utilize 4 physical hosts.
  prefs: []
  type: TYPE_NORMAL
- en: It was determined previously that any given vCenter can have a maximum of 10,000
    powered on virtual machines at any given time. This solution will need to support
    more than 25,000 powered on virtual machines; therefore, this solution will require
    3 vCenter Servers.
  prefs: []
  type: TYPE_NORMAL
- en: To balance the load across the vCenter Servers, the clusters have been as equitably
    divided as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the naming conventions used for the clusters in this example are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'vCenter Server: vc-{letter}, for example, vc-b'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clusters: cl-{letter of vCenter}-{number}, for example, cl-c-6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The vCenter Servers are named as vc-a, vc-b, vc-c, respectively. Their details
    along with the diagrams are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![vCenter Servers](img/1124EN_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram explains about vCenter Server **vc-a**. The following
    list gives the details about vc-a:'
  prefs: []
  type: TYPE_NORMAL
- en: 9 clusters of 8 hosts each (cl-a-1, cl-a-2, ..., cl-a-9)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total of 72 hosts
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total of 8,640 vDesktops (120 vDesktops per host multiplied by 72 hosts)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![vCenter Servers](img/1124EN_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram explains about vCenter Server **vc-b**. The following
    list gives the details about vc-b:'
  prefs: []
  type: TYPE_NORMAL
- en: 9 clusters of 8 hosts each (cl-b-1, cl-b-2, ..., cl-b-9)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total of 72 hosts
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total of 8,640 vDesktops (120 vDesktops per host multiplied by 72 hosts)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![vCenter Servers](img/1124EN_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram explains about vCenter Server **vc-c**. The following
    list gives the details about vc-c:'
  prefs: []
  type: TYPE_NORMAL
- en: 7 clusters each having 8 hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 cluster of 5 hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 cluster of 4 hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 cluster of 4 hosts dedicated to management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total of 69 hosts
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total of 7,800 vDesktops and approximately 30 vServers (View Connection Server,
    database server, vCenter server, and so on)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'vCenter vc-c has a cluster (cl-c-10) dedicated for hosting the infrastructure
    virtual machines. These virtual machines include:'
  prefs: []
  type: TYPE_NORMAL
- en: 3 VMware vCenter Servers (vc-a, vc-b, vc-c)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 15 View Connection Servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supporting infrastructure (if needed) such as database servers, Liquidware Labs
    TM, and so on![vCenter Servers](img/1124EN_06_25.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware Update Manager Servers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: VMware Update Manager is a solution that automated the application of patches
    to both vSphere Servers and virtual machines. It's most often used to patch vSphere
    Servers in large environments as it handles the task of placing a host in maintenance
    mode, migrating virtual machines, patch application, reboots, and normalization
    with a minimal amount of user interaction.
  prefs: []
  type: TYPE_NORMAL
- en: VMware Update Manager Servers can only be paired with one VMware vCenter Server
    instance at a time. Therefore, in this solution three VMware Update Manager Servers
    will be required (one per vCenter Server instance).
  prefs: []
  type: TYPE_NORMAL
- en: '![VMware Update Manager Servers](img/1124EN_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: VMware vCenter Server Heartbeat
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this section, we have added a note about VMware vCenter Server Heartbeat.
    In most VMware View solutions, one or more highly available VMware vCenter Servers
    are required. vCenter Server is of paramount importance because if vCenter is
    unavailable, the following problems would be faced:'
  prefs: []
  type: TYPE_NORMAL
- en: New vDesktops cannot be provisioned
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vDesktops cannot be recomposed, refreshed, or rebalanced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vDesktops cannot be deleted from the View Admin console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, vCenter Server Heartbeat is often an affordable insurance policy
    for the vCenter Servers in a VDI solution.
  prefs: []
  type: TYPE_NORMAL
- en: As noted previously, VMware Update Manager can only be linked to one instance
    of the VMware vCenter Server. However, it's important to note that a pair of vCenter
    Servers joined by VMware vCenter Server Heartbeat is considered to be only one
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the solution does not require additional VMware Update Manager Servers
    just because VMware vCenter Server Heartbeat is being leveraged.
  prefs: []
  type: TYPE_NORMAL
- en: Solution design — pools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we will cover View Connection Servers.
  prefs: []
  type: TYPE_NORMAL
- en: View Connection Servers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As illustrated next, the VMware View infrastructure introduces its own maximums
    in addition to those already imposed by the VMware vSphere infrastructure. The
    View Connection maximums are given in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Connection Servers per deployment | Maximum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 Connection Server supporting direct RDP or PCoIP | 2,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 Connection Servers (5 hot + 2 spare) supporting direct RDP or PCoIP | 10,000
    |'
  prefs: []
  type: TYPE_TB
- en: '| Maximum hosts in a cluster when not using View Composer | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Maximum hosts in a cluster when using View Composer | 8 |'
  prefs: []
  type: TYPE_TB
- en: If a solution like Unidesk TM was used in lieu of View Composer, the end design
    could support more hosts per cluster.
  prefs: []
  type: TYPE_NORMAL
- en: For the solution example, whereby 25,000 vDesktops must be supported, it's important
    to understand how many end users will be logging in at any given time. A VMware
    View Connection Server can support 2,000 direct PCoIP connections at any given
    time. In this example, all 25,000 end users could potentially log in at the same
    time. Therefore, a minimum of 13 View Connection Servers are required (2,000 *
    13 = 26,000 simultaneous direct PCoIP connections supported).
  prefs: []
  type: TYPE_NORMAL
- en: '![View Connection Servers](img/1124EN_06_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In order to provide a level of redundancy in case of a View Connection Server
    outage, it is advised to add in *n* + 2 (or more) solutions. For example, increasing
    the required number of View Connection Servers, that is, 13 to a total of 15 View
    Connection Servers, provides the ability to support a maximum of 30,000 simultaneous
    PCoIP connections. Therefore, even if two View Connection Servers fail, all 25,000
    users would be able to log in to the VDI without incident.
  prefs: []
  type: TYPE_NORMAL
- en: The 15 View Connection Servers should be placed behind a redundant load balancing
    solution and should be configured to check that the View Connection Server is
    online via a simple ping (if **Internet Control Message Protocol (ICMP)** is allowed)
    and HTTP GET on the View Connection Server's URL. The entire pool of View Connection
    Servers should be accessible by a single name, such as `view.customer.com`, whereby
    end users would use `https://view.customer.com` to access the View environment.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the HTTP GET to verify functionality of a View Connection Server,
    a server whose applicable services have stopped will not successfully reply to
    the GET command and, therefore, will be removed from the load balancing pool.
  prefs: []
  type: TYPE_NORMAL
- en: Solution design — the formulae
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are some formulae to calculate the minimum number of vCenter
    Servers, Connection Servers, and Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of vCenter Servers = Number of Desktops / 10,000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimum number of View Connection Servers = Number of Simultaneous Connections
    / 2,000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimum number of vCenter Pods = Number of vCenter Servers / 10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As detailed in this chapter, there are many design considerations to make such
    as DHCP lease time, the minimum number of vCenters, and the number of cores to
    buy in a server platform. For large environments of thousands of vDesktops, it
    may be easiest to start with the vSphere maximums and work down. For small environments
    or PoCs that don't require a massive virtual infrastructure, the concepts covered
    in this chapter are still relevant as a successful PoC can grow rapidly in adoption.
    Finally, the concept of a pod architecture, or a collection of vCenter Servers,
    is typically new to those familiar only with designing virtual server solutions
    on the VMware vSphere platform. They can take some time to understand the new
    concepts and working up against the vSphere and vCenter maximums.
  prefs: []
  type: TYPE_NORMAL
