- en: Chapter 6. Sizing the VDI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章：VDI 大小配置
- en: '**Sizing** is the process of determining how much horsepower any given component
    of the VDI solution requires, which is ideally based on metrics collected during
    the assessment phase. In most situations, the challenge will not be handling the
    average daily VDI workloads, but it will be handling the peaks. Peak loads in
    a VDI environment are often short in duration and may not be able to be mitigated
    through conventional techniques such as **VMware Distributed Resource Scheduler
    (DRS)** or manual vMotion balancing.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**大小配置**是确定 VDI 解决方案中每个组件所需计算能力的过程，理想情况下应基于评估阶段收集的度量数据。在大多数情况下，挑战不在于处理平均日常
    VDI 工作负载，而在于处理高峰负载。VDI 环境中的峰值负载通常持续时间较短，可能无法通过传统技术（如 **VMware 分布式资源调度器（DRS）**
    或手动 vMotion 平衡）来缓解。'
- en: The components discussed in earlier chapters of this book, for example, VMware
    View Connection Server, require minimal sizing considerations when compared to
    the hardware components that must be sized. The reason being that the software
    components are primarily performing relatively lightweight work and merely brokering
    connections or performing provisioning tasks, which likely aren't happening constantly.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书前几章讨论的组件，例如 VMware View 连接服务器，与必须进行配置大小调整的硬件组件相比，其大小调整的考虑因素较少。原因在于，软件组件主要执行相对轻量的工作，仅仅是代理连接或执行配置任务，这些任务可能不会持续进行。
- en: 'The following diagram shows the sizing layers of a VDI solution:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了 VDI 解决方案的大小配置层次：
- en: '![Sizing the VDI](img/1124EN_06_01.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![VDI 大小配置](img/1124EN_06_01.jpg)'
- en: For example, having a properly sized and performing database infrastructure
    is important, as slow database response times can impact both View Composer tasks
    as well as tasks within vCenter. Also, it is important to ensure that the View
    Connection Server has adequate virtual or physical resources such as CPU and memory.
    However, the primary focus of this chapter is on sizing the physical components
    of the VDI.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，拥有适当大小和性能的数据库基础设施非常重要，因为数据库响应时间缓慢可能会影响 View Composer 任务以及 vCenter 中的任务。此外，确保
    View 连接服务器具有足够的虚拟或物理资源，如 CPU 和内存，也非常重要。然而，本章的主要关注点是 VDI 物理组件的大小配置。
- en: 'To properly understand how to size a VDI, it''s important to gather proper
    metrics during the assessment phase, which was covered in [Chapter 2](ch02.html
    "Chapter 2. Solution Methodology"), *Solution Methodology*. Such metrics include
    the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要正确理解如何调整 VDI 的大小，重要的是在评估阶段收集正确的度量数据，这部分内容在[第 2 章](ch02.html "第 2 章. 解决方案方法论")，*解决方案方法论*
    中有详细介绍。此类度量数据包括以下内容：
- en: Number of concurrent users
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时在线的用户数量
- en: User classes and number of vCPUs, memory, and so on, per user class
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户类别以及每个用户类别所需的 vCPU 数量、内存等
- en: Network requirements
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络需求
- en: USB redirection frequency
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: USB 重定向频率
- en: 'This chapter will focus on the following components from a sizing perspective,
    not necessarily from a redundancy perspective. This chapter is the *n* in *n*
    + 1\. These components include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点讨论从大小调整角度看待的以下组件，而不一定从冗余角度进行分析。本章属于 *n* 中的 *n* + 1。具体组件包括：
- en: VMware View Connection Server
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware View 连接服务器
- en: VMware vCenter Server
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware vCenter 服务器
- en: Server hardware
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器硬件
- en: Network infrastructure
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络基础设施
- en: Note
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意事项
- en: Storage sizing is covered in [Chapter 8](ch08.html "Chapter 8. Sizing the Storage"),
    *Sizing the Storage*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 存储大小配置在[第 8 章](ch08.html "第 8 章. 存储大小配置")，*存储大小配置* 中有详细介绍。
- en: 'An improperly sized VDI could experience any of the following problems:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 配置不当的 VDI 可能会遇到以下问题：
- en: Slow logons
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 登录缓慢
- en: Poor PCoIP performance
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较差的 PCoIP 性能
- en: Inability to power on vDesktops due to reaching vCenter maximums
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法启动 vDesktops，因为达到了 vCenter 的最大限制
- en: Inability to log in to the VDI
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法登录 VDI
- en: Authentication errors
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认证错误
- en: Random failures
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机故障
- en: Network considerations
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络考虑事项
- en: While understanding the networking connectivity between the end users and the
    VDI is fairly obvious in a remote scenario, where the end user is removed geographically
    (for example, working from home) from the VDI, it's less obvious in a local scenario.
    While a local scenario may not blatantly cause a VDI architect to think about
    network sizing, it is still imperative to analyze and size the network component
    of a VDI solution even when all components reside on a **Local Area Network (LAN)**.
    This is the only way to truly confirm that the end user's experience should be
    as positive as possible.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在远程场景下，理解最终用户与VDI之间的网络连接相当显而易见（例如，用户在家工作，地理位置远离VDI），但在本地场景下则不太明显。虽然本地场景可能不会明显导致VDI架构师考虑网络规模问题，但即使所有组件都位于**局域网（LAN）**上，分析和调整VDI解决方案的网络组件仍然至关重要。这是确保最终用户体验尽可能积极的唯一途径。
- en: Sizing the network
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络规模调整
- en: 'As a general rule of thumb, a typical task worker requires approximately 250
    Kbps of network throughput for a positive end user experience. By generally accepted
    industry terms, a task worker is a user that has the following characteristics:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，一个典型的任务工作者需要大约250 Kbps的网络吞吐量才能获得良好的最终用户体验。根据行业普遍接受的术语，任务工作者是指具有以下特点的用户：
- en: He uses typical office applications or terminal windows
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他使用典型的办公应用程序或终端窗口
- en: He does not require multimedia
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他不需要多媒体
- en: He does not require 3D graphics
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他不需要3D图形
- en: He does not require bidirectional audio
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他不需要双向音频
- en: However, where a task worker can potentially generate significant network bandwidth
    is with the use of USB peripherals. If a task worker requires USB peripherals
    to perform his job, then it is imperative to perform a network analysis of the
    specific USB peripherals in action prior to full-scale implementation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，任务工作者在使用USB外设时，可能会产生显著的网络带宽需求。如果任务工作者需要USB外设来完成工作，那么在全面实施之前，必须对具体USB外设的使用进行网络分析。
- en: 'The list of the consumables (Kbps) is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是消耗品（Kbps）列表：
- en: PCoIP baseline = 250 Kbps
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCoIP基线 = 250 Kbps
- en: PCoIP burst headroom = 500 Kbps
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCoIP突发余量 = 500 Kbps
- en: Multimedia video = 1,024 Kbps
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多媒体视频 = 1,024 Kbps
- en: 3D graphics = 10,240 Kbps
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3D图形 = 10,240 Kbps
- en: 480p video = 1,024 Kbps
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 480p视频 = 1,024 Kbps
- en: 720p video = 4,096 Kbps
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 720p视频 = 4,096 Kbps
- en: 1080p video = 6,144 Kbps
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1080p视频 = 6,144 Kbps
- en: Bidirectional audio = 500 Kbps
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双向音频 = 500 Kbps
- en: USB peripherals = 500 Kbps
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: USB外设 = 500 Kbps
- en: Stereo audio = 500 Kbps
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 立体声音频 = 500 Kbps
- en: CD quality audio = 2,048 Kbps
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CD质量音频 = 2,048 Kbps
- en: 'The network checklist is given at [http://techsupport.teradici.com/ics/support/default.asp?deptID=15164](http://techsupport.teradici.com/ics/support/default.asp?deptID=15164).
    But before that, you will be required to create an account at this site: [http://techsupport.teradici.com](http://techsupport.teradici.com).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 网络清单请参考[http://techsupport.teradici.com/ics/support/default.asp?deptID=15164](http://techsupport.teradici.com/ics/support/default.asp?deptID=15164)。但在此之前，您需要在此网站创建一个账户：[http://techsupport.teradici.com](http://techsupport.teradici.com)。
- en: 'The other weights are as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其他权重如下：
- en: Buffer = 80 percent
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓冲区 = 80%
- en: Bandwidth offset = 105 percent
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带宽偏移 = 105%
- en: 'The minimum bandwidth to deliver acceptable performance is determined by the
    activity and requirements of the user''s session. Some baseline numbers for the
    minimum bandwidth needed for a respective user type are as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 提供可接受性能所需的最小带宽由用户会话的活动和要求决定。以下是不同用户类型所需最小带宽的一些基准数字：
- en: '| Description | Kbps |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | Kbps |'
- en: '| --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Office worker low without multimedia | 250 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 不带多媒体的低端办公人员 | 250 |'
- en: '| Office worker high without multimedia | 315 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 不带多媒体的高端办公人员 | 315 |'
- en: '| Office worker low with multimedia | 340 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 带多媒体的低端办公人员 | 340 |'
- en: '| Office worker high with multimedia | 375 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 带多媒体的高端办公人员 | 375 |'
- en: 'The following diagram is an illustration showing bandwidth provisioning of
    a given network connection:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了给定网络连接的带宽配置示意图：
- en: '![Sizing the network](img/1124EN_06_09.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![Sizing the network](img/1124EN_06_09.jpg)'
- en: In most environments, the only network traffic that should have a higher network
    priority than PCoIP is network traffic related to **Voice over IP (VoIP)** communications.
    Giving PCoIP a higher priority than VoIP could cause poor quality or loss of connections
    in certain environments with an improperly sized network. Therefore, it is recommended
    to give VoIP a higher priority than PCoIP (approximately up to 20 percent of the
    overall connection), give PCoIP traffic the second highest priority, and classify
    the remaining traffic appropriately.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数环境中，唯一应该比 PCoIP 拥有更高网络优先级的网络流量是与 **IP语音（VoIP）** 通信相关的流量。给 PCoIP 设置比 VoIP
    更高的优先级可能会导致某些网络不合适的环境中出现差的质量或连接丢失。因此，建议给 VoIP 流量分配更高的优先级（大约占总连接的 20%），给 PCoIP
    流量第二高的优先级，并适当分类其余流量。
- en: Network connection characteristics
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络连接特性
- en: Teradici has made significant improvements in the ability of the PCoIP protocol
    to handle high-latency and/or low-bandwidth scenarios. Teradici's PCoIP protocol
    is a purpose-built protocol for delivering a native desktop experience. In order
    to deliver the best possible end user experience, PCoIP will consume as much bandwidth
    as is available at a given time, up to the point where it can deliver a favorable
    end user experience. PCoIP is dynamic in nature, and as the available bandwidth
    changes, so does the amount of bandwidth that PCoIP attempts to consume. PCoIP
    initially uses **Transmission Control Protocol (TCP)** to establish the connection
    and then uses **User Datagram Protocol (UDP)** to transmit the desktop experience.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Teradici 在处理高延迟和/或低带宽场景时，显著提升了 PCoIP 协议的能力。Teradici 的 PCoIP 协议是一种专门为提供原生桌面体验而设计的协议。为了提供最佳的最终用户体验，PCoIP
    会尽可能多地使用可用带宽，直到能够提供理想的用户体验为止。PCoIP 是动态的，随着可用带宽的变化，PCoIP 尝试消耗的带宽量也会相应变化。PCoIP 最初使用
    **传输控制协议（TCP）** 来建立连接，然后使用 **用户数据报协议（UDP）** 来传输桌面体验。
- en: PCoIP also has two primary settings that should be understood, the PCoIP maximum
    bandwidth and the PCoIP bandwidth floor.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PCoIP 还具有两个主要设置，应该了解它们，分别是 PCoIP 最大带宽和 PCoIP 带宽底线。
- en: The PCoIP maximum bandwidth is the maximum amount of bandwidth a given PCoIP
    session is allowed to consume. Configuring this setting can ensure that end users
    never exceed a certain amount of bandwidth themselves. In addition, properly configuring
    the PCoIP maximum bandwidth provides a sense of insurance in a solution. Without
    limiting consumptions per session (even if the maximum is configured to be very
    generous) it is possible to have a runaway PCoIP session consuming a disproportionate
    amount of the available bandwidth. This disproportionate consumption could negatively
    impact the other users sharing the same network connection.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PCoIP 最大带宽是指某个 PCoIP 会话允许消耗的最大带宽量。配置此设置可以确保最终用户永远不会超过某个带宽限制。此外，正确配置 PCoIP 最大带宽可以为解决方案提供一种保障。如果没有限制每个会话的带宽消耗（即使最大带宽配置得非常宽松），也可能出现
    PCoIP 会话过度消耗可用带宽的情况，从而对共享同一网络连接的其他用户产生负面影响。
- en: 'The following diagram is an illustration of the bandwidth floor and the bandwidth
    maximums:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了带宽底线和带宽最大值：
- en: '![Network connection characteristics](img/1124EN_06_10.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![网络连接特性](img/1124EN_06_10.jpg)'
- en: 'The PCoIP bandwidth floor is the minimum threshold of bandwidth that must be
    available for PCoIP to throttle the stream. Following is an example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: PCoIP 带宽底线是 PCoIP 用于控制数据流的带宽最低阈值。以下是一个示例：
- en: An organization has 500 task workers and is looking to understand how large
    a network pipe they need to provide for their VMware View solution. The VDI users
    only use basic office applications and require no other capabilities.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一家组织有 500 名任务工作人员，想了解为他们的 VMware View 解决方案提供多大网络带宽。VDI 用户仅使用基本的办公应用程序，并不需要其他功能。
- en: Average Bandwidth Consumption = (Total Users * 250 Kbps) + (Special Need * Bandwidth
    Penalty) * Bandwidth Offsite * Buffer
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 平均带宽消耗 = (总用户数 * 250 Kbps) + (特殊需求 * 带宽惩罚) * 带宽外部 * 缓冲区
- en: 'So, substituting the values given in the preceding example gives us the following
    output:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将前面示例中给定的值代入公式，我们得到以下输出：
- en: Average Bandwidth Consumption = (500 * 250 Kbps) + 0 * 80 percent = 100,000
    KBps (approximately 97 Mbps)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 平均带宽消耗 = (500 * 250 Kbps) + 0 * 80% = 100,000 KBps（约为 97 Mbps）
- en: DHCP considerations
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DHCP 考虑事项
- en: While it is possible to cobble together a VDI solution that uses static **Internet
    Protocol (IP)** addresses, it is highly not recommended. Due to the potential
    volatility of a VDI and for ease of management, **Dynamic Host Configuration Protocol
    (DHCP)** is the preferred method for managing issuing the IP addresses of the
    vDesktops. When using DHCP, vDesktops do not own a specific IP address, but rather
    it leases it from a DHCP server.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以拼凑出一个使用静态 **互联网协议（IP）** 地址的 VDI 解决方案，但强烈不推荐这样做。由于 VDI 的潜在波动性以及管理的方便性，**动态主机配置协议（DHCP）**
    是管理 vDesktops IP 地址分配的首选方法。使用 DHCP 时，vDesktops 并不拥有特定的 IP 地址，而是从 DHCP 服务器租用它。
- en: 'A single DHCP scope consists of a pool of IP addresses on a particular subnet.
    A DHCP superscope allows a DHCP server to distribute IP addresses from more than
    one scope to devices on a single physical network. Proper subnetting can ensure
    that enough IP leases exist in a particular scope to serve the number of end devices
    requiring IP addresses. The following diagram shows a DHCP workflow:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 单一的 DHCP 范围由特定子网上的 IP 地址池组成。一个 DHCP 超范围允许 DHCP 服务器将多个范围中的 IP 地址分发到单一物理网络上的设备。适当的子网划分可以确保在特定范围内有足够的
    IP 租约来满足需要 IP 地址的终端设备数量。下图展示了 DHCP 工作流程：
- en: '![DHCP considerations](img/1124EN_06_11.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![DHCP 注意事项](img/1124EN_06_11.jpg)'
- en: 'The workflow of a DHCP lease allocation is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: DHCP 租约分配的工作流程如下：
- en: The client broadcasts a DHCPDISCOVER message on its physical subnet.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端在其物理子网中广播 DHCPDISCOVER 消息。
- en: Available DHCP servers on the subnet respond with an IP address by sending a
    DHCPOFFER packet back to the client.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 子网上可用的 DHCP 服务器通过发送 DHCPOFFER 包向客户端回复一个 IP 地址。
- en: A client replies with a DHCPREQUEST message to signal which DHCP server he/she
    accepted the DHCPOFFER packet from. The other DHCP servers withdraw their offer
    for a DHCP lease.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端通过 DHCPREQUEST 消息回复，以指示他/她接受来自哪个 DHCP 服务器的 DHCPOFFER 包。其他 DHCP 服务器会撤回它们的
    DHCP 租约提议。
- en: The DHCP server in the DHCPREQUEST message from the client replies with a DHCPACK
    packet to acknowledge the completion of the lease transaction.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DHCP 服务器在客户端的 DHCPREQUEST 消息中回复一个 DHCPACK 包，以确认租约交易的完成。
- en: DHCP reallocation occurs when a client that already has an address within a
    valid lease expiration window reboots or starts up after being shut down. When
    it starts back up, it will contact the DHCP server previously confirmed via a
    DHCPACK packet to verify the lease and obtain any necessary parameters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个已经有有效租约到期窗口内的地址的客户端重启或在关闭后启动时，发生 DHCP 重新分配。它重新启动时，会联系之前通过 DHCPACK 包确认的 DHCP
    服务器，以验证租约并获取任何必要的参数。
- en: After a set period of time (T1) has elapsed since the original lease allocation,
    the client will attempt to renew the lease. If the client is unable to successfully
    renew the lease, it will enter the rebinding phase (starts at T2). During the
    rebinding phase, it will attempt to obtain a lease from any available DHCP server.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始租约分配后经过一段时间（T1），客户端会尝试续租。如果客户端无法成功续租，则会进入重新绑定阶段（从 T2 开始）。在重新绑定阶段，它会尝试从任何可用的
    DHCP 服务器获取租约。
- en: '![DHCP considerations](img/1124EN_06_12.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![DHCP 注意事项](img/1124EN_06_12.jpg)'
- en: In the preceding diagram, **T[1]** is defined as 50 percent of the lease duration
    and **T[2]** is defined as 90 percent of the lease duration.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，**T[1]** 被定义为租约时长的 50%，**T[2]** 被定义为租约时长的 90%。
- en: For this example, assume a lease duration of 120 minutes (two hours).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设本示例的租约时长为 120 分钟（两个小时）。
- en: A vDesktop boots and is successfully allocated a DHCP lease for a duration of
    120 minutes from DHCP_SERVER_01\. At T1 (50 percent of 120 minutes, that is, 60
    minutes), the vDesktop attempts to renew its lease from DHCP_SERVER_01\. During
    the renewal period, the vDesktop successfully renews its DHCP lease. The lease
    clock is now reset back to a full 120 minute lease since the renew was successful.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: vDesktop 启动并成功从 DHCP_SERVER_01 获得了一个 120 分钟的 DHCP 租约。在 T1 时（120 分钟的一半，即 60 分钟），vDesktop
    尝试从 DHCP_SERVER_01 续租。在续租期间，vDesktop 成功续租了 DHCP 租约。由于续租成功，租约时钟会被重置为完整的 120 分钟。
- en: This time the vDesktop is unsuccessful during the renewal period and enters
    the rebinding period. The vDesktop successfully obtains a new DHCP lease from
    DHCP_SERVER_03 with a lease of a fresh 120 minutes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这次 vDesktop 在续租期间未成功，进入了重新绑定阶段。vDesktop 成功从 DHCP_SERVER_03 获得了一个新的 DHCP 租约，租期为
    120 分钟。
- en: In most VDI scenarios, a DHCP lease time of one hour is sufficient. Typically,
    this is considerably less than the average DHCP lease time in default scopes used
    by most organizations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数 VDI 场景中，1 小时的 DHCP 租约时间已足够。通常，这比大多数组织默认范围内的 DHCP 租约时间要短得多。
- en: If a desktop pool is set to delete a vDesktop after a user logs off, this could
    generate significant DHCP lease thrashing and a very short DHCP lease time should
    be considered (depending on the frequency of vDesktop deletions).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果桌面池设置为在用户注销后删除虚拟桌面，则可能会导致显著的 DHCP 租约波动，并应考虑设置一个非常短的 DHCP 租约时间（具体取决于虚拟桌面删除的频率）。
- en: VMware View Composer tasks such as Recompose and Refresh should maintain the
    same MAC address throughout the process as the VMX settings related to the vNIC
    should not be altered. Therefore, the original lease would attempt to be reallocated
    during the boot process.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: VMware View Composer 任务（如重新组合和刷新）应在整个过程中保持相同的 MAC 地址，因为与 vNIC 相关的 VMX 设置不应被更改。因此，在启动过程中，原始的租约将尝试重新分配。
- en: Virtual switch considerations
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟交换机注意事项
- en: Virtual switch design for VDI environments is another component that may prove
    challenging for those unfamiliar with large-scale virtual infrastructure, or those
    accustomed to designing solutions with potentially high virtual machine volatility.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 设计 VDI 环境中的虚拟交换机是另一个可能对不熟悉大规模虚拟基础设施或习惯于设计具有高虚拟机波动性的解决方案的人来说具有挑战性的组成部分。
- en: '![Virtual switch considerations](img/1124EN_06_13.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![虚拟交换机注意事项](img/1124EN_06_13.jpg)'
- en: The preceding diagram shows, at a high level, the network components of a VDI
    environment. Not shown is the abstraction (that would reside in-between the physical
    switch and the virtual switch) done by the hypervisor.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图示以高层次展示了 VDI 环境中的网络组件。未显示的是由虚拟化管理程序（hypervisor）执行的抽象（位于物理交换机和虚拟交换机之间）。
- en: When a standard vSwitch is created it has, by default, 120 ports. This parameter
    is defined at a kernel (hypervisor) layer, and any changes to the number of ports
    in a standard switch requires a reboot of the physical host.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 创建标准 vSwitch 时，默认有 120 个端口。此参数是在内核（虚拟化管理程序）层定义的，任何更改标准交换机端口数量的操作都需要重启物理主机。
- en: When a distributed vSwitch, also known as a **dvSwitch**, is created, it has,
    by default, 128 ports. This parameter can be changed dynamically and does not
    require a reboot of the physical host for changing the number of ports from its
    original value of 128.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个分布式 vSwitch（也称为**dvSwitch**）时，默认有 128 个端口。此参数可以动态更改，并且无需重启物理主机即可更改端口数量（从默认的
    128 开始）。
- en: Standard versus distributed switches
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准交换机与分布式交换机
- en: Standard vSwitches are not impacted by the loss of a VMware vCenter Server,
    and are best used by functions such as Service console, vMotion, and storage connectivity
    as they can all be easily managed from the command line. However, in large VDI
    solutions leveraging multiple **Virtual Local Area Networks (VLANs)**, dozens
    or hundreds of physical hosts, dvSwitches help to greatly streamline the virtual
    network management across the virtual infrastructure.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 标准 vSwitch 不受 VMware vCenter Server 丧失的影响，最好用于像服务控制台、vMotion 和存储连接等功能，因为这些都可以通过命令行轻松管理。然而，在利用多个**虚拟局域网（VLAN）**的大型
    VDI 解决方案中，数十台或数百台物理主机的 dvSwitch 帮助大大简化了虚拟基础设施中的虚拟网络管理。
- en: VMware vSphere hosts keep a local cache of dvSwitch, dvPortGroup, and dvPort
    information to use when the VMware vCenter Server is unavailable. The local cache
    configuration copies are read-only and cannot be manipulated by the administrator.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: VMware vSphere 主机会在 VMware vCenter Server 无法使用时，保持一个 dvSwitch、dvPortGroup 和
    dvPort 信息的本地缓存。该本地缓存配置副本是只读的，管理员无法对其进行操作。
- en: Port binding
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端口绑定
- en: '**Port binding** is the process of assigning a specific port, also known as
    a **dvPort**, to a specific **network interface controller (NIC)** on a specific
    virtual machine. Think of this assignment as analogous to taking a patch cable
    and plugging one end into the NIC on a physical desktop and the other end into
    an available switch. dvPorts decide how a virtual machine''s network traffic is
    mapped to a specific distributed port group or **dvPortGroup.**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**端口绑定**是将特定端口（也称为**dvPort**）分配给特定虚拟机上的特定**网络接口控制器（NIC）**的过程。可以将这种分配类比为将一根补丁电缆的一端插入物理桌面上的
    NIC，另一端插入可用的交换机。dvPorts 决定了虚拟机的网络流量如何映射到特定的分布式端口组或**dvPortGroup**。'
- en: 'There are three types of port bindings used by dvSwitches; they are as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: dvSwitch 使用三种类型的端口绑定，具体如下：
- en: Static binding
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态绑定
- en: Dynamic binding
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态绑定
- en: Ephemeral binding
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 临时绑定
- en: Static binding
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 静态绑定
- en: Static binding assigns an available port on the dvPortGroup of the dvSwitch
    when a vNIC is added to a virtual machine. For example, if VM009 is a powered
    off Windows 2008 virtual machine and the administrator goes into Edit Settings
    and adds an NIC on dvPortGroup VLAN 71, a dvPort from the VLAN 71 dvPortGroup
    is assigned to the NIC, assuming one is available. It does not matter if the virtual
    machine VM009 is powered on or powered off, it is still assigned a dvPort and
    the dvPort will be unavailable to other virtual machines.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当向虚拟机添加 vNIC 时，静态绑定会在 dvSwitch 的 dvPortGroup 上分配一个可用端口。例如，如果 VM009 是已关机的 Windows
    2008 虚拟机，并且管理员进入“编辑设置”并在 dvPortGroup VLAN 71 上添加一个 NIC，则会分配 VLAN 71 dvPortGroup
    上的一个 dvPort 给该 NIC，前提是有可用的 dvPort。无论虚拟机 VM009 是否已开启，它仍会被分配一个 dvPort，并且该 dvPort
    对其他虚拟机不可用。
- en: The assigned dvPort is released only when the virtual machine has been removed
    from the dvPortGroup. Virtual machines using static binding can only be connected
    to a dvPortGroup through the vCenter Server.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 仅当虚拟机已从 dvPortGroup 中移除时，分配的 dvPort 才会释放。只能通过 vCenter Server 将使用静态绑定的虚拟机连接到
    dvPortGroup。
- en: '**Advantage:** The advantage of static binding is that a virtual machine can
    be powered on even if the vCenter Server is unavailable. In addition, network
    statistics are maintained after a vMotion event and after a power cycle of the
    virtual machine.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**优势：** 静态绑定的优势在于，即使 vCenter Server 不可用，虚拟机也可以启动。此外，在 vMotion 事件和虚拟机电源周期后仍然保留网络统计信息。'
- en: '**Disadvantage:** The disadvantage of static binding is that the dvPortGroup
    cannot be overcommitted. In volatile VDI using non-persistent desktops that are
    deleted at the time of logoff, it is possible that the solution could run out
    of available dvPorts on the dvPortGroup. Static binding is strongly discouraged
    in environments leveraging VMware View Composer.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**劣势：** 静态绑定的劣势在于，dvPortGroup 无法过度使用。在使用非持久桌面的易失性 VDI 环境中，这可能会导致 dvPortGroup
    上的可用 dvPort 不足。在利用 VMware View Composer 的环境中，强烈不建议使用静态绑定。'
- en: Dynamic binding
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态绑定
- en: Dynamic binding assigns an available dvPort on the dvPortGroup when a virtual
    machine is powered on and its NIC is in the connected state. For example, if VM009
    is a Windows 2008 virtual machine and the administrator goes into Edit Settings
    and adds an NIC on dvPortGroup VLAN 71, a dvPort from VLAN 71 dvPortGroup is not
    yet assigned. Once virtual machine VM009 is powered on, it is assigned a dvPort
    on the dvPortGroup and that specific dvPort will be unavailable to other virtual
    machines.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟机开启并且其 NIC 处于连接状态时，动态绑定会在 dvPortGroup 上分配一个可用的 dvPort。例如，如果 VM009 是 Windows
    2008 虚拟机，并且管理员进入“编辑设置”并在 dvPortGroup VLAN 71 上添加一个 NIC，则 VLAN 71 dvPortGroup 上的
    dvPort 尚未分配。一旦虚拟机 VM009 开启，它将在 dvPortGroup 上分配一个 dvPort，该特定 dvPort 将对其他虚拟机不可用。
- en: The assigned dvPort is released when the virtual machine has been powered down
    or the NIC is in the disconnected state. Virtual machines using dynamic binding
    can only be connected to a dvPortGroup through the vCenter Server.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟机已经关机或者 NIC 处于断开状态时，分配的 dvPort 会被释放。只能通过 vCenter Server 将使用动态绑定的虚拟机连接到 dvPortGroup。
- en: Dynamic binding is useful in environments where there are more virtual machines
    than available dvPorts on a given dvPortGroup; however, the number of powered
    on virtual machines will not exceed the number of available dvPorts on a given
    dvPortGroup.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 动态绑定适用于虚拟机数量超过给定 dvPortGroup 上可用 dvPort 的环境；但是，已开启的虚拟机数量不会超过给定 dvPortGroup 上的可用
    dvPort 数量。
- en: '**Advantage:** The advantage of dynamic binding is that as a virtual machine
    doesn''t occupy a dvPort until it is powered on, it is possible to overcommit
    the port on a given dvPortGroup. In addition, network statistics are maintained
    after a vMotion event.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**优势：** 动态绑定的优势在于，由于虚拟机在开启之前不占用 dvPort，因此可以在给定的 dvPortGroup 上过度使用端口。此外，在 vMotion
    事件后仍然保留网络统计信息。'
- en: '**Disadvantage:** The disadvantage of dynamic binding is that as a virtual
    machine isn''t assigned a dvPort until it is powered on, it must be powered on
    by the vCenter Server. Therefore, if the vCenter Server is unavailable, the virtual
    machine will not be able to be powered on. Network statistics are not maintained
    after the power cycle of a virtual machine as the dvPort is assigned at the time
    of boot.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点：** 动态绑定的缺点在于，虚拟机直到启动后才会分配dvPort，因此它必须由vCenter Server启动。如果vCenter Server不可用，虚拟机将无法启动。由于dvPort是在启动时分配的，因此在虚拟机电源循环后，网络统计信息不会被保留。'
- en: Ephemeral binding
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 短暂绑定
- en: Ephemeral binding creates and assigns a dvPort on the dvPortGroup when a virtual
    machine is powered on and its NIC is in the connected state. For example, if VM009
    is a Windows 2008 virtual machine and the administrator goes into Edit Settings
    and adds an NIC on dvPortGroup VLAN 71, a dvPort from VLAN71 dvPortGroup is not
    yet assigned. Once virtual machine VM009 is powered on, a dvPort is first created
    and then it is assigned a dvPort on the dvPortGroup and that specific dvPort will
    be unavailable to other virtual machines.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 短暂绑定在虚拟机启动并且其NIC处于连接状态时，会在dvPortGroup上创建并分配一个dvPort。例如，如果VM009是一个Windows 2008虚拟机，管理员进入编辑设置并添加一个在dvPortGroup
    VLAN 71上的NIC，那么VLAN71 dvPortGroup尚未分配dvPort。一旦虚拟机VM009启动，首先会创建一个dvPort，并将其分配到dvPortGroup上，这个特定的dvPort将无法被其他虚拟机使用。
- en: The assigned dvPort is released when the virtual machine has been powered down
    or the NIC is in the disconnected state. Virtual machines using ephemeral binding
    can be connected to a dvPortGroup through the vCenter Server or from ESX/ESXi.
    Therefore, if the vCenter Server is unavailable, the virtual machine network connections
    can still be managed.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦虚拟机关闭或其NIC处于断开连接状态，分配的dvPort将被释放。使用短暂绑定的虚拟机可以通过vCenter Server或从ESX/ESXi连接到dvPortGroup。因此，即使vCenter
    Server不可用，虚拟机的网络连接仍然可以被管理。
- en: When a virtual machine is vMotion'd, the original dvPort is deleted from the
    source dvPortGroup and a new dvPort is created on the destination dvPortGroup.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟机进行vMotion时，原有的dvPort会从源dvPortGroup中删除，并且在目标dvPortGroup上创建一个新的dvPort。
- en: Ephemeral binding is useful in environments of high volatility, for example,
    a non-persistent VDI solution, where virtual machines are created and deleted
    often. The number of ports on a dvPortGroup is defined and limited by the number
    of ports available of the dvSwitch.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 短暂绑定在高波动性的环境中非常有用，例如非持久的VDI解决方案，在这些环境中，虚拟机经常被创建和删除。dvPortGroup上的端口数量由dvSwitch上可用的端口数量定义和限制。
- en: '**Advantage:** The advantage of ephemeral binding is that as a virtual machine
    doesn''t occupy a dvPort until it is powered on, it is possible to overcommit
    the port on a given dvPortGroup.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点：** 短暂绑定的优点在于，虚拟机在启动之前不会占用dvPort，因此可以在给定的dvPortGroup上进行端口过度分配。'
- en: '**Disadvantage:** Network statistics are not maintained after the power cycle
    of a virtual machine or after a vMotion event as the dvPort is created and assigned
    at the time of boot or vMotion.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点：** 网络统计信息在虚拟机电源循环后或vMotion事件发生后不会被保留，因为dvPort是在启动时或vMotion时创建并分配的。'
- en: Port binding and VMware View Composer
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端口绑定与VMware View Composer
- en: For VDI solutions leveraging View Composer, it is important to recognize that
    tasks such as Recompose, Rebalance, and Refresh will attempt to use the same port
    that has been assigned to the replica image.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于利用View Composer的VDI解决方案，重要的是要认识到，诸如重组（Recompose）、负载均衡（Rebalance）和刷新（Refresh）等任务会尝试使用已经分配给副本镜像的端口。
- en: Therefore, it is recommended to use dynamic or ephemeral (preferred) binding
    if VMware View Composer will be leveraged.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果要使用VMware View Composer，建议使用动态绑定或短暂绑定（推荐）。
- en: Compute considerations
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算考虑事项
- en: 'Compute is typically not an area of failure in most VDI projects, but it is
    still important to understand the computing requirements of an organization before
    implementing a final design. Programs that can cause unforeseen failure from a
    compute perspective are as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数VDI项目中，计算通常不是失败的领域，但在实施最终设计之前，了解组织的计算需求仍然非常重要。可能会导致计算方面预料之外故障的程序如下：
- en: Dragon Medical/Dragon Naturally Speaking
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dragon Medical/Dragon Naturally Speaking
- en: Defense Connect Online
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Defense Connect Online
- en: AutoCAD
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoCAD
- en: Eclipse IDE
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eclipse IDE
- en: For VDI solutions that will be based on Windows XP, one vCPU can likely be used
    to address most basic computing needs. However, for VDI solutions leveraging Windows
    Vista, or more importantly Windows 7, two vCPUs may be necessary to ensure a favorable
    end user experience.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于 Windows XP 的 VDI 解决方案，一个 vCPU 很可能能满足大多数基本计算需求。然而，对于基于 Windows Vista 或更重要的
    Windows 7 的 VDI 解决方案，可能需要两个 vCPU，以确保提供良好的最终用户体验。
- en: For the most accurate calculation of CPU requirements, a proper assessment of
    the environment should be performed. This will help identify potential pitfalls
    such as some of the applications listed previously, prior to rollout.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最准确地计算 CPU 需求，应对环境进行适当评估。这将帮助识别潜在的陷阱，比如之前列出的一些应用程序问题，以便在部署前处理。
- en: While both AMD and Intel-based x86 servers will suffice for VDI solutions, in
    large-scale and/or demanding environments, Intel-based solutions have consistently
    outperformed their AMD counterparts from a density (number of vDesktops per core)
    perspective.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于 AMD 和 Intel 的 x86 服务器都能满足 VDI 解决方案的需求，但在大规模和/或高需求环境中，基于 Intel 的解决方案在密度（每个核心的
    vDesktop 数量）方面始终优于基于 AMD 的解决方案。
- en: In VDI solutions, there is also the potential for unnecessary CPU load due to
    tasks such as antivirus scanning, poorly-tuned applications, single-threaded processes,
    added visual effects, and impacts from video or audio processing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VDI 解决方案中，还可能由于杀毒扫描、调优不当的应用程序、单线程进程、增加的视觉效果、以及视频或音频处理的影响而导致不必要的 CPU 负载。
- en: '![Compute considerations](img/1124EN_06_14.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![计算考虑](img/1124EN_06_14.jpg)'
- en: The preceding diagram illustrates a single processor with 6 cores. As a safe
    baseline, 10 vDesktops per core is used for design purposes. For basic task workers,
    this number could be significantly higher, and there are multiple reference architectures
    that validate 15 to 18 vDesktops per core. The use of the Teradici APEX offload
    card could also increase users per core density.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了一个 6 核处理器的示例。作为一个安全的基准，设计时每个核心使用 10 个 vDesktop。对于基本任务工作者，这个数字可以显著更高，且有多个参考架构验证了每个核心
    15 到 18 个 vDesktop 的情况。使用 Teradici APEX 卸载卡也可能增加每个核心的用户密度。
- en: Continuing to use 10 vDesktops per core as a baseline, and assuming that the
    server has 2 processors (of 6 cores each) that nets a total of 12 cores per physical
    server. With 12 cores per server and 10 users per core, that yields 120 users
    per physical server (6 cores per processor * 2 processors per server * 10 users
    per core). Using 1.5 GB of RAM for each vDesktop (the minimum recommendation for
    64-bit Windows 7), the same physical server needs 180 GB of RAM (1.5 GB * 120
    users). That's a relative sweet spot for memory, as most servers are configurable
    with 256 GB of RAM from the factory.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用每个核心 10 个 vDesktop 作为基准，并假设服务器有 2 个处理器（每个 6 核），那么每台物理服务器的总核心数为 12 核。每台服务器有
    12 个核心，每个核心 10 个用户，那么每台物理服务器的用户数为 120 个（每个处理器 6 核 * 每台服务器 2 个处理器 * 每个核心 10 个用户）。如果每个
    vDesktop 使用 1.5 GB RAM（这是对 64 位 Windows 7 的最低建议配置），那么同一台物理服务器需要 180 GB 的内存（1.5
    GB * 120 个用户）。这是一个相对的内存最佳点，因为大多数服务器的出厂配置内存为 256 GB。
- en: 'The following two tables have been extracted from the *Configuration Maximums,
    VMware vSphere 5.0* guide at [http://www.vmware.com/pdf/vsphere5/r50/vsphere-50-configuration-maximums.pdf](http://www.vmware.com/pdf/vsphere5/r50/vsphere-50-configuration-maximums.pdf).
    The tables explain compute maximums:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两张表格摘自 *Configuration Maximums, VMware vSphere 5.0* 指南，链接在[这里](http://www.vmware.com/pdf/vsphere5/r50/vsphere-50-configuration-maximums.pdf)。这些表格解释了计算的最大值：
- en: '| Host CPU maximums | Maximum |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 主机 CPU 最大值 | 最大值 |'
- en: '| --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Logical CPUs per host | 160 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 每个主机的逻辑 CPU 数量 | 160 |'
- en: '| Virtual machine maximums | Maximum |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 虚拟机最大值 | 最大值 |'
- en: '| --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Virtual machines per host | 512 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 每个主机的虚拟机数量 | 512 |'
- en: '| Virtual CPUs per host | 2048 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 每个主机的虚拟 CPU 数量 | 2048 |'
- en: '| Virtual CPUs per core | 25 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 每个核心的虚拟 CPU 数量 | 25 |'
- en: Given the preceding information, we know that selected processors with significantly
    more cores per processor (for example, 24 cores per processor or 32 cores per
    processor) will not help vDesktop density on a given physical server.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的信息，我们知道，选择每个处理器有显著更多核心（例如每个处理器 24 核或 32 核）的处理器并不会帮助提高某个物理服务器上 vDesktop
    的密度。
- en: 'The following table explains about memory maximums:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格解释了内存的最大值：
- en: '| Host memory maximums | Maximum |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 主机内存最大值 | 最大值 |'
- en: '| --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| RAM per host | 2 TB |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 每个主机的 RAM | 2 TB |'
- en: '| Maximum RAM allocated to service console | 800 MB |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 分配给服务控制台的最大 RAM | 800 MB |'
- en: '| Minimum RAM allocated to service console | 272 MB |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 分配给服务控制台的最小内存 | 272 MB |'
- en: The reason why increased density will not be realized (or more accurately, maximized),
    is partly due to memory limitations and also due to existing VMware vSphere limitations.
    Let's assume, for the sake of argument, that a 32-core physical server was selected
    as the standard for a given VDI solution and it was shipped with a maximum supported
    2 TB of RAM.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 增加密度未能实现（或更准确地说，未能最大化）的原因，部分是由于内存限制，也由于现有的 VMware vSphere 限制。假设为了论证的目的，选择了一个
    32 核物理服务器作为给定 VDI 解决方案的标准，并且它配备了最大支持的 2 TB 内存。
- en: Using the conservative baseline of 10 vDesktops per core, that would yield 320
    vDesktops per host, requiring 640 GB of RAM.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用保守的基准，即每个核心 10 个虚拟桌面，这将产生每个主机 320 个虚拟桌面，需要 640 GB 内存。
- en: 'The following table explains about cluster maximums:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格说明了集群最大值：
- en: '| Cluster maximums | Maximum |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 集群最大值 | 最大 |'
- en: '| --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Hosts per cluster | 32 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 每个集群的主机数 | 32 |'
- en: '| Virtual machines per cluster | 3,000 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 每个集群的虚拟机数 | 3,000 |'
- en: '| Virtual machines per host | 512 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 每个主机的虚拟机数 | 512 |'
- en: Comparing 320 vDesktops per host with the cluster maximums as defined by VMware,
    the maximum number of virtual machines per host would be reached.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个主机上的 320 个虚拟桌面与 VMware 定义的集群最大值进行比较时，每个主机的虚拟机数量将达到最大值。
- en: Furthering the analysis from *Configuration Maximums, VMware vSphere* guide
    describes, "If more than one configuration option (such as, number of virtual
    machines, number of LUNs, number of vDS ports, and so on) are used at their maximum
    limit, some of the processes running on the host might run out of memory." Therefore,
    it is advised to avoid reaching the configuration maximums when possible.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步分析来自*配置最大值，VMware vSphere*指南中描述的内容：“如果超过一个配置选项（例如虚拟机数量、LUN 数量、vDS 端口数量等）达到其最大限制，主机上运行的一些进程可能会耗尽内存。”因此，建议尽可能避免达到配置最大值。
- en: As with all portions of a VDI design, it is important to leverage real-world
    metrics, when possible, to understand how vDesktops will be used, and how they
    will impact the underlying physical infrastructure.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与 VDI 设计的所有部分一样，重要的是尽可能利用真实世界的度量数据，了解虚拟桌面将如何使用，并了解它们如何影响底层的物理基础设施。
- en: Given the preceding calculations, it is advisable to conserve capital expenditure
    on high core count processors and instead focus the funding elsewhere. In most
    environments, six, eight, or twelve core processors will be more than sufficient
    in terms of performance as well as ensuring that vSphere maximums are not reached.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前述计算，建议节省资本支出，不购买高核心数的处理器，而应将资金集中在其他地方。在大多数环境中，六核、八核或十二核处理器在性能方面已足够，并且能够确保不达到
    vSphere 的最大限制。
- en: Working with VMware vSphere maximums
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 VMware vSphere 最大值
- en: 'A strong case can be made that while VMware vSphere is by far the industry-leading
    hypervisor platform for server virtualization, its current maximums could be limiting
    in terms of mega-scale VDI environments. The following is a list of vCenter maximums
    taken from the *Configuration Maximums, VMware vSphere* guide that are most relevant
    to a VMware View solution:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 可以强烈主张，尽管 VMware vSphere 迄今为止是业界领先的服务器虚拟化平台，其当前的最大值可能在超大规模 VDI 环境中存在限制。以下是来自*配置最大值，VMware
    vSphere*指南中与 VMware View 解决方案最相关的 vCenter 最大值列表：
- en: '| vCenter Server scalability | Maximum |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| vCenter Server 的可扩展性 | 最大 |'
- en: '| --- | --- |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Hosts per vCenter Server | 1,000 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 每个 vCenter Server 上的主机数 | 1,000 |'
- en: '| Powered on virtual machines per vCenter Server | 10,000 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 每个 vCenter Server 上的虚拟机开机数 | 10,000 |'
- en: '| Registered virtual machines per vCenter Server | 15,000 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 每个 vCenter Server 注册的虚拟机数 | 15,000 |'
- en: '| Linked vCenter Servers (pod) | 10 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 链接的 vCenter Servers（Pod） | 10 |'
- en: '| Hosts in linked vCenter Servers | 3,000 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 链接的 vCenter Servers 中的主机数 | 3,000 |'
- en: '| Powered on virtual machines in linked vCenter Servers | 30,000 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 链接的 vCenter Servers 中的虚拟机开机数 | 30,000 |'
- en: '| Registered virtual machines in linked vCenter Servers | 50,000 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 在链接的 vCenter Servers 中注册的虚拟机数 | 50,000 |'
- en: The preceding limitations will be analyzed with a solution example in the next
    section.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 上述限制将在下一部分通过一个解决方案示例进行分析。
- en: Solution example — 25,000 seats of VMware View
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案示例——25,000 个 VMware View 座席
- en: A VDI architect has been hired by Company, Inc. to design a solution for 25,000
    task workers in a single building. In this scenario, the networking and storage
    will be provided and will meet the necessary requirements of the VDI solution;
    therefore, the focus is on the physical server specification and the logical design
    of the VMware vSphere and VMware View environments.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一位 VDI 架构师已被公司聘请，负责为单栋建筑中的 25,000 名任务型员工设计解决方案。在这种情况下，将提供网络和存储，并满足 VDI 解决方案的必要要求；因此，重点是物理服务器规格和
    VMware vSphere 及 VMware View 环境的逻辑设计。
- en: 'Company, Inc. is looking for the following information:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 公司正在寻找以下信息：
- en: '**Bill of materials (BOM)** for physical servers'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物料清单 (BOM)** 用于物理服务器'
- en: Logical design of the vSphere infrastructure
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSphere 基础设施的逻辑设计
- en: Logical design of the View infrastructure
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: View 基础设施的逻辑设计
- en: 'With a quick look at the requirements, the architect has determined the following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通过快速查看需求，架构师已确定以下内容：
- en: Powered on virtual machines per vCenter Server will be exceeded (limit 10,000)
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 vCenter 服务器的开机虚拟机将超出限制（限制为 10,000）
- en: Registered virtual machines per vCenter Server will be exceeded (limit 15,000)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 vCenter 服务器注册的虚拟机将超出限制（限制为 15,000）
- en: Powered on virtual machines in linked vCenter Servers will not be exceeded (limit
    30,000)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 链接的 vCenter 服务器中的开机虚拟机将不会超出限制（限制为 30,000）
- en: Registered virtual machines in linked vCenter Servers will not be exceeded (limit
    50,000)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在链接的 vCenter 服务器中注册的虚拟机将不会超出限制（限制为 50,000）
- en: Maximum hosts per vCenter Server will not be exceeded (limit 1,000)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 vCenter 服务器的最大主机数不会超出限制（限制为 1,000）
- en: Maximum virtual machines per host will not be exceeded (limit 320)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个主机的最大虚拟机数量不会超过限制（限制为 320）
- en: Solution design — physical server requirements
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案设计 — 物理服务器要求
- en: 'To support 25,000 task workers running Windows 7 vDesktops, the physical server
    sizing must be determined. Through initial testing, 10 vDesktops per core was
    a conservative estimate. As 4-core processors are being phased out, 6-core processors
    were chosen for their price and availability. Therefore, with 2 6-core processors
    per physical host, that yields 12 cores per host. Using 10 vDesktops per core
    and 12 cores per host yields 120 vDesktops per host. With 1.5 GB per vDesktop
    used for the environment, 180 GB of RAM is required for vDesktops. By allocating
    the maximum supported, 800 MB of RAM to the service console, that yields 181 GB
    of RAM required. Therefore, a server with 192 GB of RAM will support the environment
    nicely. In addition, the following vNetwork maximums exist:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持运行 Windows 7 vDesktops 的 25,000 名任务型员工，必须确定物理服务器的大小。通过初步测试，每个核心 10 个 vDesktops
    是一个保守估计。由于 4 核处理器正在逐步淘汰，选择了 6 核处理器，因为它们在价格和可用性方面更合适。因此，每个物理主机配备 2 个 6 核处理器，总计每个主机
    12 个核心。使用每个核心 10 个 vDesktops 和每个主机 12 个核心，得出每个主机 120 个 vDesktops。每个 vDesktop 使用
    1.5 GB 内存环境，需要 180 GB 的 RAM 来支持 vDesktops。通过为服务控制台分配最大支持的 800 MB RAM，总共需要 181
    GB 的 RAM。因此，配备 192 GB RAM 的服务器将很好地支持该环境。此外，以下是 vNetwork 最大值：
- en: '| vNetwork Standard & Distributed Switch | Maximum |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| vNetwork 标准 & 分布式交换机 | 最大值 |'
- en: '| --- | --- |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Total virtual network ports per host | 4,096 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 每个主机的总虚拟网络端口数量 | 4,096 |'
- en: '| Maximum active ports per host | 1,016 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 每个主机的最大活动端口 | 1,016 |'
- en: '| Distributed switches per vCenter | 32 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 每个 vCenter 的分布式交换机数量 | 32 |'
- en: 'Given the preceding maximums, the following physical host design was leveraged:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前述最大值，采用了以下物理主机设计：
- en: '| Description | Value |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | 值 |'
- en: '| --- | --- |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Cores per processor | 6 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 每个处理器的核心数 | 6 |'
- en: '| Processors per host | 2 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 每个主机的处理器 | 2 |'
- en: '| NICs per host | 8 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 每个主机的网卡数量 | 8 |'
- en: '| Memory per host (GB) | 192 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 每个主机的内存（GB） | 192 |'
- en: '| Approximate vDesktops per core | 10 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 每个核心的约 vDesktops 数量 | 10 |'
- en: '| Approximate vDesktops per host | 120 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 每个主机的约 vDesktops 数量 | 120 |'
- en: '| Standard vSwitches | 2 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 标准 vSwitches | 2 |'
- en: '| Distributed vSwitches | 1 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 分布式 vSwitches | 1 |'
- en: 'The networking configuration is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 网络配置如下：
- en: '![Solution design — physical server requirements](img/1124EN_06_15.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![解决方案设计 — 物理服务器要求](img/1124EN_06_15.jpg)'
- en: The preceding diagram represents two vNetwork standard switches and one vNetwork
    distributed switch. The first standard vSwitch, vS1, is used for service console
    and vMotion.The second standard vSwitch, vS2, is used for network-based storage.
    The only distributed vSwitch, vD1, is used for virtual machines.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图表展示了两个 vNetwork 标准交换机和一个 vNetwork 分布式交换机。第一个标准 vSwitch，vS1，用于服务控制台和 vMotion。第二个标准
    vSwitch，vS2，用于基于网络的存储。唯一的分布式 vSwitch，vD1，用于虚拟机。
- en: Solution design — the pod concept
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案设计 — Pod 概念
- en: The concept of the pod is to give architects a method of creating building blocks
    to ease the design scalability for large environments. It also provides a conceptual
    framework for the solution architecture.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Pod的概念是为架构师提供一种创建构建模块的方法，以简化大规模环境设计的可扩展性。它还为解决方案架构提供了一个概念框架。
- en: '![Solution design — the pod concept](img/1124EN_06_16.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![解决方案设计 — Pod概念](img/1124EN_06_16.jpg)'
- en: 'The main components of a VMware View pod are as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: VMware View Pod的主要组成部分如下：
- en: '**Physical network:** This includes necessary switches, VLANs, network policies,
    and other network infrastructure required to support the VDI'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物理网络：** 包括支持VDI所需的交换机、VLAN、网络策略以及其他网络基础设施。'
- en: '**vCenter blocks:** This includes hosts, vCenter cluster design, vCenter Linked
    Mode configuration, and so on'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vCenter块：** 包括主机、vCenter集群设计、vCenter链接模式配置等。'
- en: '**View Connection Server pools:** This includes View Connection Servers and
    (if applicable) View Security Servers'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**View连接服务器池：** 包括View连接服务器和（如果适用）View安全服务器。'
- en: 'This concept of a pod can be carried through with the following architecture
    types:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这种Pod的概念可以通过以下架构类型来实现：
- en: Traditional
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统架构
- en: Traditional in modular form
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模块化形式的传统架构
- en: Converged virtualization appliances
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 融合虚拟化设备
- en: 'The **traditional** architecture type involves using servers (rackmount or
    blade), network switches, storage network switches (if applicable), and storage
    arrays. A traditional architecture approach is normally sufficient for an initial
    build-out but may not offer the scale-out capabilities of other approaches. The
    following diagram shows an illustration of a typical traditional architecture
    approach where disproportionate resources exist:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**传统**架构类型涉及使用服务器（机架式或刀片服务器）、网络交换机、存储网络交换机（如果适用）和存储阵列。传统架构方法通常足够满足初始构建需求，但可能无法提供其他方法的扩展能力。以下图示展示了一个典型的传统架构方法，其中存在不成比例的资源：'
- en: '![Solution design — the pod concept](img/1124EN_06_17.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![解决方案设计 — Pod概念](img/1124EN_06_17.jpg)'
- en: For example, using the preceding diagram, sufficient compute, network, and storage
    resources may exist for the initial rollout of 400 VMware View users. In this
    example, an overabundance of storage capacity exists.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用前面的图示，可能拥有足够的计算、网络和存储资源来支持400个VMware View用户的初始部署。在这个例子中，存储容量过剩。
- en: 'The following diagram shows an illustration of a typical traditional architecture
    scale-out challenge:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了典型传统架构扩展挑战的插图：
- en: '![Solution design — the pod concept](img/1124EN_06_18.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![解决方案设计 — Pod概念](img/1124EN_06_18.jpg)'
- en: When the organization decides to add an additional 500 VMware View users, it
    runs into a problem. In the phase 1 rollout, an overabundance of storage capacity
    existed. However, to add capacity in a modular fashion, compute and network will
    still require an additional block of storage. Therefore, every addition will have
    some level of excess, which drives the price per vDesktop up due to architectural
    inefficiencies.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 当组织决定添加500个VMware View用户时，遇到了问题。在第一阶段的部署中，存储容量过剩。然而，为了以模块化的方式添加容量，计算和网络仍然需要额外的存储模块。因此，每次添加都会产生一定程度的冗余，这由于架构效率低下导致每个虚拟桌面的成本上升。
- en: An organization likely would not want to accept these inefficiencies so it would
    redesign its requirements every step of the way. Designing the scale out for every
    additional phase of a VDI solution also drives up cost through added complexity
    and man hours.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 组织可能不愿意接受这些低效，所以它会在每一步重新设计需求。为VDI解决方案的每个额外阶段设计扩展也会通过增加复杂性和人力成本推高总成本。
- en: In addition, every time a scale-out phase is re-architected, the chance of error
    becomes greater.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每次扩展阶段重新架构时，错误的发生概率会更大。
- en: The **traditional in modular form** architecture type involves using servers
    (rackmount or blade), network switches, storage network switches (if applicable),
    and storage arrays. Whereas, a traditional architecture is normally not able to
    scale proportionately, a traditional in modular form is designed to scale in building
    blocks. This approach does not need re-engineering for each scale-out phase, and
    instead an organization relies on the traditional yet modular architecture for
    predictable scale-out design.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**传统模块化架构**类型涉及使用服务器（机架式或刀片服务器）、网络交换机、存储网络交换机（如果适用）和存储阵列。而传统架构通常无法按比例扩展，而传统模块化架构则设计为可以按模块扩展。这种方法不需要在每次扩展时进行重新设计，组织可以依赖这种传统的但模块化的架构来实现可预测的扩展设计。'
- en: 'The following diagram shows an illustration of a typical traditional in modular
    form architecture approach, where proportionate resources exist:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了典型的传统模块化架构方法的示意图，其中包含了按比例分配的资源：
- en: '![Solution design — the pod concept](img/1124EN_06_19.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![解决方案设计 — pod 概念](img/1124EN_06_19.jpg)'
- en: There are typically two ways to implement a traditional in modular form architecture.
    The first is by spending the time to architect and test a customer design, where
    compute (for example, Dell blade) is combined with network switches (for example,
    Cisco) and a storage array (for example, NetApp). The danger with this approach
    is that if the person or team designing the solution has never designed a VDI
    solution before, they are likely to have a few lessons learned through the process
    that will yield a less than optimal solution. This is not to say that this approach
    is not suitable and should not be taken, but special considerations should be
    taken to ensure the architecture is sound and scalable. A seasoned VDI architect
    can take any off-the-shelf hardware and build a sustainable VDI architecture.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 实施传统模块化架构通常有两种方式。第一种是花时间为客户设计和测试架构，其中计算（例如，戴尔刀片服务器）与网络交换机（例如，思科）和存储阵列（例如，NetApp）结合使用。采用这种方法的风险在于，如果设计解决方案的团队从未设计过
    VDI 解决方案，他们可能会在过程中吸取一些经验教训，最终得出一个不太优化的解决方案。这并不是说这种方法不适用或不应该采取，但需要特别注意确保架构的可靠性和可扩展性。一位经验丰富的
    VDI 架构师可以使用任何现成的硬件来构建一个可持续的 VDI 架构。
- en: The second way to implement a traditional in modular form architecture is by
    implementing a branded solution such as the VCE Vblock (Cisco servers + Cisco
    switches + EMC storage) or FlexPod (Cisco servers + Cisco switches + NetApp storage),
    for example. These solutions are proven, scalable in a predictive manner, and
    they offer a known architecture for VDI. The drawback of these solutions is that
    they often have a high barrier to entry in terms of cost and scale out in large
    modular blocks (for example, 1,000 users at a time).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种实现传统模块化架构的方法是采用品牌解决方案，如 VCE Vblock（思科服务器 + 思科交换机 + EMC 存储）或 FlexPod（思科服务器
    + 思科交换机 + NetApp 存储）等。这些解决方案是经过验证的，可以按可预测的方式进行扩展，并为 VDI 提供已知的架构。这些解决方案的缺点是，它们通常在成本和大规模模块化扩展（例如一次扩展
    1,000 个用户）方面存在较高的门槛。
- en: The third type of architecture uses **converged virtualization appliances**.
    Converged virtualization appliances are typically 2U to 6U appliances that comprise
    of one to many ESXi servers with local storage that is often shared among the
    ESXi servers in the appliance. The storage is typically shared through a virtual
    storage appliance model, where local storage is represented as either iSCSI or
    NFS storage to one or more ESXi servers in the appliance. The converged virtualization
    appliance model is relatively new to the VDI market.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种架构类型使用 **融合虚拟化设备**。融合虚拟化设备通常是 2U 到 6U 的设备，包含一个或多个 ESXi 服务器，并且设备内的本地存储通常在这些
    ESXi 服务器之间共享。存储通常通过虚拟存储设备模型共享，其中本地存储作为 iSCSI 或 NFS 存储呈现给设备中的一个或多个 ESXi 服务器。融合虚拟化设备模型对于
    VDI 市场来说相对较新。
- en: Linked vCenter Servers
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 连接的 vCenter 服务器
- en: As the number of virtual machines per vCenter Server will be exceeded, more
    than one vCenter Server will be required for this solution.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 随着每个 vCenter Server 虚拟机数量的超出，解决方案将需要多个 vCenter Server。
- en: 'The following table illustrates the vCenter maximums:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了 vCenter 的最大值：
- en: '| vCenter Server scalability | Maximum |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| vCenter Server 可扩展性 | 最大 |'
- en: '| --- | --- |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Powered on virtual machines per vCenter Server | 10,000 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 每个 vCenter Server 启动的虚拟机 | 10,000 |'
- en: '| Registered virtual machines per vCenter Server | 15,000 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 每个 vCenter Server 注册的虚拟机 | 15,000 |'
- en: '| Linked vCenter Servers | 10 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 已链接的 vCenter 服务器 | 10 |'
- en: '| Powered on virtual machines in linked vCenter Servers | 30,000 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 在已链接的 vCenter 服务器中启动的虚拟机 | 30,000 |'
- en: '| Registered virtual machines in linked vCenter Servers | 50,000 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 在已链接的 vCenter 服务器中注册的虚拟机 | 50,000 |'
- en: 'vCenter Linked Mode has a few basic prerequisites. They are as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: vCenter 链接模式有一些基本的先决条件，具体如下：
- en: Both vCenter Servers must reside in a functional DNS environment, where **fully
    qualified domain names (FQDNs)** of each vCenter Server can be resolved properly
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个 vCenter 服务器必须位于功能正常的 DNS 环境中，在该环境下，能够正确解析每个 vCenter 服务器的 **完全限定域名（FQDN）**
- en: Any vCenter Server participating in Linked Mode must reside in an Active Directory
    domain
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何参与链接模式的 vCenter 服务器必须位于一个活动目录域中
- en: If the vCenter Servers are in separate Active Directory domains, the respective
    domains must have a two-way trust
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 vCenter 服务器位于不同的活动目录域中，则相应的域之间必须建立双向信任关系
- en: Both vCenter Servers must reside in a functional **Network Time Protocol (NTP)**
    environment, where time synchronization of the vCenter Servers is no more than
    5 minutes adrift of one another
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个 vCenter 服务器必须位于功能正常的 **网络时间协议（NTP）** 环境中，在该环境下，两个 vCenter 服务器的时间同步误差不得超过
    5 分钟
- en: Windows RPC port mapper must be allowed to open the **Remote Procedure Call
    (RPC)** ports for replication; this is covered in detail at [http://support.microsoft.com/kb/154596](http://support.microsoft.com/kb/154596)
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须允许 Windows RPC 端口映射器打开 **远程过程调用（RPC）** 端口以支持复制；有关详细信息，请参考 [http://support.microsoft.com/kb/154596](http://support.microsoft.com/kb/154596)
- en: Both VMware vCenter Servers have the VMware vCenter Standard Edition license
    (versus foundation, for example)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个 VMware vCenter 服务器均拥有 VMware vCenter 标准版许可证（例如，与基础版相比）
- en: Separate databases for each VMware vCenter Server
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 VMware vCenter 服务器使用独立的数据库
- en: '![Linked vCenter Servers](img/1124EN_06_20.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![已链接的 vCenter 服务器](img/1124EN_06_20.jpg)'
- en: VMware vCenter Linked Mode connects two or more vCenter Servers together via
    ADAM database replication to store information regarding user roles as well as
    VMware licensing. VMware vCenter Linked Mode does not do any form of database
    replication. If VMware vCenter Linked Mode would fail for any reason, the two
    (or more) vCenter Servers would still be viable as standalone instances.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: VMware vCenter 链接模式通过 ADAM 数据库复制将两个或多个 vCenter 服务器连接在一起，存储有关用户角色以及 VMware 许可的信息。VMware
    vCenter 链接模式不进行任何形式的数据库复制。如果 VMware vCenter 链接模式因任何原因失败，两个（或更多）vCenter 服务器仍然可以作为独立实例正常运行。
- en: '![Linked vCenter Servers](img/1124EN_06_21.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![已链接的 vCenter 服务器](img/1124EN_06_21.jpg)'
- en: As shown in the preceding diagram, where there are two separate vCenter Server
    instances (vCenter1 and vCenter2), the virtual data centers, clusters, resource
    pools, and virtual machines are unique to their respective instance of vCenter.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，在两个独立的 vCenter 服务器实例（vCenter1 和 vCenter2）之间，虚拟数据中心、集群、资源池和虚拟机是各自 vCenter
    实例独有的。
- en: Joining multiple vCenters together with vCenter Linked Mode forms what is known
    as a pod. A pod can consist of up to 10 vCenter Servers in Linked Mode.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 将多个 vCenter 通过 vCenter 链接模式连接在一起，形成所谓的 pod。一个 pod 最多可以包含 10 个通过链接模式连接的 vCenter
    服务器。
- en: vCenter Servers
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: vCenter 服务器
- en: Using calculations from preceding sections, this solution is expected to have
    approximately 120 vDesktops per host; this means that 209 physical hosts are needed
    to support the vDesktop portion of this solution (not taking into account a virtualized
    vCenter, database, and so on).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的计算，本解决方案预计每台主机大约支持 120 个 vDesktops；这意味着需要 209 台物理主机来支持该解决方案中的 vDesktop
    部分（不考虑虚拟化的 vCenter、数据库等）。
- en: Due to the nature of the end user population, the time they log in, the conservative
    nature of the original assessment (for example, 10 vDesktops per core), it has
    been decided that there will be no HA requirements for the vSphere Servers supporting
    vDesktops.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最终用户群体的特点、他们的登录时间，以及原始评估的保守性（例如，每个核心支持 10 个 vDesktops），决定了不需要为支持 vDesktops
    的 vSphere 服务器配置高可用性（HA）。
- en: It has also been determined that the management infrastructure including the
    View Connection Servers, vCenter Servers, database server, and a few other components
    require three physical hosts. In order to provide a level of protection, it has
    been determined to use an *n* + 1 solution and utilize 4 physical hosts.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 还已确定，管理基础设施，包括 View 连接服务器、vCenter 服务器、数据库服务器以及其他几个组件，要求三台物理主机。为了提供一定的保护级别，决定采用
    *n* + 1 解决方案，并使用四台物理主机。
- en: It was determined previously that any given vCenter can have a maximum of 10,000
    powered on virtual machines at any given time. This solution will need to support
    more than 25,000 powered on virtual machines; therefore, this solution will require
    3 vCenter Servers.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 之前已经确定，任何给定的 vCenter 在任何时刻最多只能拥有 10,000 个打开的虚拟机。此解决方案需要支持超过 25,000 个打开的虚拟机；因此，这个解决方案将需要
    3 个 vCenter 服务器。
- en: To balance the load across the vCenter Servers, the clusters have been as equitably
    divided as possible.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了平衡负载，vCenter 服务器之间的集群已经尽可能均匀地分配。
- en: 'Note that the naming conventions used for the clusters in this example are:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本例中使用的集群命名约定为：
- en: 'vCenter Server: vc-{letter}, for example, vc-b'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vCenter 服务器命名格式：vc-{字母}，例如 vc-b
- en: 'Clusters: cl-{letter of vCenter}-{number}, for example, cl-c-6'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群命名格式：cl-{vCenter 字母}-{编号}，例如 cl-c-6
- en: 'The vCenter Servers are named as vc-a, vc-b, vc-c, respectively. Their details
    along with the diagrams are as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 vCenter 服务器分别命名为 vc-a、vc-b、vc-c。它们的详细信息及图表如下：
- en: '![vCenter Servers](img/1124EN_06_22.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![vCenter Servers](img/1124EN_06_22.jpg)'
- en: 'The preceding diagram explains about vCenter Server **vc-a**. The following
    list gives the details about vc-a:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表解释了 vCenter 服务器 **vc-a**。以下是关于 vc-a 的详细信息：
- en: 9 clusters of 8 hosts each (cl-a-1, cl-a-2, ..., cl-a-9)
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 9 个每个包含 8 个主机的集群 (cl-a-1, cl-a-2, ..., cl-a-9)
- en: Total of 72 hosts
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共 72 个主机
- en: Total of 8,640 vDesktops (120 vDesktops per host multiplied by 72 hosts)
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共 8,640 个 vDesktops（每个主机 120 个 vDesktops，72 个主机）
- en: '![vCenter Servers](img/1124EN_06_23.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![vCenter Servers](img/1124EN_06_23.jpg)'
- en: 'The preceding diagram explains about vCenter Server **vc-b**. The following
    list gives the details about vc-b:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表解释了 vCenter 服务器 **vc-b**。以下是关于 vc-b 的详细信息：
- en: 9 clusters of 8 hosts each (cl-b-1, cl-b-2, ..., cl-b-9)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 9 个每个包含 8 个主机的集群 (cl-b-1, cl-b-2, ..., cl-b-9)
- en: Total of 72 hosts
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共 72 个主机
- en: Total of 8,640 vDesktops (120 vDesktops per host multiplied by 72 hosts)
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共 8,640 个 vDesktops（每个主机 120 个 vDesktops，72 个主机）
- en: '![vCenter Servers](img/1124EN_06_24.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![vCenter Servers](img/1124EN_06_24.jpg)'
- en: 'The preceding diagram explains about vCenter Server **vc-c**. The following
    list gives the details about vc-c:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表解释了 vCenter 服务器 **vc-c**。以下是关于 vc-c 的详细信息：
- en: 7 clusters each having 8 hosts
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 7 个每个包含 8 个主机的集群
- en: 1 cluster of 5 hosts
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 个包含 5 个主机的集群
- en: 1 cluster of 4 hosts
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 个包含 4 个主机的集群
- en: 1 cluster of 4 hosts dedicated to management
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 个由 4 个主机构成的集群，专门用于管理
- en: Total of 69 hosts
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共 69 个主机
- en: Total of 7,800 vDesktops and approximately 30 vServers (View Connection Server,
    database server, vCenter server, and so on)
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共 7,800 个 vDesktops 和约 30 个 vServers（View Connection Server、数据库服务器、vCenter 服务器等）
- en: 'vCenter vc-c has a cluster (cl-c-10) dedicated for hosting the infrastructure
    virtual machines. These virtual machines include:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: vCenter vc-c 拥有一个集群 (cl-c-10)，专门用于托管基础设施虚拟机。这些虚拟机包括：
- en: 3 VMware vCenter Servers (vc-a, vc-b, vc-c)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 个 VMware vCenter 服务器 (vc-a, vc-b, vc-c)
- en: 15 View Connection Servers
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 15 个 View Connection 服务器
- en: Supporting infrastructure (if needed) such as database servers, Liquidware Labs
    TM, and so on![vCenter Servers](img/1124EN_06_25.jpg)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持基础设施（如有需要），例如数据库服务器、Liquidware Labs TM 等![vCenter Servers](img/1124EN_06_25.jpg)
- en: VMware Update Manager Servers
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VMware 更新管理服务器
- en: VMware Update Manager is a solution that automated the application of patches
    to both vSphere Servers and virtual machines. It's most often used to patch vSphere
    Servers in large environments as it handles the task of placing a host in maintenance
    mode, migrating virtual machines, patch application, reboots, and normalization
    with a minimal amount of user interaction.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: VMware 更新管理器是一种自动化应用程序，用于为 vSphere 服务器和虚拟机应用补丁。它最常用于大型环境中为 vSphere 服务器打补丁，因为它能够自动执行将主机置于维护模式、迁移虚拟机、应用补丁、重启以及最小化用户交互的任务。
- en: VMware Update Manager Servers can only be paired with one VMware vCenter Server
    instance at a time. Therefore, in this solution three VMware Update Manager Servers
    will be required (one per vCenter Server instance).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: VMware 更新管理器服务器一次只能与一个 VMware vCenter 服务器实例配对。因此，此解决方案需要 3 个 VMware 更新管理器服务器（每个
    vCenter 服务器实例一个）。
- en: '![VMware Update Manager Servers](img/1124EN_06_26.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![VMware Update Manager Servers](img/1124EN_06_26.jpg)'
- en: VMware vCenter Server Heartbeat
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: VMware vCenter 服务器心跳
- en: 'In this section, we have added a note about VMware vCenter Server Heartbeat.
    In most VMware View solutions, one or more highly available VMware vCenter Servers
    are required. vCenter Server is of paramount importance because if vCenter is
    unavailable, the following problems would be faced:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们添加了关于 VMware vCenter Server Heartbeat 的说明。在大多数 VMware View 解决方案中，要求有一个或多个高可用的
    VMware vCenter Server。vCenter Server 至关重要，因为如果 vCenter 无法使用，将会面临以下问题：
- en: New vDesktops cannot be provisioned
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的 vDesktops 无法被配置
- en: vDesktops cannot be recomposed, refreshed, or rebalanced
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vDesktops 无法被重新组合、刷新或重新平衡
- en: vDesktops cannot be deleted from the View Admin console
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vDesktops 无法从 View 管理控制台删除
- en: Therefore, vCenter Server Heartbeat is often an affordable insurance policy
    for the vCenter Servers in a VDI solution.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，vCenter Server Heartbeat 通常是 VDI 解决方案中 vCenter Server 的一个经济有效的保险措施。
- en: As noted previously, VMware Update Manager can only be linked to one instance
    of the VMware vCenter Server. However, it's important to note that a pair of vCenter
    Servers joined by VMware vCenter Server Heartbeat is considered to be only one
    instance.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，VMware Update Manager 只能与一个 VMware vCenter Server 实例关联。然而，值得注意的是，通过 VMware
    vCenter Server Heartbeat 连接的一对 vCenter Server 被视为一个实例。
- en: Therefore, the solution does not require additional VMware Update Manager Servers
    just because VMware vCenter Server Heartbeat is being leveraged.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，解决方案并不要求仅仅因为 VMware vCenter Server Heartbeat 被使用，就需要额外的 VMware Update Manager
    服务器。
- en: Solution design — pools
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案设计 — 池
- en: Here, we will cover View Connection Servers.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将介绍 View Connection Servers。
- en: View Connection Servers
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: View Connection Servers
- en: 'As illustrated next, the VMware View infrastructure introduces its own maximums
    in addition to those already imposed by the VMware vSphere infrastructure. The
    View Connection maximums are given in the following table:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，VMware View 基础设施在 VMware vSphere 基础设施已施加的最大值之外，还引入了自己的最大值。View Connection
    的最大值见下表：
- en: '| Connection Servers per deployment | Maximum |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 每个部署的连接服务器 | 最大 |'
- en: '| --- | --- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 Connection Server supporting direct RDP or PCoIP | 2,000 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 1 个连接服务器支持直接 RDP 或 PCoIP | 2,000 |'
- en: '| 7 Connection Servers (5 hot + 2 spare) supporting direct RDP or PCoIP | 10,000
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 7 个连接服务器（5 个热备 + 2 个备用）支持直接 RDP 或 PCoIP | 10,000 |'
- en: '| Maximum hosts in a cluster when not using View Composer | 32 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 不使用 View Composer 时，集群中最大主机数 | 32 |'
- en: '| Maximum hosts in a cluster when using View Composer | 8 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 使用 View Composer 时，集群中最大主机数 | 8 |'
- en: If a solution like Unidesk TM was used in lieu of View Composer, the end design
    could support more hosts per cluster.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用像 Unidesk TM 这样的解决方案替代 View Composer，则最终设计可能支持每个集群更多的主机。
- en: For the solution example, whereby 25,000 vDesktops must be supported, it's important
    to understand how many end users will be logging in at any given time. A VMware
    View Connection Server can support 2,000 direct PCoIP connections at any given
    time. In this example, all 25,000 end users could potentially log in at the same
    time. Therefore, a minimum of 13 View Connection Servers are required (2,000 *
    13 = 26,000 simultaneous direct PCoIP connections supported).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 对于必须支持 25,000 个 vDesktops 的解决方案示例，了解任何时候有多少终端用户将登录是非常重要的。一个 VMware View Connection
    Server 可以支持每次最多 2,000 个直接的 PCoIP 连接。在此示例中，所有 25,000 个终端用户可能会同时登录。因此，至少需要 13 个
    View Connection Server（2,000 * 13 = 26,000 个同时直接 PCoIP 连接支持）。
- en: '![View Connection Servers](img/1124EN_06_27.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![View Connection Servers](img/1124EN_06_27.jpg)'
- en: In order to provide a level of redundancy in case of a View Connection Server
    outage, it is advised to add in *n* + 2 (or more) solutions. For example, increasing
    the required number of View Connection Servers, that is, 13 to a total of 15 View
    Connection Servers, provides the ability to support a maximum of 30,000 simultaneous
    PCoIP connections. Therefore, even if two View Connection Servers fail, all 25,000
    users would be able to log in to the VDI without incident.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 View Connection Server 故障时提供一定的冗余，建议添加 *n* + 2（或更多）个解决方案。例如，增加所需的 View Connection
    Server 数量，即将 13 增加到 15 个 View Connection Server，提供支持最多 30,000 个同时 PCoIP 连接的能力。因此，即使两个
    View Connection Server 故障，所有 25,000 个用户仍然可以顺利登录到 VDI。
- en: The 15 View Connection Servers should be placed behind a redundant load balancing
    solution and should be configured to check that the View Connection Server is
    online via a simple ping (if **Internet Control Message Protocol (ICMP)** is allowed)
    and HTTP GET on the View Connection Server's URL. The entire pool of View Connection
    Servers should be accessible by a single name, such as `view.customer.com`, whereby
    end users would use `https://view.customer.com` to access the View environment.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这 15 台 View Connection Server 应该部署在冗余的负载均衡解决方案后，并配置为通过简单的 ping（如果允许 **互联网控制报文协议（ICMP）**）和
    HTTP GET 命令检查 View Connection Server 是否在线，且检查其 URL。整个 View Connection Server 池应通过一个单一的名称进行访问，例如
    `view.customer.com`，最终用户可以通过 `https://view.customer.com` 来访问 View 环境。
- en: By leveraging the HTTP GET to verify functionality of a View Connection Server,
    a server whose applicable services have stopped will not successfully reply to
    the GET command and, therefore, will be removed from the load balancing pool.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 HTTP GET 来验证 View Connection Server 的功能，当一个服务器的相关服务停止时，它将无法成功响应 GET 命令，因此会被从负载均衡池中移除。
- en: Solution design — the formulae
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方案设计 — 公式
- en: 'The following are some formulae to calculate the minimum number of vCenter
    Servers, Connection Servers, and Pods:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些公式，用于计算最小 vCenter 服务器数、连接服务器数和 Pod 数：
- en: Minimum number of vCenter Servers = Number of Desktops / 10,000
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vCenter 服务器的最小数量 = 桌面数 / 10,000
- en: Minimum number of View Connection Servers = Number of Simultaneous Connections
    / 2,000
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: View Connection Server 的最小数量 = 同时连接数 / 2,000
- en: Minimum number of vCenter Pods = Number of vCenter Servers / 10
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vCenter Pod 的最小数量 = vCenter 服务器数 / 10
- en: Summary
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As detailed in this chapter, there are many design considerations to make such
    as DHCP lease time, the minimum number of vCenters, and the number of cores to
    buy in a server platform. For large environments of thousands of vDesktops, it
    may be easiest to start with the vSphere maximums and work down. For small environments
    or PoCs that don't require a massive virtual infrastructure, the concepts covered
    in this chapter are still relevant as a successful PoC can grow rapidly in adoption.
    Finally, the concept of a pod architecture, or a collection of vCenter Servers,
    is typically new to those familiar only with designing virtual server solutions
    on the VMware vSphere platform. They can take some time to understand the new
    concepts and working up against the vSphere and vCenter maximums.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章所述，设计中需要考虑许多因素，如 DHCP 租约时间、最小 vCenter 数量以及服务器平台中需要购买的核心数。对于大规模环境（如成千上万的 vDesktops），最简单的方法可能是从
    vSphere 的最大值开始，逐步调整。对于小型环境或不需要大规模虚拟基础设施的 PoC，尽管本章所涵盖的概念仍然适用，因为一个成功的 PoC 可以快速扩展。最后，Pod
    架构的概念，或 vCenter 服务器的集合，对于只熟悉 VMware vSphere 平台虚拟服务器解决方案设计的人来说通常是新的。他们可能需要一些时间来理解这些新概念，并了解如何与
    vSphere 和 vCenter 的最大值相对接。
