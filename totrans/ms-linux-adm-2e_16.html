<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-324"><a id="_idTextAnchor342"/>16</h1>
<h1 id="_idParaDest-325"><a id="_idTextAnchor343"/>Deploying Applications with Kubernetes</h1>
<p>Whether you are a seasoned system administrator managing containerized applications or a DevOps engineer automating app orchestration workflows, <strong class="bold">Kubernetes</strong> could be your platform of choice. This chapter will introduce you to Kubernetes and will guide you through the basic process of building and configuring a <strong class="bold">Kubernetes cluster</strong>. We’ll use Kubernetes to run and scale a simple application in a secure and highly available environment. You will also learn how to interact with Kubernetes using the <strong class="bold">command-line </strong><strong class="bold">interface</strong> (<strong class="bold">CLI</strong>).</p>
<p>By the end of this chapter, you’ll learn how to install, configure, and manage a Kubernetes cluster on-premises. We’ll also show you how to deploy and scale an application using Kubernetes.</p>
<p>Here’s a brief outline of the topics we will cover in this chapter:</p>
<ul>
<li>Introducing Kubernetes architecture and the API object model</li>
<li>Installing and configuring Kubernetes</li>
<li>Working with Kubernetes using the <code>kubectl</code> command-line tool and deploying applications</li>
</ul>
<h1 id="_idParaDest-326"><a id="_idTextAnchor344"/>Technical requirements</h1>
<p>You should be familiar with Linux and the CLI in general. A good grasp of <strong class="bold">TCP/IP networking</strong> and <strong class="bold">Docker</strong> containers would go a long way in making your journey of learning Kubernetes easier.</p>
<p>You will also need the following:</p>
<ul>
<li>A local desktop machine with a Linux distribution of your choice to install and experiment with the CLI tools used in this chapter. We will use both Debian and Ubuntu LTS.</li>
<li>A powerful desktop system with at least 8 CPU cores and at least 16 GB of RAM will allow you to replicate the necessary environment on your desktop as we’ll be devoting a relatively large section to building a Kubernetes cluster using VMs.</li>
<li>A desktop hypervisor.</li>
</ul>
<p>Now, let’s start our journey together to discover Kubernetes.</p>
<h1 id="_idParaDest-327"><a id="_idTextAnchor345"/>Introducing Kubernetes</h1>
<p>Kubernetes<a id="_idIndexMarker2276"/> is an open source <strong class="bold">container orchestrator</strong> initially developed by Google. A container orchestrator is a piece of software that automatically manages (including provisioning, deployment, and scaling) containerized applications. Assuming an application uses containerized microservices, a container orchestration system provides the following features:</p>
<ul>
<li><strong class="bold">Elastic orchestration (autoscaling)</strong>: This involves automatically starting and stopping application services (containers) based on<a id="_idIndexMarker2277"/> specific requirements and conditions – for example, launching multiple web server instances with an increasing number of requests and eventually terminating servers when the number of requests drops below a certain threshold</li>
<li><strong class="bold">Workload management</strong>: This involves optimally<a id="_idIndexMarker2278"/> deploying and distributing application services across the underlying cluster to ensure mandatory dependencies and redundancy – for example, running a web server endpoint on each cluster node for high availability</li>
<li><strong class="bold">Infrastructure abstraction</strong>: This involves providing<a id="_idIndexMarker2279"/> container runtime, networking, and load-balancing capabilities – for example, distributing the load among multiple web server containers and autoconfiguring the underlying network connectivity with a database app container</li>
<li><strong class="bold">Declarative configuration</strong>: This involves describing<a id="_idIndexMarker2280"/> and ensuring the desired state of a multi-tiered application – for example, a web server should be ready for serving requests only when the database backend is up and running, and the underlying storage is available</li>
</ul>
<p>A classic example of workload orchestration<a id="_idIndexMarker2281"/> is a video-on-demand streaming service. With <a id="_idIndexMarker2282"/>a popular new TV show in high demand, the number of streaming requests would significantly exceed the average during a regular season. With <strong class="bold">Kubernetes</strong>, we could scale out the number of web servers based on the volume of streaming sessions. We could also control the possible scale-out of some of the middle-tier components, such as database instances (serving the authentication requests) and storage cache (serving the streams). When the TV show goes out of fashion, and the number of requests drops significantly, Kubernetes terminates the surplus instances, automatically reducing the application deployment’s footprint and, consequently, the underlying costs.</p>
<p>Here are some key benefits of deploying applications with Kubernetes:</p>
<ul>
<li><strong class="bold">Speedy deployment</strong>: Application<a id="_idIndexMarker2283"/> containers are created and launched relatively fast, using either a <strong class="bold">declarative</strong> or <strong class="bold">imperative</strong> configuration model (as we’ll see in the <em class="italic">Introducing the Kubernetes object model</em> section later in this chapter)</li>
<li><strong class="bold">Quick iterations</strong>: Application upgrades are relatively straightforward, with the underlying infrastructure simply seamlessly replacing the related container</li>
<li><strong class="bold">Rapid recovery</strong>: If an application crashes or becomes unavailable, Kubernetes automatically restores the application to the desired state by replacing the related container</li>
<li><strong class="bold">Reduced operation costs</strong>: The<a id="_idIndexMarker2284"/> containerized environment and infrastructure abstraction of Kubernetes yields minimal administration and maintenance efforts with relatively low resources for running applications</li>
</ul>
<p>Now that we have introduced Kubernetes, let’s look at its basic operating principles next.</p>
<h2 id="_idParaDest-328"><a id="_idTextAnchor346"/>Understanding the Kubernetes architecture</h2>
<p>There<a id="_idIndexMarker2285"/> are three major concepts at the core of the working model of Kubernetes:</p>
<ul>
<li><strong class="bold">Declarative configuration or desired state</strong>: This concept describes the overall application state and <a id="_idIndexMarker2286"/>microservices, deploying<a id="_idIndexMarker2287"/> the required containers and related<a id="_idIndexMarker2288"/> resources, including network, storage, and load balancers, to achieve a running functional state of the application</li>
<li><strong class="bold">Controllers or controller loops</strong>: This <a id="_idIndexMarker2289"/>monitors the desired state of the <a id="_idIndexMarker2290"/>system and takes<a id="_idIndexMarker2291"/> corrective action when needed, such as replacing a failed application container or adding additional resources for scale-out workloads</li>
<li><strong class="bold">API object model</strong>: This<a id="_idIndexMarker2292"/> model represents the actual <a id="_idIndexMarker2293"/>implementation of the desired state, using various configuration objects and the interaction – the <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) – between <a id="_idIndexMarker2294"/>these objects</li>
</ul>
<p>For a better grasp of the internals of Kubernetes, we need to take a closer look at the Kubernetes object model and related API. Also, you can check out <em class="italic">Figure 16</em><em class="italic">.1</em> for a visual explanation of the Kubernetes (cluster) architecture.</p>
<h2 id="_idParaDest-329"><a id="_idTextAnchor347"/>Introducing the Kubernetes object model</h2>
<p>The Kubernetes <a id="_idIndexMarker2295"/>architecture defines a collection of objects representing the desired state of a<a id="_idIndexMarker2296"/> system. An <strong class="bold">object</strong>, in this context, is a programmatic term to describe the behavior of a subsystem. Multiple objects interact with one another via the API, shaping the desired state over time. In other words, the Kubernetes object model is the programmatic representation of the desired state.</p>
<p>So, what are these objects in Kubernetes? We’ll briefly enumerate some of the more important ones and further elaborate on each in the following sections: API server, pods, controllers, services, and storage.</p>
<p>We use these API objects to configure the system’s state, using either a declarative or imperative model:</p>
<ul>
<li>With a <strong class="bold">declarative configuration model</strong>, we describe the state of the system, usually with a configuration file or <a id="_idIndexMarker2297"/>manifest (in YAML or JSON format). Such a configuration may include and deploy multiple API objects and regard the system as a whole.</li>
<li>The <strong class="bold">imperative configuration model</strong> uses individual commands to configure and deploy specific API objects, typically <a id="_idIndexMarker2298"/>acting on a single target or subsystem.</li>
</ul>
<p>Let’s look at the API server first – the central piece in the Kubernetes object model.</p>
<h3>Introducing the API server</h3>
<p>The API server<a id="_idIndexMarker2299"/> is the core<a id="_idIndexMarker2300"/> hub of the Kubernetes object model, acting as a management endpoint for the desired state of the system. The API server exposes an HTTP REST interface using JSON payloads. It is accessible in two ways:</p>
<ul>
<li><strong class="bold">Internally</strong>: It is accessed internally by other API objects</li>
<li><strong class="bold">Externally</strong>: It is accessed externally by configuration and management workflows</li>
</ul>
<p>The API server is essentially the gateway of interaction with the Kubernetes cluster, both from the outside and within. A Kubernetes cluster is a framework of different nodes that run containerized applications. The cluster is the basic running mode of Kubernetes itself. More on the Kubernetes cluster will be provided in the <em class="italic">Anatomy of a Kubernetes cluster</em> section. A system administrator connects to the API server endpoint to configure and manage a Kubernetes cluster, typically via a CLI. Internally, Kubernetes <a id="_idIndexMarker2301"/>API objects connect to the API server to provide an update of their state. In return, the API <a id="_idIndexMarker2302"/>server may further adjust the internal configuration of the API objects toward the desired state.</p>
<p>The API objects are the building blocks of the internal configuration or desired state of a Kubernetes cluster. Let’s look at a few of these API objects next.</p>
<h3>Introducing pods</h3>
<p>A <strong class="bold">pod</strong> represents <a id="_idIndexMarker2303"/>the basic working unit in Kubernetes, running as a single- or multi-container application. A <a id="_idIndexMarker2304"/>pod is also known as the <strong class="bold">unit of scheduling</strong> in Kubernetes. In other <a id="_idIndexMarker2305"/>words, containers within the same pod are guaranteed to be deployed together on the same cluster node.</p>
<p>A pod essentially represents a microservice (or a service) within the application’s service mesh. Considering the classic example of a web application, we may have the following pods running in the cluster:</p>
<ul>
<li>Web server (Nginx)</li>
<li>Authentication (Vault)</li>
<li>Database (PostgreSQL)</li>
<li>Storage (NAS)</li>
</ul>
<p>Each of these services (or applications) runs within their pod. Multiple pods of the same application (for example, a <a id="_idIndexMarker2306"/>web server) make up a <strong class="bold">ReplicaSet</strong>. We’ll look at ReplicaSets closer in the <em class="italic">Introducing </em><em class="italic">controllers</em> section.</p>
<p>Some essential features of pods are as follows:</p>
<ul>
<li>They have an <strong class="bold">ephemeral nature</strong>. Once a pod is terminated, it is gone for good. No pod ever gets redeployed in Kubernetes. Consequently, pods don’t persist in any state unless they use persistent storage or a local volume to save their data.</li>
<li>Pods are an <strong class="bold">atomic unit</strong> – they are <a id="_idIndexMarker2307"/>either deployed or not. For a single-container pod, atomicity is almost a given. For multi-container pods, atomicity means that a pod is<a id="_idIndexMarker2308"/> deployed only when each of the constituent containers is deployed. If any <a id="_idIndexMarker2309"/>of the containers fail to deploy, the pod will not be deployed, and hence there’s no pod. If one of the containers within a running multi-container pod fails, the whole pod is terminated.</li>
<li>Kubernetes uses <strong class="bold">probes</strong> (such as <a id="_idIndexMarker2310"/>liveliness and readiness) to monitor the <a id="_idIndexMarker2311"/>health of an application inside a pod. This is because a pod could be deployed and running, but that doesn’t necessarily mean the application or service within the pod is healthy. For example, a web server pod can have a probe that checks a specific URL and decides whether it’s healthy based on the response.</li>
</ul>
<p>Kubernetes tracks the state of pods using controllers. Let’s look at controllers next.</p>
<h3>Introducing controllers</h3>
<p><strong class="bold">Controllers</strong> in<a id="_idIndexMarker2312"/> Kubernetes are control loops responsible for keeping the system in the desired state or <a id="_idIndexMarker2313"/>bringing the system closer to the desired state by constantly watching the state of the cluster. For example, a controller may detect that a pod is not responding and request the deployment of a new pod while terminating the old one. Let’s look at two key types of controllers:</p>
<ul>
<li>A controller may also add or remove pods of a specific type to and from a collection of pod replicas. Such controllers are<a id="_idIndexMarker2314"/> called <strong class="bold">ReplicaSets</strong>, and their responsibility is to accommodate a particular number of pod replicas based on the current state of the application. For example, suppose an application requires three web server pods, and one of them becomes unavailable (due to a failed probe). In that case, the ReplicaSet controller ensures that the failed pod is deleted, and a new one takes its place.</li>
<li>When deploying applications in Kubernetes, we usually don’t use ReplicaSets directly to create pods. We<a id="_idIndexMarker2315"/> use the <code>v1</code>) with several pods, all running version 1 of our application, and we want to upgrade them to version 2. Remember, pods cannot be regenerated or upgraded. Instead, we’ll define a second ReplicaSet (<code>v2</code>), creating the version 2 pods. The Deployment controller will tear down the <code>v1</code> ReplicaSet and bring up <code>v2</code>. Kubernetes performs the rollout seamlessly, with minimal to no disruption of service. The Deployment controller manages the transition between the <code>v1</code> and <code>v2</code> ReplicaSets <a id="_idIndexMarker2316"/>and even<a id="_idIndexMarker2317"/> rolls back the transition if needed.</p></li>
</ul>
<p>There are many other controller types <a id="_idIndexMarker2318"/>in Kubernetes, and we encourage you to explore them at <a href="https://kubernetes.io/docs/concepts/workloads/controllers/">https://kubernetes.io/docs/concepts/workloads/controllers/</a>.</p>
<p>As applications scale-out or terminate, the related pods are deployed or removed. Services provide access to the dynamic and transient world of pods. We’ll look at Services next.</p>
<h3>Introducing Services</h3>
<p><strong class="bold">Services</strong> provide persistent <a id="_idIndexMarker2319"/>access to the applications running in pods. It is the Services’ responsibility to ensure that the pods are accessible by routing the traffic to the corresponding application endpoints. In other words, Services provide network abstraction for communicating with pods through IP addresses, routing, and DNS resolution. As<a id="_idIndexMarker2320"/> pods are deployed or terminated based on the system’s desired state, Kubernetes dynamically updates the Service endpoint of the pods, with minimal to no disruption in terms of accessing the related applications. As users and applications access the Service endpoint’s persistent IP address, the Service will ensure that the routing information is up to date and traffic is exclusively routed to the running and healthy pods. Services can also be leveraged to load-balance the application traffic between pods and scale pods up or down based on demand.</p>
<p>So far, we have looked at Kubernetes API objects controlling the deployment, access, and life cycle of application Services. What about the persistent data that applications require? We’ll look at the Kubernetes storage next.</p>
<h3>Introducing storage</h3>
<p>Kubernetes<a id="_idIndexMarker2321"/> provides various <a id="_idIndexMarker2322"/>storage types for applications running within the cluster. The most common<a id="_idIndexMarker2323"/> are <strong class="bold">volumes</strong> and <strong class="bold">persistent volumes</strong>. Due to the ephemeral nature of pods, application data stored within a pod using volumes is lost when the pod is terminated. Persistent volumes are<a id="_idIndexMarker2324"/> defined and managed at the Kubernetes cluster level, and they are independent of pods. Applications (pods) requiring a persistent state would reserve a persistent volume (of a specific size), using a <strong class="bold">persistent volume claim</strong>. When a <a id="_idIndexMarker2325"/>pod using a persistent volume terminates, the new pod replacing the old one retrieves the current state <a id="_idIndexMarker2326"/>from the persistent volume and will continue using the underlying storage.</p>
<p>For more information on <a id="_idIndexMarker2327"/>Kubernetes storage types, please refer to <a href="https://kubernetes.io/docs/concepts/storage/">https://kubernetes.io/docs/concepts/storage/</a>.</p>
<p>Now that we are familiar with the Kubernetes API object model, let’s quickly go through the architecture of a Kubernetes cluster.</p>
<h2 id="_idParaDest-330"><a id="_idTextAnchor348"/>The anatomy of a Kubernetes cluster</h2>
<p>A Kubernetes cluster consists of one <strong class="bold">Control Plane</strong> (<strong class="bold">CP</strong>) <strong class="bold">node</strong> and one or more <strong class="bold">worker nodes</strong>. The following diagram presents a high-level view of the Kubernetes architecture:</p>
<div><div><img alt="Figure 16.1 – Kubernetes cluster architecture" src="img/B19682_16_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.1 – Kubernetes cluster architecture</p>
<p>The components of a Kubernetes cluster that are shown in the preceding figure are divided into the worker node and CP node as the two major components. The Worker nodes have different components, such as the container runtime, kubelet, and kube-proxy, and the CP node has the API server, the controller manager, and the scheduler. All of these components will be discussed in detail in the following sections.</p>
<p>First, let’s look at the Kubernetes cluster nodes shown in the preceding image in some detail next, starting with the CP node.</p>
<h3>Introducing the Kubernetes CP</h3>
<p>The <strong class="bold">Kubernetes CP</strong> provides <a id="_idIndexMarker2328"/>essential Services for deploying and orchestrating application workloads, and it runs on a dedicated node in the Kubernetes cluster – the CP node. This node, also known as the <strong class="bold">master node</strong>, implements the core components of a<a id="_idIndexMarker2329"/> Kubernetes cluster, such as resource scheduling and monitoring. It’s also the primary access point for cluster administration. Here are the key subsystems of a CP node:</p>
<ul>
<li><strong class="bold">API server</strong>: The<a id="_idIndexMarker2330"/> central communication hub between Kubernetes API objects; it also provides the cluster’s management endpoint accessible either via CLI or the Kubernetes web administration console (dashboard)</li>
<li><strong class="bold">Scheduler</strong>: This decides <a id="_idIndexMarker2331"/>when and which nodes to deploy the pods on, depending on resource allocation and administrative policies</li>
<li><strong class="bold">Controller manager</strong>: This maintains<a id="_idIndexMarker2332"/> the control loops, monitoring and shaping the desired state of the system</li>
<li><strong class="bold">etcd</strong>: Also known as <a id="_idIndexMarker2333"/>the <strong class="bold">cluster store</strong>, this is a highly available persisted database, maintaining the<a id="_idIndexMarker2334"/> state of the Kubernetes cluster and related API objects; the information in etcd is stored as key-value pairs</li>
<li><code>kubectl</code>: The <a id="_idIndexMarker2335"/>primary administrative CLI for managing and interacting with the Kubernetes cluster; <code>kubectl</code> communicates directly with the API server, and it may connect remotely to a cluster</li>
</ul>
<p>A detailed architectural overview of the Kubernetes CP<a id="_idIndexMarker2336"/> is beyond the scope of this chapter. You may explore the related concepts in more detail at <a href="https://kubernetes.io/docs/concepts/architecture/">https://kubernetes.io/docs/concepts/architecture/</a>.</p>
<p>Next, let’s take a brief look at the Kubernetes node – the workhorse of a Kubernetes cluster.</p>
<h3>Introducing the Kubernetes nodes</h3>
<p>In a <a id="_idIndexMarker2337"/>Kubernetes cluster, the <strong class="bold">nodes</strong> – also referred to as <strong class="bold">worker nodes</strong> – run the <a id="_idIndexMarker2338"/>actual application pods and<a id="_idIndexMarker2339"/> maintain their full life cycle. Nodes provide the compute capacity of Kubernetes and ensure that the workloads are uniformly distributed across the cluster when deploying and running pods. Nodes can be configured either as physical (bare metal) or VMs.</p>
<p>Let’s enumerate the key elements of a Kubernetes node:</p>
<ul>
<li><strong class="bold">Kubelet</strong>: This processes CP requests (from the scheduler) to deploy and start application pods; the kubelet<a id="_idIndexMarker2340"/> also monitors the node and pod state, reporting the related changes to the API server</li>
<li><strong class="bold">Kube-Proxy</strong>: Dynamically <a id="_idIndexMarker2341"/>configures the virtual networking environment for the applications running in the pods; it routes the network traffic, provides load balancing, and maintains the IP addresses of Services and pods</li>
<li><code>containerd</code> and Docker)</li>
</ul>
<p>All the preceding Services run on <em class="italic">each</em> node in the Kubernetes cluster, including the CP node. These components in the CP are required by special-purpose pods, providing specific CP Services, such as DNS, ingress (load balancing), and dashboard (web console).</p>
<p>For more information on Kubernetes nodes<a id="_idIndexMarker2346"/> and related architectural concepts, please visit <a href="https://kubernetes.io/docs/concepts/architecture/nodes/">https://kubernetes.io/docs/concepts/architecture/nodes/</a>.</p>
<p>Now that we have become familiar with some of the key concepts and cluster components, let’s get ready to install and configure Kubernetes.</p>
<h1 id="_idParaDest-331"><a id="_idTextAnchor349"/>Installing and configuring Kubernetes</h1>
<p>Before installing or using Kubernetes, you have to decide on the infrastructure you’ll use, whether that be on-premises or public cloud. Second, you’ll have to choose between an <strong class="bold">Infrastructure-as-a-Service</strong> (<strong class="bold">IaaS</strong>) or a <strong class="bold">Platform-as-a-Service</strong> (<strong class="bold">PaaS</strong>) model. With IaaS, you’ll have to<a id="_idIndexMarker2347"/> install, configure, manage, and maintain the Kubernetes cluster yourself, either on <a id="_idIndexMarker2348"/>physical (bare metal) or VMs. The related operation efforts are not<a id="_idIndexMarker2349"/> straightforward and should be considered carefully. If you choose<a id="_idIndexMarker2350"/> a PaaS solution, available from all major public cloud providers, you’ll be limited to only administrative tasks but saved from the burden of maintaining the underlying infrastructure.</p>
<p>In this chapter, we’ll cover only IaaS deployments of Kubernetes. For IaaS, we’ll use a local desktop environment running Ubuntu VMs.</p>
<p>For the on-premises installation, we may also choose between a lightweight desktop version of Kubernetes or a full-blown cluster with multiple nodes. Let’s look at some of the most common desktop versions of Kubernetes next.</p>
<h2 id="_idParaDest-332"><a id="_idTextAnchor350"/>Installing Kubernetes on a desktop</h2>
<p>If you’re <a id="_idIndexMarker2351"/>looking only to experiment with Kubernetes, a desktop <a id="_idIndexMarker2352"/>version may fit the bill. Desktop versions of Kubernetes usually deploy a single-node cluster on your local machine. Depending on your platform of choice, whether it be Windows, macOS, or Linux, you have plenty of Kubernetes engines to select from. Here are just a few:</p>
<ul>
<li><strong class="bold">Docker Desktop (macOS, </strong><strong class="bold">Windows)</strong>: <a href="https://www.docker.com/products/docker-desktop">https://www.docker.com/products/docker-desktop</a></li>
<li><strong class="bold">minikube (Linux, macOS, </strong><strong class="bold">Windows)</strong>: <a href="https://minikube.sigs.k8s.io/docs/">https://minikube.sigs.k8s.io/docs/</a></li>
<li><strong class="bold">Microk8s (Linux, macOS, </strong><strong class="bold">Windows)</strong>: <a href="https://microk8s.io/">https://microk8s.io/</a></li>
<li><strong class="bold">k3s (</strong><strong class="bold">Linux)</strong>: <a href="https://k3s.io/">https://k3s.io/</a></li>
</ul>
<p>In this section, we’ll show you how to install Microk8s, one of the trending Kubernetes desktop engines at the time of writing. Microk8s is available to install via the Snap Store. We will use Debian 12 as the base operating system on our test computer, so we can install Microk8s from the Snap Store. If you don’t have <code>snapd</code> installed, you will have to first proceed and install it. You will have to use the following command to install the Snap daemon:</p>
<pre class="console">
sudo apt install snapd</pre> <p>If you already have <code>snapd</code> installed, you can skip this first step. The following step will be to run the command to install the <code>snapd</code> core runtime environment needed to run the Snap Store:</p>
<pre class="console">
sudo snap install core</pre> <p>Only after <code>snap</code> is installed, you <a id="_idIndexMarker2353"/>can install <code>microk8s</code> by using the <a id="_idIndexMarker2354"/>following command:</p>
<pre class="console">
sudo snap install microk8s --classic</pre> <p>A successful installation of Microk8s should yield the following result:</p>
<div><div><img alt="Figure 16.2 – Installing Microk8s on Linux" src="img/B19682_16_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.2 – Installing Microk8s on Linux</p>
<p>As we already had <code>snapd</code> installed, we did not run the first of the commands listed previously.</p>
<p>To access the Microk8s CLI without <code>sudo</code> permissions, you’ll have to add the local user account to the <code>microk8s</code> group and also fix the permissions on the <code>~/.kube</code> directory with the following commands:</p>
<pre class="console">
sudo usermod -aG microk8s $USER
sudo chown -f -R $USER ~/.kube</pre> <p>The changes will take effect on the next login, and you can use the <code>microk8s</code> command-line utility with invocations that are not <code>sudo</code>. For example, the following command displays the help for the tool:</p>
<pre class="console">
microk8s help</pre> <p>To get the status of the local single-node Microk8s Kubernetes cluster, we run the following command:</p>
<pre class="console">
microk8s status</pre> <p>As you can see, the installation steps of Microk8s on Debian are straightforward and similar to the ones used in Ubuntu.</p>
<p>In the next section, we will<a id="_idIndexMarker2355"/> show you how to install Microk8s on a VM. This<a id="_idIndexMarker2356"/> time, we will use Ubuntu 22.04 LTS as the base operating system for the VM.</p>
<h2 id="_idParaDest-333"><a id="_idTextAnchor351"/>Installing Kubernetes on VMs</h2>
<p>In this section, we’ll get <a id="_idIndexMarker2357"/>closer to a real-world Kubernetes<a id="_idIndexMarker2358"/> environment – though at a much smaller scale – by deploying a Kubernetes cluster on Ubuntu VMs. You can use any hypervisor, such as KVM, Oracle VirtualBox, or VMware Fusion. We will use KVM as our preferred hypervisor.</p>
<p>We will create four VMs, and we’ll provision each VM with 2 vCPU cores, 2 GB RAM, and 20 GB disk capacity. You may follow the steps described in the <em class="italic">Installing Ubuntu</em> section of <a href="B19682_01.xhtml#_idTextAnchor030"><em class="italic">Chapter 1</em></a>, <em class="italic">Installing Linux</em>, using your hypervisor of choice.</p>
<p>Before we dive into the Kubernetes cluster installation details, let’s take a quick look at our lab environment.</p>
<h3>Preparing the lab environment</h3>
<p>Here are the specs of our VM environment:</p>
<ul>
<li><strong class="bold">Hypervisor</strong>: VMware <a id="_idIndexMarker2359"/>Fusion</li>
<li><strong class="bold">Kubernetes cluster</strong>: One CP node and three worker nodes</li>
<li><code>k8s-cp1</code>: <code>192.168.122.104</code></li></ul></li>
<li><code>k8s-n1</code>: <code>192.168.122.146</code></li><li><code>k8s-n2</code>: <code>192.168.122.233</code></li><li><code>k8s-n3</code>: <code>192.168.122.163</code></li></ul></li>
<li><strong class="bold">VMs</strong>: Ubuntu Server 22.04.3 LTS, 2 vCPUs, 2 GB RAM, 20 GB disk</li>
<li><code>packt</code> (on all nodes), with SSH access enabled</li>
</ul>
<p>We set the username and hostname settings on each VM node on the Ubuntu Server<a id="_idIndexMarker2360"/> installation wizard. Also, make sure to enable the OpenSSH server when prompted. Your VM IP addresses would most probably be different from those in the specs, but that shouldn’t matter. You may also choose to use static IP addresses for your VMs.</p>
<p>To make hostname resolution simple within the cluster, edit the <code>/etc/hosts</code> file on each node and add the related records. For example, we have the following <code>/etc/hosts</code> file on the CP node (<code>k8s-cp1</code>):</p>
<div><div><img alt="Figure 16.3 – The /etc/hosts file on the CP node (k8s-cp1)" src="img/B19682_16_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.3 – The /etc/hosts file on the CP node (k8s-cp1)</p>
<p>In production environments, with the firewall enabled on the cluster nodes, we have to make sure that the following rules are configured for accepting network traffic within the cluster (according to <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/):">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/):</a></p>
<div><div><img alt="Figure 16.4 – The ports used by the Kubernetes cluster nodes" src="img/B19682_16_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.4 – The ports used by the Kubernetes cluster nodes</p>
<p>The following sections assume that you have the VMs provisioned and running <a id="_idIndexMarker2361"/>according to the preceding specs. You may take some initial snapshots of your VMs before proceeding with the next steps. If anything goes wrong with the installation, you can revert to the initial state and start again.</p>
<p>Here are the steps we’ll follow to install the Kubernetes cluster:</p>
<ol>
<li>Disable swapping.</li>
<li>Install <code>containerd.</code></li>
<li>Install the <code>kubelet</code>, <code>kubeadm</code>, and Kubernetes packages.</li>
</ol>
<p>We’ll have to perform these steps on each cluster node. The related commands are also captured in the accompanying chapter source code on GitHub.</p>
<p>Let’s start with the first step and disable the memory swap on each node.</p>
<h3>Disable swapping</h3>
<p><code>swap</code> is a disk space used when the memory is full (refer to <a href="https://github.com/kubernetes/kubernetes/issues/53533">https://github.com/kubernetes/kubernetes/issues/53533</a> for more details). The<a id="_idIndexMarker2362"/> Kubernetes kubelet package doesn’t work with <code>swap</code> enabled on Linux platforms. This means that we will have to disable <code>swap</code> on all the nodes.</p>
<p>To disable <code>swap</code> immediately, we run the <a id="_idIndexMarker2363"/>following command on each VM:</p>
<pre class="console">
sudo swapoff -a</pre> <p>To persist the disabled <code>swap</code> with system reboots, we need to comment out the <code>swap</code>-related entries in <code>/etc/fstab</code>. You can do this either manually, by editing <code>/etc/fstab</code>, or with the following command:</p>
<pre class="console">
sudo sed -i '/\s*swap\s*/s/^\(.*\)$/# \1/g' /etc/fstab</pre> <p>You may want to double-check that all <code>swap</code> entries in <code>/etc/fstab</code> are disabled:</p>
<pre class="console">
cat /etc/fstab</pre> <p>We can see the <code>swap</code> mount point commented out in our <code>/</code><code>etc/fstab</code> file:</p>
<div><div><img alt="Figure 16.5 – Disabling swap entries in /etc/fstab" src="img/B19682_16_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.5 – Disabling swap entries in /etc/fstab</p>
<p>Remember to run the preceding commands on each node in the cluster. Next, we’ll look at installing the Kubernetes container runtime.</p>
<h3>Installing containerd</h3>
<p><code>containerd</code> is the<a id="_idIndexMarker2364"/> default container runtime in recent versions of Kubernetes. <code>containerd</code> implements the <a id="_idIndexMarker2365"/>CRI required<a id="_idIndexMarker2366"/> by the Kubernetes container engine abstraction layer. The related installation procedure is not straightforward, and we’ll follow the steps described in the official Kubernetes documentation in the following link at the time of this writing: <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">https://kubernetes.io/docs/setup/production-environment/container-runtimes/</a>. These steps may change at any time, so please make sure to check the latest procedure. The container runtime needs to be installed on each node of the cluster. We will proceed by installing the needed components on the <a id="_idIndexMarker2367"/>CP node, the VM called <code>k8s-cp1</code>, and then on the other nodes as well.</p>
<p>We’ll start by installing some <code>containerd</code> prerequisites:</p>
<ol>
<li>First, we enable the <code>br_netfilter</code> and <code>overlay</code> kernel modules using <code>modprobe</code>:<pre class="source-code">
<strong class="bold">sudo modprobe br_netfilter</strong>
<strong class="bold">sudo modprobe overlay</strong></pre></li> <li>We also ensure that these modules are loaded upon system reboots:<pre class="source-code">
<strong class="bold">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf</strong>
<strong class="bold">br_netfilter</strong>
<strong class="bold">overlay</strong>
<code>sysctl</code> parameters, also persisted across system reboots:<pre class="source-code">
<strong class="bold">cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/containerd.conf</strong>
<strong class="bold">net.bridge.bridge-nf-call-iptables = 1</strong>
<strong class="bold">net.bridge.bridge-nf-call-ip6tables = 1</strong>
<strong class="bold">net.ipv4.ip_forward = 1</strong>
<strong class="bold">EOF</strong></pre></li> <li>We want the preceding changes to take effect immediately, without a system reboot:<pre class="source-code">
sudo sysctl --system</pre><p class="list-inset">Here is a screenshot showing the preceding commands:</p></li> </ol>
<p class="IMG---Figure"/>
<div><div><img alt="Figure 16.6 – Setting containerd prerequisites" src="img/B19682_16_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.6 – Setting containerd prerequisites</p>
<ol>
<li value="5">Next, we will verify if specific system variables are enabled in the <code>sysctl</code> configuration by running the<a id="_idIndexMarker2368"/> following command:<pre class="source-code">
<strong class="bold">sudo sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward</strong></pre><p class="list-inset">The output of the command should be as follows:</p></li> </ol>
<div><div><img alt="Figure 16.7 – Verifying system variables in sysctl configuration" src="img/B19682_16_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.7 – Verifying system variables in sysctl configuration</p>
<p class="list-inset">Each variable should have a value of <code>1</code> in the output, just as shown in the previous screenshot.</p>
<ol>
<li value="6">Now, let’s make sure the <code>apt</code> repository is up to date before installing any new packages:<pre class="source-code">
<code>containerd</code>:<pre class="source-code">
<code>containerd</code> configuration:<pre class="source-code">
<strong class="bold">sudo mkdir -p /etc/containerd</strong>
<code>config.toml</code> inside it. The output of the command is too large to show, but it will show you on <a id="_idIndexMarker2369"/>the screen the automatically generated contents of the new file we created.</p></li> <li>We need to slightly<a id="_idIndexMarker2370"/> alter the default <code>containerd</code> configuration to use the <code>systemd</code> <code>cgroup</code> driver with the container runtime (<code>runc</code>). This change is required because the underlying platform (Ubuntu in our case) uses <code>systemd</code> as the Service manager. Open the <code>/etc/containerd/config.toml</code> file with your editor of choice, such as the following:<pre class="source-code">
<code>[plugins]</code> section of the file):<pre class="source-code">
<strong class="bold">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]</strong></pre></li> <li>Then, add the highlighted lines, adjusting the appropriate indentation (this is <em class="italic">very</em> important):<pre class="source-code">
<strong class="bold">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]</strong>
<strong class="bold">  ...</strong>
<strong class="bold">  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]</strong>
<strong class="bold">  SystemdCgroup = true</strong></pre><p class="list-inset">Here’s the resulting configuration stub:</p></li> </ol>
<div><div><img alt="Figure 16.8 – Modifying the containerd configuration" src="img/B19682_16_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.8 – Modifying the containerd configuration</p>
<ol>
<li value="12">Save the <code>/etc/containerd/config.toml</code> file and restart <code>containerd</code>:<pre class="source-code">
<code>containerd</code> Service, by using the following command:<pre class="source-code">
<strong class="bold">sudo systemctl status containerd</strong></pre><p class="list-inset">The output should show that the system is running and there are no issues.</p></li> </ol>
<p>With <code>containerd</code> installed<a id="_idIndexMarker2372"/> and configured, we can proceed with the installation of the<a id="_idIndexMarker2373"/> Kubernetes packages next.</p>
<h3>Installing Kubernetes packages</h3>
<p>To install the Kubernetes packages, we’ll follow the <a id="_idIndexMarker2374"/>steps described at <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a>. This procedure may also change over time, so please make sure to<a id="_idIndexMarker2375"/> check out the latest instructions. The steps presented next are applicable to Debian 12 and Ubuntu 22.04. Let’s begin:</p>
<ol>
<li>We first install the packages required by the Kubernetes <code>apt</code> repository:<pre class="source-code">
<code>apt</code> repository <strong class="bold">GNU Privacy Guard</strong> (<strong class="bold">GPG</strong>) public <a id="_idIndexMarker2377"/>signing key (for the latest Kubernetes 1.28 at the time of writing):<pre class="source-code">
<code>apt</code> repository to our system:<pre class="source-code">
<strong class="bold">echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list</strong></pre></li> <li>Let’s read the packages available in the new repository we just added:<pre class="source-code">
<strong class="bold">sudo apt update -y</strong></pre></li> <li>We’re now ready to install the Kubernetes packages:<pre class="source-code">
<code>apt-mark hold</code> command to pin the version of the Kubernetes packages, including <code>containerd</code>:<pre class="source-code">
<code>containerd</code> and <code>kubelet</code> Services are enabled upon system startup (reboot):<pre class="source-code">
<strong class="bold">sudo systemctl enable containerd</strong>
<code>containerd</code> Service first:<pre class="source-code">
<code>containerd</code> should be active and running. The following is a screenshot showing the output of the preceding commands:</p></li> </ol>
<div><div><img alt="Figure 16.9 – Pinning the Kubernetes packages and the running status of containerd" src="img/B19682_16_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.9 – Pinning the Kubernetes packages and the running status of containerd</p>
<ol>
<li value="9">Next, let’s check the status of the <code>kubelet</code> Service:<pre class="source-code">
<code>exited</code>:</p></li> </ol>
<div><div><img alt="Figure 16.10 – The kubelet crashing without cluster configuration" src="img/B19682_16_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.10 – The kubelet crashing without cluster configuration</p>
<p>As shown in the preceding screenshot, <code>kubelet</code> is looking for the <a id="_idIndexMarker2381"/>Kubernetes cluster, which is not set up yet. We can see that <code>kubelet</code> attempts to start and activate itself but keeps crashing, as it cannot locate the required configuration.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Please install the required Kubernetes packages on <em class="italic">all</em> cluster nodes following the previous steps before proceeding with the next section.</p>
<p>Next, we’ll bootstrap (initialize) the Kubernetes cluster using <code>kubeadm</code>.</p>
<h3>Introducing kubeadm</h3>
<p><code>kubeadm</code> is a helper<a id="_idIndexMarker2382"/> tool for<a id="_idIndexMarker2383"/> creating a Kubernetes cluster and essentially has two invocations:</p>
<ul>
<li><code>kubeadm init</code>: This bootstraps or initializes a Kubernetes cluster</li>
<li><code>kubeadm join</code>: This adds a node to a Kubernetes cluster</li>
</ul>
<p>The default invocation of <code>kubadm init [flags]</code> – with no flags – performs the following tasks:</p>
<ol>
<li><code>kubeadm init</code> ensures that we have the minimum system resources in terms of CPU and memory, the required user permissions, and a supported CRI-compliant container runtime. If any of these checks fail, <code>kubeadm init</code> stops the execution of creating the cluster. If the <a id="_idIndexMarker2384"/>checks succeed, <code>kubeadm</code> proceeds to the next step.</li>
<li><code>kubeadm init</code> creates a<a id="_idIndexMarker2385"/> self-signed CA used by Kubernetes to generate the certificates required to authenticate and run trusted workloads within the cluster. The CA files are stored in the <code>/etc/kubernetes/pki</code>/ directory and are distributed on each node upon joining the cluster.</li>
<li><code>kubeadm init</code> creates a default set of kubeconfig files required to bootstrap the cluster. The kubeconfig files are stored in the <code>/</code><code>etc/kubernetes/</code> directory.</li>
<li><code>kubelet</code> daemon. Examples of static pods are the API server, the controller manager, scheduler, and etcd. Static pod manifests are configuration files describing the CP pods. <code>kubeadm init</code> generates the static pod manifests during the cluster bootstrapping process. The manifest files are stored in the <code>/etc/kubernetes/manifests/</code> directory. The <code>kubelet</code> Service monitors this location and, when it finds a manifest, deploys the corresponding static pod.</li>
<li><code>kubelet</code> daemon deploys the static pods, <code>kubeadm</code> queries <code>kubelet</code> for the static pods’ state. When the static pods are up and running, <code>kubeadm init</code> proceeds with the next stage.</li>
<li><code>kubeadm init</code> follows the Kubernetes best practice of<a id="_idIndexMarker2392"/> tainting the CP to avoid user pods running on the CP node. The obvious reason is to preserve CP resources exclusively for system-specific workloads.</li>
<li><code>kubeadm init</code> generates a bootstrap token that can be shared with a trusted node to join the cluster.</li>
<li><code>kubeadm init</code> creates and deploys the <em class="italic">DNS</em> and <em class="italic">kube-proxy</em> add-on pods.</li>
</ol>
<p>The stages of a Kubernetes cluster’s bootstrapping process are highly customizable. <code>kubeadm init</code>, when invoked without additional parameters, runs all the tasks in the preceding order. Alternatively, a system administrator may invoke the <code>kubeadm</code> command with different option parameters to control and run any of the stages mentioned.</p>
<p>For more information about <code>kubeadm</code>, please refer to the utility’s help with the following command:</p>
<pre class="console">
kubeadm help</pre> <p>For more information about bootstrapping a Kubernetes cluster using <code>kubeadm</code>, including installing, troubleshooting, and customizing components, you may refer to the official Kubernetes documentation at <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/</a>.</p>
<p>In the next section, we’ll bootstrap a Kubernetes cluster using <code>kubeadm</code> to generate a cluster configuration file and then invoke <code>kubeadm init</code> to use this configuration. We’ll bootstrap our cluster by creating<a id="_idIndexMarker2396"/> the Kubernetes CP node next.</p>
<h3>Creating a Kubernetes CP node</h3>
<p>In order to<a id="_idIndexMarker2397"/> create the CP node, we will use a networking and security solution named <code>kubeadm</code>, and afterwards, we will use different tools to apply the <a id="_idIndexMarker2398"/>configurations. The choice of Calico is purely subjective, but it is needed for managing communications between workloads and components. For more information about Calico, please visit the following link: <a href="https://docs.tigera.io/calico/latest/about/">https://docs.tigera.io/calico/latest/about/</a>.</p>
<p>The commands are performed on the <code>k8s-cp1</code> host in our VM environment. As the hostname suggests, we choose <code>k8s-cp1</code> as the CP node of our Kubernetes cluster. Now, let’s get to work and configure our Kubernetes CP node:</p>
<ol>
<li>We’ll start by downloading the Calico manifest for <strong class="bold">overlay networking</strong>. The overlay network – also known as <strong class="bold">software-defined network</strong> (<strong class="bold">SDN</strong>) – is a logical networking layer that accommodates a secure and <a id="_idIndexMarker2399"/>seamless network communication between the pods over a physical network that may not be accessible for configuration. Exploring the internals of cluster networking is beyond the scope of <a id="_idIndexMarker2400"/>this chapter, but we encourage you to read more at <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">https://kubernetes.io/docs/concepts/cluster-administration/networking/</a>. You’ll also find references to the Calico networking add-on. To download the related manifest, we run the following command:<pre class="source-code">
<code>calico.yaml</code> file in the current directory (<code>/home/packt/</code>) that we’ll use with <code>kubectl</code> to configure pod networking later in the process.</p></li> <li>Next, let’s open the <code>calico.yaml</code> file using a text editor and look for the following lines (starting with <em class="italic">line 3672</em>):<pre class="source-code">
<strong class="bold"># - name: CALICO_IPV4POOL_CIDR</strong>
<code>CALICO_IPV4POOL_CIDR</code> points to the network range associated with the pods. If the related subnet conflicts in any way with your local environment, you’ll have to change it here. We’ll leave the setting as is.</p></li> <li>Next, we’ll create a <a id="_idIndexMarker2401"/>default cluster configuration file using <code>kubeadm</code>. The cluster configuration file describes the settings of the Kubernetes cluster we’re building. Let’s name this file <code>k8s-config.yaml</code>:<pre class="source-code">
<code>k8s-config.yaml</code> file we just generated and mention a few changes that we’ll have to make. We will open it using the <code>localAPIEndpoint.advertiseAddress</code> configuration parameter – the IP address of the API server endpoint. The default value is <code>1.2.3.4</code>, and we need to change it to the IP address of the VM running the CP node (<code>k8s-cp1</code>), in our case, <code>192.168.122.104</code>. Refer to the <em class="italic">Preparing the lab environment</em> section earlier in this chapter. You’ll have to enter the IP address matching your environment:</li>
</ol>
<div><div><img alt="Figure 16.11 – Modifying the advertiseAddress configuration parameter" src="img/B19682_16_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.11 – Modifying the advertiseAddress configuration parameter</p>
<ol>
<li value="5">The next change we need to make is pointing the <code>nodeRegistration.criSocket</code> configuration <a id="_idIndexMarker2403"/>parameter to the <code>containerd</code> socket <code>(/run/containerd/containerd.sock</code>) and the name (<code>k8s-cp1</code>):</li>
</ol>
<div><div><img alt="Figure 16.12 – Changing the criSocket configuration parameter" src="img/B19682_16_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.12 – Changing the criSocket configuration parameter</p>
<ol>
<li value="6">Next, we change the <code>kubernetesVersion</code> parameter to match the version of our Kubernetes environment:</li>
</ol>
<div><div><img alt="Figure 16.13 – Changing the kubernetesVersion parameter" src="img/B19682_16_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.13 – Changing the kubernetesVersion parameter</p>
<p class="list-inset">The default value is <code>1.28.0</code>, but <a id="_idIndexMarker2404"/>our Kubernetes version, using the following command, is <code>1.28.2</code>:</p>
<pre class="source-code">
<strong class="bold">kubeadm version</strong></pre> <p class="list-inset">The output is as follows:</p>
<div><div><img alt="Figure 16.14 – Retrieving the current version of Kubernetes" src="img/B19682_16_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.14 – Retrieving the current version of Kubernetes</p>
<ol>
<li value="7">Our final modification of the cluster configuration file sets the <code>cgroup</code> driver of <code>kubelet</code> to <code>systemd</code>, matching the <code>cgroup</code> driver of <code>containerd</code>. Please note that <code>systemd</code> is the underlying platform’s Service manager (in Ubuntu), hence the need to yield related Service control to the Kubernetes daemons. The corresponding configuration block is not yet present in <code>k8s-config.yaml</code>. We can add it manually to the end of the file or<a id="_idIndexMarker2405"/> with the<a id="_idIndexMarker2406"/> following command:<pre class="source-code">
<strong class="bold">cat &lt;&lt;EOF | cat &gt;&gt; k8s-config.yaml</strong>
<strong class="bold">---</strong>
<strong class="bold">apiVersion: kubelet.config.k8s.io/v1beta1</strong>
<strong class="bold">kind: KubeletConfiguration</strong>
<strong class="bold">cgroupDriver: systemd</strong>
<code>kubeadm init</code> command with the <code>--config</code> option pointing to the cluster configuration file (<code>k8s-config.yaml</code>), and with the <code>--cri-socket</code> option parameter pointing to the <code>containerd</code> socket:<pre class="source-code">
<strong class="bold">sudo kubeadm init --config=k8s-config.yaml</strong></pre><p class="list-inset">The preceding command takes a couple of minutes to run. A successful bootstrap of the Kubernetes cluster <a id="_idIndexMarker2407"/>completes with the following output:</p></li> </ol>
<div><div><img alt="Figure 16.15 – Successfully bootstrapping the Kubernetes cluster" src="img/B19682_16_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.15 – Successfully bootstrapping the Kubernetes cluster</p>
<p class="list-inset">At this point, our Kubernetes<a id="_idIndexMarker2408"/> CP node is up and running. In the output, we highlighted the relevant excerpts for the following commands:</p>
<ul>
<li>A successful message (<strong class="bold">1</strong>)</li>
<li>Configuring the current user as the Kubernetes cluster administrator (<strong class="bold">2</strong>)</li>
<li>Joining new nodes to the Kubernetes cluster (<strong class="bold">3</strong>)</li>
</ul>
<p class="list-inset">We recommend taking the time to go over the complete output and identify the related information for each of the <code>kubeadm init</code> tasks, as captured in the <em class="italic">Introducing kubeadm</em> section earlier in this chapter.</p>
<ol>
<li value="9">Next, to configure the current user as the Kubernetes cluster administrator, we run the following commands:<pre class="source-code">
<strong class="bold">mkdir -p ~/.kube</strong>
<strong class="bold">sudo cp -i /etc/kubernetes/admin.conf ~/.kube/config</strong>
<strong class="bold">sudo chown $(id -u):$(id -g) ~/.kube/config</strong></pre></li> <li>With our cluster up and running, let’s deploy the Calico networking manifest to create the pod network:<pre class="source-code">
<code>kubectl</code> command to list all the pods in the system:<pre class="source-code">
<strong class="bold">kubectl get pods --all-namespaces</strong></pre><p class="list-inset">The command yields the following output:</p></li> </ol>
<div><div><img alt="Figure 16.16 – Retrieving the pods in the Kubernetes cluster" src="img/B19682_16_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.16 – Retrieving the pods in the Kubernetes cluster</p>
<p class="list-inset">The <code>--all-namespaces</code> option retrieves the pods across all resource groups in the cluster. Kubernetes uses <strong class="bold">namespaces</strong> to organize resources. For now, the only pods running in our cluster are <strong class="bold">system pods</strong>, as we haven’t deployed any <strong class="bold">user </strong><strong class="bold">pods</strong> yet.</p>
<ol>
<li value="12">The following command retrieves the current nodes in the cluster:<pre class="source-code">
<code>k8s-cp1</code> as the only node configured in the Kubernetes cluster, running as a CP node:</p></li> </ol>
<div><div><img alt="Figure 16.17 – Listing the current nodes in the Kubernetes cluster" src="img/B19682_16_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.17 – Listing the current nodes in the Kubernetes cluster</p>
<ol>
<li value="13">You may recall that <a id="_idIndexMarker2411"/>prior to bootstrapping the Kubernetes cluster, the <code>kubelet</code> Service was continually crashing (and attempting to restart). With the cluster up and running, the status of the <code>kubelet</code> daemon<a id="_idIndexMarker2412"/> should be <code>active</code> and <code>running</code>:<pre class="source-code">
<strong class="bold">sudo systemctl status kubelet</strong></pre><p class="list-inset">The output shows the following:</p></li> </ol>
<div><div><img alt="Figure 16.18 – A healthy kubelet in the cluster" src="img/B19682_16_18.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.18 – A healthy kubelet in the cluster</p>
<ol>
<li value="14">We encourage you to check out the manifests created in the <code>/etc/kubernetes/manifests/</code> directory for each cluster component using the following command:<pre class="source-code">
<strong class="bold">ls /etc/kubernetes/manifests/</strong></pre><p class="list-inset">The output shows the configuration files describing the static (system) pods, corresponding to the API server, controller manager, scheduler, and etcd:</p></li> </ol>
<div><div><img alt="Figure 16.19 – The static pod configuration files in /etc/kubernetes/manifests/" src="img/B19682_16_19.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.19 – The static pod configuration files in /etc/kubernetes/manifests/</p>
<ol>
<li value="15">You may also look at the kubeconfig files in <code>/etc/kubernetes</code>:<pre class="source-code">
<strong class="bold">ls /etc/kubernetes/</strong></pre><p class="list-inset">As you may recall from the <em class="italic">Introducing kubeadm</em> section earlier in this chapter, the kubeconfig files are used by the cluster components to communicate and authenticate with the API server.</p></li> </ol>
<p>As we have used the <code>kubectl</code> utility <a id="_idIndexMarker2413"/>quite extensively in this section, you can visit the official documentation to find out more about the commands and options available for it at the following link: <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands">https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands</a>.</p>
<p>Next, let’s add the worker nodes to our Kubernetes cluster.</p>
<h3>Joining a node to a Kubernetes cluster</h3>
<p>As previously noted, before <a id="_idIndexMarker2414"/>adding a node to the Kubernetes cluster, you’ll need to<a id="_idIndexMarker2415"/> run the preliminary steps described in the <em class="italic">Preparing the lab environment</em> section earlier in this chapter.</p>
<p>To join a node to the cluster, we’ll need both the <code>kubeadm join</code> command were provided in the output at the end of the bootstrapping process with <code>kubeadm init</code>. Refer to the <em class="italic">Creating a Kubernetes CP node</em> section earlier in this chapter. Keep in mind that the bootstrap token expires in 24 hours. If you forget to copy the command, you can retrieve the related information by running the following commands in the CP node’s terminal (on <code>k8s-cp1</code>).</p>
<p>To proceed, follow these steps:</p>
<ol>
<li>Retrieve the current bootstrap tokens:<pre class="source-code">
<code>abcdef.0123456789abcdef</code>):</p></li> </ol>
<div><div><img alt="Figure 16.20 – Getting the current bootstrap tokens" src="img/B19682_16_20.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.20 – Getting the current bootstrap tokens</p>
<ol>
<li value="2">Get the CA certificate hash:<pre class="source-code">
<strong class="bold">openssl x509 -pubkey \</strong>
<strong class="bold">    -in /etc/kubernetes/pki/ca.crt | \</strong>
<strong class="bold">    openssl rsa -pubin -outform der 2&gt;/dev/null | \</strong>
<strong class="bold">    openssl dgst -sha256 -hex | sed 's/^.* //'</strong></pre><p class="list-inset">The output is as follows:</p></li> </ol>
<div><div><img alt="Figure 16.21 – Getting the CA certificate hash" src="img/B19682_16_21.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.21 – Getting the CA certificate hash</p>
<ol>
<li value="3">You may<a id="_idIndexMarker2416"/> also generate a new bootstrap token<a id="_idIndexMarker2417"/> with the<a id="_idIndexMarker2418"/> following command:<pre class="source-code">
<code>kubeadm join</code> command with the required parameters:</p><pre class="source-code"><strong class="bold">kubeadm token create --print-join-command</strong></pre></li> </ol>
<p>Now that the token has been created, we can proceed to the next bootstrapping steps.</p>
<p>In the following steps, we’ll use our initial tokens as displayed in the output at the end of the bootstrapping process. So, let’s switch to the node’s command-line terminal (on <code>k8s-n1</code>) and run the following command:</p>
<ol>
<li>Make sure to invoke <code>sudo</code>, or the command will fail with insufficient permissions:<pre class="source-code">
<strong class="bold">sudo kubeadm join 192.168.122.104:6443 \</strong>
<strong class="bold">    --token abcdef.0123456789abcdef \</strong>
<code>k8s-cp1</code>):<pre class="source-code">
<code>k8s-n1</code>) added to the cluster:</p></li> </ol>
<div><div><img alt="Figure 16.22 – The new node (k8s-n1) added to the cluster" src="img/B19682_16_22.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.22 – The new node (k8s-n1) added to the cluster</p>
<ol>
<li value="3">We encourage you to repeat the process of joining the other two cluster nodes (<code>k8s-n2</code> and <code>k8s-n3</code>). During the join, while the CP pods are being deployed on the new node, you may temporarily see a <code>NotReady</code> status for the new node if you query the nodes on the CP node (<code>k8s-cp1</code>) too fast. The process should take a while. In the end, we should have all three nodes showing <code>Ready</code> in the output of the <code>kubectl get nodes</code> command (on <code>k8s-cp1</code>):</li>
</ol>
<div><div><img alt="Figure 16.23 – The Kubernetes cluster with all nodes running" src="img/B19682_16_23.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.23 – The Kubernetes cluster with all nodes running</p>
<p>We have now <a id="_idIndexMarker2421"/>completed the installation of our Kubernetes cluster, with <a id="_idIndexMarker2422"/>a CP node and three worker nodes. We used a local (on-premises) VM environment, but the same process would also apply to a<a id="_idIndexMarker2423"/> hosted IaaS solution running in a private or public cloud.</p>
<p>In the next section, we’ll explore the <code>kubectl</code> CLI to a certain extent and use it to create and manage Kubernetes resources. Then, we’ll look at deploying and scaling applications using the imperative and declarative deployment models in Kubernetes.</p>
<h1 id="_idParaDest-334"><a id="_idTextAnchor352"/>Working with Kubernetes</h1>
<p>In this section, we’ll use<a id="_idIndexMarker2424"/> real-world examples of interacting with a Kubernetes cluster. Since we’ll be using the <code>kubectl</code> CLI to a considerable extent, we’re going to take a deep dive into some of its more common usage patterns. Then, we will turn our focus to deploying applications to a Kubernetes cluster. We’ll be using the on-premises environment we built in the <em class="italic">Installing Kubernetes on </em><em class="italic">VMs</em> section.</p>
<p>Let’s start by taking a closer look at <code>kubectl</code> and its usage.</p>
<h2 id="_idParaDest-335"><a id="_idTextAnchor353"/>Using kubectl</h2>
<p><code>kubectl</code> is the primary tool<a id="_idIndexMarker2425"/> for managing a Kubernetes cluster and its resources. <code>kubectl</code> communicates with the cluster’s API server endpoint using the Kubernetes REST API. The general syntax of a <code>kubectl</code> command is as follows:</p>
<pre class="console">
kubectl [command] [TYPE] [NAME] [flags]</pre> <p>In general, <code>kubectl</code> commands execute <strong class="bold">CRUD operations</strong> – CRUD stands for <strong class="bold">Create</strong>, <strong class="bold">Read</strong>, <strong class="bold">Update</strong>, and <strong class="bold">Delete</strong> – against Kubernetes<a id="_idIndexMarker2426"/> resources, such as pods, Deployments, and Services.</p>
<p>One of the essential features of <code>kubectl</code> is the command output format, either in YAML, JSON, or plain text. The output format is handy when creating or editing application Deployment manifests. We can capture the YAML output of a <code>kubectl</code> command (such as create a resource) to a file. Later, we can reuse the manifest file to perform the same operation (or sequence of operations) in a declarative way. This brings us to the two basic Deployment paradigms of Kubernetes:</p>
<ul>
<li><code>kubectl</code> commands to operate on specific resources</li>
<li><code>kubectl apply</code> command, usually <a id="_idIndexMarker2428"/>targeting a set of resources with a single invocation</li>
</ul>
<p>We’ll look at these two Deployment models more closely in the <em class="italic">Deploying applications</em> section later in this chapter. For now, let’s get back to exploring the <code>kubectl</code> command further. Here’s a short list of some of the most common <code>kubectl</code> commands:</p>
<ul>
<li><code>create</code>, <code>apply</code>: These create resources imperatively/declaratively</li>
<li><code>get</code>: This reads resources</li>
<li><code>edit</code>, <code>set</code>: These update resources or specific features of objects</li>
<li><code>delete</code>: This deletes resources</li>
<li><code>run</code>: This starts a pod</li>
<li><code>exec</code>: This executes a command in a pod container</li>
<li><code>describe</code>: This displays detailed information about resources</li>
<li><code>explain</code>: This provides resource-related documentation</li>
<li><code>logs</code>: This shows the logs in pod containers</li>
</ul>
<p>A couple of frequently <a id="_idIndexMarker2429"/>used parameters of the <code>kubectl</code> command are also worth mentioning:</p>
<ul>
<li><code>--dry-run</code>: This runs the command without modifying the system state while still providing the output as if it executed normally</li>
<li><code>--output</code>: This specifies various formats for the command output: <code>yaml</code>, <code>json</code>, and <code>wide</code> (additional information in plain text)</li>
</ul>
<p>In the following sections, we’ll look at multiple examples of using the <code>kubectl</code> command. Always keep in mind the general pattern of the command:</p>
<div><div><img alt="Figure 16.24 – The general usage pattern of kubectl" src="img/B19682_16_24.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.24 – The general usage pattern of kubectl</p>
<p>We recommend that you check out the <a id="_idIndexMarker2430"/>complete <code>kubectl</code> command reference at <a href="https://kubernetes.io/docs/reference/kubectl/overview/">https://kubernetes.io/docs/reference/kubectl/overview/</a>. While you are becoming proficient with <code>kubectl</code>, you may also want to keep the related cheat sheet at hand, which you can find at the following link: <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">https://kubernetes.io/docs/reference/kubectl/cheatsheet/</a>.</p>
<p>Now, let’s prepare our <code>kubectl</code> environment to interact with the Kubernetes cluster we built earlier with VMs. You may skip the next section if you prefer to use <code>kubectl</code> on the CP node.</p>
<h3>Connecting to a Kubernetes cluster from a local machine</h3>
<p>In this section, we<a id="_idIndexMarker2431"/> configure the <code>kubectl</code> CLI running locally on our Linux desktop to control a remote Kubernetes cluster. We are running Debian 12 on our local machine.</p>
<p>First, we will need to install <code>kubectl</code> on our system. We will observe the installation instructions at <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/">https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/</a>. We will download the latest <code>kubectl</code> release with the following command:</p>
<pre class="console">
curl -LO "<code>kubectl</code> with the following command:</p>
<pre class="console">
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl</pre> <p>Now that the package is installed, we can test the installation using the following command:</p>
<pre class="console">
kubectl version --client</pre> <p>The output of the<a id="_idIndexMarker2432"/> preceding commands is shown in the following screenshot:</p>
<div><div><img alt="Figure 16.25 – Installing kubectl locally on a Debian system" src="img/B19682_16_25.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.25 – Installing kubectl locally on a Debian system</p>
<p>We want to add (merge) yet another cluster configuration to our environment. This time, we connect to an on-premises Kubernetes CP, and we’ll use <code>kubectl</code> to update kubeconfig. Here are the steps we’ll be taking:</p>
<ol>
<li>We first copy kubeconfig from the CP node (<code>k8s-cp1</code>, <code>192.168.122.104</code>) to a temporary location <code>(/tmp/config.cp</code>):<pre class="source-code">
<strong class="bold">scp packt@192.168.122.104:~/.kube/config /tmp/config.cp</strong></pre></li> <li>Finally, we can move the new kubeconfig file to the new location:<pre class="source-code">
<strong class="bold">mv /tmp/config.cp ~/.kube/config</strong></pre></li> <li>Optionally, we can clean up the temporary files created in the process:<pre class="source-code">
<strong class="bold">rm ~/.kube/config.old /tmp/config.cp</strong></pre></li> <li>Let’s get a view of the current kubeconfig contexts:<pre class="source-code">
<code>kubernetes-admin@kubernetes</code>) and cluster name (<code>kubernetes</code>):</p></li> </ol>
<div><div><img alt="Figure 16.26 – The new kubeconfig contexts" src="img/B19682_16_26.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.26 – The new kubeconfig contexts</p>
<ol>
<li value="5">For consistency, let’s change the on-premises cluster’s context name to <code>k8s-local</code> and make it the default context in our <code>kubectl</code> environment:<pre class="source-code">
<strong class="bold">kubectl config rename-context \</strong>
<strong class="bold">    kubernetes-admin@kubernetes \</strong>
<strong class="bold">    k8s-local</strong>
<code>kubectl</code> context becomes <code>k8s-local</code>, and we’re now interacting with our on-premises Kubernetes cluster (<code>kubernetes</code>). The output is shown in the following screenshot:</p></li> </ol>
<div><div><img alt="Figure 16.27 – The current context set to the on-premises Kubernetes cluster" src="img/B19682_16_27.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.27 – The current context set to the on-premises Kubernetes cluster</p>
<p>Next, we look at some of<a id="_idIndexMarker2434"/> the most common <code>kubectl</code> commands used with everyday Kubernetes administration tasks.</p>
<h3>Working with kubectl</h3>
<p>One of the first commands we <a id="_idIndexMarker2435"/>run when connected to a Kubernetes cluster is the following:</p>
<pre class="console">
kubectl cluster-info</pre> <p>The command shows the IP address and port of the API server listening on the CP node, among other information:</p>
<div><div><img alt="Figure 16.28 – The Kubernetes cluster information shown" src="img/B19682_16_28.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.28 – The Kubernetes cluster information shown</p>
<p>The <code>cluster-info</code> command can also help to debug and diagnose cluster-related issues:</p>
<pre class="console">
kubectl cluster-info dump</pre> <p>To get a detailed view of the cluster nodes, we run the following command:</p>
<pre class="console">
kubectl get nodes --output=wide</pre> <p>The <code>--output=wide</code> (or <code>-o wide</code>) flag yields detailed information about cluster nodes. The output in the following illustration has been cropped to show it more clearly:</p>
<div><div><img alt="Figure 16.29 – Getting detailed information about cluster nodes" src="img/B19682_16_29.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.29 – Getting detailed information about cluster nodes</p>
<p>The following command retrieves the pods running in the default namespace:</p>
<pre class="console">
kubectl get pods</pre> <p>As of now, we don’t have any user pods running, and the command returns the following:</p>
<pre class="console">
No resources found in default namespace.</pre> <p>To list all the pods, we append the <code>--all-namespaces</code> flag to the preceding command:</p>
<pre class="console">
kubectl get pods --all-namespace</pre> <p>The output shows all pods <a id="_idIndexMarker2436"/>running in the system. Since these are exclusively system pods, they are associated with the <code>kube-system</code> namespace:</p>
<div><div><img alt="Figure 16.30 – Getting all pods in the system" src="img/B19682_16_30.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.30 – Getting all pods in the system</p>
<p>We would get the<a id="_idIndexMarker2437"/> same output if we specified <code>kube-system</code> with the <code>--</code><code>namespace</code> flag:</p>
<pre class="console">
kubectl get pods --namespace kube-system</pre> <p>For a comprehensive view of all resources running in the system, we run the following command:</p>
<pre class="console">
kubectl get all --all-namespaces</pre> <p>So far, we have only mentioned some of the more common object types, such as nodes, pods, and Services. There are many others, and we can view them with the following command:</p>
<pre class="console">
kubectl api-resources</pre> <p>The output includes the<a id="_idIndexMarker2438"/> name of the API object types (such as <code>nodes</code>), their short name or alias (such as <code>no</code>), and whether they can be organized in namespaces (such as <code>false</code>):</p>
<div><div><img alt="Figure 16.31 – Getting all API object types (cropped)" src="img/B19682_16_31.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.31 – Getting all API object types (cropped)</p>
<p>Suppose you want to find out more about specific API objects, such as <code>nodes</code>. Here’s where the <code>explain</code> command comes in handy:</p>
<pre class="console">
kubectl explain nodes</pre> <p>The output is as follows:</p>
<div><div><img alt="Figure 16.32 – Showing nodes’ detailed information (cropped)" src="img/B19682_16_32.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.32 – Showing nodes’ detailed information (cropped)</p>
<p>The output provides<a id="_idIndexMarker2439"/> detailed documentation about the <code>nodes</code> API object type, including the related API fields. One of the API fields is <code>apiVersion</code>, describing the versioning schema of an object. You may view the related documentation with the following command:</p>
<pre class="console">
kubectl explain nodes.apiVersion</pre> <p>The output is as follows:</p>
<div><div><img alt="Figure 16.33 – Details about the apiVersion field (cropped)" src="img/B19682_16_33.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.33 – Details about the apiVersion field (cropped)</p>
<p>We encourage you to use the <code>explain</code> command to learn about the various Kubernetes API object types in a cluster. Please note that the <code>explain</code> command provides documentation about <em class="italic">resource types</em>. It should not be confused with the <code>describe</code> command, which shows detailed information about the <em class="italic">resources</em> in the system.</p>
<p>The following commands display cluster node-related information about <em class="italic">all</em> nodes, and then node <code>k8s-n1</code> in particular:</p>
<pre class="console">
kubectl describe nodes
kubectl describe nodes k8s-n1</pre> <p>For every <code>kubectl</code> command, you can invoke <code>--help</code> (or <code>-h</code>) to get context-specific help. Here are a few examples:</p>
<pre class="console">
kubectl --help
kubectl config -h
kubectl get pods -h</pre> <p>The <code>kubectl</code> CLI is<a id="_idIndexMarker2440"/> relatively rich in commands, and becoming proficient with it may take some time. Occasionally, you may find yourself looking for a specific command or remembering its correct spelling or use. The <code>auto-complete</code> bash for <code>kubectl</code> comes to the rescue. We’ll show you how to enable this next.</p>
<h3>Enabling kubectl autocompletion</h3>
<p>With <code>kubectl</code> autocompletion, you’ll get<a id="_idIndexMarker2441"/> context-sensitive suggestions when you hit the <em class="italic">Tab</em> key twice while typing the <code>kubectl</code> commands.</p>
<p>The <code>kubectl</code> autocompletion feature depends<a id="_idIndexMarker2442"/> on <code>bash-completion</code>. Most Linux platforms have <code>bash-completion</code> enabled by default. Otherwise, you’ll have to install the related package manually. On Ubuntu, for example, you install it with the following command:</p>
<pre class="console">
sudo apt-get install -y bash-completion</pre> <p>Next, you need to source the <code>kubectl</code> autocompletion in your shell (or similar) profile:</p>
<pre class="console">
echo "source &lt;(kubectl completion bash)" &gt;&gt; ~/.bashrc</pre> <p>The changes will take effect on your next login to the terminal or immediately if you source the <code>bash</code> profile:</p>
<pre class="console">
source ~/.bashrc</pre> <p>With the <code>kubectl</code> autocomplete active, you’ll get context-sensitive suggestions when you hit the <em class="italic">Tab</em> key twice while typing the command. For example, the following sequence provides all the available resources when you try to create one:</p>
<pre class="console">
kubectl create [Tab][Tab]</pre> <p>When typing the <code>kubectl create</code> command and pressing the <em class="italic">Tab</em> key twice, the result will be a list of resources available for the command:</p>
<div><div><img alt="Figure 16.34 – Autocompletion use with kubectl" src="img/B19682_16_34.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.34 – Autocompletion use with kubectl</p>
<p>The <code>kubectl</code> autocompletion reaches every part of the syntax: command, resource (type and name), and flags.</p>
<p>Now that we know more about using the <code>kubectl</code> command, it’s time to turn our attention to deploying applications in Kubernetes.</p>
<h2 id="_idParaDest-336"><a id="_idTextAnchor354"/>Deploying applications</h2>
<p>When we introduced<a id="_idIndexMarker2443"/> the <code>kubectl</code> command and its usage pattern at the beginning of the <em class="italic">Using kubectl</em> section, we touched upon the two ways of creating application resources in Kubernetes: imperative and declarative.</p>
<p>We’ll look at both of these models closely in this section while deploying a simple web application. Let’s start with the imperative model first.</p>
<h3>Working with imperative Deployments</h3>
<p>As a quick refresher, with imperative <a id="_idIndexMarker2444"/>Deployments, we follow a sequence of <code>kubectl</code> commands to create the required resources and get to the cluster’s desired state, such as running the application. Declarative Deployments accomplish the same, usually with a single <code>kubectl</code> <code>apply</code> command using a manifest file describing multiple resources.</p>
<h4>Creating a Deployment</h4>
<p>Let’s begin by creating a Deployment first. We’ll name our Deployment <code>packt</code>, based on a demo Nginx container we’re pulling from the public Docker <a id="_idIndexMarker2445"/>registry (<code>docker.io/nginxdemos/hello</code>):</p>
<pre class="console">
kubectl create deployment <strong class="bold">packt</strong> --image=<strong class="bold">nginxdemos/hello</strong></pre> <p>The command output shows that our Deployment was created successfully:</p>
<pre class="console">
deployment.apps/packt created</pre> <p>We just created a Deployment with a ReplicaSet containing a single pod running a web server application. We should note that our application is managed by the controller manager within an app Deployment stack (<code>deployment.apps</code>). Alternatively, we could just deploy a simple application pod (<code>packt-web</code>) with the following command:</p>
<pre class="console">
kubectl run <code>pod/packt-web</code>), is not part of a Deployment:</p>
<pre class="console">
pod/packt-web created</pre> <p>We’ll see later in this section that this pod is not part of a ReplicaSet and so is not managed by the controller manager.</p>
<p>Let’s look at the state of our system by querying the pods for detailed information:</p>
<pre class="console">
kubectl get pods -o wide</pre> <p>Let’s analyze the output:</p>
<div><div><img alt="Figure 16.35 – Getting the application pods with detailed information" src="img/B19682_16_35.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.35 – Getting the application pods with detailed information</p>
<p>In the preceding output, you can see the series of commands described, and we can also see that our pods are up and running and that Kubernetes deployed them on separate nodes:</p>
<ul>
<li><code>packt-579bb9c999-rtvzr</code>: On cluster node <code>k8s-n1</code></li>
<li><code>packt-web</code>: On cluster node <code>k8s-n3</code></li>
</ul>
<p>Running the pods on different nodes is due to internal load balancing and resource distribution in the Kubernetes cluster.</p>
<p>The application pod managed<a id="_idIndexMarker2446"/> by the controller is <code>packt-579bb9c999-rtvzr</code>. Kubernetes generates a unique name for our managed pod by appending a <code>579bb9c999</code>) and a <code>rtvzr</code>) to the name of the Deployment (<code>packt</code>). The pod template hash and pod ID are unique within a ReplicaSet.</p>
<p>In contrast, the standalone pod (<code>packt-web</code>) is left as is since it’s not part of an application Deployment. Let’s describe both pods to obtain more information about them. We’ll start with the managed pod first. Don’t forget to use the <code>kubectl</code> autocompletion (by pressing the <em class="italic">Tab</em> key twice):</p>
<pre class="console">
kubectl describe pod packt-5dc77bb9bf-bnzsc</pre> <p>The related output is relatively large. Here are some relevant snippets:</p>
<div><div><img alt="Figure 16.36 – Pod information" src="img/B19682_16_36.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.36 – Pod information</p>
<p>In contrast, the same<a id="_idIndexMarker2447"/> command for the standalone pod (<code>packt-web</code>) would be slightly different without featuring the <code>Controlled </code><code>By</code> field:</p>
<pre class="console">
kubectl describe pod packt-web</pre> <p>Here are the corresponding excerpts:</p>
<div><div><img alt="Figure 16.37 – Relevant pod information" src="img/B19682_16_37.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.37 – Relevant pod information</p>
<p>You can also venture out to any of the cluster nodes where <a id="_idIndexMarker2448"/>our pods are running and take a closer look at the related containers. Let’s take node <code>k8s-n3</code> (<code>192.168.122.163</code>), for example, where our standalone pod (<code>packt-web</code>) is running. We’ll SSH into the node’s terminal first:</p>
<pre class="console">
ssh packt@192.168.122.163</pre> <p>Then we’ll use the <code>containerd</code> runtime to query the containers in the system:</p>
<pre class="console">
sudo crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps</pre> <p>The output shows the following:</p>
<div><div><img alt="Figure 16.38 – Getting the containers running on a cluster node" src="img/B19682_16_38.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.38 – Getting the containers running on a cluster node</p>
<p>Next, we’ll show you how to access processes running inside the pods.</p>
<h4>Accessing processes in pods</h4>
<p>Let’s switch back to our local (on our local machine, not the VM) <code>kubectl</code> environment and run the following <a id="_idIndexMarker2449"/>command to access the shell in the container running the <code>packt-web</code> pod:</p>
<pre class="console">
kubectl exec -it packt-web -- /bin/sh</pre> <p>The command<a id="_idIndexMarker2450"/> takes us inside the container to an interactive shell prompt. Here, we can run commands as if we were logged in to the <code>packt-web</code> host using the terminal. The interactive session is produced using the <code>-it</code> option – interactive terminal – or <code>--</code><code>interactive --tty</code>.</p>
<p>Let’s run a few commands, starting with the process explorer:</p>
<pre class="console">
ps aux</pre> <p>Here’s a relevant excerpt from the output, showing the processes running inside the <code>packt-web</code> container, and some commands running inside it:</p>
<div><div><img alt="Figure 16.39 – The processes running inside the packt-web container" src="img/B19682_16_39.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.39 – The processes running inside the packt-web container</p>
<p>We can also retrieve the IP address with the following command:</p>
<pre class="console">
ifconfig | grep 'inet addr:' | cut -d: -f2 | awk '{print $1}' | grep -v '127.0.0.1'</pre> <p>The output shows the pod’s IP address (as seen in <em class="italic">Figure 16</em><em class="italic">.39</em>):</p>
<pre class="console">
172.16.57.193</pre> <p>We can also retrieve the hostname with the following command:</p>
<pre class="console">
hostname</pre> <p>The output shows the pod name (as seen in <em class="italic">Figure 16</em><em class="italic">.39</em>):</p>
<pre class="console">
packt-web</pre> <p>Let’s leave the container shell with the <code>exit</code> command (as shown in <em class="italic">Figure 16</em><em class="italic">.39</em>) or by pressing <em class="italic">Ctrl</em> + <em class="italic">D</em>. With the <code>kubectl</code> <code>exec</code> command, we can run any process inside a pod, assuming that the related process exists.</p>
<p>We’ll experiment next by <a id="_idIndexMarker2451"/>testing the <code>packt-web</code> application pod using <code>curl</code>. We should note that, at this time, the only way to access the web server endpoint of <code>packt-web</code> is via its internal IP address. Previously, we used the <code>kubectl</code> <code>get pods -o wide</code> and <code>describe</code> commands to retrieve detailed information regarding pods, including the pod’s IP address. You can also use the following one-liner to retrieve the pod’s IP:</p>
<pre class="console">
kubectl get pods packt-web -o jsonpath='{.status.podIP}{"\n"}'</pre> <p>In our case, the command returns <code>172.16.57.193</code>. We used the <code>-o jsonpath</code> output option to specify the JSON query for a specific field, <code>{.status.podIP}</code>. Remember that the pod’s IP is only accessible within the pod network (<code>172.16.0.0/16</code>) inside the cluster. Following is a screenshot showing the output:</p>
<div><div><img alt="Figure 16.40 – Testing the application pod" src="img/B19682_16_40.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.40 – Testing the application pod</p>
<p>Consequently, we need to probe the <code>packt-web</code> endpoint using a <code>curl</code> command that has originated within the pod network. An easy way to accomplish such a task is to run a test pod with the <code>curl</code> utility installed:</p>
<ol>
<li>The following command runs a pod named <code>test</code>, based on the <code>curlimages/curl</code> Docker image:<pre class="source-code">
<code>sleep</code> command due to the Docker entry point of the corresponding image, which simply runs a <code>curl</code> command and then exits. Without <code>sleep</code>, the pod would keep coming up and crashing. With the <code>sleep</code> command, we delay the execution of the <code>curl</code> entry point to prevent the exit. The output is shown in the following screenshot:</li>
</ol>
<div><div><img alt="Figure 16.41 – Running a test with curl on the pod" src="img/B19682_16_41.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.41 – Running a test with curl on the pod</p>
<ol>
<li value="3">Now, we can run a simple <code>curl</code> command using the <code>test</code> pod targeting the <code>packt-web</code> web server endpoint:<pre class="source-code">
<strong class="bold">kubectl exec test -- curl http://172.16.57.193</strong></pre></li> <li>We’ll get an HTTP response and a corresponding <strong class="bold">access log trace</strong> (from the Nginx server running in the <a id="_idIndexMarker2452"/>pod) accounting for the request. A snippet of the output is as follows:</li>
</ol>
<div><div><img alt="Figure 16.42 – The response of running the curl test" src="img/B19682_16_42.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.42 – The response of running the curl test</p>
<ol>
<li value="5">To view the logs on the <code>packt-web</code> pod, we run the following command:<pre class="source-code">
<strong class="bold">kubectl logs packt-web</strong></pre><p class="list-inset">The output is<a id="_idIndexMarker2453"/> as follows:</p></li> </ol>
<div><div><img alt="Figure 16.43 – Logs for packt-web pod" src="img/B19682_16_43.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.43 – Logs for packt-web pod</p>
<ol>
<li value="6">The logs in the <code>packt-web</code> pod are produced by Nginx and redirected to <code>stdout</code> and <code>stderr</code>. We can <a id="_idIndexMarker2454"/>easily verify this with the following command:<pre class="source-code">
<strong class="bold">kubectl exec packt-web -- ls -la /var/log/nginx</strong></pre><p class="list-inset">The output shows the related symlinks:</p></li> </ol>
<div><div><img alt="Figure 16.44 – Related symlinks" src="img/B19682_16_44.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.44 – Related symlinks</p>
<ol>
<li value="7">When you’re done using the <code>test</code> pod, you can delete it with the following command:<pre class="source-code">
<strong class="bold">kubectl delete pods test</strong></pre></li> </ol>
<p>Now that we have deployed our first application inside a Kubernetes cluster, let’s look at how to expose the related endpoint to the world. Next, we will expose Deployments via a Service.</p>
<h4>Exposing Deployments as Services</h4>
<p>Now, let’s rewind to the command we used previously to create the <code>packt</code> Deployment. Don’t run it. Here it is just as a refresher:</p>
<pre class="console">
kubectl create deployment packt --image=nginxdemos/hello</pre> <p>The command carried out the<a id="_idIndexMarker2455"/> following sequence:</p>
<ol>
<li>It created a Deployment (<code>packt</code>).</li>
<li>The Deployment created a ReplicaSet (<code>packt-579bb9c999</code>).</li>
<li>The ReplicaSet created the pod (<code>packt-579bb9c999-rtvzr</code>).</li>
</ol>
<p>We can verify that with the following commands:</p>
<pre class="console">
kubectl get deployments -l app=packt
kubectl get replicasets -l app=packt
kubectl get pods -l app=packt</pre> <p>In the preceding commands, we used the <code>--label-columns (-l)</code> flag to filter results by the <code>app=packt</code> label, denoting the <code>packt</code> Deployment’s resources.</p>
<p>We encourage you to take a closer look at each of these resources using the <code>kubectl describe</code> command. Don’t forget to use the <code>kubectl</code> autocomplete feature when typing in the commands:</p>
<pre class="console">
kubectl describe deployment packt | more
kubectl describe replicaset packt | more
kubectl describe pod packt-5dc77bb9bf-bnzsc | more</pre> <p>The <code>kubectl</code> <code>describe</code> command could be very resourceful when troubleshooting applications or pod Deployments. Look inside the <em class="italic">Events</em> section in the related output for clues on pods failing to start, errors, if any, and possibly understand what went wrong.</p>
<p>Now that we have deployed our first application inside a Kubernetes cluster, let’s look at how to expose the related endpoint to the world.</p>
<p>So far, we have deployed an<a id="_idIndexMarker2456"/> application (<code>packt</code>) with a single pod (<code>packt-579bb9c999-rtvzr</code>) running an Nginx web server listening on port <code>80</code>. As we explained earlier, at this time, we can only <a id="_idIndexMarker2457"/>access the pod within the pod network, which is internal to the cluster. In this section, we’ll show you how to expose the application (or Deployment) to be accessible from the outside world. Kubernetes uses the Service API object, consisting of a <strong class="bold">proxy</strong> and a <strong class="bold">selector</strong> routing the network traffic to application pods in a Deployment. To proceed, you can follow these steps:</p>
<ol>
<li>The following command creates a Service for our Deployment (<code>packt</code>):<pre class="source-code">
<strong class="bold">kubectl expose deployment packt \</strong>
<strong class="bold">    --port=80 \</strong>
<strong class="bold">    --target-port=80 \</strong>
<code>--type=NodePort</code> flag, the Service type would be <code>ClusterIP</code> by default, and the Service endpoint would only be accessible within the cluster.</pre></li> <li>Let’s take a closer look at our Service (<code>packt</code>):<pre class="source-code">
<code>10.105.111.243</code>) and the<a id="_idIndexMarker2458"/> ports the Service is listening on for TCP traffic (<code>80:32664/TCP</code>):</p><ul><li>port <code>80</code>: Within the <a id="_idIndexMarker2459"/>cluster</li><li>port <code>32664</code>: Outside the cluster, on any of the nodes</li></ul><p class="list-inset">We should note that the cluster IP is only accessible within the cluster and not from the outside:</p></li> </ol>
<div><div><img alt="Figure 16.45 – The service exposing the packt deployment" src="img/B19682_16_45.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.45 – The Service exposing the packt Deployment</p>
<p class="list-inset">Also, <code>EXTERNAL-IP</code> (<code>&lt;none&gt;</code>) should not be mistaken for the cluster node’s IP address where our Service is accessible. The external IP is usually a load balancer IP address configured by a cloud provider hosting the Kubernetes cluster (configurable via the <code>--</code><code>external-ip</code> flag).</p>
<ol>
<li value="3">We should now be able to access our application outside the cluster by pointing a browser to any of the cluster nodes on port <code>32664</code>. To get a list of our cluster nodes with their respective IP addresses and hostnames, we can run the following command:<pre class="source-code">
<strong class="bold">kubectl get nodes -o jsonpath='{range .items[*]}{.status.addresses[*].address}{"\n"}'</strong></pre><p class="list-inset">The output is as follows:</p></li> </ol>
<div><div><img alt="Figure 16.46 – List of cluster nodes" src="img/B19682_16_46.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.46 – List of cluster nodes</p>
<ol>
<li value="4">Let’s choose the CP node (<code>192.168.122.104/k8s-cp1</code>) and enter <a id="_idIndexMarker2460"/>the following address<a id="_idIndexMarker2461"/> in a browser: http://192.168.122.104:32664.<p class="list-inset">The web request from the browser is directed to the Service endpoint (<code>packt</code>), which routes the related network packets to the application pod (<code>packt-579bb9c999-rtvzr</code>). The <code>packt</code> web application responds with a simple Nginx <code>172.16.215.65</code>) and name (<code>packt-579bb9c999-rtvzr</code>):</p></li>
</ol>
<div><div><img alt="Figure 16.47 – Accessing the packt application service" src="img/B19682_16_47.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.47 – Accessing the packt application Service</p>
<ol>
<li value="5">To verify that the <a id="_idIndexMarker2462"/>information on the web page is accurate, you may <a id="_idIndexMarker2463"/>run the following <code>kubectl</code> command, retrieving similar information:<pre class="source-code">
<strong class="bold">kubectl get pod packt-579bb9c999-rtvzr -o jsonpath='{.status.podIP}{"\n"}{.metadata.name}{"\n"}'</strong></pre><p class="list-inset">The output of the<a id="_idIndexMarker2464"/> command will be the internal IP address and the name of the pod, as shown in the following:</p></li> </ol>
<div><div><img alt="Figure 16.48 – Verify the information with the kubectl command" src="img/B19682_16_48.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.48 – Verify the information with the kubectl command</p>
<p>Suppose we have high traffic targeting our application, and we’d like to scale out the ReplicaSet controlling our pods. We’ll show you how to accomplish this task in the next section.</p>
<h4>Scaling application Deployments</h4>
<p>Currently, we have a single pod in the <code>packt</code> Deployment. In order to scale the <a id="_idIndexMarker2465"/>application Deployments, we have to obtain information about the running replicas, to scale them to the desired number and test it. Here are the steps to take:</p>
<ol>
<li>To retrieve the relevant details about the number of running replicas, we run the following command:<pre class="source-code">
<strong class="bold">kubectl describe deployment packt</strong></pre><p class="list-inset">The relevant excerpt in the output is as follows:</p></li> </ol>
<div><div><img alt="Figure 16.49 – Pod details" src="img/B19682_16_49.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.49 – Pod details</p>
<ol>
<li value="2">Let’s scale up our <code>packt</code> Deployment to<a id="_idIndexMarker2466"/> 10 replicas with the following command:<pre class="source-code">
<code>packt</code> Deployment, we’ll see 10 pods running:<pre class="source-code">
<strong class="bold">kubectl get pods -l app=packt</strong></pre><p class="list-inset">The output is as follows:</p></li> </ol>
<div><div><img alt="Figure 16.50 – Scaling up the deployment replicas" src="img/B19682_16_50.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.50 – Scaling up the Deployment replicas</p>
<ol>
<li value="4">Incoming requests to our application Service endpoint (<code>http://192.168.122.104:32664</code>) will be load balanced between the pods. To illustrate this behavior, we can either use <code>curl</code> or a text-based browser at the command line to avoid the caching-related<a id="_idIndexMarker2467"/> optimizations of a modern desktop browser. For a better illustration, we’ll use <strong class="bold">Lynx</strong>, a simple text-based browser. On our Debian 12 desktop, the package is already installed. You can install it with the following command:<pre class="source-code">
<strong class="bold">sudo apt-get install -y lynx</strong></pre></li> <li>Next, we point Lynx to our application endpoint:<pre class="source-code">
<strong class="bold">lynx 172.16.191.6:32081</strong></pre><p class="list-inset">If we refresh the page with <em class="italic">Ctrl</em> + <em class="italic">R</em> every few seconds, we observe that the server address and name change based on the current pod processing the request:</p></li> </ol>
<div><div><img alt="Figure 16.51 – Load balancing requests across pods" src="img/B19682_16_51.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.51 – Load balancing requests across pods</p>
<p class="list-inset">You can exit the<a id="_idIndexMarker2468"/> Lynx browser by typing <code>Q</code> and then pressing <em class="italic">Enter</em>.</p>
<ol>
<li value="6">We can scale back our Deployment (<code>packt</code>) to three replicas (or any other non-zero positive number) with<a id="_idIndexMarker2469"/> the following command:<pre class="source-code">
<code>packt</code> application pods, we can see the surplus pods terminating until only three pods are remaining:<pre class="source-code">
<strong class="bold">kubectl get pods -l app=packt</strong></pre><p class="list-inset">The output is as follows:</p></li> </ol>
<div><div><img alt="Figure 16.52 – Scaling back to three pods" src="img/B19682_16_52.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.52 – Scaling back to three pods</p>
<ol>
<li value="8">Before concluding our imperative Deployments, let’s clean up all the resources we have created thus far:<pre class="source-code">
<strong class="bold">kubectl delete service packt</strong>
<strong class="bold">kubectl delete deployment packt</strong>
<strong class="bold">kubectl delete pod packt-web</strong></pre></li> <li>The following command should reflect a clean slate:<pre class="source-code">
<strong class="bold">kubectl get all</strong></pre><p class="list-inset">The output<a id="_idIndexMarker2470"/> is as follows:</p></li> </ol>
<div><div><img alt="Figure 16.53 – The cluster in a default state" src="img/B19682_16_53.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.53 – The cluster in a default state</p>
<p>In the next section, we’ll look<a id="_idIndexMarker2471"/> at how to deploy resources and applications declaratively in the Kubernetes cluster.</p>
<h3>Working with declarative Deployments</h3>
<p>At the heart of a<a id="_idIndexMarker2472"/> declarative Deployment is a manifest file. Manifest files are generally in YAML format and authoring them usually involves a mix of autogenerated code and manual editing. The manifest is then deployed using the <code>kubectl </code><code>apply</code> command:</p>
<pre class="console">
kubectl apply -f MANIFEST</pre> <p>Deploying resources declaratively in Kubernetes involves the following stages:</p>
<ul>
<li>Creating a manifest file</li>
<li>Updating the manifest</li>
<li>Validating the manifest</li>
<li>Deploying the manifest</li>
<li>Iterating between the preceding stages</li>
</ul>
<p>To illustrate the declarative model, we follow the example of deploying a simple Hello World web application to the cluster. The result will be similar to our previous approach of using the<a id="_idIndexMarker2473"/> imperative method.</p>
<p>So, let’s start by creating a manifest for our Deployment.</p>
<h4>Creating a manifest</h4>
<p>When we created our <code>packt</code> Deployment imperatively, we used the <a id="_idIndexMarker2474"/>following command (don’t run it just yet!):</p>
<pre class="console">
kubectl create deployment packt --image=nginxdemos/hello</pre> <p>The following command will simulate the same process without changing the system state:</p>
<pre class="console">
kubectl create deployment packt --image=nginxdemos/hello \
    <strong class="bold">--dry-run=client</strong> <strong class="bold">--output=yaml</strong></pre> <p>We used the following additional options (flags):</p>
<ul>
<li><code>--dry-run=client</code>: This runs the command in the local <code>kubectl</code> environment (<em class="italic">client</em>) without modifying the system state</li>
<li><code>--output=yaml</code>: This formats the command output as YAML</li>
</ul>
<p>The output of the command is as follows:</p>
<div><div><img alt="Figure 16.54 – Simulating a manifest creation" src="img/B19682_16_54.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.54 – Simulating a manifest creation</p>
<p>We can use the previous command’s output to analyze the changes to be made to the system. Then we can redirect it to a file (<code>packt.yaml</code>) serving as a draft of our Deployment manifest:</p>
<pre class="console">
kubectl create deployment packt --image=nginxdemos/hello \
    --dry-run=client --output=yaml &gt; <code>packt.yaml</code>. From here, we can edit the file to accommodate more complex<a id="_idIndexMarker2475"/> configurations. For now, we’ll leave the manifest as is and proceed with the next stage in our declarative Deployment workflow.</p>
<h4>Validating a manifest</h4>
<p>Before deploying a manifest, we recommend validating the Deployment, especially if you<a id="_idIndexMarker2476"/> edited the file manually. Editing mistakes can happen, particularly when working with complex YAML files with multiple indentation levels.</p>
<p>The following command validates the <code>packt.yaml</code> Deployment manifest:</p>
<pre class="console">
kubectl apply -f packt.yaml --dry-run=client</pre> <p>A successful validation yields the following output:</p>
<pre class="console">
deployment.apps/packt created (dry run)</pre> <p>If there are any errors, we should edit the manifest file and correct them prior to deployment. Our manifest looks good, so let’s go ahead and deploy it.</p>
<h4>Deploying a manifest</h4>
<p>To deploy the <code>packt.yaml</code> manifest, we use the following command:</p>
<pre class="console">
kubectl apply -f packt.yaml</pre> <p>A successful Deployment shows<a id="_idIndexMarker2477"/> the following message:</p>
<pre class="console">
deployment.apps/packt created</pre> <p>We can check the deployed resources with the following command:</p>
<pre class="console">
kubectl get all -l app=packt</pre> <p>The output shows that the <code>packt</code> Deployment<a id="_idIndexMarker2478"/> resources created declaratively are up and running:</p>
<div><div><img alt="Figure 16.55 – The deployment resources created declaratively" src="img/B19682_16_55.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.55 – The Deployment resources created declaratively</p>
<p>Next, we want to expose our Deployment using a Service.</p>
<h4>Exposing the Deployment with a Service</h4>
<p>We’ll repeat the preceding workflow by <a id="_idIndexMarker2479"/>creating, validating, and deploying the Service manifest (<code>packt-svc.yaml</code>). For brevity, we simply enumerate the related commands:</p>
<ol>
<li>Create the manifest file (<code>packt-svc.yaml</code>) for the Service exposing our Deployment (<code>packt</code>):<pre class="source-code">
<strong class="bold">kubectl expose deployment packt \</strong>
<strong class="bold">    --port=80 \</strong>
<strong class="bold">    --target-port=80 \</strong>
<strong class="bold">    --type=NodePort \</strong>
<strong class="bold">    --dry-run=client --output=yaml &gt; packt-svc.yaml</strong></pre><p class="list-inset">We explained the preceding command previously in the <em class="italic">Exposing Deployments as </em><em class="italic">Services</em> section.</p></li> <li>Next, we’ll validate<a id="_idIndexMarker2480"/> the Service Deployment manifest:<pre class="source-code">
<strong class="bold">kubectl apply -f packt-svc.yaml --dry-run=client</strong></pre></li> <li>If the validation is successful, we deploy the Service manifest:<pre class="source-code">
<code>packt</code> resources:<pre class="source-code">
<code>packt</code> application resources, including the Service endpoint (<code>service/packt</code>) listening on port <code>31380</code>:</p></li> </ol>
<div><div><img alt="Figure 16.56 – The packt application resources deployed" src="img/B19682_16_56.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.56 – The packt application resources deployed</p>
<ol>
<li value="5">Using a browser, <code>curl</code>, or Lynx, we can access our application by targeting <a id="_idIndexMarker2481"/>any of the cluster nodes on port <code>31380</code>. Let’s use the CP node (<code>k8s-cp1</code>, <code>192.168.122.104</code>) by pointing our browser to <code>http://192.168.122.104:31380</code>:</li>
</ol>
<div><div><img alt="Figure 16.57 – Accessing the packt application endpoint" src="img/B19682_16_57.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.57 – Accessing the packt application endpoint</p>
<p>If we want to change the existing configuration of a resource in our application Deployment, we can update the related manifest and redeploy it. In the next section, we’ll modify the Deployment to accommodate a scale-out scenario.</p>
<h4>Updating a manifest</h4>
<p>Suppose our application is taking a high number of requests, and we’d like to add more pods to our <a id="_idIndexMarker2482"/>Deployment to handle the traffic. We need to change the <code>spec.replicas</code> configuration setting in the <code>pack.yaml</code> manifest:</p>
<ol>
<li>Using your editor of choice, edit the <code>packt.yaml</code> file and locate the following configuration section:<pre class="source-code">
spec:
  replicas: 1</pre><p class="list-inset">Change the value from <code>1</code> to <code>10</code> for additional application pods in the ReplicaSet controlled by the <code>packt</code> Deployment. The configuration becomes the following:</p><pre class="source-code">spec:
  replicas: 10</pre></li> <li>Save the manifest file and <a id="_idIndexMarker2483"/>redeploy with the following command:<pre class="source-code">
<code>packt</code> Deployment has been reconfigured:</p><pre class="source-code"><code>packt</code> resources in the cluster, we should see the new pods up and running:<pre class="source-code">
<code>packt</code> Deployment, including the additional pods deployed in the cluster:</p></li> </ol>
<div><div><img alt="Figure 16.58 – The additional pods added for application scale-out" src="img/B19682_16_58.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.58 – The additional pods added for application scale-out</p>
<p class="list-inset">We encourage you to test with the scale-out environment and verify the load balancing workload described in the <em class="italic">Scaling application Deployments</em> section earlier in this chapter.</p>
<ol>
<li value="4">Let’s scale back our <a id="_idIndexMarker2484"/>Deployment to three pods, but this time by updating the related manifest on the fly with the following command:<pre class="source-code">
<strong class="bold">kubectl edit deployment packt</strong></pre><p class="list-inset">The command will open our default editor in the system (<strong class="bold">vi</strong>) to make the desired change:</p></li> </ol>
<div><div><img alt="Figure 16.59 – Making deployment changes on the fly" src="img/B19682_16_59.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.59 – Making Deployment changes on the fly</p>
<ol>
<li value="5">After saving and exiting the editor, we’ll get a message suggesting that our Deployment (<code>packt</code>) has<a id="_idIndexMarker2485"/> been updated:<pre class="source-code">
<code>kubectl edit</code> will not be reflected in the Deployment manifest (<code>packt.yaml</code>). Nevertheless, the related configuration changes are persisted in the cluster (etcd).</p></li> <li>We can verify our updated Deployment with the help of the following command:<pre class="source-code">
<strong class="bold">kubectl get deployment packt</strong></pre><p class="list-inset">The output now shows only three pods running in our Deployment:</p></li> </ol>
<div><div><img alt="Figure 16.60 – Showing the number of deployments" src="img/B19682_16_60.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.60 – Showing the number of Deployments</p>
<ol>
<li value="7">Before wrapping up, let’s clean up our resources once again with the following commands to bring the cluster back to the default state:<pre class="source-code">
<strong class="bold">kubectl delete service packt</strong>
<strong class="bold">kubectl delete deployment packt</strong></pre></li> </ol>
<p>We have shown you how to use Kubernetes on bare metal, and in the next section, we will briefly point you to some useful resources for using Kubernetes in the cloud.</p>
<h2 id="_idParaDest-337"><a id="_idTextAnchor355"/>Running Kubernetes in the cloud</h2>
<p>Managed<a id="_idIndexMarker2486"/> Kubernetes Services are<a id="_idIndexMarker2487"/> fairly common among public cloud providers. Amazon <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>), <strong class="bold">Azure Kubernetes Services</strong> (<strong class="bold">AKS</strong>), and <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>) are the major cloud offerings of <a id="_idIndexMarker2488"/>Kubernetes at the time of this<a id="_idIndexMarker2489"/> writing. In this section, we’ll not focus on any of these solutions, but we will provide you with solid resources on how to use Kubernetes in the cloud. For more advanced titles on this subject, please check the <em class="italic">Further reading</em> section of this chapter.</p>
<p>We should note that we just scratched the surface of deploying and managing Kubernetes clusters. Yet, here we are, at a significant milestone, where we deployed our first Kubernetes clusters on-premises. We have reached the end of this journey here, but we trust that you’ll take it to the next level and further explore the exciting domain of application Deployment and scaling with Kubernetes. Let’s now summarize what we have learned in this chapter.</p>
<h1 id="_idParaDest-338"><a id="_idTextAnchor356"/>Summary</h1>
<p>We began this chapter with a high-level overview of the Kubernetes architecture and API object model, introducing the most common cluster resources, such as pods, Deployments, and Services. Next, we took on the relatively challenging task of building an on-premises Kubernetes cluster from scratch using VMs. We explored various CLI tools for managing Kubernetes cluster resources on-premises. At the high point of our journey, we focused on deploying and scaling applications in Kubernetes using imperative and declarative Deployment scenarios.</p>
<p>We believe that novice Linux administrators will benefit greatly from the material covered in this chapter and become more knowledgeable in managing resources across hybrid clouds and on-premises distributed environments, deploying applications at scale, and working with CLI tools. We believe that the structured information in this chapter will also help seasoned system administrators refresh some of their knowledge and skills in the areas covered.</p>
<p>It’s been a relatively long chapter, and we barely skimmed the surface of the related field. We encourage you to explore some resources captured in the <em class="italic">Further reading</em> section and strengthen your knowledge regarding some key areas of Kubernetes environments, such as networking, security, and scale.</p>
<p>In the next chapter, we’ll stay within the application deployment realm and look at <strong class="bold">Ansible</strong>, a platform for accelerating application delivery on-premises and in the cloud.</p>
<h1 id="_idParaDest-339"><a id="_idTextAnchor357"/>Questions</h1>
<p>Here are a few questions for refreshing or pondering upon some of the concepts you’ve learned in this chapter:</p>
<ol>
<li>Enumerate some of the essential Services of a Kubernetes CP node. How do the worker nodes differ?</li>
<li>What command did we use to bootstrap a Kubernetes cluster?</li>
<li>What is the difference between imperative and declarative Deployments in Kubernetes?</li>
<li>What is the <code>kubectl</code> command for deploying a pod? How about the command for creating a Deployment?</li>
<li>What is the <code>kubectl</code> command to access the shell within a pod container?</li>
<li>What is the <code>kubectl</code> command to query all resources related to a Deployment?</li>
<li>How do you scale out a Deployment in Kubernetes? Can you think of the different ways (commands) in which to accomplish the task?</li>
<li>How do you delete all resources related to a Deployment in Kubernetes?</li>
</ol>
<h1 id="_idParaDest-340"><a id="_idTextAnchor358"/>Further reading</h1>
<p>The following resources may help you to consolidate your knowledge of Kubernetes further:</p>
<ul>
<li>Kubernetes documentation online: <a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a></li>
<li>The <code>kubectl</code> cheat sheet: <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">https://kubernetes.io/docs/reference/kubectl/cheatsheet/</a></li>
<li><em class="italic">Kubernetes and Docker: The Container Masterclass [Video]</em>, <em class="italic">Cerulean Canvas</em>, Packt Publishing</li>
<li><em class="italic">Mastering Kubernetes – Third Edition</em>, Gigi Sayfan, Packt Publishing</li>
</ul>
<p>The following is a short list of useful links for deploying Kubernetes on Azure, Amazon, and Google:</p>
<ul>
<li>Amazon EKS:<ul><li><a href="https://docs.aws.amazon.com/eks/index.html">https://docs.aws.amazon.com/eks/index.html</a></li><li><a href="https://docs.aws.amazon.com/eks/latest/userguide/sample-deployment.html">https://docs.aws.amazon.com/eks/latest/userguide/sample-deployment.html</a></li></ul></li>
<li>AKS:<ul><li><a href="https://azure.microsoft.com/en-us/services/kubernetes-service/">https://azure.microsoft.com/en-us/services/kubernetes-service/</a></li><li><a href="https://learn.microsoft.com/en-us/azure/aks/tutorial-kubernetes-deploy-cluster?tabs=azure-cli">https://learn.microsoft.com/en-us/azure/aks/tutorial-kubernetes-deploy-cluster?tabs=azure-cli</a></li><li><a href="https://learn.microsoft.com/en-us/azure/aks/learn/quick-kubernetes-deploy-portal?tabs=azure-cli">https://learn.microsoft.com/en-us/azure/aks/learn/quick-kubernetes-deploy-portal?tabs=azure-cli</a></li></ul></li>
<li>GKE:<ul><li><a href="https://cloud.google.com/kubernetes-engine">https://cloud.google.com/kubernetes-engine</a></li><li><a href="https://cloud.google.com/build/docs/deploying-builds/deploy-gke">https://cloud.google.com/build/docs/deploying-builds/deploy-gke</a></li><li><a href="https://cloud.google.com/kubernetes-engine/docs/deploy-app-cluster">https://cloud.google.com/kubernetes-engine/docs/deploy-app-cluster</a></li></ul></li>
</ul>
</div>
</body></html>