<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Sizing the VDI"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Sizing the VDI</h1></div></div></div><p>
<span class="strong"><strong>Sizing</strong></span> is the process of determining how much horsepower any given component of the VDI solution requires, which is ideally based on metrics collected during the assessment phase. In most situations, the challenge will not be handling the average daily VDI workloads, but it will be handling the peaks. Peak loads in a VDI environment are often short in duration and may not be able to be mitigated through conventional techniques such as<span class="strong"><strong> VMware Distributed Resource Scheduler (DRS)</strong></span> or manual vMotion balancing.<a id="id223" class="indexterm"/>
</p><p>The components discussed in earlier chapters of this book, for example, VMware View Connection Server, require minimal sizing considerations when compared to the hardware components that must be sized. The reason being that the software components are primarily performing relatively lightweight work and merely brokering connections or performing provisioning tasks, which likely aren't happening constantly.</p><p>The following diagram shows the sizing layers of a VDI solution:<a id="id224" class="indexterm"/>
</p><div class="mediaobject"><img src="graphics/1124EN_06_01.jpg" alt="Sizing the VDI"/></div><p>For example, having a properly sized and performing database infrastructure is important, as slow database response times can impact both View Composer tasks as well as tasks within vCenter. Also, it is important to ensure that the View Connection Server has adequate virtual or physical resources such as CPU and memory. However, the primary focus of this chapter is on sizing the physical components of the VDI.<a id="id225" class="indexterm"/>
</p><p>To properly understand how to size a VDI, it's important to gather proper metrics during the assessment phase, which was covered in<a class="link" href="ch02.html" title="Chapter 2. Solution Methodology"> Chapter 2</a>,<span class="emphasis"><em> Solution Methodology</em></span>. Such metrics include the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Number of concurrent users</li><li class="listitem" style="list-style-type: disc">User classes and number of vCPUs, memory, and so on, per user class</li><li class="listitem" style="list-style-type: disc">Network requirements</li><li class="listitem" style="list-style-type: disc">USB redirection frequency</li></ul></div><p>This chapter will focus on the following components from a sizing perspective, not necessarily from a redundancy perspective. This chapter is the<span class="emphasis"><em> n</em></span> in<span class="emphasis"><em> n</em></span> + 1. These components include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">VMware View Connection Server</li><li class="listitem" style="list-style-type: disc">VMware vCenter Server</li><li class="listitem" style="list-style-type: disc">Server hardware</li><li class="listitem" style="list-style-type: disc">Network infrastructure</li></ul></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note18"/>Note</h3><p>Storage sizing is covered in<a class="link" href="ch08.html" title="Chapter 8. Sizing the Storage"> Chapter 8</a>, <span class="emphasis"><em>Sizing the Storage</em></span>.</p></div><p>An improperly sized VDI could experience any of the following problems:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Slow logons</li><li class="listitem" style="list-style-type: disc">Poor PCoIP performance</li><li class="listitem" style="list-style-type: disc">Inability to power on vDesktops due to reaching vCenter maximums</li><li class="listitem" style="list-style-type: disc">Inability to log in to the VDI</li><li class="listitem" style="list-style-type: disc">Authentication errors</li><li class="listitem" style="list-style-type: disc">Random failures</li></ul></div><div class="section" title="Network considerations"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec01"/>Network considerations</h1></div></div></div><p>While understanding the networking connectivity between the end users and the VDI is fairly obvious in a remote scenario, where the end user is removed geographically (for example, working from home) from the VDI, it's less obvious in a local scenario. While a local scenario may not blatantly cause a VDI architect to think about network sizing, it is still imperative to analyze and size the network component of a VDI solution even when all components reside on a<span class="strong"><strong> Local Area Network (LAN)</strong></span>. This is the only way to truly confirm that the end user's experience should be as positive as possible.<a id="id226" class="indexterm"/>
</p><div class="section" title="Sizing the network"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec01"/>Sizing the network</h2></div></div></div><p>As a general rule of thumb, a typical task worker requires approximately 250 Kbps of network throughput for a positive end user experience. By generally accepted industry terms, a task worker is a user that has the following characteristics:<a id="id227" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">He uses typical office applications or terminal windows</li><li class="listitem" style="list-style-type: disc">He does not require multimedia</li><li class="listitem" style="list-style-type: disc">He does not require 3D graphics</li><li class="listitem" style="list-style-type: disc">He does not require bidirectional audio</li></ul></div><p>However, where a task worker can potentially generate significant network bandwidth is with the use of USB peripherals. If a task worker requires USB peripherals to perform his job, then it is imperative to perform a network analysis of the specific USB peripherals in action prior to full-scale implementation.</p><p>The list of the consumables (Kbps) is as follows:<a id="id228" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">PCoIP baseline = 250 Kbps</li><li class="listitem" style="list-style-type: disc">PCoIP burst headroom = 500 Kbps</li><li class="listitem" style="list-style-type: disc">Multimedia video = 1,024 Kbps</li><li class="listitem" style="list-style-type: disc">3D graphics = 10,240 Kbps</li><li class="listitem" style="list-style-type: disc">480p video = 1,024 Kbps</li><li class="listitem" style="list-style-type: disc">720p video = 4,096 Kbps</li><li class="listitem" style="list-style-type: disc">1080p video = 6,144 Kbps</li><li class="listitem" style="list-style-type: disc">Bidirectional audio = 500 Kbps</li><li class="listitem" style="list-style-type: disc">USB peripherals = 500 Kbps</li><li class="listitem" style="list-style-type: disc">Stereo audio = 500 Kbps</li><li class="listitem" style="list-style-type: disc">CD quality audio = 2,048 Kbps</li></ul></div><p>The network checklist is given at<a class="ulink" href="http://techsupport.teradici.com/ics/support/default.asp?deptID=15164"> http://techsupport.teradici.com/ics/support/default.asp?deptID=15164</a>. But before that, you will be required to create an account at this site:<a class="ulink" href="http://techsupport.teradici.com"> http://techsupport.teradici.com</a>.</p><p>The other weights are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Buffer = 80 percent</li><li class="listitem" style="list-style-type: disc">Bandwidth offset = 105 percent</li></ul></div><p>The minimum bandwidth to deliver acceptable performance is determined by the activity and requirements of the user's session. Some baseline numbers for the minimum bandwidth needed for a respective user type are as follows:<a id="id229" class="indexterm"/>
</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Description</p>
</th><th style="text-align: left" valign="bottom">
<p>Kbps</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Office worker low without multimedia</p>
</td><td style="text-align: left" valign="top">
<p>250</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Office worker high without multimedia</p>
</td><td style="text-align: left" valign="top">
<p>315</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Office worker low with multimedia</p>
</td><td style="text-align: left" valign="top">
<p>340</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Office worker high with multimedia</p>
</td><td style="text-align: left" valign="top">
<p>375</p>
</td></tr></tbody></table></div><p>The following diagram is an illustration showing bandwidth provisioning of a given network connection:<a id="id230" class="indexterm"/>
</p><div class="mediaobject"><img src="graphics/1124EN_06_09.jpg" alt="Sizing the network"/></div><p>In most environments, the only network traffic that should have a higher network priority than PCoIP is network traffic related to <span class="strong"><strong> Voice over IP (VoIP)</strong></span> communications. Giving PCoIP a higher priority than VoIP could cause poor quality or loss of connections in certain environments with an improperly sized network. Therefore, it is recommended to give VoIP a higher priority than PCoIP (approximately up to 20 percent of the overall connection), give PCoIP traffic the second highest priority, and classify the remaining traffic appropriately.<a id="id231" class="indexterm"/>
</p><div class="section" title="Network connection characteristics"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec01"/>Network connection characteristics</h3></div></div></div><p>Teradici has made significant improvements in the ability of the PCoIP protocol to handle high-latency and/or low-bandwidth scenarios. Teradici's PCoIP protocol is a purpose-built protocol for delivering a native desktop experience. In order to deliver the best possible end user experience, PCoIP will consume as much bandwidth as is available at a given time, up to the point where it can deliver a favorable end user experience. PCoIP is dynamic in nature, and as the available bandwidth changes, so does the amount of bandwidth that PCoIP attempts to consume. PCoIP initially uses<span class="strong"><strong> Transmission Control Protocol (TCP)</strong></span> to establish the connection and then uses<span class="strong"><strong> User Datagram Protocol (UDP)</strong></span> to transmit the desktop experience.<a id="id232" class="indexterm"/>
</p><p>PCoIP also has two primary settings that should be understood, the PCoIP maximum bandwidth and the PCoIP bandwidth floor.<a id="id233" class="indexterm"/>
</p><p>The PCoIP maximum bandwidth is the maximum amount of bandwidth a given PCoIP session is allowed to consume. Configuring this setting can ensure that end users never exceed a certain amount of bandwidth themselves. In addition, properly configuring the PCoIP maximum bandwidth provides a sense of insurance in a solution. Without limiting consumptions per session (even if the maximum is configured to be very generous) it is possible to have a runaway PCoIP session consuming a disproportionate amount of the available bandwidth. This disproportionate consumption could negatively impact the other users sharing the same network connection.<a id="id234" class="indexterm"/>
</p><p>The following diagram is an illustration of the bandwidth floor and the bandwidth maximums:</p><div class="mediaobject"><img src="graphics/1124EN_06_10.jpg" alt="Network connection characteristics"/></div><p>The PCoIP bandwidth floor is the minimum threshold of bandwidth that must be available for PCoIP to throttle the stream. Following is an example:<a id="id235" class="indexterm"/>
</p><p>An organization has 500 task workers and is looking to understand how large a network pipe they need to provide for their VMware View solution. The VDI users only use basic office applications and require no other capabilities.</p><p>Average Bandwidth Consumption = (Total Users * 250 Kbps) + (Special Need * Bandwidth Penalty) * Bandwidth Offsite * Buffer</p><p>So, substituting the values given in the preceding example gives us the following output:</p><p>Average Bandwidth Consumption = (500 * 250 Kbps) + 0 * 80 percent = 100,000 KBps (approximately 97 Mbps)</p></div></div><div class="section" title="DHCP considerations"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec02"/>DHCP considerations</h2></div></div></div><p>While it is possible to cobble together a VDI solution that uses static<span class="strong"><strong> Internet Protocol (IP)</strong></span> addresses, it is highly not recommended. Due to the potential volatility of a VDI and for ease of management,<span class="strong"><strong> Dynamic Host Configuration Protocol (DHCP)</strong></span> is the preferred method for managing issuing the IP addresses of the vDesktops. When using DHCP, vDesktops do not own a specific IP address, but rather it leases it from a DHCP server.<a id="id236" class="indexterm"/>
</p><p>A single DHCP scope consists of a pool of IP addresses on a particular subnet. A DHCP superscope allows a DHCP server to distribute IP addresses from more than one scope to devices on a single physical network. Proper subnetting can ensure that enough IP leases exist in a particular scope to serve the number of end devices requiring IP addresses. The following diagram shows a DHCP workflow:</p><div class="mediaobject"><img src="graphics/1124EN_06_11.jpg" alt="DHCP considerations"/></div><p>The workflow of a DHCP lease allocation is as follows:<a id="id237" class="indexterm"/>
</p><div class="orderedlist"><ol class="orderedlist"><li class="listitem">The client broadcasts a DHCPDISCOVER message on its physical subnet.</li><li class="listitem">Available DHCP servers on the subnet respond with an IP address by sending a DHCPOFFER packet back to the client.</li><li class="listitem">A client replies with a DHCPREQUEST message to signal which DHCP server he/she accepted the DHCPOFFER packet from. The other DHCP servers withdraw their offer for a DHCP lease.</li><li class="listitem">The DHCP server in the DHCPREQUEST message from the client replies with a DHCPACK packet to acknowledge the completion of the lease transaction.</li></ol></div><p>DHCP reallocation occurs when a client that already has an address within a valid lease expiration window reboots or starts up after being shut down. When it starts back up, it will contact the DHCP server previously confirmed via a DHCPACK packet to verify the lease and obtain any necessary parameters.<a id="id238" class="indexterm"/>
</p><p>After a set period of time (T1) has elapsed since the original lease allocation, the client will attempt to renew the lease. If the client is unable to successfully renew the lease, it will enter the rebinding phase (starts at T2). During the rebinding phase, it will attempt to obtain a lease from any available DHCP server.</p><div class="mediaobject"><img src="graphics/1124EN_06_12.jpg" alt="DHCP considerations"/></div><p>In the preceding diagram,<span class="strong"><strong> T<sub>1</sub></strong></span> is defined as 50 percent of the lease duration and<span class="strong"><strong> T<sub>2</sub></strong></span> is defined as 90 percent of the lease duration.<a id="id239" class="indexterm"/>
</p><p>For this example, assume a lease duration of 120 minutes (two hours).</p><p>A vDesktop boots and is successfully allocated a DHCP lease for a duration of 120 minutes from DHCP_SERVER_01. At T1 (50 percent of 120 minutes, that is, 60 minutes), the vDesktop attempts to renew its lease from DHCP_SERVER_01. During the renewal period, the vDesktop successfully renews its DHCP lease. The lease clock is now reset back to a full 120 minute lease since the renew was successful.<a id="id240" class="indexterm"/>
</p><p>This time the vDesktop is unsuccessful during the renewal period and enters the rebinding period. The vDesktop successfully obtains a new DHCP lease from DHCP_SERVER_03 with a lease of a fresh 120 minutes.</p><p>In most VDI scenarios, a DHCP lease time of one hour is sufficient. Typically, this is considerably less than the average DHCP lease time in default scopes used by most organizations.</p><p>If a desktop pool is set to delete a vDesktop after a user logs off, this could generate significant DHCP lease thrashing and a very short DHCP lease time should be considered (depending on the frequency of vDesktop deletions).</p><p>VMware View Composer tasks such as Recompose and Refresh should maintain the same MAC address throughout the process as the VMX settings related to the vNIC should not be altered. Therefore, the original lease would attempt to be reallocated during the boot process.<a id="id241" class="indexterm"/>
</p></div><div class="section" title="Virtual switch considerations"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec03"/>Virtual switch considerations</h2></div></div></div><p>Virtual switch design for VDI environments is another component that may prove challenging for those unfamiliar with large-scale virtual infrastructure, or those accustomed to designing solutions with potentially high virtual machine volatility.<a id="id242" class="indexterm"/>
</p><div class="mediaobject"><img src="graphics/1124EN_06_13.jpg" alt="Virtual switch considerations"/></div><p>The preceding diagram shows, at a high level, the network components of a VDI environment. Not shown is the abstraction (that would reside in-between the physical switch and the virtual switch) done by the hypervisor.</p><p>When a standard vSwitch is created it has, by default, 120 ports. This parameter is defined at a kernel (hypervisor) layer, and any changes to the number of ports in a standard switch requires a reboot of the physical host.<a id="id243" class="indexterm"/>
</p><p>When a distributed vSwitch, also known as a<span class="strong"><strong> dvSwitch</strong></span>, is created, it has, by default, 128 ports. This parameter can be changed dynamically and does not require a reboot of the physical host for changing the number of ports from its original value of 128.<a id="id244" class="indexterm"/>
</p><div class="section" title="Standard versus distributed switches"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec02"/>Standard versus distributed switches</h3></div></div></div><p>Standard vSwitches are not impacted by the loss of a VMware vCenter Server, and are best used by functions such as Service console, vMotion, and storage connectivity as they can all be easily managed from the command line. However, in large VDI solutions leveraging multiple<span class="strong"><strong> Virtual Local Area Networks (VLANs)</strong></span>, dozens or hundreds of physical hosts, dvSwitches help to greatly streamline the virtual network management across the virtual infrastructure.<a id="id245" class="indexterm"/>
</p><p>VMware vSphere hosts keep a local cache of dvSwitch, dvPortGroup, and dvPort information to use when the VMware vCenter Server is unavailable. The local cache configuration copies are read-only and cannot be manipulated by the administrator.</p></div><div class="section" title="Port binding"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec03"/>Port binding</h3></div></div></div><p>
<span class="strong"><strong>Port binding</strong></span> is the process of assigning a specific port, also known as a<span class="strong"><strong> dvPort</strong></span>, to a specific<span class="strong"><strong> network interface controller (NIC)</strong></span> on a specific virtual machine. Think of this assignment as analogous to taking a patch cable and plugging one end into the NIC on a physical desktop and the other end into an available switch. dvPorts decide how a virtual machine's network traffic is mapped to a specific distributed port group or<span class="strong"><strong> dvPortGroup.</strong></span>
<a id="id246" class="indexterm"/>
</p><p>There are three types of port bindings used by dvSwitches; they are as follows:<a id="id247" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Static binding</li><li class="listitem" style="list-style-type: disc">Dynamic binding</li><li class="listitem" style="list-style-type: disc">Ephemeral binding</li></ul></div><div class="section" title="Static binding"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec01"/>Static binding</h4></div></div></div><p>Static binding assigns an available port on the dvPortGroup of the dvSwitch when a vNIC is added to a virtual machine. For example, if VM009 is a powered off Windows 2008 virtual machine and the administrator goes into Edit Settings and adds an NIC on dvPortGroup VLAN 71, a dvPort from the VLAN 71 dvPortGroup is assigned to the NIC, assuming one is available. It does not matter if the virtual machine VM009 is powered on or powered off, it is still assigned a dvPort and the dvPort will be unavailable to other virtual machines.<a id="id248" class="indexterm"/>
</p><p>The assigned dvPort is released only when the virtual machine has been removed from the dvPortGroup. Virtual machines using static binding can only be connected to a dvPortGroup through the vCenter Server.</p><p>
<span class="strong"><strong>Advantage:</strong></span> The advantage of static binding is that a virtual machine can be powered on even if the vCenter Server is unavailable. In addition, network statistics are maintained after a vMotion event and after a power cycle of the virtual machine.<a id="id249" class="indexterm"/>
</p><p>
<span class="strong"><strong>Disadvantage:</strong></span> The disadvantage of static binding is that the dvPortGroup cannot be overcommitted. In volatile VDI using non-persistent desktops that are deleted at the time of logoff, it is possible that the solution could run out of available dvPorts on the dvPortGroup. Static binding is strongly discouraged in environments leveraging VMware View Composer.<a id="id250" class="indexterm"/>
</p></div><div class="section" title="Dynamic binding"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec02"/>Dynamic binding</h4></div></div></div><p>Dynamic binding assigns an available dvPort on the dvPortGroup when a virtual machine is powered on and its NIC is in the connected state. For example, if VM009 is a Windows 2008 virtual machine and the administrator goes into Edit Settings and adds an NIC on dvPortGroup VLAN 71, a dvPort from VLAN 71 dvPortGroup is not yet assigned. Once virtual machine VM009 is powered on, it is assigned a dvPort on the dvPortGroup and that specific dvPort will be unavailable to other virtual machines.<a id="id251" class="indexterm"/>
</p><p>The assigned dvPort is released when the virtual machine has been powered down or the NIC is in the disconnected state. Virtual machines using dynamic binding can only be connected to a dvPortGroup through the vCenter Server.</p><p>Dynamic binding is useful in environments where there are more virtual machines than available dvPorts on a given dvPortGroup; however, the number of powered on virtual machines will not exceed the number of available dvPorts on a given dvPortGroup.</p><p>
<span class="strong"><strong>Advantage:</strong></span> The advantage of dynamic binding is that as a virtual machine doesn't occupy a dvPort until it is powered on, it is possible to overcommit the port on a given dvPortGroup. In addition, network statistics are maintained after a vMotion event.<a id="id252" class="indexterm"/>
</p><p>
<span class="strong"><strong>Disadvantage:</strong></span> The disadvantage of dynamic binding is that as a virtual machine isn't assigned a dvPort until it is powered on, it must be powered on by the vCenter Server. Therefore, if the vCenter Server is unavailable, the virtual machine will not be able to be powered on. Network statistics are not maintained after the power cycle of a virtual machine as the dvPort is assigned at the time of boot.<a id="id253" class="indexterm"/>
</p></div><div class="section" title="Ephemeral binding"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec03"/>Ephemeral binding</h4></div></div></div><p>Ephemeral binding creates and assigns a dvPort on the dvPortGroup when a virtual machine is powered on and its NIC is in the connected state. For example, if VM009 is a Windows 2008 virtual machine and the administrator goes into Edit Settings and adds an NIC on dvPortGroup VLAN 71, a dvPort from VLAN71 dvPortGroup is not yet assigned. Once virtual machine VM009 is powered on, a dvPort is first created and then it is assigned a dvPort on the dvPortGroup and that specific dvPort will be unavailable to other virtual machines.<a id="id254" class="indexterm"/>
</p><p>The assigned dvPort is released when the virtual machine has been powered down or the NIC is in the disconnected state. Virtual machines using ephemeral binding can be connected to a dvPortGroup through the vCenter Server or from ESX/ESXi. Therefore, if the vCenter Server is unavailable, the virtual machine network connections can still be managed.</p><p>When a virtual machine is vMotion'd, the original dvPort is deleted from the source dvPortGroup and a new dvPort is created on the destination dvPortGroup.</p><p>Ephemeral binding is useful in environments of high volatility, for example, a non-persistent VDI solution, where virtual machines are created and deleted often. The number of ports on a dvPortGroup is defined and limited by the number of ports available of the dvSwitch.</p><p>
<span class="strong"><strong>Advantage:</strong></span> The advantage of ephemeral binding is that as a virtual machine doesn't occupy a dvPort until it is powered on, it is possible to overcommit the port on a given dvPortGroup.<a id="id255" class="indexterm"/>
</p><p>
<span class="strong"><strong>Disadvantage:</strong></span> Network statistics are not maintained after the power cycle of a virtual machine or after a vMotion event as the dvPort is created and assigned at the time of boot or vMotion.<a id="id256" class="indexterm"/>
</p></div></div><div class="section" title="Port binding and VMware View Composer"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec04"/>Port binding and VMware View Composer</h3></div></div></div><p>For VDI solutions leveraging View Composer, it is important to recognize that tasks such as Recompose, Rebalance, and Refresh will attempt to use the same port that has been assigned to the replica image.<a id="id257" class="indexterm"/>
</p><p>Therefore, it is recommended to use dynamic or ephemeral (preferred) binding if VMware View Composer will be leveraged.</p></div></div></div></div>
<div class="section" title="Compute considerations"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec02"/>Compute considerations</h1></div></div></div><p>Compute is typically not an area of failure in most VDI projects, but it is still important to understand the computing requirements of an organization before implementing a final design. Programs that can cause unforeseen failure from a compute perspective are as follows:<a id="id258" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Dragon Medical/Dragon Naturally Speaking</li><li class="listitem" style="list-style-type: disc">Defense Connect Online</li><li class="listitem" style="list-style-type: disc">AutoCAD</li><li class="listitem" style="list-style-type: disc">Eclipse IDE</li></ul></div><p>For VDI solutions that will be based on Windows XP, one vCPU can likely be used to address most basic computing needs. However, for VDI solutions leveraging Windows Vista, or more importantly Windows 7, two vCPUs may be necessary to ensure a favorable end user experience.</p><p>For the most accurate calculation of CPU requirements, a proper assessment of the environment should be performed. This will help identify potential pitfalls such as some of the applications listed previously, prior to rollout.</p><p>While both AMD and Intel-based x86 servers will suffice for VDI solutions, in large-scale and/or demanding environments, Intel-based solutions have consistently outperformed their AMD counterparts from a density (number of vDesktops per core) perspective.</p><p>In VDI solutions, there is also the potential for unnecessary CPU load due to tasks such as antivirus scanning, poorly-tuned applications, single-threaded processes, added visual effects, and impacts from video or audio processing.</p><div class="mediaobject"><img src="graphics/1124EN_06_14.jpg" alt="Compute considerations"/></div><p>The preceding diagram illustrates a single processor with 6 cores. As a safe baseline, 10 vDesktops per core is used for design purposes. For basic task workers, this number could be significantly higher, and there are multiple reference architectures that validate 15 to 18 vDesktops per core. The use of the Teradici APEX offload card could also increase users per core density.<a id="id259" class="indexterm"/>
</p><p>Continuing to use 10 vDesktops per core as a baseline, and assuming that the server has 2 processors (of 6 cores each) that nets a total of 12 cores per physical server. With 12 cores per server and 10 users per core, that yields 120 users per physical server (6 cores per processor * 2 processors per server * 10 users per core). Using 1.5 GB of RAM for each vDesktop (the minimum recommendation for 64-bit Windows 7), the same physical server needs 180 GB of RAM (1.5 GB * 120 users). That's a relative sweet spot for memory, as most servers are configurable with 256 GB of RAM from the factory.</p><p>The following two tables have been extracted from the<span class="emphasis"><em> Configuration Maximums, VMware vSphere 5.0</em></span> guide at<a class="ulink" href="http://www.vmware.com/pdf/vsphere5/r50/vsphere-50-configuration-maximums.pdf"> http://www.vmware.com/pdf/vsphere5/r50/vsphere-50-configuration-maximums.pdf</a>. The tables explain compute maximums:<a id="id260" class="indexterm"/>
</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Host CPU maximums</p>
</th><th style="text-align: left" valign="bottom">
<p>Maximum</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Logical CPUs per host</p>
</td><td style="text-align: left" valign="top">
<p>160</p>
</td></tr></tbody></table></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Virtual machine maximums</p>
</th><th style="text-align: left" valign="bottom">
<p>Maximum</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Virtual machines per host</p>
</td><td style="text-align: left" valign="top">
<p>512</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Virtual CPUs per host</p>
</td><td style="text-align: left" valign="top">
<p>2048</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Virtual CPUs per core</p>
</td><td style="text-align: left" valign="top">
<p>25</p>
</td></tr></tbody></table></div><p>Given the preceding information, we know that selected processors with significantly more cores per processor (for example, 24 cores per processor or 32 cores per processor) will not help vDesktop density on a given physical server.<a id="id261" class="indexterm"/>
</p><p>The following table explains about memory maximums:<a id="id262" class="indexterm"/>
</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Host memory maximums</p>
</th><th style="text-align: left" valign="bottom">
<p>Maximum</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>RAM per host</p>
</td><td style="text-align: left" valign="top">
<p>2 TB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Maximum RAM allocated to service console</p>
</td><td style="text-align: left" valign="top">
<p>800 MB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Minimum RAM allocated to service console</p>
</td><td style="text-align: left" valign="top">
<p>272 MB</p>
</td></tr></tbody></table></div><p>The reason why increased density will not be realized (or more accurately, maximized), is partly due to memory limitations and also due to existing VMware vSphere limitations. Let's assume, for the sake of argument, that a 32-core physical server was selected as the standard for a given VDI solution and it was shipped with a maximum supported 2 TB of RAM.</p><p>Using the conservative baseline of 10 vDesktops per core, that would yield 320 vDesktops per host, requiring 640 GB of RAM.</p><p>The following table explains about cluster maximums:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Cluster maximums</p>
</th><th style="text-align: left" valign="bottom">
<p>Maximum</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Hosts per cluster</p>
</td><td style="text-align: left" valign="top">
<p>32</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Virtual machines per cluster</p>
</td><td style="text-align: left" valign="top">
<p>3,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Virtual machines per host</p>
</td><td style="text-align: left" valign="top">
<p>512</p>
</td></tr></tbody></table></div><p>Comparing 320 vDesktops per host with the cluster maximums as defined by VMware, the maximum number of virtual machines per host would be reached.</p><p>Furthering the analysis from<span class="emphasis"><em> Configuration Maximums, VMware vSphere</em></span> guide describes, "If more than one configuration option (such as, number of virtual machines, number of LUNs, number of vDS ports, and so on) are used at their maximum limit, some of the processes running on the host might run out of memory." Therefore, it is advised to avoid reaching the configuration maximums when possible.</p><p>As with all portions of a VDI design, it is important to leverage real-world metrics, when possible, to understand how vDesktops will be used, and how they will impact the underlying physical infrastructure.</p><p>Given the preceding calculations, it is advisable to conserve capital expenditure on high core count processors and instead focus the funding elsewhere. In most environments, six, eight, or twelve core processors will be more than sufficient in terms of performance as well as ensuring that vSphere maximums are not reached.</p></div>
<div class="section" title="Working with VMware vSphere maximums"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec03"/>Working with VMware vSphere maximums</h1></div></div></div><p>A strong case can be made that while VMware vSphere is by far the industry-leading hypervisor platform for server virtualization, its current maximums could be limiting in terms of mega-scale VDI environments. The following is a list of vCenter maximums taken from the<span class="emphasis"><em> Configuration Maximums, VMware vSphere</em></span> guide that are most relevant to a VMware View solution:<a id="id263" class="indexterm"/>
</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>vCenter Server scalability</p>
</th><th style="text-align: left" valign="bottom">
<p>Maximum</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Hosts per vCenter Server</p>
</td><td style="text-align: left" valign="top">
<p>1,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Powered on virtual machines per vCenter Server</p>
</td><td style="text-align: left" valign="top">
<p>10,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Registered virtual machines per vCenter Server</p>
</td><td style="text-align: left" valign="top">
<p>15,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Linked vCenter Servers (pod)</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Hosts in linked vCenter Servers</p>
</td><td style="text-align: left" valign="top">
<p>3,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Powered on virtual machines in linked vCenter Servers</p>
</td><td style="text-align: left" valign="top">
<p>30,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Registered virtual machines in linked vCenter Servers</p>
</td><td style="text-align: left" valign="top">
<p>50,000</p>
</td></tr></tbody></table></div><p>The preceding limitations will be analyzed with a solution example in the next section.</p><div class="section" title="Solution example — 25,000 seats of VMware View"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec04"/>Solution example — 25,000 seats of VMware View</h2></div></div></div><p>A VDI architect has been hired by Company, Inc. to design a solution for 25,000 task workers in a single building. In this scenario, the networking and storage will be provided and will meet the necessary requirements of the VDI solution; therefore, the focus is on the physical server specification and the logical design of the VMware vSphere and VMware View environments.</p><p>Company, Inc. is looking for the following information:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Bill of materials (BOM)</strong></span> for physical servers<a id="id264" class="indexterm"/></li><li class="listitem" style="list-style-type: disc">Logical design of the vSphere infrastructure</li><li class="listitem" style="list-style-type: disc">Logical design of the View infrastructure</li></ul></div><p>With a quick look at the requirements, the architect has determined the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Powered on virtual machines per vCenter Server will be exceeded (limit 10,000)</li><li class="listitem" style="list-style-type: disc">Registered virtual machines per vCenter Server will be exceeded (limit 15,000)</li><li class="listitem" style="list-style-type: disc">Powered on virtual machines in linked vCenter Servers will not be exceeded (limit 30,000)</li><li class="listitem" style="list-style-type: disc">Registered virtual machines in linked vCenter Servers will not be exceeded (limit 50,000)</li><li class="listitem" style="list-style-type: disc">Maximum hosts per vCenter Server will not be exceeded (limit 1,000)</li><li class="listitem" style="list-style-type: disc">Maximum virtual machines per host will not be exceeded (limit 320)</li></ul></div><div class="section" title="Solution design — physical server requirements"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec05"/>Solution design — physical server requirements</h3></div></div></div><p>To support 25,000 task workers running Windows 7 vDesktops, the physical server sizing must be determined. Through initial testing, 10 vDesktops per core was a conservative estimate. As 4-core processors are being phased out, 6-core processors were chosen for their price and availability. Therefore, with 2 6-core processors per physical host, that yields 12 cores per host. Using 10 vDesktops per core and 12 cores per host yields 120 vDesktops per host. With 1.5 GB per vDesktop used for the environment, 180 GB of RAM is required for vDesktops. By allocating the maximum supported, 800 MB of RAM to the service console, that yields 181 GB of RAM required. Therefore, a server with 192 GB of RAM will support the environment nicely. In addition, the following vNetwork maximums exist:<a id="id265" class="indexterm"/>
</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>vNetwork Standard &amp; Distributed Switch</p>
</th><th style="text-align: left" valign="bottom">
<p>Maximum</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Total virtual network ports per host</p>
</td><td style="text-align: left" valign="top">
<p>4,096</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Maximum active ports per host</p>
</td><td style="text-align: left" valign="top">
<p>1,016</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Distributed switches per vCenter</p>
</td><td style="text-align: left" valign="top">
<p>32</p>
</td></tr></tbody></table></div><p>Given the preceding maximums, the following physical host design was leveraged:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Description</p>
</th><th style="text-align: left" valign="bottom">
<p>Value</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Cores per processor</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Processors per host</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>NICs per host</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Memory per host (GB)</p>
</td><td style="text-align: left" valign="top">
<p>192</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Approximate vDesktops per core</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Approximate vDesktops per host</p>
</td><td style="text-align: left" valign="top">
<p>120</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Standard vSwitches</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Distributed vSwitches</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr></tbody></table></div><p>The networking configuration is as follows:<a id="id266" class="indexterm"/>
</p><div class="mediaobject"><img src="graphics/1124EN_06_15.jpg" alt="Solution design — physical server requirements"/></div><p>The preceding diagram represents two vNetwork standard switches and one vNetwork distributed switch. The first standard vSwitch, vS1, is used for service console and vMotion.The second standard vSwitch, vS2, is used for network-based storage. The only distributed vSwitch, vD1, is used for virtual machines.<a id="id267" class="indexterm"/>
</p></div><div class="section" title="Solution design — the pod concept"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec06"/>Solution design — the pod concept</h3></div></div></div><p>The concept of the pod is to give architects a method of creating building blocks to ease the design scalability for large environments. It also provides a conceptual framework for the solution architecture.<a id="id268" class="indexterm"/>
</p><div class="mediaobject"><img src="graphics/1124EN_06_16.jpg" alt="Solution design — the pod concept"/></div><p>The main components of a VMware View pod are as follows:<a id="id269" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Physical network:</strong></span> This includes necessary switches, VLANs, network policies, and other network infrastructure required to support the VDI</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>vCenter blocks:</strong></span> This includes hosts, vCenter cluster design, vCenter Linked Mode configuration, and so on</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>View Connection Server pools:</strong></span> This includes View Connection Servers and (if applicable) View Security Servers</li></ul></div><p>This concept of a pod can be carried through with the following architecture types:<a id="id270" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Traditional</li><li class="listitem" style="list-style-type: disc">Traditional in modular form</li><li class="listitem" style="list-style-type: disc">Converged virtualization appliances</li></ul></div><p>The<span class="strong"><strong> traditional</strong></span> architecture type involves using servers (rackmount or blade), network switches, storage network switches (if applicable), and storage arrays. A traditional architecture approach is normally sufficient for an initial build-out but may not offer the scale-out capabilities of other approaches. The following diagram shows an illustration of a typical traditional architecture approach where disproportionate resources exist:<a id="id271" class="indexterm"/>
</p><div class="mediaobject"><img src="graphics/1124EN_06_17.jpg" alt="Solution design — the pod concept"/></div><p>For example, using the preceding diagram, sufficient compute, network, and storage resources may exist for the initial rollout of 400 VMware View users. In this example, an overabundance of storage capacity exists.</p><p>The following diagram shows an illustration of a typical traditional architecture scale-out challenge:<a id="id272" class="indexterm"/>
</p><div class="mediaobject"><img src="graphics/1124EN_06_18.jpg" alt="Solution design — the pod concept"/></div><p>When the organization decides to add an additional 500 VMware View users, it runs into a problem. In the phase 1 rollout, an overabundance of storage capacity existed. However, to add capacity in a modular fashion, compute and network will still require an additional block of storage. Therefore, every addition will have some level of excess, which drives the price per vDesktop up due to architectural inefficiencies.<a id="id273" class="indexterm"/>
</p><p>An organization likely would not want to accept these inefficiencies so it would redesign its requirements every step of the way. Designing the scale out for every additional phase of a VDI solution also drives up cost through added complexity and man hours.<a id="id274" class="indexterm"/>
</p><p>In addition, every time a scale-out phase is re-architected, the chance of error becomes greater.</p><p>The<span class="strong"><strong> traditional in modular form</strong></span> architecture type involves using servers (rackmount or blade), network switches, storage network switches (if applicable), and storage arrays. Whereas, a traditional architecture is normally not able to scale proportionately, a traditional in modular form is designed to scale in building blocks. This approach does not need re-engineering for each scale-out phase, and instead an organization relies on the traditional yet modular architecture for predictable scale-out design.<a id="id275" class="indexterm"/>
</p><p>The following diagram shows an illustration of a typical traditional in modular form architecture approach, where proportionate resources exist:</p><div class="mediaobject"><img src="graphics/1124EN_06_19.jpg" alt="Solution design — the pod concept"/></div><p>There are typically two ways to implement a traditional in modular form architecture. The first is by spending the time to architect and test a customer design, where compute (for example, Dell blade) is combined with network switches (for example, Cisco) and a storage array (for example, NetApp). The danger with this approach is that if the person or team designing the solution has never designed a VDI solution before, they are likely to have a few lessons learned through the process that will yield a less than optimal solution. This is not to say that this approach is not suitable and should not be taken, but special considerations should be taken to ensure the architecture is sound and scalable. A seasoned VDI architect can take any off-the-shelf hardware and build a sustainable VDI architecture.<a id="id276" class="indexterm"/>
</p><p>The second way to implement a traditional in modular form architecture is by implementing a branded solution such as the VCE Vblock (Cisco servers + Cisco switches + EMC storage) or FlexPod (Cisco servers + Cisco switches + NetApp storage), for example. These solutions are proven, scalable in a predictive manner, and they offer a known architecture for VDI. The drawback of these solutions is that they often have a high barrier to entry in terms of cost and scale out in large modular blocks (for example, 1,000 users at a time).<a id="id277" class="indexterm"/>
</p><p>The third type of architecture uses<span class="strong"><strong> converged virtualization appliances</strong></span>. Converged virtualization appliances are typically 2U to 6U appliances that comprise of one to many ESXi servers with local storage that is often shared among the ESXi servers in the appliance. The storage is typically shared through a virtual storage appliance model, where local storage is represented as either iSCSI or NFS storage to one or more ESXi servers in the appliance. The converged virtualization appliance model is relatively new to the VDI market.<a id="id278" class="indexterm"/>
</p><div class="section" title="Linked vCenter Servers"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec04"/>Linked vCenter Servers</h4></div></div></div><p>As the number of virtual machines per vCenter Server will be exceeded, more than one vCenter Server will be required for this solution.<a id="id279" class="indexterm"/>
</p><p>The following table illustrates the vCenter maximums:<a id="id280" class="indexterm"/>
</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>vCenter Server scalability</p>
</th><th style="text-align: left" valign="bottom">
<p>Maximum</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Powered on virtual machines per vCenter Server</p>
</td><td style="text-align: left" valign="top">
<p>10,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Registered virtual machines per vCenter Server</p>
</td><td style="text-align: left" valign="top">
<p>15,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Linked vCenter Servers</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Powered on virtual machines in linked vCenter Servers</p>
</td><td style="text-align: left" valign="top">
<p>30,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Registered virtual machines in linked vCenter Servers</p>
</td><td style="text-align: left" valign="top">
<p>50,000</p>
</td></tr></tbody></table></div><p>vCenter Linked Mode has a few basic prerequisites. They are as follows:<a id="id281" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Both vCenter Servers must reside in a functional DNS environment, where<span class="strong"><strong> fully qualified domain names (FQDNs)</strong></span> of each vCenter Server can be resolved properly<a id="id282" class="indexterm"/></li><li class="listitem" style="list-style-type: disc">Any vCenter Server participating in Linked Mode must reside in an Active Directory domain<a id="id283" class="indexterm"/></li><li class="listitem" style="list-style-type: disc">If the vCenter Servers are in separate Active Directory domains, the respective domains must have a two-way trust</li><li class="listitem" style="list-style-type: disc">Both vCenter Servers must reside in a functional<span class="strong"><strong> Network Time Protocol (NTP)</strong></span> environment, where time synchronization of the vCenter Servers is no more than 5 minutes adrift of one another<a id="id284" class="indexterm"/></li><li class="listitem" style="list-style-type: disc">Windows RPC port mapper must be allowed to open the<span class="strong"><strong> Remote Procedure Call (RPC)</strong></span> ports for replication; this is covered in detail at<a class="ulink" href="http://support.microsoft.com/kb/154596"> http://support.microsoft.com/kb/154596</a><a id="id285" class="indexterm"/></li><li class="listitem" style="list-style-type: disc">Both VMware vCenter Servers have the VMware vCenter Standard Edition license (versus foundation, for example)</li><li class="listitem" style="list-style-type: disc">Separate databases for each VMware vCenter Server</li></ul></div><div class="mediaobject"><img src="graphics/1124EN_06_20.jpg" alt="Linked vCenter Servers"/></div><p>VMware vCenter Linked Mode connects two or more vCenter Servers together via ADAM database replication to store information regarding user roles as well as VMware licensing. VMware vCenter Linked Mode does not do any form of database replication. If VMware vCenter Linked Mode would fail for any reason, the two (or more) vCenter Servers would still be viable as standalone instances.<a id="id286" class="indexterm"/>
</p><div class="mediaobject"><img src="graphics/1124EN_06_21.jpg" alt="Linked vCenter Servers"/></div><p>As shown in the preceding diagram, where there are two separate vCenter Server instances (vCenter1 and vCenter2), the virtual data centers, clusters, resource pools, and virtual machines are unique to their respective instance of vCenter.<a id="id287" class="indexterm"/>
</p><p>Joining multiple vCenters together with vCenter Linked Mode forms what is known as a pod. A pod can consist of up to 10 vCenter Servers in Linked Mode.</p></div><div class="section" title="vCenter Servers"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec05"/>vCenter Servers</h4></div></div></div><p>Using calculations from preceding sections, this solution is expected to have approximately 120 vDesktops per host; this means that 209 physical hosts are needed to support the vDesktop portion of this solution (not taking into account a virtualized vCenter, database, and so on).<a id="id288" class="indexterm"/>
</p><p>Due to the nature of the end user population, the time they log in, the conservative nature of the original assessment (for example, 10 vDesktops per core), it has been decided that there will be no HA requirements for the vSphere Servers supporting vDesktops.</p><p>It has also been determined that the management infrastructure including the View Connection Servers, vCenter Servers, database server, and a few other components require three physical hosts. In order to provide a level of protection, it has been determined to use an<span class="emphasis"><em> n</em></span> + 1 solution and utilize 4 physical hosts.</p><p>It was determined previously that any given vCenter can have a maximum of 10,000 powered on virtual machines at any given time. This solution will need to support more than 25,000 powered on virtual machines; therefore, this solution will require 3 vCenter Servers.<a id="id289" class="indexterm"/>
</p><p>To balance the load across the vCenter Servers, the clusters have been as equitably divided as possible.<a id="id290" class="indexterm"/>
</p><p>Note that the naming conventions used for the clusters in this example are:<a id="id291" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">vCenter Server: vc-{letter}, for example, vc-b</li><li class="listitem" style="list-style-type: disc">Clusters: cl-{letter of vCenter}-{number}, for example, cl-c-6</li></ul></div><p>The vCenter Servers are named as vc-a, vc-b, vc-c, respectively. Their details along with the diagrams are as follows:</p><div class="mediaobject"><img src="graphics/1124EN_06_22.jpg" alt="vCenter Servers"/></div><p>The preceding diagram explains about vCenter Server<span class="strong"><strong> vc-a</strong></span>. The following list gives the details about vc-a:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">9 clusters of 8 hosts each (cl-a-1, cl-a-2, ..., cl-a-9)<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Total of 72 hosts</li><li class="listitem" style="list-style-type: disc">Total of 8,640 vDesktops (120 vDesktops per host multiplied by 72 hosts)</li></ul></div></li></ul></div><div class="mediaobject"><img src="graphics/1124EN_06_23.jpg" alt="vCenter Servers"/></div><p>The preceding diagram explains about vCenter Server<span class="strong"><strong> vc-b</strong></span>. The following list gives the details about vc-b:<a id="id292" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">9 clusters of 8 hosts each (cl-b-1, cl-b-2, ..., cl-b-9)<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Total of 72 hosts</li><li class="listitem" style="list-style-type: disc">Total of 8,640 vDesktops (120 vDesktops per host multiplied by 72 hosts)</li></ul></div></li></ul></div><div class="mediaobject"><img src="graphics/1124EN_06_24.jpg" alt="vCenter Servers"/></div><p>The preceding diagram explains about vCenter Server<span class="strong"><strong> vc-c</strong></span>. The following list gives the details about vc-c:<a id="id293" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">7 clusters each having 8 hosts</li><li class="listitem" style="list-style-type: disc">1 cluster of 5 hosts</li><li class="listitem" style="list-style-type: disc">1 cluster of 4 hosts</li><li class="listitem" style="list-style-type: disc">1 cluster of 4 hosts dedicated to management<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Total of 69 hosts</li><li class="listitem" style="list-style-type: disc">Total of 7,800 vDesktops and approximately 30 vServers (View Connection Server, database server, vCenter server, and so on)</li></ul></div></li></ul></div><p>vCenter vc-c has a cluster (cl-c-10) dedicated for hosting the infrastructure virtual machines. These virtual machines include:<a id="id294" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">3 VMware vCenter Servers (vc-a, vc-b, vc-c)</li><li class="listitem" style="list-style-type: disc">15 View Connection Servers</li><li class="listitem" style="list-style-type: disc">Supporting infrastructure (if needed) such as database servers, Liquidware Labs TM, and so on<a id="id295" class="indexterm"/><div class="mediaobject"><img src="graphics/1124EN_06_25.jpg" alt="vCenter Servers"/></div></li></ul></div></div><div class="section" title="VMware Update Manager Servers"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec06"/>VMware Update Manager Servers</h4></div></div></div><p>VMware Update Manager is a solution that automated the application of patches to both vSphere Servers and virtual machines. It's most often used to patch vSphere Servers in large environments as it handles the task of placing a host in maintenance mode, migrating virtual machines, patch application, reboots, and normalization with a minimal amount of user interaction.<a id="id296" class="indexterm"/>
</p><p>VMware Update Manager Servers can only be paired with one VMware vCenter Server instance at a time. Therefore, in this solution three VMware Update Manager Servers will be required (one per vCenter Server instance).<a id="id297" class="indexterm"/>
</p><div class="mediaobject"><img src="graphics/1124EN_06_26.jpg" alt="VMware Update Manager Servers"/></div><div class="section" title="VMware vCenter Server Heartbeat"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl5sec01"/>VMware vCenter Server Heartbeat</h5></div></div></div><p>In this section, we have added a note about VMware vCenter Server Heartbeat. In most VMware View solutions, one or more highly available VMware vCenter Servers are required. vCenter Server is of paramount importance because if vCenter is unavailable, the following problems would be faced:<a id="id298" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">New vDesktops cannot be provisioned</li><li class="listitem" style="list-style-type: disc">vDesktops cannot be recomposed, refreshed, or rebalanced</li><li class="listitem" style="list-style-type: disc">vDesktops cannot be deleted from the View Admin console</li></ul></div><p>Therefore, vCenter Server Heartbeat is often an affordable insurance policy for the vCenter Servers in a VDI solution.</p><p>As noted previously, VMware Update Manager can only be linked to one instance of the VMware vCenter Server. However, it's important to note that a pair of vCenter Servers joined by VMware vCenter Server Heartbeat is considered to be only one instance.</p><p>Therefore, the solution does not require additional VMware Update Manager Servers just because VMware vCenter Server Heartbeat is being leveraged.</p></div></div></div><div class="section" title="Solution design — pools"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec07"/>Solution design — pools</h3></div></div></div><p>Here, we will cover View Connection Servers.<a id="id299" class="indexterm"/>
</p><div class="section" title="View Connection Servers"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec07"/>View Connection Servers</h4></div></div></div><p>As illustrated next, the VMware View infrastructure introduces its own maximums in addition to those already imposed by the VMware vSphere infrastructure. The View Connection maximums are given in the following table:<a id="id300" class="indexterm"/>
</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Connection Servers per deployment</p>
</th><th style="text-align: left" valign="bottom">
<p>Maximum</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1 Connection Server supporting direct RDP or PCoIP</p>
</td><td style="text-align: left" valign="top">
<p>2,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>7 Connection Servers (5 hot + 2 spare) supporting direct RDP or PCoIP</p>
</td><td style="text-align: left" valign="top">
<p>10,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Maximum hosts in a cluster when not using View Composer</p>
</td><td style="text-align: left" valign="top">
<p>32</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Maximum hosts in a cluster when using View Composer</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td></tr></tbody></table></div><p>If a solution like Unidesk TM was used in lieu of View Composer, the end design could support more hosts per cluster.</p><p>For the solution example, whereby 25,000 vDesktops must be supported, it's important to understand how many end users will be logging in at any given time. A VMware View Connection Server can support 2,000 direct PCoIP connections at any given time. In this example, all 25,000 end users could potentially log in at the same time. Therefore, a minimum of 13 View Connection Servers are required (2,000 * 13 = 26,000 simultaneous direct PCoIP connections supported).<a id="id301" class="indexterm"/>
</p><div class="mediaobject"><img src="graphics/1124EN_06_27.jpg" alt="View Connection Servers"/></div><p>In order to provide a level of redundancy in case of a View Connection Server outage, it is advised to add in<span class="emphasis"><em> n</em></span> + 2 (or more) solutions. For example, increasing the required number of View Connection Servers, that is, 13 to a total of 15 View Connection Servers, provides the ability to support a maximum of 30,000 simultaneous PCoIP connections. Therefore, even if two View Connection Servers fail, all 25,000 users would be able to log in to the VDI without incident.</p><p>The 15 View Connection Servers should be placed behind a redundant load balancing solution and should be configured to check that the View Connection Server is online via a simple ping (if<span class="strong"><strong> Internet Control Message Protocol (ICMP)</strong></span> is allowed) and HTTP GET on the View Connection Server's URL. The entire pool of View Connection Servers should be accessible by a single name, such as<code class="literal"> view.customer.com</code>, whereby end users would use<code class="literal"> https://view.customer.com</code> to access the View environment.<a id="id302" class="indexterm"/>
</p><p>By leveraging the HTTP GET to verify functionality of a View Connection Server, a server whose applicable services have stopped will not successfully reply to the GET command and, therefore, will be removed from the load balancing pool.</p></div></div><div class="section" title="Solution design — the formulae"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec08"/>Solution design — the formulae</h3></div></div></div><p>The following are some formulae to calculate the minimum number of vCenter Servers, Connection Servers, and Pods:<a id="id303" class="indexterm"/>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Minimum number of vCenter Servers = Number of Desktops / 10,000</li><li class="listitem" style="list-style-type: disc">Minimum number of View Connection Servers = Number of Simultaneous Connections / 2,000</li><li class="listitem" style="list-style-type: disc">Minimum number of vCenter Pods = Number of vCenter Servers / 10</li></ul></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec04"/>Summary</h1></div></div></div><p>As detailed in this chapter, there are many design considerations to make such as DHCP lease time, the minimum number of vCenters, and the number of cores to buy in a server platform. For large environments of thousands of vDesktops, it may be easiest to start with the vSphere maximums and work down. For small environments or PoCs that don't require a massive virtual infrastructure, the concepts covered in this chapter are still relevant as a successful PoC can grow rapidly in adoption. Finally, the concept of a pod architecture, or a collection of vCenter Servers, is typically new to those familiar only with designing virtual server solutions on the VMware vSphere platform. They can take some time to understand the new concepts and working up against the vSphere and vCenter maximums.</p></div></body></html>