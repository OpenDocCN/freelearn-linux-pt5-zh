- en: Tangled Web? Not At All!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading from a web page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading a web page as plain text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A primer on cURL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing unread Gmail e-mails from the command line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing data from a website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image crawler and downloader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web photo album generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Twitter command-line client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing word definitions via a web server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding broken links in a website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking changes to a website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Posting to a web page and reading the response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading a video from the Internet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing text with OTS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating text from the command line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Web has become the face of technology and the central access point for data
    processing. Shell scripts cannot do everything that languages such as PHP can
    do on the Web, but there are many tasks for which shell scripts are ideally suited.
    We will explore recipes to download and parse website data, send data to forms,
    and automate website-usage tasks and similar activities. We can automate many
    activities that we perform interactively through a browser with a few lines of
    scripting. The functionality provided by the HTTP protocol and command-line utilities
    enables us to write scripts to solve many web-automation needs.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading from a web page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Downloading a file or a web page is simple. A few command-line download utilities
    are available to perform this task.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`wget` is a flexible file download command-line utility that can be configured
    with many options.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A web page or a remote file can be downloaded using `wget`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to specify multiple download URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, the downloaded files are named the same as the URL, and the download
    information and progress is written to `stdout`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `-O` option specifies the output filename. If a file with that name already
    exists, it will be replaced by the downloaded file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `-o` option specifies a `logfile` instead of printing logs to `stdout`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Using the preceding command will print nothing on the screen. The log or progress
    will be written to the log and the output file will be `dloaded_file.img`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a chance that downloads might break due to unstable Internet connections.
    The `-t` option specifies how many times the utility will retry before giving
    up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Use a value of `0` to force `wget` to keep trying infinitely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `wget` utility has options to fine-tune behavior and solve problems.
  prefs: []
  type: TYPE_NORMAL
- en: Restricting the download speed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When there is limited bandwidth with many applications sharing it, a large
    file can devour all the bandwidth and starve other processes (perhaps interactive
    users). The `wget` option `-limit-rate` will specify the maximum bandwidth for
    the download job, allowing all applications fair access to the Internet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this command, `k` (kilobyte) specifies the speed limit. You can also use
    `m` for megabyte.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `-quota` (or `-Q`) option specifies the maximum size of the download. `wget`
    will stop when the quota is exceeded. This is useful when downloading multiple
    files to a system with limited space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Resume downloading and continue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If `wget` gets interrupted before the download is complete, it can be resumed
    where it left off with the `-c` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Copying a complete website (mirroring)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`wget` can download a complete website by recursively collecting the URL links
    and downloading them like a crawler. To download the pages, use the `--mirror`
    option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `-l` option specifies the depth of web pages as levels. This means that
    it will traverse only that number of levels. It is used along with `-r` (recursive).
    The `-N` argument is used to enable time stamping for the file. `URL` is the base
    URL for a website for which the download needs to be initiated. The `-k` or `--convert-links`
    option instructs `wget` to convert the links to other pages to the local copy.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise discretion when mirroring other websites. Unless you have permission,
    only perform this for your personal use and don't do it too frequently.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing pages with HTTP or FTP authentication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `--user` and `--password` arguments provide the username and password to
    websites that require authentication.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: It is also possible to ask for a password without specifying the password inline.
    For this, use `--ask-password` instead of the `--password` argument.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading a web page as plain text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web pages are simply text with HTML tags, JavaScript, and CSS. The HTML tags
    define the content of the web page, which we can parse for specific content. Bash
    scripts can parse web pages. An HTML file can be viewed in a web browser to see
    it properly formatted or processed with tools described in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing a text document is simpler than parsing HTML data because we aren't
    required to strip off the HTML tags. **Lynx** is a command-line web browser that downloads
    a web page as plain text.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lynx is not installed in all distributions, but is available via the package
    manager.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `-dump` option downloads a web page as pure ASCII. The next recipe shows
    how to send that ASCII version of the page to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This command will list all the hyperlinks (`<a href="link">`) separately under
    a heading `References`, as the footer of the text output. This lets us parse links
    separately with regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the plain text version of `text` using the `cat` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: A primer on cURL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**cURL** transfers data to or from a server using the HTTP, HTTPS, or FTP protocols.
    It supports `POST`, cookies, authentication, downloading partial files from a
    specified offset, referer, user agent string, extra headers, limiting speed, maximum
    file size, progress bar, and more. cURL is useful for maintaining a website, retrieving
    data, and checking server configurations.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike `wget`, cURL is not included in all Linux distros; you may have to install
    it with your package manager.
  prefs: []
  type: TYPE_NORMAL
- en: By default, cURL dumps downloaded files to `stdout`, and progress information
    to `stderr`. To disable displaying progress information, use the `--silent` option.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `curl` command performs many functions, including downloading, sending different
    HTTP requests, and specifying HTTP headers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To dump the downloaded file to `stdout`, use the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `-O` option specifies sending the downloaded data into a file with the filename
    parsed from the URL. Note that the URL must be a full page URL, not just a site
    name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `-o` option specifies the output file name. With this option you can specify
    only the site name to retrieve the home page.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `-silent` option prevents the `curl` command from displaying progress information:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `-progress` option displays progress bar while downloading:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: cURL downloads web pages or remote files to your local system. You can control
    the destination filename with the `-O` and `-o` options, and verbosity with the
    `-silent` and `-progress` options.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding sections, you learned how to download files. cURL supports
    more options to fine tune its behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing and resuming downloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: cURL can resume a download from a given offset. This is useful if you have a
    per-day data limit and a large file to download.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: offset is an integer value in bytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'cURL doesn''t require us to know the exact byte offset, if we want to resume
    downloading a file. If you want cURL to figure out the correct resume point, use
    the `-C -` option, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: cURL will automatically figure out where to restart the download of the specified
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the referer string with cURL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Referer** field in the HTTP header identifies the page that led to the
    current web page. When a user clicks on a link on web page A to go to web page
    B, the referer header string for page B will contain the URL of page A.
  prefs: []
  type: TYPE_NORMAL
- en: Some dynamic pages check the referer string before returning the HTML data.
    For example, a web page may display a Google logo when a user navigates to a website
    from Google, and display a different page when the user types the URL.
  prefs: []
  type: TYPE_NORMAL
- en: A web developer can write a condition to return a Google page if the referer
    is www.google.com, or return a different page if not.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use `--referer` with the `curl` command to specify the referer string,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Cookies with cURL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`curl` can specify and store the cookies encountered during HTTP operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `-cookie``COOKIE_IDENTIFER` option specifies which cookies to provide.
    Cookies are defined as `name=value`. Multiple cookies should be delimited with
    a semicolon (`;`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `-cookie-jar` option specifies the file to store cookies in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Setting a user agent string with cURL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some web pages that check the user agent won't work if there is no user agent
    specified. For example, some old websites require **Internet Explorer** (**IE**).
    If a different browser is used, they display a message that the site must be viewed
    with IE. This is because the website checks for a user agent. You can set the
    user agent with `curl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `--user-agent` or `-A` option sets the user agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Additional headers can be passed with cURL. Use `-H "Header"` to pass additional
    headers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: There are many different user agent strings across multiple browsers and crawlers
    on the Web. You can find a list of some of them at [http://www.useragentstring.com/pages/useragentstring.php](http://www.useragentstring.com/pages/useragentstring.php).
  prefs: []
  type: TYPE_NORMAL
- en: Specifying a bandwidth limit on cURL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When bandwidth is shared among multiple users, we can limit the download rate
    with the `--limit-rate` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The rate can be specified with `k` (kilobyte) or `m` (megabyte).
  prefs: []
  type: TYPE_NORMAL
- en: Specifying the maximum download size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `--max-filesize` option specifies the maximum file size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `curl` command will return a non-zero exit code if the file size exceeds
    the limit or a zero if the download succeeds.
  prefs: []
  type: TYPE_NORMAL
- en: Authenticating with cURL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `curl` command's  `-u` option performs HTTP or FTP authentication.
  prefs: []
  type: TYPE_NORMAL
- en: 'The username and password can be specified using `-u username:password`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'If you prefer to be prompted for the password, provide only a username:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Printing response headers excluding data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Examining headers is sufficient for many checks and statistics. For example,
    we don't need to download an entire page to confirm it is reachable. Just reading
    the HTTP response is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Another use case for examining the HTTP header is to check the `Content-Length`
    field to determine the file size or the `Last-Modified` field to see if the file
    is newer than a current copy before downloading.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `-I` or `-head` option outputs only the HTTP headers, without downloading
    the remote file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Posting to a web page and reading the response* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing unread Gmail e-mails from the command line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gmail is a widely-used free e-mail service from Google: [http://mail.google.com/](http://mail.google.com/).
    It allows you to read your mail via a browser or an authenticated RSS feeds. We
    can parse the RSS feeds to report the sender name and subject. This is a quick
    way to scan unread e-mails without opening the web browser.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go through a shell script to parse the RSS feeds for Gmail to display
    the unread mails:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output resembles this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: If you use a Gmail account with two-factor authentication, you will have to
    generate a new key for this script and use it. Your regular password won't work.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The script uses cURL to download the RSS feed. You can view the format of the
    incoming data by logging in to your Gmail account and viewing [https://mail.google.com/mail/feed/atom](https://mail.google.com/mail/feed/atom).
  prefs: []
  type: TYPE_NORMAL
- en: cURL reads the RSS feed with the user authentication provided by the `-u user:pass`
    argument. When you use `-u user` without the password cURL, it will interactively
    ask for the password.
  prefs: []
  type: TYPE_NORMAL
- en: '`tr -d ''\n''`: This removes the newline characters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sed ''s:</entry>:\n:g''`: This replaces every `</entry>` element with a newline,
    so each e-mail entry is delimited by a new line and, hence, mails can be parsed
    one-by-one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next block of script that needs to be executed as one single expression
    uses `sed` to extract the relevant fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This script matches the title with the `<title>\(.*\)<\/title` regular expression,
    the sender name with the `<author><name>\([^<]*\)<\/name>` regular expression,
    and e-mail using `<email>\([^<]*\)`. Sed uses back referencing to display the
    author, title, and subject of the e-mail into an easy to read format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`\1` corresponds to the first substring match (title), `\2` for the second
    substring match (name), and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: The `SHOW_COUNT=5` variable is used to take the number of unread mail entries
    to be printed on the terminal.
  prefs: []
  type: TYPE_NORMAL
- en: '`head` is used to display only the `SHOW_COUNT*3` lines from the first line.
    `SHOW_COUNT` is multiplied by three in order to show three lines of output.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *A primer on cURL* recipe in this chapter explains the `curl` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Using sed to perform text replacement* recipe in [Chapter 4](22424a9e-fea7-49de-9589-ea32aeb0b829.xhtml),
    *Texting and Driving,* explains the `sed` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing data from a website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `lynx`, `sed`, and `awk` commands can be used to mine data from websites.
    You might have come across a list of actress rankings in a *Searching and mining
    text inside a file with grep *recipe in [Chapter 4](22424a9e-fea7-49de-9589-ea32aeb0b829.xhtml),
    *Texting and Driving*; it was generated by parsing the [http://www.johntorres.net/BoxOfficefemaleList.html](http://www.johntorres.net/BoxOfficefemaleList.html)
    web page.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go through the commands used to parse details of actresses from the
    website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lynx is a command-line web browser; it can dump a text version of a website
    as we will see in a web browser, instead of returning the raw HTML as `wget` or
    cURL does. This saves the step of removing HTML tags. The `-nolist` option shows
    the links without numbers. Parsing and formatting the lines that contain Rank
    is done with `sed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: These lines are then sorted according to the ranks.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Using sed to perform text replacement* recipe in [Chapter 4](22424a9e-fea7-49de-9589-ea32aeb0b829.xhtml),
    *Texting and Driving*, explains the `sed` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Downloading a web page as plain text* recipe in this chapter explains the
    `lynx` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image crawler and downloader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Image crawlers** download all the images that appear in a web page. Instead
    of going through the HTML page to pick the images by hand, we can use a script
    to identify the images and download them automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This Bash script will identify and download the images from a web page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'An example usage is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The image downloader script reads an HTML page, strips out all tags except `<img>`,
    parses `src="img/URL"` from the `<img>` tag, and downloads them to the specified
    directory. This script accepts a web page URL and the destination directory as
    command-line arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `[ $# -ne 3 ]` statement checks whether the total number of arguments to
    the script is three, otherwise it exits and returns a usage example. Otherwise,
    this code parses the URL and destination directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The `while` loop runs until all the arguments are processed. The `shift` command
    shifts arguments to the left so that `$1` will take the next argument's value;
    that is, `$2`, and so on. Hence, we can evaluate all arguments through `$1` itself.
  prefs: []
  type: TYPE_NORMAL
- en: The `case` statement checks the first argument (`$1`). If that matches `-d`,
    the next argument must be a directory name, so the arguments are shifted and the
    directory name is saved. If the argument is any other string it is a URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of parsing arguments in this way is that we can place the -d
    argument anywhere in the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Or:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '`egrep -o "<img src=[^>]*>"` will print only the matching strings, which are
    the `<img>` tags including their attributes. The `[^>]*` phrase matches all the
    characters except the closing `>`, that is, `<img src="img/image.jpg">`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`sed''s/<img src=\"\([^"]*\).*/\1/g''` extracts the `url` from the `src="img/url"`
    string.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of image source paths: relative and absolute. **Absolute
    paths** contain full URLs that start with `http://` or `https://`. Relative URLs
    starts with `/` or `image_name` itself. An example of an absolute URL is `http://example.com/image.jpg`.
    An example of a relative URL is `/image.jpg`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For relative URLs, the starting `/` should be replaced with the base URL to
    transform it to `http://example.com/image.jpg`. The script initializes `baseurl`
    by extracting it from the initial URL with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previously described `sed` command is piped into another
    sed command to replace a leading `/` with `baseurl`, and the results are saved
    in a file named for the script''s PID: (`/tmp/$$.list`).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The final `while` loop iterates through each line of the list and uses curl
    to download the images. The `--silent` argument is used with `curl` to avoid extra
    progress messages from being printed on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *A primer on cURL* recipe in this chapter explains the `curl` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Using sed to perform text replacement* recipe in [Chapter 4](22424a9e-fea7-49de-9589-ea32aeb0b829.xhtml),
    *Texting and Driving*  explains the `sed` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Searching and mining text inside a file with grep* recipe in [Chapter 4](22424a9e-fea7-49de-9589-ea32aeb0b829.xhtml),
    *Texting and Driving*, explains the `grep` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web photo album generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web developers frequently create photo albums of full-size and thumbnail images.
    When a thumbnail is clicked, a large version of the picture is displayed. This
    requires resizing and placing many images. These actions can be automated with
    a simple Bash script. The script creates thumbnails, places them in exact directories,
    and generates the code fragment for `<img>` tags automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This script uses a `for` loop to iterate over every image in the current directory.
    The usual Bash utilities such as `cat` and `convert` (from the Image Magick package)
    are used. These will generate an HTML album, using all the images, in `index.html`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This Bash script will generate an HTML album page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the script as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The initial part of the script is used to write the header part of the HTML
    page.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following script redirects all the contents up to `EOF1` to `index.html`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The header includes the HTML and CSS styling.
  prefs: []
  type: TYPE_NORMAL
- en: '`for img in *.jpg *.JPG;` iterates over the filenames and evaluates the body
    of the loop.'
  prefs: []
  type: TYPE_NORMAL
- en: '`convert "$img" -resize "100x" "thumbs/$img"` creates images 100px-wide as
    thumbnails.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following statement generates the required `<img>` tag and appends it to
    `index.html`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the footer HTML tags are appended with `cat` as  in the first part
    of the script.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Web photo album generator* recipe in this chapter explains `EOF` and `stdin`
    redirection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Twitter command-line client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Twitter** is the hottest micro-blogging platform, as well as the latest buzz
    word for online social media now. We can use Twitter API to read tweets on our
    timeline from the command line!'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recently, Twitter stopped allowing people to log in using plain HTTP Authentication,
    so we must use OAuth to authenticate ourselves. A full explanation of OAuth is
    out of the scope of this book, so we will use a library which makes it easy to
    use OAuth from Bash scripts. Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the `bash-oauth` library from [https://github.com/livibetter/bash-oauth/archive/master.zip](https://github.com/livibetter/bash-oauth/archive/master.zip),
    and unzip it to any directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to that directory and then inside the subdirectory `bash-oauth-master`, run
    `make install-all` as root.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to [https://apps.twitter.com/](https://apps.twitter.com/) and register a
    new app. This will make it possible to use OAuth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After registering the new app, go to your app's settings and change Access type
    to Read and Write.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, go to the Details section of the app and note two things, Consumer Key
    and Consumer Secret, so that you can substitute these in the script we are going
    to write.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Great, now let's write the script that uses this.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This Bash script uses the OAuth library to read tweets or send your own updates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the script as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, we use the source command to include the `TwitterOAuth.sh` library,
    so we can use its functions to access Twitter. The `TO_init` function initializes
    the library.
  prefs: []
  type: TYPE_NORMAL
- en: Every app needs to get an OAuth token and token secret the first time it is
    used. If these are not present, we use the `TO_access_token_helper` library function
    to acquire them. Once we have the tokens, we save them to a `config` file so we
    can simply source it the next time the script is run.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `TO_statuses_home_timeline` library function fetches the tweets from Twitter.
    This data is retuned as a single long string in JSON format, which starts like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Each tweet starts with the `"created_at"` tag and includes a `text` and a `screen_name`
    tag. The script will extract the text and screen name data and display only those
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: The script assigns the long string to the `TO_ret` variable.
  prefs: []
  type: TYPE_NORMAL
- en: The JSON format uses quoted strings for the key and may or may not quote the
    value. The key/value pairs are separated by commas, and the key and value are
    separated by a colon (`:`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first `sed` replaces each `"` character set with a newline, making each
    key/value a separate line. These lines are piped to another `sed` command to replace
    each occurrence of `":` with a tilde (~), which creates a line like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The final `awk` script reads each line. The `-F~` option splits the line into
    fields at the tilde, so `$1` is the key and `$2` is the value. The `if` command
    checks for `text` or `screen_name`. The text is first in the tweet, but it's easier
    to read if we report the sender first; so the script saves a `text` return until
    it sees a `screen_name`, then prints the current value of `$2` and the saved value
    of the text.
  prefs: []
  type: TYPE_NORMAL
- en: The `TO_statuses_update` library function generates a tweet. The empty first
    parameter defines our message as being in the default format, and the message
    is a part of the second parameter.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Using sed to perform text replacement* recipe in [Chapter 4](22424a9e-fea7-49de-9589-ea32aeb0b829.xhtml),
    *Texting and Driving*,  explains the `sed` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Searching and mining text inside a file with grep* recipe in [Chapter 4](22424a9e-fea7-49de-9589-ea32aeb0b829.xhtml),
    *Texting and Driving*, explains the `grep` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing word definitions via a web server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several dictionaries on the Web offer an API to interact with their website
    via scripts. This recipe demonstrates how to use a popular one.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to use `curl`, `sed`, and `grep` for this define utility. There
    are a lot of dictionary websites where you can register and use their APIs for
    personal use for free. In this example, we are using Merriam-Webster''s dictionary
    API. Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to [http://www.dictionaryapi.com/register/index.htm](http://www.dictionaryapi.com/register/index.htm),
    and register an account for yourself. Select Collegiate Dictionary and Learner''s
    Dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in using the newly created account and go to My Keys to access the keys.
    Note the key for the learner's dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script will display a word definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the script like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use `curl` to fetch the data from the dictionary API web page by specifying
    our API `Key ($apikey)`, and the word we want the definition for (`$1`). The result
    contains definitions in the `<dt>` tags, selected with `grep`. The `sed` command
    removes the tags. The script selects the required number of lines from the definitions
    and uses `nl` to add a line number to each line.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Using sed to perform text replacement* recipe in [Chapter 4](22424a9e-fea7-49de-9589-ea32aeb0b829.xhtml)
    explains the `sed` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Searching and mining text inside a file with grep* recipe in [Chapter 4](22424a9e-fea7-49de-9589-ea32aeb0b829.xhtml),
    *Texting and Driving*, explains the `grep` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding broken links in a website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Websites must be tested for broken links. It's not feasible to do this manually
    for large websites. Luckily, this is an easy task to automate. We can find the
    broken links with HTTP manipulation tools.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use `lynx` and `curl` to identify the links and find broken ones. Lynx
    has the `-traversal` option, which recursively visits pages on the website and
    builds a list of all hyperlinks. cURL is used to verify each of the links.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script uses `lynx` and `curl` to find the broken links on a web page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`lynx -traversal URL` will produce a number of files in the working directory.
    It includes a `reject.dat` file, which will contain all the links in the website.
    `sort -u` is used to build a list by avoiding duplicates. Then, we iterate through
    each link and check the header response using `curl -I`. If the first line of
    the header contains HTTP/ and either `OK` or `200`, it means that the link is
    valid. If the link is not valid, it is rechecked and tested for a `301`-*link
    moved*-reply. If that test also fails, the broken link is printed on the screen.'
  prefs: []
  type: TYPE_NORMAL
- en: From its name, it might seem like `reject.dat` should contain a list of URLs
    that were broken or unreachable. However, this is not the case, and lynx just
    adds all the URLs there.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that `lynx` generates a file called `traverse.errors`, which contains
    all the URLs that had problems in browsing. However, `lynx` will only add URLs
    that return `HTTP 404 (not found)`, and so we will lose other errors (for instance,
    `HTTP 403 Forbidden`). This is why we manually check for statuses.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Downloading a web page as plain text* recipe in this chapter explains the
    `lynx` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *A primer on cURL* recipe in this chapter explains the `curl` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking changes to a website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracking website changes is useful for both web developers and users. Checking
    a website manually is impractical, but a change tracking script can be run at
    regular intervals. When a change occurs, it generates a notification.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracking changes in terms of Bash scripting means fetching websites at different
    times and taking the difference using the `diff` command. We can use `curl` and
    `diff` to do this.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This Bash script combines different commands, to track changes in a web page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at the output of the `track_changes.sh` script on a website you control.
    First we'll see the output when a web page is unchanged, and then after making
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you should change `MyWebSite.org` to your website name.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, run the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Second, run the command again:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Third, run the following command after making changes to the web page:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The script checks whether the script is running for the first time using `[
    ! -e "last.html" ];`. If `last.html` doesn't exist, it means that it is the first
    time, and the web page must be downloaded and saved as `last.html`.
  prefs: []
  type: TYPE_NORMAL
- en: If it is not the first time, it downloads the new copy (`recent.html`) and checks
    the difference with the diff utility. Any changes will be displayed as diff output.
    Finally, `recent.html` is copied to `last.html`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that changing the website you are checking will generate a huge diff file
    the first time you examine it. If you need to track multiple pages, you can create
    a folder for each website you intend to watch.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *A primer on cURL* recipe in this chapter explains the `curl` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Posting to a web page and reading the response
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`POST` and `GET` are two types of request in HTTP to send information to or
    retrieve information from a website. In a `GET` request, we send parameters (name-value
    pairs) through the web page URL itself. The POST command places the key/value
    pairs in the message body instead of the URL. `POST` is commonly used when submitting
    long forms or to conceal information submitted from a casual glance.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this recipe, we will use the sample `guestbook` website included in the
    **tclhttpd** package. You can download tclhttpd from [http://sourceforge.net/projects/tclhttpd](http://sourceforge.net/projects/tclhttpd)
    and then run it on your local system to create a local web server. The guestbook
    page requests a name and URL which it adds to a guestbook to show who has visited
    a site when the user clicks on the Add me to your guestbook button.
  prefs: []
  type: TYPE_NORMAL
- en: This process can be automated with a single `curl` (or `wget`) command.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Download the tclhttpd package and `cd` to the `bin` folder. Start the tclhttpd
    daemon with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The format to POST and read the HTML response from the generic website resembles
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The curl command prints a response page like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '`-d` is the argument used for posting. The string argument for `-d` is similar
    to the `GET` request semantics. `var=value` pairs are to be delimited by `&`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can post the data using `wget` using `--post-data "string"`. Consider the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Use the same format as cURL for name-value pairs. The text in output.html is
    the same as that returned by the cURL command.
  prefs: []
  type: TYPE_NORMAL
- en: The string to the post arguments (for example, to `-d` or `--post-data`) should
    always be given in quotes. If quotes are not used, `&` is interpreted by the shell
    to indicate that this should be a background process.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at the website source (use the View Source option from the web
    browser), you will see an HTML form defined, similar to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Here, `newguest.cgi` is the target URL. When the user enters the details and
    clicks on the Submit button, the name and URL inputs are sent to `newguest.cgi`
    as a `POST` request, and the response page is returned to the browser.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *A primer on cURL* recipe in this chapter explains the `curl` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The  *Downloading from a web page *recipe in this chapter explains the `wget`
    command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading a video from the Internet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many reasons for downloading a video. If you are on a metered service,
    you might want to download videos during off-hours when the rates are cheaper.
    You might want to watch videos where the bandwidth doesn't support streaming,
    or you might just want to make certain that you always have that video of cute
    cats to show your friends.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One program for downloading videos is `youtube-dl`. This is not included in
    most distributions and the repositories may not be up-to-date, so it's best to
    go to the `youtube-dl` main site at [http://yt-dl.org](http://yt-dl.org).
  prefs: []
  type: TYPE_NORMAL
- en: You'll find links and information on that page for downloading and installing
    `youtube-dl`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using `youtube-dl` is easy. Open your browser and find a video you like. Then
    copy/paste that URL to the `youtube-dl` command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: While `youtube-dl` is downloading the file it will generate a status line on
    your terminal.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `youtube-dl` program works by sending a `GET` message to the server, just
    as a browser would do. It masquerades as a browser so that YouTube or other video
    providers will download a video as if the device were streaming.
  prefs: []
  type: TYPE_NORMAL
- en: The `-list-formats` (`-F`) option will list the available formats a video is
    available in, and the `-format` (`-f`) option will specify which format to download.
    This is useful if you want to download a higher-resolution video than your Internet
    connection can reliably stream.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing text with OTS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Open Text Summarizer** (**OTS**) is an application that removes the fluff
    from a piece of text to create a succinct summary.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `ots` package is not part of most Linux standard distributions, but it
    can be installed with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `OTS` application is easy to use. It reads text from a file or from `stdin`
    and generates the summary to `stdout`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Or
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The `OTS` application can also be used with `curl` to summarize information
    from websites. For example, you can use `ots` to summarize longwinded blogs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `curl` command retrieves the page from a blog site and passes the page to
    `sed`. The `sed` command uses a regular expression to replace all the HTML tags,
    a string that starts with a less-than symbol and ends with a greater-than symbol,
    with a blank. The stripped text is passed to `ots`, which generates a summary
    that's displayed by less.
  prefs: []
  type: TYPE_NORMAL
- en: Translating text from the command line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google provides an online translation service you can access via your browser.
    Andrei Neculau created an **awk** script that will access that service and do
    translations from the command line.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The command line translator is not included on most Linux distributions, but
    it can be installed directly from Git like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `trans` application will translate into the language in your locale environment
    variable by default.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'You can control the language being translated from and to with an option before
    the text. The format for the option is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'To translate from English to French, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `trans` program is about 5,000 lines of awk code that uses `curl` to communicate
    with the Google, Bing, and Yandex translation services.
  prefs: []
  type: TYPE_NORMAL
