<html><head></head><body><div class="chapter" title="Chapter&#xA0;1.&#xA0;Instant Varnish Cache How-to"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Instant Varnish Cache How-to</h1></div></div></div><p>Welcome to <span class="emphasis"><em>Instant Varnish Cache How-to</em></span>. In this book, we will cover the basics of setting up a Varnish Cache server instance in front of your website, how to identify cacheable portions of it, and how to get the best performance from your cache mechanism and policies.</p><p>Varnish Cache is a caching reverse proxy—often referred to as an HTTP accelerator, which sits between your application server and the client's requests. Its main goal is to avoid unnecessary duplicated work when the generated response is known to be the same, and with its flexible framework, it allows you to manipulate requests and also stitches together the <span class="strong"><strong>Edge Side Includes</strong></span> (<span class="strong"><strong>ESI</strong></span>) parts.</p><p>Most of the sample codes found in this book were written for a specific use-case and should be applied with caution. Hopefully, these examples will inspire you to write your own solutions for your scenario, and deploy a fast and reliable Varnish Cache inside your infrastructure.</p><p>This book targets system administrators and web developers with previous knowledge of the HTTP protocol. I've made a few assumptions throughout the book regarding the HTTP protocol and if you ever find yourself in need of an extended explanation, you can always refer to the HTTP Version 1.1 documentation at <a class="ulink" href="http://www.w3.org/Protocols/rfc2616/rfc2616.html">http://www.w3.org/Protocols/rfc2616/rfc2616.html</a> or the Varnish Cache 3.0 documentation at <a class="ulink" href="https://www.varnish-cache.org/docs/3.0/">https://www.varnish-cache.org/docs/3.0/</a>.</p><div class="section" title="Installing Varnish Cache (Must know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec08"/>Installing Varnish Cache (Must know)</h1></div></div></div><p>The Varnish Cache binaries are very likely to be provided by your Linux distribution package repository, but the version you get from that repository might not be as up-to-date as it should be.</p><p>The Varnish Cache official repository provides versions for Red Hat- or Debian-based distributions, FreeBSD, and there's also the possibility to install it manually from a tarball file.</p><p>In the following recipe, we will install Varnish Cache on a Linux CentOS box using the <code class="literal">varnish-cache.org</code> repository and the <span class="strong"><strong>yum</strong></span> package manager.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec06"/>Getting ready</h2></div></div></div><p>The following example will use both Varnish Cache 3.0.3 and  64-bit Linux CentOS 6.</p><p>It's recommended to try it out first on a virtual machine, since your caching policy will need to be adjusted and debugged before you go live.</p><p>If you don't have virtualization software yet, I recommend the Oracle VirtualBox at <a class="ulink" href="https://www.virtualbox.org/">https://www.virtualbox.org/</a> for its easy-to-use interface.</p><p>For other Linux distributions, you can find the correct <code class="literal">varnish-cache.org</code> repository at <a class="ulink" href="https://www.varnish-cache.org/releases/">https://www.varnish-cache.org/releases/</a>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec07"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Add the <code class="literal">varnish-cache.org</code> repository to your CentOS box by typing the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># sudo rpm --nosignature -i http://repo.varnish-cache.org/redhat/varnish-3.0/el5/noarch/varnish-release-3.0-1.noarch.rpm</strong></span>
</pre></div></li><li class="listitem">Install Varnish Cache by typing the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># sudo yum install varnish</strong></span>
</pre></div></li><li class="listitem">Start the Varnish Cache daemon by typing the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># sudo service varnish start</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec08"/>How it works...</h2></div></div></div><p>Adding the <code class="literal">varnish-cache.org</code> repository allows us to keep our varnish server up-to-date and running with stable versions only.</p><p>If there's no <code class="literal">varnish-cache.org</code> repository for your Linux distribution, or in case you decide to compile it by hand, you'll need to download a tarball file from <a class="ulink" href="http://repo.varnish-cache.org/source/ ">http://repo.varnish-cache.org/source/ </a>and resolve the dependencies before you use the GNU <code class="literal">make</code> and <code class="literal">make install </code>commands.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec09"/>There's more...</h2></div></div></div><p>Always check if the startup script service was correctly added to the runlevels list by typing the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># sudo chkconfig --list varnish</strong></span>
</pre></div><p>You should see runlevels 2, 3, 4, and 5 marked as <code class="literal">on</code>, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>varnish0:off 1:off 2:on 3:on 4:on 5:on 6:off</strong></span>
</pre></div><p>In case the service is turned <code class="literal">off</code> in any of these runlevels, turn them <code class="literal">on</code> by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># sudo chkconfig varnish on</strong></span>
</pre></div><p>This will ensure that in case of a power outage or accidental server restart, our Varnish Cache instance will be up and running as soon as possible.</p><p>Startup scripts (also known as initialization script), are scripts that are typically run at system boot time. Most of them start services and set initial system parameters. For more information, visit <a class="ulink" href="https://access.redhat.com/knowledge/docs/en-US/ Red_Hat_Enterprise_Linux/6/html/Installation_Guide/s2-boot-init-shutdown-init.html">https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Installation_Guide/s2-boot-init-shutdown-init.html</a>.</p><p>The runlevels list determines which programs are executed at system startup. More information about how runlevels works can be found at <a class="ulink" href="https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Installation_Guide/s1-boot-init-shutdown-sysv.html">https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Installation_Guide/s1-boot-init-shutdown-sysv.html</a>.</p></div></div></div>
<div class="section" title="Varnish Cache server daemon options (Must know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec09"/>Varnish Cache server daemon options (Must know)</h1></div></div></div><p>Now that we have installed the Varnish Cache service and it's running, we need to take a moment to make sure that our startup parameters are correct.</p><p>The default post-installation storage method declared in the startup script is file, and we will change that to a memory-based storage in order to maximize our performance boost.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note02"/>Note</h3><p>Bear in mind that our entire data is supposed to fit into memory. In case it doesn't, stick with the default storage type (file).</p></div></div><p>Some parameters such as the number of open files, the location of the configuration file, and others do not require adjusting to server specifications and should work as provided.</p><p>The daemon options go from setting the amount of space for storage and the type of storage used up, to thread pooling and hashing algorithms.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec10"/>Getting ready</h2></div></div></div><p>The <code class="literal">varnish </code>file at <code class="literal">/etc/sysconfig </code>is where all daemon options are conveniently located and also the resource for the startup script. You can also start the daemon manually and set the configuration parameters by passing arguments to it, but there's no need to do it since everything is already packed and ready. Every change to the <code class="literal">varnish</code> file at <code class="literal">/etc/sysconfig</code> is only valid after a full restart of the Varnish Cache service.</p><p>To restart the Varnish Cache, use the following provided script:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># sudo service varnish restart</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip02"/>Tip</h3><p>When you perform a full restart, the cache is completely wiped. Be careful.</p></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec11"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open the <code class="literal">varnish</code> file from <code class="literal">/etc/sysconfig</code> using your favorite text editor (I'm using vim) by typing the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># sudo vim /etc/sysconfig/varnish</strong></span>
</pre></div><p>Take your time and read all the comments so that you can understand every bit of this file, since it's loaded by the startup script (<code class="literal">/etc/init.d/varnish</code>).</p></li><li class="listitem">Find the <code class="literal">VARNISH_STORAGE</code> parameter and change the default value to <code class="literal">malloc ,${VARNISH_STORAGE_SIZE}</code>.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>VARNISH_STORAGE="malloc,${VARNISH_STORAGE_SIZE}"</strong></span>
</pre></div></li><li class="listitem">Set the <code class="literal">VARNISH_STORAGE_SIZE </code>parameter to about 85 percent of your available ram.<p>On a 16-GB RAM system, we can allocate 14-GB for storage and leave the remaining 2-GB for OS usage by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>VARNISH_STORAGE_SIZE=14G</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec12"/>How it works...</h2></div></div></div><p>Both methods of storage—file and malloc—make use of file and memory resources but in a slightly different manner.</p><p>While the file storage type will allocate the entire cache size on a file, and tell the OS to map that file to memory (if possible) in order to gain speed, the malloc will request the storage size to the OS, and let it decide about how to divide and swap to file, what it can't fit into memory.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip03"/>Tip</h3><p>
<span class="strong"><strong>Don't get fooled by the name</strong></span>
</p><p>The file storage does not keep data in file when you restart your Varnish Cache server.</p></div></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec13"/>There's more...</h2></div></div></div><p>A new and still experimental storage type called persistent will work in a similar fashion for the file storage, but not every object will persist, since it can't handle situations where there's no more space left on the disk. The main benefit of this new storage type would be having a warmed up cache when recovering from an outage, since the objects are still available on disk.</p><p>The warm-up phase can also be performed with a tool called <code class="literal">varnishreplay</code>, but it requires much more time, since you will need an access logfile to replay it to Varnish Cache.</p><p>You can find more information about the persistent storage type on <a class="ulink" href="https://www.varnish-cache.org/trac/wiki/ArchitecturePersistentStorage">https://www.varnish-cache.org/trac/wiki/ArchitecturePersistentStorage</a>.</p></div></div>
<div class="section" title="Connecting to backend servers (Should know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Connecting to backend servers (Should know)</h1></div></div></div><p>A backend server can be defined as any HTTP server from which Varnish Cache can request and fetch data. In this recipe, we will define our backend servers, probe those servers for their health status, and direct our clients' requests to the correct backend servers.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec14"/>Getting ready</h2></div></div></div><p>If you have a server architecture diagram, that's a good place to start listing all the required servers and grouping them, but you'll also need some technical data about those servers. You may find this information in a server monitoring diagram, where you will find the IP addresses, ports, and luckily a probing URL for health checks.</p><div class="mediaobject"><img src="graphics/0403OS_03_1.jpg" alt="Getting ready"/></div><p>In our case, the main VCL configuration file <code class="literal">default.vcl</code> is located at <code class="literal">/etc/varnish</code> and defines the configuration that the Varnish Cache will use during the life cycle of the request, including the backend servers list.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec15"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open the <code class="literal">default vcl</code> file by using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># sudo vim /etc/varnish/default.vcl</strong></span>
</pre></div></li><li class="listitem">A simple backend declaration would be:<div class="informalexample"><pre class="programlisting">backend server01 {
  .host = "localhost";
  .port = "8080";
}</pre></div><p>This small block of code indicates the name of the backend (<code class="literal">server01</code>), and also the hostname or IP, and which port to connect to.</p></li><li class="listitem">Save the file and reload the configuration using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># sudo service varnish reload</strong></span>
</pre></div><p>At this point, Varnish will proxy every request to the first declared backend using its default VCL file. Give it a try and access a known URL (like the index of your website) through the Varnish Cache and make sure that the content is delivered as it would be without Varnish.</p><p>For testing purposes, this is an okay backend declaration, but we need to make sure that our backend servers are up and waiting for requests before we really start to direct web traffic to them.</p></li><li class="listitem">Let's include a probing request to our backend:<div class="informalexample"><pre class="programlisting">backend website {
  .host = "localhost";
  .port = "8080";
  .probe = {
    .url = "/favicon.ico";
    .timeout = 60ms;
    .interval = 2s;
    .window = 5;
    .threshold = 3;
  }
}</pre></div><p>Varnish will now probe the backend server using the provided URL with a timeout of 60 ms, every couple of seconds.</p><p>To determine if a backend is healthy, it will analyze the last five probes. If three of them result in <code class="literal">200 – OK</code>, the backend is marked as Healthy and the requests are forwarded to this backend; if not, the backend is marked as Sick and will not receive any incoming requests until it's Healthy again.</p></li><li class="listitem">Probe the backend servers that require additional information:<p>In case your backend server requires extra headers or has an HTTP basic authentication, you can change the probing from <code class="literal">URL</code> to <code class="literal">Request</code> and specify a raw HTTP request. When using the <code class="literal">Request</code> probe, you'll always need to provide a <code class="literal">Connection: close</code> header or it will not work. This is shown in the following code snippet:</p><div class="informalexample"><pre class="programlisting">backend api {
  .host = "localhost";
  .port = "8080";
  .probe = {
    .request = 
    "GET /status HTTP/1.1"
    "Host: www.yourhostname.com"
    "Connection: close"
    "X-API-Key: e4d909c290d0fb1ca068ffaddf22cbd0"
    "Accept: application/json"
    .timeout = 60ms;
    .interval = 2s;
    .window = 5;
    .threshold = 3;
  }
}</pre></div></li><li class="listitem">Choose a backend server based on incoming data:<p>After declaring your backend servers, you can start directing the clients' requests. The most common way to choose which backend server will respond to a request is according to the incoming URL, as shown in the following code snippet:</p><div class="informalexample"><pre class="programlisting">vcl_recv {
  if ( req.url ~ "/api/") {
    set req.backend = api;
  } else {
    Set req.backend = website;
  }
}</pre></div><p>Based on the preceding configuration, all requests that contain <code class="literal">/api/</code> (<code class="literal">www.yourdomain.com/api/</code>) in the URL will be sent to the backend named <code class="literal">api</code> and the others will reach the backend named <code class="literal">website</code>.</p><p>You can also pick the correct backend server, based on User-agent header, Client IP (geo-based), and pretty much every information that comes with the request.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec16"/>How it works...</h2></div></div></div><p>By probing your backend servers, you can automate the removal of a sick backend from your cluster, and by doing so, you avoid delivering a broken page to your customer. As soon as your backend starts to behave normally, Varnish will add it back to the cluster pool.</p><p>Directing requests to the appropriate backend server is a great way to make sure that every request reaches its destination, and gives you the flexibility to provide content based on the incoming data, such as a mobile device or an API request.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec17"/>There's more...</h2></div></div></div><p>If you have lots of servers to be declared as backend, you can declare probes as a separated configuration block and make reference to that block later at the backend specifications, avoiding repetition and improving the code's readability.</p><div class="informalexample"><pre class="programlisting">probe favicon {
  .url = "/favicon.ico";
  .timeout = 60ms;
  .interval = 2s;
  .window = 5;
  .threshold = 3;
}

probe robots {
  .url = "/robots.txt";
  .timeout = 60ms;
  .interval = 2s;
  .window = 5;
  .threshold = 3;
}

backend server01 {
  .host = "localhost";
  .port = "8080";
  .probe = favicon;
}

backend server02 {
  .host = "localhost";
  .port = "8080";
  .probe = robots;
}</pre></div><p>The <code class="literal">server01</code> server will use the probe named <code class="literal">favicon</code>, and the <code class="literal">server02</code> server will use the probe named <code class="literal">robots</code>.</p></div></div>
<div class="section" title="Load balance requests (Should know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec11"/>Load balance requests (Should know)</h1></div></div></div><p>Balancing requests is a great way to share workload across the cluster pool and avoid overloading a single server instance, keeping the overall health of the system. There's also the possibility to direct VIP customers to a dedicated cluster pool, guaranteeing them the best user experience.</p><p>By using a director group, Varnish will manage and spread the incoming requests to those servers included in it. Having the servers constantly checked, Varnish can take care of the sick servers and maintain everything as if there was no problem at all.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec18"/>Getting ready</h2></div></div></div><p>There are six types of director groups to be configured: random, client, hash, round-robin, DNS, and fallback. While the random director is self-explanatory (randomly distributes requests), a DNS director can be used to spread to an entire network of servers. The hash director will always choose a backend based on the hash of the incoming URL and the fallback director can be used in emergency cases when servers behave oddly.</p><p>The two most common directors are the round-robin and client directors.</p><p>The round-robin can be used to spread requests one by one to the entire cluster of servers no matter what is requested, and the client director can be used to create a sticky-session based on unique information provided by the client, such as an IP address.</p><p>In this recipe we will create both the client and round-robin balancers to spread requests across application servers.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec19"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Group the backend servers to load balance requests by using the following code snippet:<div class="informalexample"><pre class="programlisting">director dr1 round-robin {
  { .backend = server01 }
  { .backend = server02 }
  { .backend = server03 }
  { .backend = server04 }
}</pre></div><p>In the preceding example, we have declared that our director named <code class="literal">dr1</code> is a <code class="literal">round-robin</code> director, and inside this director there are four backend servers to be balanced. Backend servers <code class="literal">server01</code> to <code class="literal">server04</code> have already been configured earlier and this declaration is only referring to them.</p></li><li class="listitem">Create a sticky-session pool of servers by using the following code snippet:<div class="informalexample"><pre class="programlisting">director dr1 client {
  { .backend = server01 }
  { .backend = server02 }
  { .backend = server03 }
  { .backend = server04 }
}</pre></div><p>At first, there's absolutely no difference from the <code class="literal">round-robin</code> declaration to the <code class="literal">client</code> one, but inside your <code class="literal">vcl_recv</code> subroutine, you'll need to specify what identifies a unique client. Varnish will use, as default, the client IP address, but if you have other services in front of your Varnish Cache (such as a firewall), you'll need to rewrite the value of the <code class="literal">client.identity</code> variable. In the following example, we'll use the <code class="literal">X-Forwarded-For</code> header to get the clients' real IP address.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>The <code class="literal">X-Forwarded-For</code> header is used to maintain information lost in the proxying process. For more information visit <a class="ulink" href="http://tools.ietf.org/html/draft-ietf-appsawg-http-forwarded-10">http://tools.ietf.org/html/draft-ietf-appsawg-http-forwarded-10</a>.</p></div></div><div class="informalexample"><pre class="programlisting">sub vcl_recv {
  set client.identity = req.http.X-Forwarded-For;
}</pre></div><p>A sticky-session pool is necessary to direct clients to specific parts of your website that requires an HTTP session or authorization, such as shopping cart/checkout and login/logout pages, without breaking their session. Those parts of your website may be critical to your business, and having a sticky-session dedicated cluster can prioritize paying customers while the others are still browsing the products and will not interfere with the checkout process performance. </p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec20"/>How it works...</h2></div></div></div><p>By using directors to load balance the requests, we can obtain greater service availability and provide paying customers with an improved user experience.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec21"/>There's more...</h2></div></div></div><p>Sometimes it's not possible for all the servers to be identical inside a cluster. Some servers may have more available ram or more processors than others, and to balance requests based on the weakest server in the cluster is not the best way to solve this problem, since the higher end servers would be underused.</p><p>Weight-based load balancing improves the balance of the system by taking into account a pre-assigned weight for each server as shown in the following code snippet:</p><div class="informalexample"><pre class="programlisting">director dr1 client {
  { .backend = server01 ; .weight=2; }
  { .backend = server02 ; .weight=2; }
  { .backend = server03 ; .weight=2; }
  { .backend = server04 ; .weight=1; }
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip04"/>Tip</h3><p>Weighting the servers is only possible in the random or client directors.</p></div></div></div></div>
<div class="section" title="The Varnish Configuration Language (Should know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec12"/>The Varnish Configuration Language (Should know)</h1></div></div></div><p>The <span class="strong"><strong>Varnish Configuration Language</strong></span> (<span class="strong"><strong>VCL</strong></span>) is a domain-specific language used to define the caching policy. It might seem a bit confusing at first to learn another DSL instead of using an already known language, but in the next recipe you will find out how easy it is to define how your cache should behave using the VCL.</p><p>Every written VCL code will be compiled to binary code when you start your Varnish Cache. So if you forget a semi-colon, for example, the code will not compile and the daemon will continue using the last compiled version of your configuration file, as long as you use the reload function from the startup script and don't restart your server.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec22"/>Getting ready</h2></div></div></div><p>The main configuration file <code class="literal">default.vcl </code>used to define the caching policy is entirely written in VCL and is located at <code class="literal">/etc/varnish/</code>.</p><p>Most of the configuration is written into subroutines and follows a pre-defined flow during the request and response phase.</p><div class="mediaobject"><img src="graphics/0403OS_05_1.jpg" alt="Getting ready"/></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec23"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Creating comments in the code:<p>You can use <code class="literal">//</code>, <code class="literal">#</code>,or <code class="literal">/* your comment inside */</code>for creating a comment in your code:</p><div class="informalexample"><pre class="programlisting">// commented line
# commented line</pre></div></li><li class="listitem">Assignment and logical operators:<p>Assignments can be done with a single equals sign <code class="literal">=</code>.</p><div class="informalexample"><pre class="programlisting">set client.identity = req.http.x-forwarded-for;</pre></div><p>Comparisons can be done with <code class="literal">==</code> for matching conditions or <code class="literal">!=</code>for not matching conditions.</p><div class="informalexample"><pre class="programlisting">if (req.restarts == 0)
if (req.request != "GET")</pre></div><p>Logical operations can be done with <code class="literal">&amp;&amp;</code> for AND operations, and || for OR operations.</p><div class="informalexample"><pre class="programlisting">if (req.request != "GET"&amp;&amp;req.request != "HEAD")</pre></div></li><li class="listitem">Regular expressions:<p>Varnish uses the <span class="strong"><strong>Perl-compatible regular expressions</strong></span> (<span class="strong"><strong>PCRE Regex</strong></span>).</p><p>The contains operator for validating regex is <code class="literal">~</code> (tilde) and the does not contain operator is <code class="literal">!~</code>.</p><div class="informalexample"><pre class="programlisting">if (req.url !~ "^/images/")
if (req.url ~ "\.(jpg|jpeg|png|gif)$")</pre></div></li><li class="listitem">VCL functions:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">regsub()</code> and <code class="literal">regsuball()</code>: They work by changing a provided string whenever it matches a regex expression. The difference between the two functions is that the <code class="literal">regsub()</code>function replaces only the first match and the <code class="literal">regsuball()</code>function replaces all matching occurrences.<p>Both functions expect a <code class="literal">original_string</code>, regex, <code class="literal">substitute_stringformat</code> as input:</p><div class="informalexample"><pre class="programlisting">regsub(req.url, "\?.*", "");</pre></div><p>This expression will remove any character located after a <code class="literal">?</code>sign (including the question mark itself). Removing the parameters from the requested URL during the <code class="literal">vcl_hash</code> phase will help you avoid storing duplicated content (be careful not to end up serving the wrong content).</p></li><li class="listitem" style="list-style-type: disc"><code class="literal">purge</code>: It is used to invalidate stale content, keeping the cache fresh. A good rule of thumb is to send an HTTP <code class="literal">PURGE</code> request to Varnish Cache whenever your backend receives an HTTP <code class="literal">POST</code> or <code class="literal">DELETE</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ban()</code> and <code class="literal">ban_url()</code>: They create a filter, instructing if a cached object is supposed to be delivered or not. Adding a new filter to the ban list will not remove the content that is already cached—what it really does is exclude the matching cached objects from any subsequent requests, forcing a cache miss.<div class="informalexample"><pre class="programlisting">ban_url("\.xml$") 
ban("req.http.host ~ " + yourdomain.com )</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>Too many entries in the ban list will consume extra CPU space.</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">return(</code>: It determines to which subroutine the request should proceed to. By calling <code class="literal">return(lookup)</code> method, we're instructing Varnish to stop executing the current subroutine and proceed to the <code class="literal">lookup</code> subroutine.<div class="informalexample"><pre class="programlisting">return(restart)
return(lookup)
return(pipe)
return(pass)</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hash_data(</code>: It is responsible for setting the hash key used to store an object and it is only available inside the <code class="literal">vcl_hash</code> subroutine. The default value for the <code class="literal">hash_data</code> variable is <code class="literal">req.url</code> (requested URL), but it may not suit your needs. In a multi-domain website, a hash key conflict may occur, since the default <code class="literal">hash_data</code> for the homepage would be <code class="literal">/</code> in all domains. Concatenating the value of the <code class="literal">req.http.Host</code> variable would add the domain name to the hash key, making it unique.</li><li class="listitem" style="list-style-type: disc"><code class="literal">error</code>: It is used to interrupt a request or response whenever an error arises and an error page needs to be displayed. The first argument is the HTTP error code and the second is a string with the error code message.<div class="informalexample"><pre class="programlisting">if (!client.ip ~ purge) {
  error 405 "Not allowed.";
}</pre></div></li></ul></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec24"/>How it works...</h2></div></div></div><p>All the VCL written code is translated to C language and compiled into a shared object, which will be linked to the server process when it is reloaded. Any coding mistake found during this phase, such as a missing semi-colon at the end of a line, will generate the following compiler error indicating the line and what the compiler was expecting:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Message from VCC-compiler:</strong></span>
<span class="strong"><strong>Expected ';' got '}'</strong></span>
<span class="strong"><strong>(program line 174), at</strong></span>
<span class="strong"><strong>('input' Line 93 Pos 9)</strong></span>
<span class="strong"><strong>}</strong></span>
<span class="strong"><strong>--------#</strong></span>
<span class="strong"><strong>Running VCC-compiler failed, exit 1</strong></span>
<span class="strong"><strong>VCL compilation failed</strong></span>
</pre></div><p>In the preceding example, the VCC compiler informs that it was expecting a <code class="literal">;</code>(semi-colon) on line <code class="literal">93</code> and found a <code class="literal">}</code> (curly bracket) instead.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec25"/>There's more...</h2></div></div></div><p>To find out how Varnish is translating your VCL code to C language, run the daemon with the <code class="literal">-C</code> argument that will instruct the daemon to compile and print the C language code to the console.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># varnishd -C -f /etc/varnish/default.vcl</strong></span>
</pre></div><p>If you decide to insert an inline C code inside your VCL, analyzing the generated code will help you to debug it.</p><p>You will also find out that inside the generated C code the default VCL configuration is still present, even if you have deleted it. This is a self-defense mechanism which makes sure that a request always has a valid <code class="literal">return()</code> statement.</p></div></div>
<div class="section" title="Handling HTTP request vcl_recv (Should know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Handling HTTP request vcl_recv (Should know)</h1></div></div></div><p>The <code class="literal">vcl_recv</code> routine is the first subroutine to be executed when a request comes in. At this point, you can normalize URL, add or subtract HTTP headers, strip or delete cookies, define which backend or director will respond to the request, control access, and much more.</p><p>First, we will take a look at the default <code class="literal">vcl_recv </code>subroutine and increment its behavior to suit our needs.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec26"/>Getting ready</h2></div></div></div><p>Open the <code class="literal">default.vcl</code> file at <code class="literal">/etc/varnish</code> and find the <code class="literal">vcl_recv</code> routine (<code class="literal">sub vcl_recv</code>).</p><p>The following block of codes are presented as an explanation of the default behavior and in the <span class="emphasis"><em>How to do it...</em></span> section, we will modify them.</p><div class="informalexample"><pre class="programlisting">if (req.restarts == 0) {
  if (req.http.x-forwarded-for) {
    set req.http.X-Forwarded-For =
    req.http.X-Forwarded-For + ", " + client.ip;
  } else {
    set req.http.X-Forwarded-For = client.ip;
  }
}</pre></div><p>The <code class="literal">req.restarts</code> object is an internal counter that indicates how many times the request was restarted (the prefix <code class="literal">req</code> indicates that this variable is of the type request). Restarting is commonly done when a backend server is not responding or an expected error arises, and by doing so, you can choose another backend server, rewrite the URL, or take other actions to prevent an error page.</p><p>This specific block of code is executed only when the request is not restarted, and it will append the client IP to an <code class="literal">X-Forwarded-For</code> header, if present. If not, Varnish will create the <code class="literal">X-Forwarded-For</code> header and assign the client IP to it.</p><div class="informalexample"><pre class="programlisting">  if (req.request != "GET"&amp;&amp;
    req.request != "HEAD"&amp;&amp;
    req.request != "PUT"&amp;&amp;
    req.request != "POST"&amp;&amp;
    req.request != "TRACE"&amp;&amp;
    req.request != "OPTIONS"&amp;&amp;
    req.request != "DELETE") {
    /* Non-RFC2616 or CONNECT which is weird. */
    return (pipe);
  }</pre></div><p>If the incoming request is not a known HTTP method, Varnish will pipe it to the backend and let it handle the request.</p><div class="informalexample"><pre class="programlisting">if (req.request != "GET"&amp;&amp;req.request != "HEAD") {
  /* We only deal with GET and HEAD by default */
  return (pass);
}</pre></div><p>Since delivering cached content can only be done in <code class="literal">GET</code> and <code class="literal">HEAD</code> HTTP methods, we need to pass the request that do not match to the backend servers.</p><div class="informalexample"><pre class="programlisting">if (req.http.Authorization || req.http.Cookie) {
  /* Not cacheable by default */
  return (pass);
}</pre></div><p>If the request contains an authorization or cookies header, we should not try to look up for it in cache, since the content of this request is specific to the client.</p><div class="informalexample"><pre class="programlisting">return (lookup);</pre></div><p>The final statement inside the <code class="literal">vcl_recv</code> subroutine instructs Varnish to look up the requested URL in cache. When the request does not match any of the previous conditions, which instructs Varnish not to look up for the content (like cookies, authorization, HTTP <code class="literal">POST</code>, and others), it will deliver a cached version of the requested content.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec27"/>How to do it...</h2></div></div></div><p>After understanding the default <code class="literal">vcl_recv</code> subroutine, we should identify the sections of our website that can be cached and the ones that cannot be cached. For some sections of the website, you still may want to deliver a cached version even if a user sends a cookie, for example, static content such as CSS, XML, and JPEG files.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Stripping cookies from requests:<div class="informalexample"><pre class="programlisting">if (req.url ~ "\.(png|gif|jpeg|jpg|ico|swf|css|js|txt|xml)"
|| req.url ~ "/static/"
|| req.url ~ "/images/") {
  unset req.http.cookie;
}</pre></div><p>If the requested URL ends in any of the extensions (<code class="literal">.png</code>, <code class="literal">.jpeg</code>, <code class="literal">.ico</code>, and so on) listed in the condition, the cookies will be removed before the request continues through the VCL. The same behavior will happen for anything under the <code class="literal">static</code> and <code class="literal">images</code> directory.</p><p>Remember that Varnish will execute the code in a sequential way, so the first return statement that matches will be executed, and all the code after that will not be processed.</p></li><li class="listitem">Define which backend or director will receive the request:<div class="informalexample"><pre class="programlisting">if(req.url ~ "/java/"
  set req.backend = java;
} else {
  set req.backend = php;
}</pre></div><p>Every request that contains a <code class="literal">/java/</code> URL will hit the <code class="literal">java</code> director and everything else will hit the <code class="literal">php</code> director. This is a really basic example, and in reality you will probably use a regular expression to match sections of the requested URL.</p></li><li class="listitem">Create security barriers to avoid unauthorized access:<div class="informalexample"><pre class="programlisting">if (req.url ~ "\.(conf|log|old|properties|tar|war)$") {
  error 403 "Forbidden";
}</pre></div><p>If you want to, you can pass the request directly to the error subroutine <code class="literal">vcl_error</code> (with an HTTP error code and message), instead of using a return statement.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec28"/>How it works...</h2></div></div></div><p>Getting your subroutines right is one of the most important steps to get a good hit ratio, since the <code class="literal">vcl_recv</code> and <code class="literal">vcl_fetch</code> subroutines are probably where you will write 80 percent of your VCL code.</p><p>By stripping cookies of known static content, we get rid of requests that would always hit the backend for the same content like a header or footer, since the default behavior is to pass requests that contain cookies. Be extremely careful when stripping cookies, otherwise users may be locked outside your website if you remove sensitive data.</p><p>The <code class="literal">vcl_recv</code> subroutine is also the one where you control how balanced your cluster/directors will be, whenever a request passes through Varnish.</p></div></div>
<div class="section" title="Handling HTTP request vcl_hash (Should know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec14"/>Handling HTTP request vcl_hash (Should know)</h1></div></div></div><p>The <code class="literal">vcl_hash</code> subroutine is the subroutine that is executed after the <code class="literal">vcl_recv</code> subroutine. Its responsibility is to generate a hash that will become the key in the memory map of stored objects and play an important role in achieving a high hit ratio.</p><p>The default value for an object's key is its URL and it probably suits most of the cases. If you have too many SEO friendly URLs, and purging becomes a problem since those URLs are dynamically generated, using the <code class="literal">vcl_hash</code> subroutine to normalize these keys will help you remove stale content from cache.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec29"/>Getting ready</h2></div></div></div><p>This is the default action for the <code class="literal">vcl_hash</code> subroutine and the object's key will be of the <code class="literal">URL + host</code> format, in case an HTTP <code class="literal">host</code> header is present, or of the <code class="literal">URL + server ip</code> format, when it is not.</p><div class="informalexample"><pre class="programlisting">sub vcl_hash {
  hash_data(req.url);
  if (req.http.host) {
    hash_data(req.http.host);
  } else {
    hash_data(server.ip);
  }
  return (hash);
}</pre></div><p>Whenever the <code class="literal">hash_data()</code> function is called, the provided value is appended to the current key.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec30"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Avoiding double caching for <code class="literal">www.youdomain.com</code> and <code class="literal">yourdomain.com</code>:<div class="informalexample"><pre class="programlisting">sub vcl_hash {
  hash_data(req.url);
  if (req.http.host) {
    set req.http.host = regsub(req.http.host,"^www\.yourdomain\.com$", "yourdomain.com");
    hash_data(req.http.host);
  } else {
    hash_data(server.ip);
  }
  return (hash);
}</pre></div><p>Adding the HTTP host normalization step before it is appended to the <code class="literal">hash_data()</code> function will prevent your cache from generating different keys for <code class="literal">www.yourdomain.com</code> and <code class="literal">www.yourdomain.com</code>, when the content is the same.</p></li><li class="listitem">Removing domain name (host) from static files of multi-language websites:<div class="informalexample"><pre class="programlisting">if (req.url ~ "\.(png|gif|jpeg|jpg|ico|swf|css|js|txt|xml)"{
  set req.http.X-HASH = regsub(req.url,".*\.yourdomain\.[a-zA-Z]{2,3}\.[a-zA-Z]{2}", "");
}
hash_data(req.http.X-HASH);</pre></div><p>Removing a domain name from the object's key can help you achieve a higher hit ratio by serving the same image for all web pages that speak the same language but have different domain names.</p><p>Creating a temporary header <code class="literal">http X-HASH</code> to rewrite and modify everything you need before passing to the <code class="literal">hash_data()</code> function can make your code more readable. By removing the domain name, the generated object's keys for <code class="literal">http://www.youdomain.com/images/example.png</code> and <code class="literal">http://www.youdomain.co.uk/images/example.png</code> will be the same.</p></li><li class="listitem">Normalizing SEO URLs for easier purging:<div class="informalexample"><pre class="programlisting">if (req.url ~ "\.(html|htm)(\?[a-z0-9=]+)?$" {
  set req.http.X-HASH = regsub(req.http.X-HASH, "^/.*__", "/__");
}
hash_data(req.http.X-HASH);</pre></div><p>Taking <code class="literal">http://yourdomain.com/a-long-seo-friendly-url-that-can-make-your-life-harder__3458.html</code> as an example of a SEO-friendly URL, removing this object from cache can become harder if the generated SEO URL is based on an updated field (as you will lose reference to the old key value), or if you have an asynchronous process that purges content, based on the timestamp of objects.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec31"/>How it works...</h2></div></div></div><p>Normalizing the object's key can make your life easier whenever you need to purge a content that is shared across multiple domains, or whenever you have a SEO friendly website in which it is necessary to update content as soon as it becomes stale.</p><p>Avoiding duplicated content will save extra memory space and provide you with a higher hit ratio, improving the overall performance of your website.</p><p>Take some time to familiarize yourself with your website contents, in order to identify files and sections that can benefit from a normalized key.</p></div></div>
<div class="section" title="Handling HTTP request vcl_pipe and vcl_pass (Should know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec15"/>Handling HTTP request vcl_pipe and vcl_pass (Should know)</h1></div></div></div><p>The main difference between these two subroutines is that the <code class="literal">vcl_pipe</code> subroutine will transmit the bytes back and forth after a pipe instruction is executed at the <code class="literal">vcl_recv</code> subroutine (and the subsequent VCL code will not be processed), no matter what is in it, while the <code class="literal">vcl_pass</code> subroutine will transmit the request to the backend without caching the generated response.</p><p>Both subroutines respect a <code class="literal">keep-alive</code> header. As long as the connection is still open, the <code class="literal">vcl_pipe</code> subroutine will keep transmitting bytes back and forth without analyzing any of the subsequent requests, preventing cached content from being delivered since the pipe is still active.</p><p>Piping or passing a request can be useful when users reach a protected section of the website.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec32"/>Getting ready</h2></div></div></div><p>The following is the default <code class="literal">vcl_pipe</code> subroutine:</p><div class="informalexample"><pre class="programlisting">sub vcl_pipe {
      # Note that only the first request to the backend will have
      # X-Forwarded-For set.  If you use X-Forwarded-For and want to
      # have it set for all requests, make sure to have:
      # set bereq.http.connection = "close";
      # here. It is not set by default as it might 
  # break some broken web applications, like IIS with NTLM    
  # authentication.
      return (pipe);
}</pre></div><p>The default behavior is that if you do not set the HTTP <code class="literal">connection</code> header as <code class="literal">close</code>, a client might be piped once, and then all other subsequent requests made with a keep-alive header will use the same pipe, overloading your backend servers.</p><p>This is the default <code class="literal">vcl_pass</code> subroutine:</p><div class="informalexample"><pre class="programlisting">sub vcl_pass {
      return (pass);
}</pre></div><p>The default <code class="literal">vcl_pass</code> behavior is pretty much self-explanatory. The request will be passed to the backend and the generated response will not be cached.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec33"/>How to do it...</h2></div></div></div><p>Add a connection header to piped requests:</p><div class="informalexample"><pre class="programlisting">sub vcl_pipe {
      set bereq.http.connection = "close";
      return (pipe);
}</pre></div><p>Every piped request will be closed after it is processed.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec34"/>How it works...</h2></div></div></div><p>By piping requests, you can stream large objects, but you need to be careful or all other subsequent requests for that same client will also be piped.</p><p>Passing requests is the default action for protected or personalized sections of your website and requires no extra work.</p></div></div>
<div class="section" title="Handling HTTP response vcl_fetch (Should know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec16"/>Handling HTTP response vcl_fetch (Should know)</h1></div></div></div><p>The <code class="literal">vcl_fetch</code> subroutine is the first subroutine to deal with the response phase and it plays an important role on caching policies and <span class="strong"><strong>Edge-side Include</strong></span> (<span class="strong"><strong>ESI</strong></span>). When dealing with a legacy system that does not provide a <code class="literal">cache-control</code> header, you can hardcode a time to live (ttl) value to the content that should be cached.</p><p>While you can manipulate requests based on client-provided data using the <code class="literal">req.*</code> variable in the <code class="literal">vcl_recv</code> subroutine, you can do the same data manipulation in the <code class="literal">vcl_fetch</code> subroutine, but with data provided by a backend server using the <code class="literal">beresp.*</code> variable (<code class="literal">beresp</code> = backend response).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip05"/>Tip</h3><p>For more information about edge-side include visit <a class="ulink" href="http://www.w3.org/TR/esi-lang">http://www.w3.org/TR/esi-lang</a>.</p></div></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec35"/>Getting ready</h2></div></div></div><p>First we will take a look at the default <code class="literal">vcl_fetch</code> subroutine:</p><div class="informalexample"><pre class="programlisting">sub vcl_fetch {
      if (beresp.ttl &lt;= 0s ||
          beresp.http.Set-Cookie ||
          beresp.http.Vary == "*") {
                    /*
                     * Mark as "Hit-For-Pass" for the next 2 minutes
                     */
                    set beresp.ttl = 120 s;
                    return (hit_for_pass);
      }
      return (deliver);
}</pre></div><p>The default <code class="literal">vcl_fetch</code> behavior will not cache the response if your backend server provides a zero or negative ttl value, a <code class="literal">Set-cookie</code> header, or a <code class="literal">Vary</code> header. Instead, Varnish will cache a dummy object that instructs the next requests for this URL to be passed for the next two minutes. This is called hit-for-pass.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec36"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Overriding the default time to live of a cached object:<div class="informalexample"><pre class="programlisting">if ( req.url ~ "\.(png|gif|jpeg|jpg|ico|css|js|txt|xml)(\?[a-z0-9=]+)?$"){
  set beresp.ttl = 1d;
}</pre></div><p>Using the <code class="literal">set beresp.ttl = 1d</code> instruction, our static files will be stored in cache for one day. If our backend server provides an HTTP <code class="literal">cache-control</code> or <code class="literal">expires</code> header with a different time frame, we will override it with the set command.</p></li><li class="listitem">Stripping cookies for static content:<div class="informalexample"><pre class="programlisting">if ( req.url ~ "/static/") {
  set beresp.ttl = 30m;
  unset beresp.http.set-cookie;
}</pre></div><p>Removing the HTTP <code class="literal">set-cookie</code> header from the response allows us to sanitize the object before inserting it into memory.</p></li><li class="listitem">Restart requests that failed:<div class="informalexample"><pre class="programlisting">if ( beresp.status&gt;= 500 &amp;&amp;req.request != "POST") {
  return(restart);
}</pre></div><p>In case the backend server returns a 500+ HTTP error code (server-side error) and the original request was not an HTTP <code class="literal">POST</code>, we will restart it and try a different backend. Restarting will take the request back to the <code class="literal">vcl_recv</code> subroutine.</p><p>To do so, we also need to tweak the <code class="literal">vcl_recv</code> subroutine so that the restarted request can pick another backend instead of our original failed server.</p><div class="informalexample"><pre class="programlisting">sub vcl_recv {
  if (req.restarts == 0) {
    # Try the director first.
    set req.backend = director1;
  } else if (req.restarts == 1) {
    # Director has failed and we will try the backend 1.
    set req.backend = b1;
  } else if (req.restarts == 2) {
    # Backend 1 has failed. Try backend 2.
    set req.backend = b2;
  } else {
    # All backend servers have failed. Go to error page.
    error 503 "Service unavailable";
  }
}</pre></div></li><li class="listitem">Inspecting why the response was not cached:<p>Sending a debug header alongside the response can help you understand the behavior of the cache and why a specific content was not cached.</p><div class="informalexample"><pre class="programlisting">sub vcl_fetch {
  if (beresp.ttl&lt;= 0s) {
    # Cannot cache. Backend provided an expired TTL
    set beresp.http.X-Cacheable = "NO:ExpiredTTL";
  } elsif (req.http.Cookie) {
    # Presence of cookies.
    set beresp.http.X-Cacheable = "NO:Cookies";
  } elsif (beresp.http.Cache-Control ~ "private") {
    # Cache-control is private
    set beresp.http.X-Cacheable = "NO:Cache-Control=private";
  } else {
    set beresp.http.X-Cacheable = "YES";
  }
  return(deliver);
}</pre></div><p>The preceding code will check for the presence of an expired time to live, cookies, or if the <code class="literal">Cache-control</code> header instructed that the content is private.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec37"/>How it works...</h2></div></div></div><p>Stripping cookies from a response will help you clean up data that should not be stored along with the object in cache, but it may be risky to do so, since cookies are used to authenticate users or track user steps. Be extra sure that the user experience will not be affected.</p><p>It is also possible to restart a request that would otherwise end up in an error inside the <code class="literal">vcl_error</code> subroutine, but there's no need to take that extra step if you can restart it as soon as the backend server throws an error. This is not a bulletproof method, but it can most certainly help you when a bad deploy happens in one of the cluster servers.</p></div></div>
<div class="section" title="Handling HTTP response vcl_deliver (Should know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec17"/>Handling HTTP response vcl_deliver (Should know)</h1></div></div></div><p>The <code class="literal">vcl_deliver</code> subroutine is the last step before a response returns to the client and can be used for server obfuscation, adding debug headers, and a last overall headers' clean up.</p><p>Because the <code class="literal">vcl_deliver</code> subroutine is executed after the object is placed into cache, all manipulation that happens inside the <code class="literal">vcl_deliver</code> subroutine will not be persisted.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec38"/>Getting ready</h2></div></div></div><p>This is the default <code class="literal">vcl_deliver</code> behavior:</p><div class="informalexample"><pre class="programlisting">sub vcl_deliver {
  return (deliver);
}</pre></div><p>As you can see, the default <code class="literal">vcl_deliver</code> subroutine is very straightforward and in fact it does not modify anything.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec39"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Remove the server name for obfuscating purposes:<div class="informalexample"><pre class="programlisting">sub vcl_deliver {
  unset resp.http.Server;
  return (deliver);
}</pre></div><p>Obfuscating the HTTP <code class="literal">Server</code> header is a good way to avoid exposing what type of server and version you are using behind your Varnish Cache.</p></li><li class="listitem">Removing extra headers added by Varnish:<div class="informalexample"><pre class="programlisting">sub vcl_deliver {
  unset resp.http.Server;
  unset resp.http.Via;
  unset resp.http.Age;
  unset resp.http.X-Varnish;
  return (deliver);
}</pre></div><p>The HTTP <code class="literal">Age</code> header provides information about how long this object has been cached, while the HTTP <code class="literal">Via</code> header informs who delivered the response. The <code class="literal">X-Varnish</code> header returns two values: the first one is the request ID that originated the cached object, and the second one is the present request ID.</p><p>There is absolutely no need to remove those headers, but it is a good way to not give away unnecessary information.</p></li><li class="listitem">Add cache <code class="literal">hit</code> or <code class="literal">miss</code> debug headers:<div class="informalexample"><pre class="programlisting">sub vcl_deliver {
  if (obj.hits&gt; 0) {
    set resp.http.X-Cache = "HIT";
  } else {
    set resp.http.X-Cache = "MISS";
  }
  unset resp.http.Via;
  unset resp.http.Age;
  unset resp.http.X-Varnish;
  unset resp.http.Server;
  return (deliver);
}</pre></div><p>Inside the <code class="literal">vcl_deliver</code> subroutine, you can read the object's <code class="literal">hit</code> counter variable to determine if the object is being served from cache or by the backend.</p><p>Adding a debug header in order to know if the content was served from cache helps you to identify objects that should be served from cache, but are not.</p></li></ol></div></div></div>
<div class="section" title="Handling HTTP response vcl_error (Should know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec18"/>Handling HTTP response vcl_error (Should know)</h1></div></div></div><p>The <code class="literal">vcl_error</code> subroutine handles odd behaviors from backend servers, and can also be used when you want to deny access to a request or redirect requests to a new location.</p><p>Another good use of the <code class="literal">vcl_error</code> subroutine is to deliver a maintenance page while you are working on the backstage, rolling out patches, or deploying a new version of your website (in case you cannot have multiple versions deployed at the same time).</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec40"/>Getting ready</h2></div></div></div><p>We will take a look at the default <code class="literal">vcl_error</code> subroutine:</p><div class="informalexample"><pre class="programlisting">sub vcl_error {
      set obj.http.Content-Type = "text/html; charset=utf-8";
      set obj.http.Retry-After = "5";
      synthetic {"
    &lt;?xml version="1.0" encoding="utf-8"?&gt;
    &lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
     "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"&gt;
    &lt;html&gt;
        &lt;head&gt;
          &lt;title&gt;"} + obj.status + " " + obj.response + {"&lt;/title&gt;
        &lt;/head&gt;
        &lt;body&gt;
          &lt;h1&gt;Error "} + obj.status + " " + obj.response + {"&lt;/h1&gt;
          &lt;p&gt;"} + obj.response + {"&lt;/p&gt;
          &lt;h3&gt;Guru Meditation:&lt;/h3&gt;
          &lt;p&gt;XID: "} + req.xid + {"&lt;/p&gt;
          &lt;hr&gt;
          &lt;p&gt;Varnish cache server&lt;/p&gt;
        &lt;/body&gt;
    &lt;/html&gt;
  "};
      return (deliver);
}</pre></div><p>The default <code class="literal">vcl_error</code> subroutine will generate a simple HTML page informing the status of the response and the ID of the request.</p><p>Custom error pages can be declared inside the synthetic block.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec41"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Maintenance page:<p>Whenever you need to take the entire website offline (as a last resort), you can redirect users straight from the request phase (<code class="literal">vcl_recv</code>) to the <code class="literal">vcl_error</code> subroutine and deliver a static page that warns the users about the scheduled maintenance.</p><div class="informalexample"><pre class="programlisting">sub vcl_recv {
  error 503 "Service unavailable";
}
sub vcl_error{
  if ( obj.status == 503 ) {
    set obj.http.Content-Type = "text/html; charset=utf-8";
    set obj.http.Retry-After = "5";
    synthetic {"
      ### Insert here the HTML for the maintenance page ###
    "}
  }
}</pre></div></li><li class="listitem">Redirecting users:<p>Inside the <code class="literal">vcl_error</code> subroutine you can read and write to requests, objects, and responses variables. In case of an unexpected failure, you can redirect customers to an <span class="strong"><strong>Oops! We're sorry</strong></span> friendly page or even to the website home page.</p><div class="informalexample"><pre class="programlisting">sub vcl_error {
  if (obj.status&gt;= 500) {
    set obj.http.location = "http://www.yourwebsite.com";
    set obj.status = 302;
    return (deliver);
  }
}</pre></div><p>Keep in mind that the <code class="literal">301</code> HTTP code means Moved Permanently, so dealing with redirections based on a system failure cannot be assigned as a <code class="literal">301</code> status. A <code class="literal">302</code> HTTP code informs that the resource was temporarily moved.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec42"/>How it works...</h2></div></div></div><p>Redirecting customers to a friendly error page is a good way to avoid users from having a bad experience, therefore favoring a competitor's website.</p></div></div>
<div class="section" title="Caching static content (Should know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec19"/>Caching static content (Should know)</h1></div></div></div><p>A static web page can be defined as a page that is pre-built and delivered exactly the same way every time it is loaded. Even if the content of a page is updated from time to time (let's say every 10 to 15 minutes) with the latest news or products, you can still cache that page for a smaller period and benefit from not having to process that same response for every customer inside that time frame.</p><p>Other cacheable static contents that can make your website faster are the CSS, JavaScript, and image files which will probably be cached for a longer period of time than your HTML pages.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec43"/>Getting ready</h2></div></div></div><p>First, you need to identify your static files or sections and define a time to live based on how long you expect a specific type of file to change its content.</p><p>Creating a list of files and sections helps you grouping them under a single VCL condition (rule) based on their expected ttl:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>File type</p>
</th><th style="text-align: left" valign="bottom">
<p>Time period</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Image files (JPG, PNG, GIF)</p>
</td><td style="text-align: left" valign="top">
<p>1 week</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>JavaScript</p>
</td><td style="text-align: left" valign="top">
<p>1 hour</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>CSS</p>
</td><td style="text-align: left" valign="top">
<p>1 hour</p>
</td></tr></tbody></table></div><p>The best place to define your ttl caching policies is inside the <code class="literal">vcl_fetch</code> subroutine because it is the last step before an object is inserted to cache.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec44"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Define a time to live based on file type:<div class="informalexample"><pre class="programlisting">sub vcl_fetch {
  if ( req.url ~ " \.(png|jpg|gif)$") {
    set beresp.ttl = 1w;
    unset beresp.http.Set-Cookie;
  }
  if ( req.url ~ " \.(css|js)$") {
    set beresp.ttl = 1h;
    unset beresp.http.Set-Cookie;
  }
  if ( req.url ~ " \.(html)$") {
    set beresp.ttl = 15m;
    unset beresp.http.Set-Cookie;
  }
}</pre></div><p>As you have probably already noticed, Varnish takes a numeric argument followed by a single letter, which identifies a scale of time:</p><p>S = seconds</p><p>H = hours</p><p>D = days</p><p>W = weeks</p></li><li class="listitem">Disabling cache:<div class="informalexample"><pre class="programlisting">sub vcl_fecth {
  if ( req.url ~ "/donotcache\.html") {
    set beresp.ttl = 0s;
  }
}</pre></div><p>Setting the ttl to zero seconds will create an already expired object, forcing it not to be cached.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec45"/>How it works...</h2></div></div></div><p>Avoiding duplicated workload is the first step to a faster website, and achieving a higher hit ratio for your static content is where you have to primarily focus before moving to more complex caching policies.</p><p>Make sure your backend is not already defining a caching policy with <code class="literal">cache-control</code> or <code class="literal">expire</code> headers, as you probably will be better off obeying those policies.</p><p>Un-setting cookies before the object is placed into memory is mandatory or else you will end up caching unnecessary and sensitive information.</p></div></div>
<div class="section" title="Cookies, sessions, and authorization (Become an expert)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec20"/>Cookies, sessions, and authorization (Become an expert)</h1></div></div></div><p>There are many ways to identify unique users in our website. In this recipe, we will cover the three most used ones: cookies, sessions, and authorization. Whenever one of them occurs, you must be extremely careful with the request and generated response since they contain user-specific data and cannot be delivered to a different user.</p><p>Caching user-specific content requires an extra step, since only the same user will be able to access that cached object. Delivering user-specific content to someone else can lead to session hijacking, user's private information leakage, and many other security problems.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec46"/>Getting ready</h2></div></div></div><p>An HTTP cookie, also referred to as browser cookie, is the result of a <code class="literal">set-cookie</code> response header and it is used to store small pieces of data on the client side (browser) for later usage on subsequent requests. The top reason for using cookies is to identify a unique user and use the stored information to generate personalized content or to track its steps.</p><p>HTTP session is generated on the server side and only the ID of that interaction is sent to the client in the form of a cookie. A session is used to keep all the information on the server side, and the user only handles the session ID (token), unlike what happens with a regular cookie. Session tokens are often specific to programming languages.</p><p>HTTP authorization is the most basic kind of HTTP security that can be implemented in a website (often called HTTP Basic Auth), and it is executed via an <code class="literal">authorization</code> header. HTTP authorization will often encrypt the username and password in a base 64 fashion.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec47"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Avoiding cache for user-specific requests:<div class="informalexample"><pre class="programlisting">sub vcl_recv {
  if (req.http.Authorization || req.http.Cookie) {
    return (pass);
  }
}</pre></div><p>Since sessions are stored inside cookies, you will only need to check for the presence of a session as a URL parameter, like the <code class="literal">JSESSIONID</code> (for Java systems).</p></li><li class="listitem">Caching user specific information:<p>If you need to create a per-user cache, the following example of code will set the cookie along with the original <code class="literal">hash_data</code>. Caching responses with cookies will often lead to more problems than solutions.</p><div class="informalexample"><pre class="programlisting">sub vcl_recv {
  if (req.http.Cookie) {
    return (lookup);
  }
}
sub vcl_hash {
  hash_data(req.url);
  if ( req.http.Cookie ~ "cookie_set_for_unique_users" ) {
    hash_data(req.http.cookie);
  }
}</pre></div><p>This is actually a basic example on how to deal with requests that contain cookies and need to be cached. You should write your own regex (using the <code class="literal">regsub</code> function) to extract only the value of the cookie that you will use to append to the default object's key.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec48"/>How it works...</h2></div></div></div><p>Storing user-specific content in cache can be helpful to deliver a faster response, but it is not recommended since the non-authenticated users represent the biggest portion of the requests, therefore, the authenticated users will not have trouble if they do not hit the cache at all.</p><p>Unless you have a website that requires all users (or at least half of them) to be logged in, it is better to let the requests pass to backend servers and avoid the trouble of leaking private information and other security concerns.</p></div></div>
<div class="section" title="HTTP cache headers (Should know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec21"/>HTTP cache headers (Should know)</h1></div></div></div><p>The HTTP is a long-established protocol for exchanging information between clients and servers, and the headers are an important part of that communication. HTTP headers define what the client is requesting and how they expect it to be responded to by the server.</p><p>Along with a request, you will probably find a handful of headers such as accept, accept-encoding, keep-alive, host, and many others. In a <code class="literal">GET</code> method, the request will ask for information about a specific resource and attach headers to allow the server to know how the response should be formed.</p><p>On the response phase of the communication, the server will also attach server-specific headers to let the client know how it should treat the response and, if instructed, store it (browser cache).</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec49"/>Getting ready</h2></div></div></div><p>In this section, we will focus primarily on the HTTP header called cache-control which is covered in the section 13 of the Hypertext Transfer Protocol Version 1.1 (<a class="ulink" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html">http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html</a>).</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec50"/>How to do it...</h2></div></div></div><p>The HTTP protocol Version 1.1 introduced the <code class="literal">cache-control</code> response header as an alternative to the <code class="literal">expires</code> header implemented in HTTP 1.0. The main difference between them is that <code class="literal">expires</code> takes a date value and the <code class="literal">cache-control</code> can receive an age value.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Expires: Fri, 30 Oct 1998 14:19:41 GMT</strong></span>
<span class="strong"><strong>Cache-Control: max-age=3600</strong></span>
</pre></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The <code class="literal">cache-control</code> header can also indicate what level of cache should be done for that response. This type of control is passed as a directive and can assume the following values:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">public</code>: Any type of cache mechanism (server, proxy, or browser) can cache the content of that response.</li><li class="listitem" style="list-style-type: disc"><code class="literal">private</code>: Indicates that the response (or part of it) is specific to a single client and should not be cached by a "shared" cache mechanism.</li><li class="listitem" style="list-style-type: disc"><code class="literal">no-cache</code>: Forces a validation request to the origin server before delivering the cached copy to the user. It is primarily used for authentication when a cached copy of the response cannot be delivered to an unauthorized client.</li><li class="listitem" style="list-style-type: disc"><code class="literal">no-store</code>: The response cannot be cached at all.</li></ul></div></li><li class="listitem">The expiration attributes (ttl) of the <code class="literal">cache-control</code> header are:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">max-age</code>: Defines for what period of time the response will be cached.</li><li class="listitem" style="list-style-type: disc"><code class="literal">s-maxage</code>: Exactly the same as max-age, but it only applies to proxy caches.</li></ul></div></li></ol></div><p>Opposite to the <code class="literal">cache-control</code> header, the <code class="literal">expires</code> header receives an HTTP-date (RFC 1123) as value that indicates until when the content of the response is considered valid. After it expires, a new representation will be requested from the origin server.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip06"/>Tip</h3><p>The HTTP-date specification can be found at section 3.3.1 of the HTTP protocol (<a class="ulink" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html">http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html</a>).</p></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec51"/>How it works...</h2></div></div></div><p>The HTTP protocol is the foundation in which Varnish relies and includes a number of elements to make caching policies easier. The best approach to setting different times for cached objects (when using a proxy-cache) is to code the time to live policies inside your application server, by sending a <code class="literal">cache-control</code> header alongside with the response and thus avoiding to hardcode the ttl inside the VCL code. In case you are dealing with a legacy system that is no longer maintained, you will probably need to hardcode the ttl to create a cache object.</p><p>Varnish will respect the cache headers values unless it is told not to. So whenever a content should be cached and it is not, check the response headers for any indication of a <code class="literal">no-store</code>, <code class="literal">no-cache</code>, or maybe an already expired time value.</p></div></div>
<div class="section" title="Invalidating cached content (Should know)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec22"/>Invalidating cached content (Should know)</h1></div></div></div><p>Up until now, we have only seen ways of caching contents of your website, but delivering stale data to clients is undesirable when an updated version of that content has already been deployed—this is actually worst than not having a cache at all.</p><p>A simple example of bad content (stale) is the representation of a product that is on sale, and after the discount expires, the server keeps showing its discounted value. Since you would not cache a sensitive portion of your website, like the shopping cart, the user would see a discount price at the product detail page and a higher price at the shopping cart.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec52"/>Getting ready</h2></div></div></div><p>There are actually two ways of invalidating cached contents, and it is very important that you understand the difference between them.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Purge</strong></span>: It is used to remove a single object from cache and must be implemented inside both the <code class="literal">vcl_hit</code> and <code class="literal">vcl_miss</code> subroutines. A single object can contain multiple cached versions if the content is delivered with a <code class="literal">vary</code> header, and the purge method will remove all existing variations.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Ban</strong></span>: It is useful for removing large amounts of contents at the same time by using regular expressions as match. In case you need to ban all JPEGs (<code class="literal">.jpg</code>) or any other regex match, the ban method is faster than the purge method because the latter will invalidate them one by one, thus taking a lot longer to process.</li></ul></div><p>Be aware that while the purge will remove the content from cache, the ban method does not remove it right away—what it actually does is create a ban list that is checked every time a request comes, trying to find out if the content should be delivered from cache or if a newer version should be fetched from the backend server. At this point, you must probably have concluded that this will generate extra workload and a big ban list filter may increase the load on your server.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec53"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Purge:<p>When using any method of invalidation, it is always a good idea to have an <span class="strong"><strong>Access Control List</strong></span> (<span class="strong"><strong>ACL</strong></span>) with the authorized hosts.</p><div class="informalexample"><pre class="programlisting">acl purge {
  "localhost";
  "10.1.0.0"/24;
  "172.16.11.0"/23;
}</pre></div><p>In the preceding example, we are adding <code class="literal">localhost</code> and all hosts from the <code class="literal">10.1.0.0</code> and <code class="literal">172.16.11.0</code> networks to an ACL called <code class="literal">purge</code> (you can name the ACL any way you want).</p><div class="informalexample"><pre class="programlisting">sub vcl_hit {
  if (req.request == "PURGE") {
    if (!client.ip ~ purge) {
      error 405 "Not allowed.";
    } else {
      purge;
      error 200 "Purged.";
    }
  }
  return (deliver);
}

sub vcl_miss {
  if (req.request == "PURGE") {
    if (!client.ip ~ purge) {
      error 405 "Not allowed.";
    } else {
      purge;
      error 200 "Not in cache.";
    }
  }
  return (fetch);
}</pre></div><p>Even though the <code class="literal">PURGE</code> method is not present in the HTTP protocol, it is actually a <code class="literal">GET</code> method with a different name that is used to direct requests to the purge method inside the VCL.</p><p>The exact requested object might not generate a hit and a variant of that object might be in cache, so using the purge method inside the <code class="literal">vcl_miss</code> subroutine is necessary to remove all variants.</p></li><li class="listitem">Ban:<p>Ban can be done through VCL code:</p><div class="informalexample"><pre class="programlisting">sub vcl_recv {
  if (req.request == "BAN") {
    if (!client.ip ~ purge) {
      error 405 "Not allowed.";
    }
    ban("req.http.host == " + req.http.host + "&amp;&amp;req.url == " + req.url);
    error 200 "Banned";
  }
}</pre></div><p>But, it can also can be done through the varnish CLI:</p><div class="informalexample"><pre class="programlisting">ban req.url ~ "\.jpg$"</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec54"/>How it works...</h2></div></div></div><p>While you may increase the ttl of cached objects in hope that it will improve the chances of a cache hit, serving stale content is bad and should be avoided at any cost. Implementing a cache invalidation policy is as important as getting objects into cache and should be treated as a top priority.</p><p>You should always purge content whenever your application server receives an update to its own entities. Since this behavior is not always present in legacy systems, you may need to remove stale data using the varnish CLI.</p></div></div>
<div class="section" title="Compressing the response (Become an expert)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec23"/>Compressing the response (Become an expert)</h1></div></div></div><p>HTTP compression can be described as a way to reduce the transferred data between servers and client. Reducing the amount of transmitted data will result in a faster loading website, and in the new cloud datacenter era, it may even reduce your network usage costs.</p><p>Compression can be performed mainly on text content such as HTML, CSS, JS, XML files, but it does not mean that other type of files cannot be compressed. A common mistake is to compress files that are already natively compressed, such as a PNG image file. This odd behavior will only reduce performance as the compression and decompression processes will actually consume more time and will not result in a smaller file.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec55"/>Getting ready</h2></div></div></div><p>Only after Version 3.0 of the Varnish Cache, the gzip compression method is possible and is definitely encouraged.</p><p>Varnish default behavior is to compress the response before delivering it to the client, searching for the presence of the <code class="literal">accept-encoding</code> header in the request. You can change this behavior by setting the <code class="literal">http_gzip_support</code> parameter in the Varnish daemon.</p><p>Even if the <code class="literal">accept-encoding</code> header is present in the request, this does not guarantee that the cached object will be stored as a compressed representation. In the following section, we will compress objects before inserting them into cache.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec56"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Compressing before storing:<p>Setting the <code class="literal">do_gzip</code> variable to <code class="literal">true</code> inside the <code class="literal">vcl_fetch</code> subroutine will enable the gzip compression before the content is stored.</p><div class="informalexample"><pre class="programlisting">sub vcl_fetch {
  if (beresp.http.content-type ~ "text") {
    set beresp.do_gzip = true;
  }
}</pre></div></li><li class="listitem">Compressing specific content:<div class="informalexample"><pre class="programlisting">sub vcl_fetch {
  if ( req.url ~ "\.(html|htm|css|js|txt|xml)(\?[a-z0-9=]+)?$" ) {
    set beresp.do_gzip = true;
  }
}</pre></div><p>You should not try to compress files that are already natively compressed such as JPEGs, MP3s, and so on, avoiding unnecessary workload.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec57"/>How it works...</h2></div></div></div><p>Compressing the response will result in a smaller network footprint, a faster loading time of pages, and in reduced costs if your datacenter charges you for network bandwidth usage.</p><p>Since most of the search engines will add relevance to faster websites, consider this tool as one of the most important implementation inside your VCL code.</p></div></div>
<div class="section" title="Monitoring the hit ratio (Become an expert)"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec24"/>Monitoring the hit ratio (Become an expert)</h1></div></div></div><p>Varnish default installation comes with many secondary tools that can make the debugging and analysis tasks easier and faster, including <code class="literal">varnishlog</code> (to access the shared memory log), <code class="literal">varnishncsa</code> (to generate an access log in the apache common format), <code class="literal">varnishhist</code> (to create a histogram with hit-miss along with time), <code class="literal">varnishadm</code> (for administrative interface), and the <code class="literal">varnishstat</code> that we will cover in this recipe.</p><p>Analyzing the HTTP headers contained inside the requests and responses will solve most of the problems you may encounter while debugging. An HTTP header viewer such as the Firefox add-on <span class="strong"><strong>Live HTTP Headers</strong></span> will be very helpful to sort out why a response is not being cached, but some other type of bugs will take extra investigation.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec58"/>Getting ready</h2></div></div></div><p>Before trying to improve your cache by reducing the number of repeated objects in memory and refactoring your VCL code to make it slimmer, take a minute to see how well you cache is performing live.</p><p>If you Varnish instance has a low hit ratio percentage, you may need to implement more aggressive caching policies by forcing to cache static content that is not being stored due to a misconfigured header.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec59"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open the <code class="literal">varnishstat</code> monitoring tool.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># varnishstat</strong></span>
</pre></div></li><li class="listitem">Type in your server console screen the preceding command to execute the Varnish statistics tool.<p>You will be presented with a screen full of statistics and we will go through the most important ones here is this section.</p><div class="mediaobject"><img src="graphics/0403_17_01.jpg" alt="How to do it..."/></div><p>The upper left part of the <code class="literal">varnishstat</code> shows the uptime of the server (Days+Hours:Minutes:Seconds).</p><p>The <span class="strong"><strong>Hitrate ratio</strong></span> indicates the time frame, in seconds, of the collected data.</p><p>The <span class="strong"><strong>Hitrate avg</strong></span> numbers show the percentage of hits (multiply by 100) according to the time frame right above them.</p><p>In this example, the hit ratio average was 77 percent for the last 10 seconds, as it also was in the remaining time frames.</p><p>The following data is formatted as:</p><p>Raw data / realtime (per second) / since boot (per second)</p><div class="mediaobject"><img src="graphics/0403_17_02.jpg" alt="How to do it..."/></div><p>8.99 connections accepted per second.</p><p>199.71 requests being made.</p><p>The preceding lines show the client connections and requests. The client connections are expected to be lower than the requests, since <code class="literal">KeepAlive</code> headers were sent with the connections.</p><div class="mediaobject"><img src="graphics/0403_17_03.jpg" alt="How to do it..."/></div><p>162.76 cache hits per second.</p><p>33.95 cache misses per second.</p><p>A high cache miss counter is the first sign of a bad caching policy that needs revision. There is not a magic number when it comes to hit ratio average, but if your cache is not effective, there is no point in caching.</p><div class="mediaobject"><img src="graphics/0403_17_04.jpg" alt="How to do it..."/></div><p>0.00 connections to the backend.</p><p>37.94 connections reused to the backend.</p></li><li class="listitem">Do not be scared if you find out that in the last second no connections were made to the backend servers. This is a good sign of connections being reused, which avoid unnecessary handshake between servers. We are aiming for a high backend reuse.</li><li class="listitem">There are many other counters to retrieve information from the statistics about the health of your Varnish instance and whether there is a problem with it or not. You should pay close attention to the hit ratio and how well Varnish is connecting to the backend servers.</li></ol></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip07"/>Tip</h3><p>To quit the <code class="literal">varnishstat</code> interface just hit <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>C</em></span>.</p></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec60"/>How it works...</h2></div></div></div><p>Monitoring the hit ratio average, the backend connections and other information after a new VCL has been deployed is the key to identifying early problems before it crashes and a server restart is required.</p><p>The smallest mistake while coding a personalized hash key or misplacing a return statement can ruin your cache and make your backend servers go down with the increased and unexpected load.</p></div></div></body></html>