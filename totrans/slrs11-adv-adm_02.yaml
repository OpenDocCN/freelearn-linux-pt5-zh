- en: Chapter 2. ZFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating ZFS storage pools and filesystems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing with ZFS faults and properties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a ZFS snapshot and clone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing a backup in a ZFS filesystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling logs and caches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing devices in storage pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring spare disks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling ZFS snapshots and clones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing with COMSTAR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirroring the root pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZFS shadowing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring ZFS sharing with the SMB share
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting and getting other ZFS properties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing with the ZFS swap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ZFS is a 128-bit transactional filesystem offered by Oracle Solaris 11, and
    it supports 256 trillion directory entries, does not have any upper limit of files,
    and is always consistent on disk. Oracle Solaris 11 makes ZFS its default filesystem,
    which provides some features such as storage pool, snapshots, clones, and volumes.
    When administering ZFS objects, the first step is to create a ZFS storage pool.
    It can be made from full disks, files, and slices, considering that the minimum
    size of any mentioned block device is 128 MB. Furthermore, when creating a ZFS
    pool, the possible RAID configurations are stripe (Raid 0), mirror (Raid 1), and
    RAID-Z (a kind of RAID-5). Both the mirror and RAID-Z configurations support a
    feature named self-healing data that works by protecting data. In this case, when
    a bad block arises in a disk, the ZFS framework fetches the same block from another
    replicated disk to repair the original bad block. RAID-Z presents three variants:
    raidz1 (similar to RAID-5) that uses at least three disks (two data and one parity),
    raidz2 (similar to RAID-6) that uses at least five disks (3D and 2P), and raidz3
    (similar to RAID-6, but with an additional level of parity) that uses at least
    eight disks (5D and 3P).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating ZFS storage pools and filesystems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start playing with ZFS, the first step is to create a storage pool, and afterwards,
    all filesystems will be created inside these storage pools. To accomplish the
    creation of a storage pool, we have to decide which raid configuration we will
    use (stripe, mirror, or RAID-Z) to create the storage pool and, afterwards, the
    filesystems on it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To follow this recipe, it is necessary to use a virtual machine (VMware or VirtualBox)
    that runs Oracle Solaris 11 with 4 GB RAM and eight 4 GB disks. Once the virtual
    machine is up and running, log in as the root user and open a terminal.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A storage pool is a logical object, and it represents the physical characteristics
    of the storage and must be created before anything else. To create a storage pool,
    the first step is to list all the available disks on the system and choose what
    disks will be used by running the following command as the root role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the selection of disks, create a `zpool create` storage pool and
    verify the information about this pool using the `zpool list` and `zpool status`
    commands. Before these steps, we have to decide the pool configuration: stripe
    (default), mirror, raidz, raidz2, or raidz3\. If the configuration isn''t specified,
    stripe (raid0) will be assumed as default. Then, a pool is created by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To list the pool, execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify the status of the pool, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Although it''s out of the scope of this chapter, we can list some related performance
    information by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If necessary, a second and third storage pool can be created using the same
    commands but taking different disks and, in this case, by changing to the `mirror`
    and `raidz` configurations, respectively. This task is accomplished by running
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the storage pools are created, it''s time to create filesystems in these
    pools. First, let''s create a filesystem named `zfs_stripe_1` in the `oracle_stripe_1`
    pool. Execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Repeating the same syntax, it''s easy to create two new filesystems named `zfs_mirror_1`
    and `zfs_raidz_1` in `oracle_mirror_1` and `oracle_raidz_1`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The listing of recently created filesystems is done by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The ZFS engine has automatically created the mount-point directory for all
    the created filesystems, and it has been mounted on them. This can also be verified
    by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The last two lines confirm that the ZFS filesystems that we created are already
    mounted and ready to use.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This recipe has taught us how to create a storage pool with different configurations
    such as stripe, mirror, and raidz. Additionally, we learned how to create filesystems
    in these pools.
  prefs: []
  type: TYPE_NORMAL
- en: Playing with ZFS faults and properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ZFS is completely oriented by properties that can change the behavior of storage
    pools and filesystems. This recipe will touch upon important properties from ZFS,
    and we will learn how to handle them.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To follow this recipe, it is necessary to use a virtual machine (VMware or VirtualBox)
    that runs Oracle Solaris 11 with 4 GB RAM and eight 4 GB disks. Once the virtual
    machine is up and running, log in as the root user and open a terminal.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Every ZFS object has properties that can be accessed and, most of the time,
    changed. For example, to get the pool properties, we must execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Some useful information from the previous output is that the free space is
    3.97 GB (the `free` property), the pool is online (the `health` property), and
    `0%` of the total capacity was used (the `capacity` property). If we need to know
    about any problem related to the pool (referring to the `health` property), it''s
    recommended that you get this information by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Another fantastic method to check whether all data in the specified storage
    pool is okay is using the `zpool scrub` command that examines whether the checksums
    are correct, and for replicated devices (such as mirror and raidz configurations),
    the `zpool scrub` command repairs any discovered problem. To follow the `zpool
    scrub` results, the `zpool status` command can be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After some time, if everything went well, the same `zpool` status command should
    show the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'During an analysis of possible disk errors, the following `zpool history` command,
    which shows all the events that occurred on the pool, could be interesting and
    suitable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The Oracle Solaris Fault Manager, through its `fmd` daemon, is a framework
    that receives any information related to potential problems that were detected
    by the system, diagnoses these problems and, eventually, takes a proactive action
    to keep the system integrity such as disabling a memory module. Therefore, this
    framework offers the following `fmadm` command that, when used with the `faulty`
    argument, displays information about resources that the Oracle Solaris Fault Manager
    believes to be faulty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `dmesg` command confirms any suspicious hardware error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'From the `zpool status` command, there are some possible values for the `status`
    field:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ONLINE`:. This means that the pool is good'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FAULTED`: This means that the pool is bad'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OFFLINE`: This means that the pool was disabled by the administrator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DEGRADED`: This means that something (likely a disk) is bad, but the pool
    is still working'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`REMOVED`: This means that a disk was hot-swapped'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UNAVAIL`: This means that the device or virtual device can be opened'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returning to ZFS properties, it''s easy to get property information from a
    ZFS filesystem by running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The previous two commands deserve an explanation—`zfs list –r` shows all the
    datasets (filesystems, snapshots, clones, and so on) under the `oracle_mirror_1`
    storage pool. Additionally, `zfs get all oracle_mirror_1/zfs_mirror_1` displays
    all the properties from the `zfs_mirror_1` filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: There are many filesystem properties (some of them are read-only and others
    read-write), and it's advisable to know some of them. Almost all are inheritable—a
    child (for example, a snapshot or clone object) inherits a configured value for
    a parent object (for example, a filesystem).
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting a property value is done by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The old mount point was renamed to the `/oracle_mirror_1/another_point` directory
    and remounted again. Later, we'll return to this point and review some properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it''s necessary, a ZFS filesystem has to be renamed by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Oracle Solaris 11 automatically altered the mount point of the renamed filesystem
    and remounted it again.
  prefs: []
  type: TYPE_NORMAL
- en: 'To destroy a ZFS filesystem or storage pool, there can''t be any process that
    accesses the dataset. For example, if we try to delete the `zfs_test` filesystem
    when a process is using the directory, we get an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This case presents several possibilities—first (and the most recommended) is
    to understand what processes or applications are using the mentioned filesystem.
    Once the guilty processes or applications are found, the next step is to stop
    them. Therefore, everything is solved without losing any data. However, if there
    isn''t any possibility to find the guilty processes, then killing the offending
    process(es) would be a feasible and unpredictable option, where data loss would
    be probable. Finally, using the `-f` option would cause a *forced destroy*, which,
    obviously, is not advisable and would probably cause data loss. The following
    is the second procedure (killing the problematic process) by running the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We used the `fuser` command that enables us to look for processes that access
    a specific file or directory. Therefore, according to the previous two outputs,
    there''s a process using the `/oracle_stripe_1/zfs_test_1` filesystem, and the
    `ps –ef` command reveals that `bash` is the guilty process, which is correct because
    we changed the mount point before trying to delete it. To solve this, it would
    be enough to leave the `/oracle_stripe_1/zfs_test_1` directory. Nonetheless, if
    we didn''t know how to solve the problem, the last resource would be to kill the
    offending process by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'At this time, there isn''t a process accessing the filesystem, so it''s possible
    to destroy it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify whether the filesystem was correctly destroyed, execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything worked fine, and the filesystem was destroyed. Nonetheless, if there
    was a snapshot or clone under this filesystem (we''ll review and learn about them
    in the next recipe), we wouldn''t have been able to delete the filesystem, and
    we should use the same command with the`–r` option (for snapshots inside) or `–R`
    (for snapshots and clones inside). From here, it''s also possible to destroy the
    whole pool using the `zpool destroy` command. Nevertheless, we should take care
    of a single detail—if there isn''t any process using any filesystem from the pool
    to be destroyed, Oracle Solaris 11 doesn''t prompt any question about the pool
    destruction. Everything inside the pool is destroyed without any question (so
    different from the Windows system, which prompts a warning before a dangerous
    action). To prove this statement, in the next example, we''re going to create
    one filesystem in the `oracle_stripe_1` pool, put some information into it, and,
    at the end, we''re going to destroy all pools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Taking the `zpool` and `zfs` commands, we created, listed, renamed, and destroyed
    pools and filesystems. Furthermore, we learned how to view properties and alter
    them, especially the mount point property that's very essential for daily ZFS
    administration. We also learned how to see the pool history, monitor the pool,
    and gather important information about related pool failures.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a ZFS snapshot and clone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A ZFS snapshot and clone play fundamental roles in the ZFS framework and in
    Oracle Solaris 11, as there are many uses for these features, and one of them
    is to execute backup and restore files from the ZFS filesystem. For example, a
    snapshot could be handy when either there is some corruption in the ZFS filesystem
    or a user loses a specific file. Using ZFS snapshots makes it possible to completely
    rollback the ZFS filesystem to a specific point or date.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To follow this recipe, it is necessary to use a virtual machine (VMware or VirtualBox)
    that runs Oracle Solaris 11 with 4 GB RAM and eight 4 GB disks. Once the virtual
    machine is up and running, log in as the root user and open a terminal.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating a snapshot is a fundamental task that can be executed by running the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Before continuing, I suggest that we copy some big files to the `pool_1/fs_1`
    filesystem. In this case, I used files that I already had on my system, but you
    can copy anything into the filesystem. Run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we create the snapshot by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, snapshots aren''t shown even when using the `zfs list -r` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This behavior is controlled by the `listsnapshots` property (its value is `off`
    by default) from the pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s necessary to alter `listsnapshots` to `on` to change this behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'It worked as planned. However, when executing the previous command, all datasets
    (filesystems and snapshots) are listed. To list only snapshots, it is necessary
    to specify a filter using the`–t` option as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The previous command has shown only the existing snapshots as expected. An interesting
    fact is that snapshots live inside filesystems, and initially, they don't take
    any space on disk. However, as the filesystem is being altered, snapshots take
    free space, and this could be a big concern. Considering this, the `SIZE` property
    equals zero and `REFER` equals `63.1M`, which is the exact size of the `pool_1/fs_1`
    filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: The `REFER` field deserves an explanation—when snapshots are explained in any
    IT area, the classification is the same. There are physical snapshots and logical
    snapshots. Physical snapshots take the same space from a reference filesystem,
    and both don't have any impact on each other during the read/write operations.
    The creation of the snapshot takes a long time, because it's a kind of "copy"
    of everything from the reference filesystem. In this case, the snapshot is a static
    picture that represents the filesystem at the exact time when the snapshot was
    created. After this initial time, snapshots won't be synchronized with the reference
    filesystem anymore. If the administrator wants both synchronized, they should
    do it manually.
  prefs: []
  type: TYPE_NORMAL
- en: The other classification, logical snapshots, is very different from the first
    one. When a logical snapshot is made, only pointers to data from the reference
    filesystem are created, but there is no data inside the snapshot. This process
    is very fast and takes little disk space. The disadvantage is that any read operation
    impacts the reference filesystem. There are two additional effects—when some data
    changes in the reference filesystem, the operating system copies the data to be
    modified to the snapshot before being modified itself (this process is called
    **copy** **on write** (**COW**)). Why? Because of our previous explanation that
    snapshots are a static picture of an exact time from the reference filesystem.
    If some data changes, the snapshot has to be unaltered, and it must contain the
    same data from the time that it was created. A second and worse effect is that
    if the reference filesystem is lost, every snapshot becomes invalid. Why? Because
    the reference filesystem doesn't exist anymore, and all pointers become invalid.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return to the `REFER` field explanation; it means how much data in the reference
    filesystem is being referenced by a pointer in the snapshot. A clone is a copy
    of a filesystem, and it''s based on snapshots, so to create a clone, a snapshot
    must be made first. However, there''s a fundamental difference between a clone
    and snapshot—a snapshot is a read-only object, and a clone is a read/write object.
    Therefore, it''s possible to write in a clone as we''re able to write in a filesystem.
    Other interesting facts are that as the snapshot must exist before creating a
    clone, the clone is dependent on the snapshot, and both must be created in the
    same pool. Create a pool by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at this output, it''s complicated to distinguish a clone from a
    filesystem. Nonetheless, we could gather enough details to be able to distinguish
    the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The `origin` property doesn''t show anything relevant to pools and snapshots,
    but when this property is analyzed on a clone context, it shows us that the clone
    originated from the `pool1_/fs_1@snap1` snapshot. Therefore, it''s feasible to
    confirm that `pool_1/fs_1@snap1` is indeed a snapshot by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In ZFS, the object creation order is `pool` | `filesystem` | `snapshot` | `clone`.
    So, the destruction order should be the inverse: `clone` | `snapshot` | `filesystem`
    | `pool`. It''s possible to skip steps using special options that we''ll learn
    about later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we try to destroy a filesystem that contains a snapshot, the
    following error will be shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In the same way, if we try to destroy a snapshot without removing the clone
    first, the following message will be shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The last two cases have shown that it''s necessary to follow the right order
    to destroy datasets in ZFS. Execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'When the correct sequence is followed, it''s possible to destroy each dataset
    one by one, although, as we mentioned earlier, it would be possible to skip steps.
    The next sequence shows how this is possible. Execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we used the `-R` option, and everything was destroyed—including the
    clone, snapshot, and filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned how to manage snapshots and clones, including how to create, list,
    distinguish, and destroy them. Finally, this closes our review about the fundamentals
    of ZFS.
  prefs: []
  type: TYPE_NORMAL
- en: Performing a backup in a ZFS filesystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ten years ago, I didn't think about learning how to use any backup software,
    and honestly, I didn't like this kind of software because I thought it was so
    simple. Nowadays, I can see why I was so wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Administering and managing backup software is the most fundamental activity
    in IT, acting as the last line of defense against hackers. By the way, hackers
    are winning the war using all types of resources—malwares, Trojans, viruses, worms,
    and spywares, and only backups of file servers and applications can save a company.
  prefs: []
  type: TYPE_NORMAL
- en: Oracle Solaris 11 offers a simple solution composed of two commands (`zfs send`
    and `zfs recv`) to back up ZFS filesystem data. During the backup operation, data
    is generated as a stream and sent (using the `zfs send` command) through the network
    to another Oracle Solaris 11 system that receives this stream (using `zfs recv`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Oracle Solaris 11 is able to produce two kinds of streams: the replication
    stream, which includes the filesystem and all its dependent datasets (snapshots
    and clones), and the recursive stream, which includes the filesystems and clones,
    but excludes snapshots. The default stream type is the replication stream.'
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will show you how to execute a backup and restore operation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To follow this recipe, it's necessary to have two virtual machines (VMware or
    VirtualBox) that run Oracle Solaris 11, with 4 GB RAM each and eight 4 GB disks.
    The systems used in this recipe are named `solaris11-1` and `solaris11-2`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All the ZFS backup operations are based on snapshots. This procedure will do
    everything from the beginning—creating a pool, filesystem, and snapshot and then
    executing the backup. Execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following commands remove some files from the `backuptest_pool/zfs1` filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We omitted a very interesting fact about snapshots—when any file is deleted
    from the filesystem, it doesn''t disappear forever. There is a hidden directory
    named `.zfs` inside each filesystem; it contains snapshots, and all the removed
    files go to a subdirectory inside this hidden directory. Let''s look at the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this information about the localization of deleted files, any file could
    be restored, and even better, it would be possible to revert the filesystem to
    the same content as when the snapshot was taken. This operation is named `rollback`,
    and it can be executed using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Every single file was restored to the filesystem, as nothing had happened.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going a step ahead, let''s see how to back up the filesystem data to another
    system that runs Oracle Solaris 11\. The first step is to connect to another system
    (`solaris 11-2`) and create and prepare a pool to receive the backup stream from
    the `solaris11-1` source system by running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We enabled the `readonly` property from `away_pool`. Why? Because we have to
    keep the metadata consistent while receiving data from another host and afterwards
    too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing this procedure, the next step is to execute the remote backup from
    the `solaris11-1` source machine, sending all filesystem data to the `solaris11-2`
    target machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We used the `ssh` command to send all data through a secure tunnel, but we could
    have used the `netcat` command (it's included in Oracle Solaris, and there's more
    information about it on [http://netcat.sourceforge.net/](http://netcat.sourceforge.net/))
    if security isn't a requirement.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can verify that all data is present on the target machine by executing
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'According to this output, the remote backup, using the `zfs send` and `zfs
    recv` commands, has worked as expected. The restore operation is similar, so let''s
    destroy every file from the `backuptest_pool/zfs1` filesystem in the first system
    (`solaris11-1`) as well as its snapshot by running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'From the second machine (`solaris11-2`), the restore procedure can be executed
    by running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The restore operation is similar to what we did during the backup, but we have
    to change the direction of the command where the `solaris11-1` system is the target
    and `solaris11-2` is the source now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we used the `ssh` command to make a secure transmission between
    the systems. Again, we could have used another tool such as `netcat` and the methodology
    would have done the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to the `solaris11-1` system, verify that all data was recovered by
    running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: ZFS is amazing. The backup and restore operations are simple to execute, and
    everything has worked so well. The removed files are back.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On ZFS, the restore and backup operations are done through two commands: `zfs
    send` and `zfs recv`. Both operations are based on snapshots, and they make it
    possible to save data on the same machine or on another machine. During the explanation,
    we also learned about the snapshot rollback procedure.'
  prefs: []
  type: TYPE_NORMAL
- en: Handling logs and caches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ZFS has some very interesting internal structures that can greatly improve the
    performance of the pool and filesystem. One of them is **ZFS intent log** (**ZIL**),
    which was created to get more intensive and sequential write request performance,
    making more **Input/Output** **Operations Per Second** (**IOPS**) possible and
    saving any transaction record in the memory until transaction groups (known as
    TXG) are flushed to the disk or a request is received. When using ZIL, all of
    the write operations are done on ZIL, and afterwards, they are committed to the
    filesystem, helping prevent any data loss.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the ZIL space is allocated from the main storage pool, but this could
    fragment data. Oracle Solaris 11 allows us to decide where ZIL will be held. Most
    implementations put ZIL on a dedicated disk or, even better, on a mirrored configuration
    using SSD disks or flash memory devices, being appropriated to highlight that
    log devices for ZIL shouldn't be confused with database logfiles' disks. Usually,
    ZIL device logs don't have a size bigger than half of the RAM size, but other
    aspects must be considered to provide a consistent guideline when making its sizing.
  prefs: []
  type: TYPE_NORMAL
- en: Another very popular structure of ZFS is the **Adaptive Replacement Cache**
    (**ARC**), which increases to occupy almost all free memory (RAM minus 1 GB) of
    Oracle Solaris 11, but without pushing the application data out of memory. A very
    positive aspect of ARC is that it improves the reading performance a lot, because
    if data can be found in the memory (ARC), there isn't a necessity of taking any
    information from disks.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond ARC, there's another type of cache named L2ARC, which is similar to a
    cache level 2 between the main memory and the disk. L2ARC complements ARC, and
    using SSD disks is suitable for this type of cache, given that one of the more
    productive scenarios is when L2ARC is deployed as an accelerator for random reads.
    Here's a very important fact to be remembered—L2ARC writes data to the cache devices
    (SSD disks) in an asynchronous way, so L2ARC is not recommended for intensive
    (sequential) writes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe is going to use a virtual machine (from VirtualBox or VMware) with
    4 GB of memory, Oracle Solaris 11 (installed), and at least eight 4 GB disks.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two methods to configure a log object in a pool—either the pool is
    created with log devices (at the same time) or log devices are added after the
    pool''s creation. The latter method is used more often, so the following procedure
    takes this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next command, we''ll add a log in the mirror mode, which is very appropriate
    to prevent a single point of failure. So, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Perfect! The mirrored log was added as expected. It's appropriate to explain
    about the `mirror-0` and `mirror-1` objects from `zpool status`. Both objects
    are virtual devices. When a pool is created, the disks that were chosen are organized
    under a structure named virtual devices (`vdev`), and then, this `vdev` object
    is presented to the pool. In a rough way, a pool is composed of virtual devices,
    and each virtual device is composed of disks, slices, files, or any volume presented
    by other software or storage. Virtual devices are generated when the `stripe`,
    `mirror`, and `raidz` pools are created. Additionally, they are also created when
    a log and cache are inserted into the pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a disk log removal is necessary, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'It would be possible to remove both log disks at once by specifying `mirror-1`
    (the virtual device), which represents the logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'As we explained at the beginning of this procedure, it''s usual to add logs
    after a pool has been created, but it would be possible and easy to create a pool
    and, at the same time, include the log devices during the creation process by
    executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the explanation about the L2ARC cache at the beginning of the
    recipe, it''s also possible to add a cache object (L2ARC) into the ZFS pool using
    a syntax very similar to the one used when adding log objects by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, like log devices, a pool could be created including cache devices
    in a single step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: It worked as expected! However, it's necessary to note that cache objects can't
    be mirrored as we did when adding log devices, and they can't be part of a RAID-Z
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Removing a cache device from a pool is done by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: A final and important warning—every time `cache` objects are added into a pool,
    wait until the data comes into cache (the warm-up phase). It usually takes around
    2 hours.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ARC, L2ARC, and ZIL are common structures in ZFS administration, and we learned
    how to create and remove both logs and cache from the ZFS pool. There are very
    interesting procedures and recommendations about performance and tuning that includes
    these objects, but it's out of the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Managing devices in storage pools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Manipulating and managing devices are common tasks when working with a ZFS storage
    pool, and more maintenance activities involve adding, deleting, attaching, and
    detaching disks. According to Oracle, ZFS supports raid0 (`stripe`), raid1 (`mirror`),
    raidz (similar to raid5, with one parity disk), raidz2 (similar to raid6, but
    uses two parity disks), and raidz3 (three parity disks), and additionally, there
    could be a combination such as raid 0+1 or raid 1+0.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe is going to use a virtual machine (from VirtualBox or VMware) with
    4 GB of memory, a running Oracle Solaris 11 installation, and at least eight 4
    GB disks.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'According to the previous recipes, the structure of a mirrored pool is `pool`
    | `vdev` | `disks`, and the next command shouldn''t be new to us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Eventually, in a critical environment, it could be necessary to increase the
    size of the pool, given that there are some ways to accomplish it. However, not
    all of them are correct, because this procedure must be done with care to keep
    the redundancy. For example, the next command fails to increase the redundancy
    because only one disk is added, and in this case, we would have two vdevs, the
    first being `vdev` (`mirror-0`) with two disks concatenated and a second `vdev`
    that doesn''t have any redundancy. If the second `vdev` fails, the entire pool
    is lost. Oracle Solaris notifies us about the problem when we try this wrong configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: If we wanted to proceed even with this notification, it would be enough to add
    the `-f` option, but this isn't recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second example is very similar to the first one, and we tried to add two
    disks instead of only one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Again, the error remains because we added two disks, but we haven't mirrored
    them. In this case, the explanation is the same, and we would have a single point
    of failure if we tried to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the correct method to expand the pool and keep the tolerance against
    failure is by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'It worked! The final configuration is one that is similar to RAID 1+0, where
    there are two mirrored vdevs and all the data is spread over them. In this case,
    if the pool has a failure disk in any vdevs, data information is preserved. Furthermore,
    there are two vdevs in the pool: `mirror-0` and `mirror-1`.     If we wished to remove a single disk from a mirror, it could be done by executing
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'If the plan is to remove the whole mirror (`vdev`), execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: All deletions were done successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'A mirrored pool with two disks is fine and is used very often, but some companies
    require a more resilient configuration with three disks. To use a more realistic
    case, let''s create a mirrored pool with two disks, create a filesystem inside
    it, copy some aleatory data into this filesystem (the reader can choose any data),
    and finally, add a third disk. Perform the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, in the preceding command, we could have copied any data. Finally, the
    command that executes our task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding command, we attached a new disk (`c8t10d0`) to a mirrored
    pool and specified where the current data would be copied from (`c8t9d0`). After
    resilvering (resynchronization), the pool organization is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Now, the `mir_pool3` pool is a three-way mirror pool, and all data is resilvered
    (resynchronized).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some maintenance procedures require that we disable a disk to prevent any reading
    or writing operation on this device. Thus, when this disk is put to the `offline`
    state, it remains `offline` even after a reboot. Considering our existing three-way
    mirrored pool, the last device can be put in `offline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: There are some interesting findings—the `c8t10d0` disk is `OFFLINE`, `vdev`
    (`mirror-0`) is in the `DEGRADED` state, and the `mir_pool3` pool is in the `DEGRADED`
    state too.
  prefs: []
  type: TYPE_NORMAL
- en: 'The opposite operation to change the status of a disk to `ONLINE` is very easy,
    and while the pool is being resilvered, its status will be `DEGRADED`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the most useful and interesting tasks when managing pools is disk replacement,
    which only happens when there are pools using one of the following configurations:
    `raid1`, `raidz`, `raidz2`, or `raid3`. Why? Because a disk replacement couldn''t
    compromise the data availability, and only these configurations can ensure this
    premise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two kinds of replacement exist:'
  prefs: []
  type: TYPE_NORMAL
- en: Replacement of a failed device by another in the same slot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacement of a failed device by another from another slot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both methods are straight and easy to execute. For example, we're using VirtualBox
    in this example, and to simulate the first case, we're going to power off Oracle
    Solaris 11 (`solaris11-1`), remove the disk that will be replaced (`c8t10d0`),
    create a new one in the same slot, and power on the virtual machine again (`solaris11-1`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before performing all these steps, we''ll copy more data (here, it can be any
    data of your choice) to the `zfs1` filesystem inside the `mir_pool3` pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'On the VirtualBox Manager, click on the virtual machine with `solaris11-1`,
    go to **Settings**, and then go to **Storage**. Once there, remove the disks from
    slot 10 and create another disk at the same place (slot 10). After the physical
    replacement is done, power on the virtual machine (`solaris11-1`) again. After
    the login, open a terminal and execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'As the `c8t10d0` device was exchanged for a new one, the `zpool status mir_pool3`
    command shows that it''s unavailable (`UNAVAIL`). This is the expected status.
    According to the previous explanation, the idea is that the failed disk is exchanged
    for another one in the same slot. Execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The `c8t10d0` disk was replaced and is being resilvered now. This time, we need
    to wait for the resilvering to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we''re executing the replacement for a disk from another slot, the procedure
    is easier. For example, in the following steps, we''re replacing the `c8t9d0`
    disk with `c8t3d0` by executing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Again, after the resync process is over, everything will be okay.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Managing disks is the most important task when working with ZFS. In this section,
    we learned how to add, remove, attach, detach, and replace a disk. All these processes
    will take a long time on a normal daily basis.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring spare disks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a big company environment, there are a hundred disks working 24/7, and literally,
    it's impossible to know when a disk will fail. Imagine lots of disks failing during
    the day and how much time the replacement operations would take. This pictured
    context is useful to show the importance of spare disks. When deploying spare
    disks in a pool in a system, if any disk fails, the spare disk will take its place
    automatically, and data availability won't be impacted.
  prefs: []
  type: TYPE_NORMAL
- en: In the ZFS framework, spare disks are configured per storage pool, and after
    the appropriate configuration, even when a disk fails, nothing is necessary. The
    ZFS makes the entire replacement job automatic.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe requires a virtual machine (VirtualBox or VMware) that runs Oracle
    Solaris 11 with 4 GB RAM and at least eight disks of 4 GB each.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A real situation using spare disks is where there''s a mirrored pool, so to
    simulate this scenario, let''s execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding spare disks in this pool is done by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned earlier, spare disks will be used only when something wrong
    happens to the disks. To test the environment with spare disks, a good practice
    is shutting down Oracle Solaris 11 (`shutdown –y –g0`), removing the `c8t3d0`
    disk (SCSI slot 3) from the virtual machine''s configuration, and turning on the
    virtual machine again. The status of `mir_pool4` presented by Oracle Solaris 11
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Perfect! The disk that was removed is being shown as unavailable (`UNAVAIL`),
    and the `c8t5d0` spare disk has taken its place (`INUSE`). The pool is shown as
    `DEGRADED` to notify the administrator that a main disk is facing problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s return to the configuration—power off the virtual machine,
    reinsert the removed disk again to the same SCSI slot 3, and power on the virtual
    machine. After completing all the steps, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the output, the `c8d5d0` spare disk continues to show its status
    as `INUSE` even when the `c8t3d0` disk is online again. To signal to the spare
    disk that `c8t3d0` is online again before Oracle Solaris updates it, execute the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: ZFS is amazing. Initially, the `c8t3d0` disk has come online again, but the
    `c8t5d0` spare disk was still in use (`INUSE`). Afterwards, we ran the `zpool
    online mir_pool4 c8t3d0` command to confirm the online status of `c8t3d0`, and
    the spare disk (`c8t5d0`) became available and started acting as a spare disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, remove the spare disk by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, you saw how to configure spare disks, and some experiments
    were done to explain its exact working.
  prefs: []
  type: TYPE_NORMAL
- en: Handling ZFS snapshots and clones
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ZFS snapshot is a complex theme that can have its functionality extended using
    the hold and release operations. Additionally, other tasks such as renaming snapshots,
    promoting clones, and executing differential snapshots are crucial in daily administration.
    All these points will be covered in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe can be followed using a virtual machine (VirtualBox or VMware) with
    4 GB RAM, a running Oracle Solaris 11 application, and at least eight disks with
    4 GB each.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From what we learned in the previous recipes, let''s create a pool and a filesystem,
    and populate this filesystem with any data (readers can copy any data into this
    filesystem) and two snapshots by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Deleting a snapshot is easy as we already saw it previously in the chapter,
    and if it''s necessary, it can be done by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Like the operation of removing a snapshot, renaming it is done by running the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Both actions (renaming and destroying) are common operations that are done
    when handling snapshots. Nonetheless, the big question that comes up is whether
    it would be possible to prevent a snapshot from being deleted. This is where a
    new snapshot operation named `hold` can help us. When a snapshot is put in `hold`
    status, it can''t be removed. This behavior can be configured by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'To list the snapshots on hold, execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Through the `zfs hold keep` command, the snapshot was left in suspension, and
    afterwards, we tried to remove it without success because of the hold. If there
    were other descendants from the `simple_pool/zfs1` filesystem, it would be possible
    to hold all of them by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'An important detail must be reinforced here—a snapshot can only be destroyed
    when it''s released, and there''s a property named `userrefs` that tells whether
    the snapshot is being held or not. Using this information, the releasing and destruction
    operations can be executed in a row by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Going a little further, Oracle Solaris 11 allows us to determine what has changed
    in a filesystem when comparing two snapshots. To understand how it works, the
    first step is to take a new snapshot named `snap_1`. Afterwards, we have to alter
    the content of the `simple_pool/zfs1` filesystem to take a new snapshot (`snap_2`)
    and determine what has changed in the filesystem. The entire procedure is accomplished
    by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command is the most important from this procedure because it
    takes the differential snapshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: The previous command has shown that the new file in `/simple_pool_1/zfs1` is
    the `hosts` file, and it was expected according to our previous setup. The `+`
    identifier indicates that a file or directory was added, the `-` identifier indicates
    that a file or directory was removed, the `M` identifier indicates that a file
    or directory was modified, and the `R` identifier indicates that a file or directory
    was renamed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are reaching the end of this section, we should remember that earlier
    in this chapter, we reviewed how to make a clone from a snapshot, but not all
    operations were shown. The fact about clone is that it is possible to promote
    it to a normal filesystem and, eventually, remove the original filesystem (if
    necessary) because there isn''t a clone as a descendant anymore. Let''s verify
    the preceding sentence by running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Until this point, everything is okay. The next command shows us that `simple_pool_1/zfs1_clone`
    is indeed a clone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The next command promotes the existing clone to an independent filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: We're able to prove that `simple_pool_1/zfs1_clone1` is a new filesystem because
    the clone didn't require any space (size of `25K`), and the recently promoted
    clone to filesystem takes 63.1M now. Moreover, the `origin` property doesn't point
    to a snapshot object anymore.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section has explained how to create, destroy, hold, and release a snapshot,
    as well as how to promote a clone to a real filesystem. Furthermore, you saw how
    to determine the difference between two snapshots.
  prefs: []
  type: TYPE_NORMAL
- en: Playing with COMSTAR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Common Protocol SCSI Target** (**COMSTAR**) is a framework that was introduced
    in Oracle Solaris 11; this makes it possible for Oracle Solaris 11 to access disks
    in another system that is running any operating system (Oracle Solaris, Oracle
    Enterprise Linux, and so on). This access happens through the network using protocols
    such as **iSCSI**, **Fibre** **Channel over Ethernet** (**FCoE**), or **Fibre
    Channel** (**FC**).'
  prefs: []
  type: TYPE_NORMAL
- en: One big advantage of using COMSTAR is that Oracle Solaris 11 is able to reach
    the disks on another machine without using a HBA board (very expensive) for an
    FC channel access. There are also disadvantages such as the fact that dump devices
    don't support the iSCSI disks offered by COMSTAR and the network infrastructure
    can become overloaded.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section requires two virtual machines that run Oracle Solaris 11, both
    with 4 GB RAM and eight 4 GB disks. Additionally, both virtual machines must be
    in the same network and have access to each other.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A good approach when configuring iSCSI is to have an initial plan, a well-defined
    list of disks that will be accessed using iSCSI, and to determine which system
    will be the initiator (`solaris11-2`) and the target (`solaris11-1`). Therefore,
    let''s list the existing disks by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the previous two commands, the `c8t3d0` and `c8t12d0` disks are
    available for use. Nevertheless, unfortunately, the COMSTAR software isn''t installed
    in Oracle Solaris 11 by default; we have to install it to use the iSCSI protocol
    on the `solaris11-1` system. Consequently, using the IPS framework that was configured
    in [Chapter 1](part0015_split_000.html#page "Chapter 1. IPS and Boot Environments"),
    *IPS and Boot Environments*, we can confirm whether the appropriate package is
    or isn''t installed on the system by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'The iSCSI target feature was installed through a package named `storage-server`,
    but the feature is only enabled if the `stmf` service is also enabled. Therefore,
    let''s enable the service by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: At this point, the system is ready to be configured as an iSCSI target. Before
    proceeding, let's learn a new concept about ZFS.
  prefs: []
  type: TYPE_NORMAL
- en: ZFS has a nice feature named ZFS volumes that represent and work as block devices.
    ZFS volumes are identified as devices in `/dev/zvol/dsk/rdsk/pool/[volume_name]`.
    The other nice thing about ZFS volumes is that after they are created, the size
    of the volume is reserved in the pool.
  prefs: []
  type: TYPE_NORMAL
- en: It's necessary to create a ZFS volume and, afterwards, a **Logical Unit** (**LUN**)
    from this ZFS volume to use iSCSI in Oracle Solaris 11\. Eventually, less experienced
    administrators don't know that the LUN concept comes from the storage world (Oracle,
    EMC, and Hitachi). A storage box presents a volume (configured as raid0, raid1,
    raid5, and so on) to the operating system, and this volume is known as LUN, but
    from the operating system's point view, it's only a simple disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s create a ZFS volume. The first step is to create a pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to create a volume (in this case, using a size of 2 GB) by
    running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, as a requirement to present the volume through the network using iSCSI,
    it''s necessary to create LUN from the `mypool_iscsi/myvolume` volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Our main concern is to make the recently created LUN viewable from any host
    that needs to access it. So, let''s configure the access that is available and
    permitted from all hosts by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Currently, the iSCSI target service can be disabled; now, it must be checked
    and enabled if necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s important to realize the dependencies from this service by executing
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the iSCSI target service is enabled, let''s create a new iSCSI target.
    Remember that to access the available disks through the network and using iSCSI,
    we have to create a target (something like an access port or an iSCSI server)
    to enable this access. Then, to create a target in the `solaris11-1` machine,
    execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'The iSCSI target has some important default properties, and one of them determines
    whether an authentication scheme will be required or not. The following output
    confirms that authentication (`auth`) isn''t enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: From here, we are handling two systems—`solaris11-1` (`192.168.1.106`), which
    was configured as the iSCSI target, and `solaris11-2` (`192.168.1.109`), which
    will be used as an initiator. By the way, we should remember that an iSCSI initiator
    is a kind of iSCS client that's necessary to access iSCSI disks offered by other
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'To configure an initiator, the first task is to verify that the iSCSI initiator
    service and its dependencies are enabled by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'The configured initiator has some very interesting properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: According to the preceding output, `Authentication Type` is configured to `NONE`;
    this is the same configuration for the target. For now, it's appropriate because
    both systems must have the same authentication scheme.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before the iSCSI configuration procedure, there are three methods to find an
    iSCSI disk on another system: static, send target, and iSNS. However, while all
    of them certainly have a specific use for different scenarios, a complete explanation
    about these methods is out of scope. Therefore, we will choose the *send target*
    method that is a kind of automatic mechanism to find iSCSI disks in internal networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify the configured method and to enable the send targets methods, execute
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'The `solaris11-1` system was configured as an iSCSI target, and we created
    a LUN in this system to be accessed by the network. On the `solaris11-2` system
    (iSCSI initiator), we have to register the iSCSI target system (`solaris11-1`)
    to discover which LUNs are available to be accessed. To accomplish these tasks,
    execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: The previous command shows the configured target on the `solaris11-1` system
    (first line of the output).
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm the successfully added target, iSCSI LUNs available from the iSCSI
    target (`solaris11-1`) are shown by the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'The iSCSI volume (presented as a disk for the iSCSI initiator) from the `solaris11-1`
    system was found, and it can be used normally as it is a local device. To test
    it, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: Normally, this configuration (without authentication) is the configuration that
    we'll see in most companies, although it isn't recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Some businesses require that all data communication be authenticated, requiring
    both the iSCSI target and initiator to be configured with an authentication scheme
    where a password is set on the iSCSi target (`solaris11-1`), forcing the same
    credential to be set on the iSCSI initiator (`solaris11-2`).
  prefs: []
  type: TYPE_NORMAL
- en: When managing authentication, it's possible to configure the iSCSI authentication
    scheme using the CHAP method (unidirectional or bidirectional) or even RADIUS.
    As an example, we're going to use CHAP unidirectional where the client (`solaris
    11-2`, the iSCSI initiator) executes the login to the server (`solaris11-1`, the
    iSCSI target) to access the iSCSI target devices (LUNs or, at the end, ZFS volumes).
    However, if a bidirectional authentication was used, both the target and initiator
    should present a CHAP password to authenticate each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the `solaris11-1` system, list the current target''s configuration by executing
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the output, currently, the authentication isn''t configured to
    use the CHAP authentication. Therefore, it can be done by executing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s great, but there isn''t any enabled password to make the authentication
    happen. Thus, we have to set a password (`packt1234567`) to complete the target
    configuration. By the way, the password is long because the CHAP password must
    have 12 characters at least:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'On the `solaris11-2` system, the CHAP authentication must be set up to make
    it possible for the initiator to log in to the target; now, execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'On the `solaris11-2` system (initiator), we have to confirm that it continues
    using the iSCSI dynamic discovery (`sendtargets`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'The same password from the target (`packt1234567`) must be set on the `solaris11-2`
    system (initiator). Moreover, the CHAP authentication also must be configured
    by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'Verifying the authentication configuration from the initiator node and available
    targets can be done using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have to update the device tree configuration using the `devfsadm`
    command to confirm that the target is available for the initiator (`solaris11-2`)
    access. If everything has gone well, the iSCSI disk will be visible using the
    `format` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'As a simple example, the following commands create a pool and filesystem using
    the iSCSI disk that was discovered and configured in the previous steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! The iSCSI configuration with the CHAP authentication has worked smoothly.
    Now, to consolidate all the acquired knowledge, the following commands undo all
    the iSCSI configurations, first on the initiator (`solaris11-2`) and afterwards
    on the target (`solaris11-1`), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'By updating the device tree (the `devfsadm` and `format` commands), we can
    see that the iSCSI disk has disappeared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the unconfiguring process must be done on the target (`solaris11-2`).
    First, list the existing LUNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the existing LUN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'List the currently configured targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'Delete the existing targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'Destroy the pool that contains the iSCSI disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we did it. There isn't an iSCSI configuration anymore.
  prefs: []
  type: TYPE_NORMAL
- en: A few months ago, I wrote a tutorial that explains how to configure a free VTL
    software that emulates a tape robot, and at the end of document, I explained how
    to connect to this VTL from Oracle Solaris 11 using the iSCSI protocol. It's very
    interesting to see a real case about how to use the iSCSI initiator to access
    an external application. Check the references at the end of this chapter to learn
    more about this VTL document.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, you learned about all the iSCSI configurations using COMSTAR
    with and without the CHAP authentication. Moreover, the undo configuration steps
    were also provided.
  prefs: []
  type: TYPE_NORMAL
- en: Mirroring the root pool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, systems running very critical applications without a working mirrored
    boot disk is something unthinkable. However, when working with ZFS, the mirroring
    process of the boot disk is smooth and requires few steps to accomplish it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To follow this recipe, it's necessary to have a virtual machine (VirtualBox
    or VMware) that runs Oracle Solaris 11 with 4 GB RAM and a disk the same size
    as the existing boot disk. This example uses an 80 GB disk.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before thinking about boot disk mirroring, the first thing to do is check is
    the `rpool` health:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'According to this output, `rpool` is healthy, so the next step is to choose
    a disk with a size that is equal to or bigger than the original `rpool` disk.
    Then, we need to call the `format` tool and prepare it to receive the same data
    from the original disk as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we''ve chosen which will be the mirrored disk, the second disk has to
    be attached to the existing root pool (`rpool`) to mirror the boot and system
    files. Remember that the mirroring process will include all the snapshots from
    the filesystem under the `rpool` disk. The mirroring process is initiated by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Make sure that you wait until resilvering is done before rebooting.
  prefs: []
  type: TYPE_NORMAL
- en: 'To follow the mirroring process, execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'To avoid executing the previous command several times, it would be simpler
    to make a script as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `rpool` pool is completely mirrored as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After adding the second disk (mirror disk) into the `rpool` pool and after the
    entire mirroring process has finished, the system can be booted using the alternative
    disk (through BIOS, we're able to initialize the system from the mirrored disk).
    For example, this example was done using VirtualBox, so the alternative disk can
    be chosen using the *F12* key.
  prefs: []
  type: TYPE_NORMAL
- en: ZFS shadowing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most companies have very heterogeneous environments where some machines are
    outdated and others are new. Usually, it's required to copy data from the old
    machine to a new machine that runs Oracle Solaris 11, and it's a perfect time
    to use an excellent feature named Shadow Migration. This feature can be used to
    copy (migrate) data through NFS or locally (between two machines), and the filesystem
    types that can be used as the origin are UFS, VxFS (from Symantec), and surely,
    the fantastic ZFS.
  prefs: []
  type: TYPE_NORMAL
- en: An additional and very attractive characteristic of this feature is the fact
    that a client application doesn't need to wait for the data migration to be complete
    at the target, and it can access all data that was already migrated. If the required
    data wasn't copied to the new machine (target) while being accessed, then ZFS
    will fail through to the source (original data).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe requires two virtual machines (`solaris11-1` and `solaris11-2`)
    with Oracle Solaris 11 installed and 4 GB RAM each. Furthermore, the example will
    show you how to migrate data from an existing filesystem (`/shadowing_pool/origin_filesystem`)
    in the `solaris11-2` system (source) to the `solaris11-1` system (target or destination).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember that the source machine is the `solaris11-2` system (from where the
    data will be migrated), and the `solaris11-1` system is the destination or target.
    Therefore, the first step to handle shadowing is to install the `shadow-migration`
    package on the destination machine to where the data will be migrated, by executing
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'After the installation of the package, it''s suggested that you check whether
    the shadowing service is enabled, by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'As the shadowing service isn''t enabled, run the following command to enable
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: On the second machine (`solaris11-2`, the source host), the filesystem to be
    migrated must be shared in a read-only mode using NFS. Why must it be read-only
    ? Because the content can't change during the migration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set up a test ZFS filesystem to be migrated using Shadow Migration and
    to make the filesystem read-only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command copies some data (readers can copy anything) to the `shadowing_pool/origin_filesystem`
    filesystem from `solaris11-2` to simulate a real case of migration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'Share the origin filesystem as read-only data (`-o ro`) using the NFS service
    by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'On the first machine (`solaris11-1`), which is the destination where data will
    be migrated (copied), check whether the NFS share is okay and reachable by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'The system is all in place. The shadowing process is ready to start from the
    second system (`solaris11-2`) to the first system (`solaris11-1`). This process
    will create the `shadowed_pool/shad_filesystem` filesystem by executing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: 'The shadowing process can be tracked by running the `shadowstat` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'The finished shadowing task is verified by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'The shadowing process worked! Moreover, the same operation is feasible to be
    accomplished using two local ZFS filesystems (the previous process was done through
    NFS between the `solaris11-2` and `solaris11-1` systems). Thus, the entire recipe
    can be repeated to copy some files to the source filesystem (it can be any data
    we want) and to start the shadowing activity by running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything has worked perfectly as expected, but in this case, we used two
    local ZFS filesystems instead of using the NFS service. Therefore, the completed
    process can be checked and finished by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The shadow migration procedure was explained in two contexts—using a remote
    filesystem through NFS and using local filesystems. In both cases, it's necessary
    to set the read-only mode for the source filesystem. Furthermore, you learned
    how to monitor the shadowing using `shadowstat` and even the `shadow` property.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring ZFS sharing with the SMB share
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Oracle Solaris 11 has introduced a new feature that enables a system to share
    its filesystems through the **Server Message Block** (**SMB**) and **Common Internet
    File System** (**CIFS**) protocols, both being very common in the Windows world.
    In this section, we're going to configure two filesystems and access these using
    CIFS.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe requires two virtual machines (VMware or VirtualBox) that run Oracle
    Solaris 11, with 4 GB memory each, and some test disks with 4 GB. Furthermore,
    we'll require an additional machine that runs Windows (for example, Windows 7)
    to test the CIFS shares offered by Oracle Solaris 11.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To begin the recipe, it''s necessary to install the smb service by executing
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a pool and two filesystems inside it by executing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: Another crucial configuration is to set mandatory locking (the `nbmand` property)
    for each filesystem, which will be offered by CIFS, because Unix usually uses
    advisory locking and SMB uses mandatory locking. A very quick explanation about
    these kinds of locks is that an advisory lock doesn't prevent non-cooperating
    clients (or processes) from having read or write access to a shared file. On the
    other hand, mandatory clients prevent any non-cooperating clients (or processes)
    from having read or write access to shared file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can accomplish this task by running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: 'Our initial setup is ready. The following step shares the `cifs_pool/zfs_cifs_1`
    and `cifs_pool/zfs_cifs_2` filesystems through the SMB protocol and configures
    a share name (`name`), protocol (`prot`), and path (`file system path`). Moreover,
    a cache client (`csc`) is also configured to smooth the performance when the filesystem
    is overused:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to enable the SMB share feature for each filesystem, we must set the
    `sharesmb` attribute to `on`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: 'The SMB Server service isn''t enabled by default. By the way, the **Service
    Management Facility** (**SMF**) still wasn''t introduced, but the `svcs –a` command
    lists all the installed services and shows which services are online, offline,
    or disabled. As we are interested only in the `smb/server` service, we can use
    the `grep` command to filter the target service by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: 'The `smb/server` service is disabled, and to enable it, you need to execute
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: 'A suitable test is to list the shares provided by the SMB server either by
    getting the value of the `share` filesystem property or by executing the `share`
    command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: 'To proceed with a real test that accesses an SMB share, let''s create a regular
    user named `aborges` and assign a password to him by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: 'The user `aborges` needs to be enabled in the SMB service, so execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm that the user `aborges` was created and enabled for the SMB service,
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the previous output, a **security identifier** (**SID**) was assigned
    to the user `aborges`. The next step is to enable the SMB authentication by adding
    a new library (`pam_smb_passwd.so.1`) in the authentication scheme by executing
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: 'The best way to test all the steps until here is to verify that the shares
    are currently being offered to the other machine (`solaris11-2`) by running the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: 'To show which shares are available from the `solaris11-1` host, run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: 'To mount the first ZFS share (`zfs_cifs_1`) using the SMB service on `solaris11-2`
    from `solaris11-1`, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: 'The mounted filesystem is an SMB filesystem (`-F smbfs`), and it''s easy to
    check its content by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: 'SMB is very common in Windows environments, and then, it would be nice to access
    these shares from a Windows machine (Windows 7 in this case) by accessing the
    network shares by going to the **Start** menu and typing `\\192.168.1.119` as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/00008.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'From the previous screenshot, there are two shares being offered to us: `zfs_cifs_1`
    and `zfs_cifs_2`. Therefore, we can try to access one of them by double-clicking
    it and filling out the credentials as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/00009.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As expected, the username and password are required according to the rules
    from the Windows system that enforce the `[Workgroup][Domain]\[user]` syntax.
    So, after we fill the textboxes, the `zfs_cifs_1 file system` content is shown
    as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/00010.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Everything has worked as we expected, and if we need to undo the SMB sharing
    offered by the `solaris11-1` system, it''s easy to do so by executing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, the CIFS sharing in Oracle Solaris 11 was also explained in
    a step-by-step procedure that showed us how to configure and access CIFS shares.
  prefs: []
  type: TYPE_NORMAL
- en: Setting and getting other ZFS properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing ZFS properties is one of the secrets when we are working with the ZFS
    filesystem, and this is the reason why understanding the inherence concept is
    very important.
  prefs: []
  type: TYPE_NORMAL
- en: 'One ZFS property can usually have three origins as source: `local` (the property
    value was set locally), `default` (the property wasn''t set either locally or
    by inheritance), and `inherited` (the property was inherited from an ancestor).
    Additionally, two other values are possible: `temporary` (the value isn''t persistent)
    and `none` (the property is read-only, and its value was generated by ZFS). Based
    on these key concepts, the sections are going to present different and interesting
    properties for daily administration.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe can be followed using two virtual machines (VirtualBox or VMware)
    with Oracle Solaris 11 installed, 4 GB RAM, and eight disks of at least 4 GB.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Working as a small review, datasets such as pools, filesystems, snapshots,
    and clones have several properties that administrators are able to list, handle,
    and configure. Therefore, the following commands will create a pool and three
    filesystems under this pool. Additionally, we are going to copy some data (a reminder
    again—we could use any data) into the first filesystem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: 'To get all the properties from a pool and filesystem, execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: Both commands have a similar syntax, and we've got all the properties from the
    `prop_pool` pool and the `prop_pool/zfs_1` filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: In the *ZFS shadowing* section, we touched the NFS subject, and some filesystems
    were shared using the `share` command. Nonetheless, they could have been shared
    using ZFS properties, such as `sharenfs`, that have a value equal to `off` by
    default (when we use this value, it isn't managed by ZFS and is still using `/etc/dfs/dfstab`).
    Let's take the `sharenfs` property, which will be used to highlight some basic
    concepts about properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, the property listing is too long; it is faster to get only one property''s
    value by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, the same property can be got recursively by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: From the last three outputs, we noticed that the `sharenfs` property is disabled
    on the pool and filesystems, and this is the default value set by Oracle Solaris
    11.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sharenfs` property can be enabled by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: 'As `sharenfs` was set to `on` for `prop_pool/zfs_1`, the source value has changed
    to `local`, indicating that this value wasn''t inherited, but it was set locally.
    Therefore, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: 'The NFS sharing can be confirmed by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating a new file stem under `zfs_1` shows us an interesting characteristic.
    Execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: 'The new `zfs_4` filesystem has the `sharenfs` property inherited from the `upper
    zfs_1` filesystem; now execute the following command to list all the inherited
    properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: That's great! The new `zfs_4` filesystem has inherited the `sharenfs` property,
    and it appears in the `share` output command.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good question is whether a filesystem will be able to fill all the space
    of a pool. Yes, it will be able to! Now, this is the reason for ZFS having several
    properties related to the amount of space on the disk. The first of them, the
    `quota` property, is a well-known property that limits how much space a dataset
    (filesystem in this case) can fill in a pool. Let''s take an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: 'All filesystems struggle to use the same space (`3.52G`), and one of them can
    fill more space than the other (or all the free space), so it is possible that
    a filesystem suffered a "run out space" error. A solution would be to limit the
    space a filesystem can take up by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: The `zfs_3` filesystem space was limited to 1 GB, and it can't exceed this threshold.
    Nonetheless, there isn't any additional guarantee that it has 1 GB to fill. This
    is subtle—it can't exceed 1 GB, but there is no guarantee that even 1 GB is enough
    for doing it. Another serious detail—this quota space is shared by the filesystem
    and all the descendants such as snapshots and clones. Finally and obviously, it
    isn't possible to set a quota value lesser than the currently used space of the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: A solution for this apparent problem is the `reservation` property. When using
    `reservation`, the space is guaranteed for the filesystem, and nobody else can
    take this space. Sure, it isn't possible to make a reservation above the quota
    or maximum free space, and the same rule is followed—the reservation is for a
    filesystem and its descendants.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the `reservation` property is set to a value, this amount is discounted
    from the total available pool space, and the used pool space is increased by the
    same value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: 'Each dataset under `prop_pool` has its `reservation` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: 'The `reservation` property is configured to a specific value (for example,
    512 MB), given that this amount is subtracted from the pool''s available space
    and added to its used space. Now, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: The concern about space is usually focused on a total value for the whole pool,
    but it's possible to limit the available space for individual users or groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting the quota for users is done through the `userquota` property and for
    groups using the `groupquota` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: 'Getting the used and quota space from users and groups is done by executing
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: 'Removing all the quota values that were set until now is done through the following
    sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, you saw some properties such as `sharenfs`, `quota`, `reservation`,
    `userquota`, and `groupquota`. All of the properties alter the behavior of the
    ZFS pool, filesystems, snapshots, and clones. Moreover, there are other additional
    properties that can improve the ZFS functionality, and I suggest that readers
    look for all of them in *ZFS Administration Guide*.
  prefs: []
  type: TYPE_NORMAL
- en: Playing with the ZFS swap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the toughest jobs in Oracle Solaris 11 is to calculate the optimal size
    of the swap area. Roughly, the operating system's virtual memory is made from
    a sum of RAM and swap, and its correct provisioning helps the application's performance.
    Unfortunately, when Oracle Solaris 11 is initially installed, the correct swap
    size can be underestimated or overestimated, given that any possible mistake can
    be corrected easily. This section will show you how to manage this issue.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe requires a virtual machine (VMware or VirtualBox) with Oracle Solaris
    11 installed and 4 GB RAM. Additionally, it's necessary to have access to eight
    4 GB disks.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'According to Oracle, there is an estimate during the installation process that
    Solaris needs around one-fourth of the RAM space for a swap area in the disk.
    However, for historical reasons, administrators still believe in the myth that
    swap space should be equal or bigger than twice the RAM size for any situation.
    Surely, it should work, but it isn''t necessary. Usually (not a rule, but observed
    many times), it should be something between 0.5 x RAM and 1.5 x RAM, excluding
    exceptions such as when predicting a database installation. Remember that the
    swap area can be a dedicated partition or a file; the best way to list the swap
    areas (and their free space) is by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: 'From the previous output, the meaning of each column is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`swapfile`: This shows that swap areas come from two ZFS volumes `(/dev/zvol/dsk/rpool/swap`
    and `/dev/zvol/dsk/rpool/newswap`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dev`: This shows the major and minor number of swap devices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`swaplo`: This shows the minimum possible swap space, which is limited to the
    memory page size and its respective value is usually obtained as units of sectors
    (512 bytes) by executing the `pagesize` command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`blocks`: This is the total swap space in sectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`free`: This is the free swap space (4 GB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An alternative way to collect information about the swap area is using the
    same `swap` command with the `–s` option, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: 'From this command output, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '`519668k bytes allocated`: This is a swap space that indicates the amount of
    swap space that already has been used earlier but is not necessarily in use this
    time. Therefore, it''s reserved and available to be used when required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`400928k reserved`: This is the virtual swap space that was reserved (heap
    segment and anonymous memory) for future use, and this time, it isn''t allocated
    yet. Usually, the swap space is reserved when the virtual memory for a process
    is created. Anonymous memory refers to pages that don''t have a counterpart in
    the disk (any filesystem). They are moved to a swap area because the shortage
    of RAM (physical memory) occurs many times because of the sum of stack, shared
    memory, and process heap, which is larger than the available physical memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`946696k used`: This is total amount of swap space that is reserved or allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`4260372k available`: This is the amount of swap space available for future
    allocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Until now, you''ve learned how to monitor swap areas. From now, let''s see
    how to add and delete swap space on Oracle Solaris 11 by executing the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: 'Two lines (`rpool/newswap` and `rpool/swap`) prove that the swap space has
    a size of 4 GB (2 GB + 2 GB), and both datasets are ZFS volumes, which can be
    verified by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: 'Continuing from the previous section (getting and setting properties), the
    swap space can be changed by altering the `volsize` property if the pool has free
    space. Then, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple way to increase the swap space would be by changing the `volsize`
    value. Then, execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: 'Eventually, it''s necessary to add a new volume because the free space on a
    pool isn''t enough, so it can be done by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the swap volume has been created, the next step is to add it as a swap
    device by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the new swap device must be included in the `vfstab` file under `etc`
    to be mounted during the Oracle Solaris 11 boot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: 'Last but not least, the task of removing the swap area is very simple. First,
    the entry in `/etc/vfstab` needs to be deleted. Before removing the swap areas,
    they need to be listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: 'Second, the swap volume must be unregistered from the system by running the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: 'Earlier, the `rpool/newswap` volume was increased. However, it would be impossible
    to decrease it because `rpool/newswap` was in use (busy). Now, as the first 2
    GB space from this volume was removed, this 2 GB part isn''t in use at this moment,
    and the total volume (3 GB) can be reduced. Execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: An overview of the recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You saw how to add, remove, and monitor the swap space using the ZFS framework.
    Furthermore, You learned some very important concepts such as reserved, allocated,
    and free swap.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Oracle Solaris Administration -* *ZFS File Systems* at [http://docs.oracle.com/cd/E23824_01/html/821-1448/preface-1.html#scrolltoc](http://docs.oracle.com/cd/E23824_01/html/821-1448/preface-1.html#scrolltoc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How to configure a* *free VTL (Virtual Tape Library)* at [http://alexandreborgesbrazil.files.wordpress.com/2013/09/how-to-configure-a-free-vtl1.pdf](http://alexandreborgesbrazil.files.wordpress.com/2013/09/how-to-configure-a-free-vtl1.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Oracle Solaris* *Tunable Parameters Reference Manual* at [http://docs.oracle.com/cd/E23823_01/html/817-0404/preface-1.html#scrolltoc](http://docs.oracle.com/cd/E23823_01/html/817-0404/preface-1.html#scrolltoc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Oracle Solaris* *Administration: SMB and Windows Interoperability* at [http://docs.oracle.com/cd/E23824_01/html/821-1449/toc.html](http://docs.oracle.com/cd/E23824_01/html/821-1449/toc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Playing with Swap Monitoring and Increasing Swap Space Using ZFS Volumes In*
    *Oracle Solaris 11.1* (by Alexandre Borges) at [http://www.oracle.com/technetwork/articles/servers-storage-admin/monitor-swap-solaris-zfs-2216650.html](http://www.oracle.com/technetwork/articles/servers-storage-admin/monitor-swap-solaris-zfs-2216650.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Playing with ZFS Encryption* *In Oracle Solaris 11* (by Alexandre Borges)
    at [http://www.oracle.com/technetwork/articles/servers-storage-admin/solaris-zfs-encryption-2242161.html](http://www.oracle.com/technetwork/articles/servers-storage-admin/solaris-zfs-encryption-2242161.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
