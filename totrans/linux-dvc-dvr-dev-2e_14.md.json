["```\n #include <linux/dma-mapping.h>\n```", "```\nvoid *dma_alloc_coherent(struct device *dev, size_t size,\n                      dma_addr_t *dma_handle, gfp_t flag) \n```", "```\nvoid dma_free_coherent(struct device *dev, size_t size,\n                 void *cpu_addr, dma_addr_t dma_handle);\n```", "```\nenum dma_data_direction {\n     DMA_BIDIRECTIONAL = 0,\n     DMA_TO_DEVICE = 1,\n     DMA_FROM_DEVICE = 2,\n     DMA_NONE = 3,\n};\n```", "```\ndma_addr_t dma_map_single(struct device *dev, void *ptr,\n        size_t size, enum dma_data_direction direction);\n```", "```\nvoid dma_unmap_single(struct device *dev, \n                      dma_addr_t dma_addr, size_t size, \n                      enum dma_data_direction direction);\nint dma_mapping_error(struct device *dev, \n                      dma_addr_t dma_addr);\n```", "```\nstruct scatterlist {\n     unsigned long page_link;\n     unsigned int     offset;\n     unsigned int     length;\n     dma_addr_t       dma_address;\n     unsigned int     dma_length;\n};\n```", "```\nvoid sg_init_table(struct scatterlist *sgl, \n                   unsigned int nents)\nvoid sg_set_buf(struct scatterlist *sg, const void *buf,\n                unsigned int buflen)\nint dma_map_sg(struct device *dev, \n               struct scatterlist *sglist, int nents,\n               enum dma_data_direction dir);\n```", "```\nu32 *wbuf, *wbuf2, *wbuf3;\nwbuf = kzalloc(SDMA_BUF_SIZE, GFP_DMA);\nwbuf2 = kzalloc(SDMA_BUF_SIZE, GFP_DMA);\nwbuf3 = kzalloc(SDMA_BUF_SIZE/2, GFP_DMA);\nstruct scatterlist sg[3];\nsg_init_table(sg, 3);\nsg_set_buf(&sg[0], wbuf, SDMA_BUF_SIZE);\nsg_set_buf(&sg[1], wbuf2, SDMA_BUF_SIZE);\nsg_set_buf(&sg[2], wbuf3, SDMA_BUF_SIZE/2);\nret = dma_map_sg(dev, sg, 3, DMA_TO_DEVICE);\nif (ret != 3) {\n     /*handle this error*/\n}\n/* As of now you can use 'ret' or 'sg_dma_len(sgl)' to retrieve the\n * length of the scatterlist array.\n */\n```", "```\nvoid dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg,\n                          enum dma_data_direction dir, int nents)\n```", "```\ndma_unmap_sg(dev, sg, 3, DMA_TO_DEVICE);\n```", "```\nvoid dma_sync_sg_for_cpu(struct device *dev,\n                     struct scatterlist *sg,\n                     int nents,\n                     enum dma_data_direction direction);\nvoid dma_sync_sg_for_device(struct device *dev,\n                     struct scatterlist *sg, int nents,\n                     enum dma_data_direction direction);\nvoid dma_sync_single_for_cpu(struct device *dev, \n                     dma_addr_t addr, size_t size,\n                     enum dma_data_direction dir)\nvoid dma_sync_single_for_device(struct device *dev,\n                     dma_addr_t addr, size_t size,\n                     enum dma_data_direction dir)\n```", "```\n#include <linux/completion.h> \n```", "```\nDECLARE_COMPLETION(my_comp);\n```", "```\nstruct completion my_comp;\ninit_completion(&my_comp);\n```", "```\nvoid wait_for_completion(struct completion *comp);\n```", "```\nvoid complete(struct completion *comp);\nvoid complete_all(struct completion *comp);\n```", "```\n#include <linux/dmaengine.h>\n```", "```\nstruct dma_device {\n    unsigned int chancnt;\n    unsigned int privatecnt;\n    struct list_head channels;\n    struct list_head global_node;\n    struct dma_filter filter;\n    dma_cap_mask_t  cap_mask;\n    u32 src_addr_widths;\n    u32 dst_addr_widths;\n    u32 directions;\n    int (*device_alloc_chan_resources)(\n                                  struct dma_chan *chan);\n    void (*device_free_chan_resources)(\n                                  struct dma_chan *chan);\n    struct dma_async_tx_descriptor \n     *(*device_prep_dma_memcpy)(\n        struct dma_chan *chan, dma_addr_t dst, \n        dma_addr_t src, size_t len, unsigned long flags);\n    struct dma_async_tx_descriptor \n      *(*device_prep_dma_memset)(\n       struct dma_chan *chan, dma_addr_t dest, int value,\n       size_t len, unsigned long flags);\n    struct dma_async_tx_descriptor\n      *(*device_prep_dma_memset_sg)(\n         struct dma_chan *chan, struct scatterlist *sg,\n         unsigned int nents, int value,\n         unsigned long flags);\n    struct dma_async_tx_descriptor  \n      *(*device_prep_dma_interrupt)(\n         struct dma_chan *chan, unsigned long flags);\n    struct dma_async_tx_descriptor \n      *(*device_prep_slave_sg)(\n         struct dma_chan *chan, struct scatterlist *sgl,\n           unsigned int sg_len,\n           enum dma_transfer_direction direction,\n           unsigned long flags, void *context);\n    struct dma_async_tx_descriptor \n      *(*device_prep_dma_cyclic)(\n           struct dma_chan *chan, dma_addr_t buf_addr,\n           size_t buf_len, size_t period_len,\n           enum dma_transfer_direction direction,\n           unsigned long flags);\n     void (*device_caps)(struct dma_chan *chan,\n                     struct dma_slave_caps *caps);\n     int (*device_config)(struct dma_chan *chan,\n                     struct dma_slave_config *config);\n     void (*device_synchronize)(struct dma_chan *chan);\n     enum dma_status (*device_tx_status)(\n              struct dma_chan *chan, dma_cookie_t cookie,\n              struct dma_tx_state *txstate);\n     void (*device_issue_pending)(struct dma_chan *chan);\n     void (*device_release)(struct dma_device *dev);\n};\n```", "```\nenum dma_transaction_type {\n    DMA_MEMCPY,     /* Memory to memory copy */\n    DMA_XOR,  /* Memory to memory XOR*/\n    DMA_PQ,   /* Memory to memory P+Q computation */\n    DMA_XOR_VAL, /* Memory buffer parity check using \n                  * XOR */\n    DMA_PQ_VAL,  /* Memory buffer parity check using \n                  * P+Q */\n    DMA_INTERRUPT,  /* The device can generate dummy\n                     * transfer that will generate \n                     * interrupts */\n    DMA_MEMSET_SG,  /* Prepares a memset operation over a \n                     * scatter list */\n    DMA_SLAVE,      /* Slave DMA operation, either to or\n                     * from a device */\n    DMA_PRIVATE,    /* channels are not to be used \n                     * for global memcpy. Usually\n                     *used with DMA_SLAVE */\n    DMA_SLAVE,      /* Memory to device transfers */\n    DMA_CYCLIC,     /* can handle cyclic tranfers */\n    DMA_INTERLEAVE, /* Memory to memory interleaved\n                     * transfer */\n}\n```", "```\ndma_cap_set(DMA_SLAVE, sdma->dma_device.cap_mask);\ndma_cap_set(DMA_CYCLIC, sdma->dma_device.cap_mask);\ndma_cap_set(DMA_MEMCPY, sdma->dma_device.cap_mask);\n```", "```\n#define SDMA_DMA_DIRECTIONS (BIT(DMA_DEV_TO_MEM) | \\\n                     BIT(DMA_MEM_TO_DEV) | \\\n                       BIT(DMA_DEV_TO_DEV))\n[...]\nsdma->dma_device.directions = SDMA_DMA_DIRECTIONS;\n```", "```\nstruct dma_chan {\n     struct dma_device *device;\n     struct device *slave;\n     dma_cookie_t cookie;\n     dma_cookie_t completed_cookie;\n[...]\n};\n```", "```\nstruct dma_async_tx_descriptor {\n     dma_cookie_t cookie;\n     struct dma_chan *chan;\n     dma_async_tx_callback callback;\n     void *callback_param;\n[...]\n};\n```", "```\nint dma_set_mask_and_coherent(struct device *dev,\n                              u64 mask);\n```", "```\nint dma_set_mask(struct device *dev, u64 mask);\nint dma_set_coherent_mask(struct device *dev, u64 mask);\n```", "```\n#define PLAYBACK_ADDRESS_BITS DMA_BIT_MASK(32)\n#define RECORD_ADDRESS_BITS DMA_BIT_MASK(24)\nstruct my_sound_card *card;\nstruct device *dev;\n...\nif (!dma_set_mask(dev, PLAYBACK_ADDRESS_BITS)) {\n     card->playback_enabled = 1;\n} else {\n    card->playback_enabled = 0;\n    dev_warn(dev,\n     \"%s: Playback disabled due to DMA limitations\\n\",\n     card->name);\n}\nif (!dma_set_mask(dev, RECORD_ADDRESS_BITS)) {\n    card->record_enabled = 1;\n} else {\n    card->record_enabled = 0;\n    dev_warn(dev, \n          \"%s: Record disabled due to DMA limitations\\n\",\n          card->name);\n}\n```", "```\nstruct dma_chan *dma_request_channel(\n                      const dma_cap_mask_t *mask,\n                      dma_filter_fn fn, void *fn_param);\n```", "```\ndma_cap_mask my_dma_cap_mask;\nstruct dma_chan *chan;\ndma_cap_zero(my_dma_cap_mask);\n/* Memory 2 memory copy */\ndma_cap_set(DMA_MEMCPY, my_dma_cap_mask); \nchan = dma_request_channel(my_dma_cap_mask, NULL, NULL);\n```", "```\ntypedef bool (*dma_filter_fn)(struct dma_chan *chan,\n                void *filter_param);\n```", "```\nvoid dma_release_channel(struct dma_chan *chan)\n```", "```\nroot@raspberrypi4-64:~# ls /sys/class/dma/\ndma0chan0  dma0chan1  dma0chan2  dma0chan3  dma0chan4  dma0chan5  dma0chan6  dma0chan7  dma1chan0  dma1chan1\n```", "```\nroot@raspberrypi4-64:~# cat /sys/class/dma/dma0chan0/in_use \n1\nroot@raspberrypi4-64:~# cat /sys/class/dma/dma0chan1/in_use \n1\nroot@raspberrypi4-64:~# cat /sys/class/dma/dma0chan2/in_use \n1\nroot@raspberrypi4-64:~# cat /sys/class/dma/dma0chan3/in_use \n0\nroot@raspberrypi4-64:~# cat /sys/class/dma/dma0chan4/in_use \n0\nroot@raspberrypi4-64:~# cat /sys/class/dma/dma0chan5/in_use \n0\nroot@raspberrypi4-64:~# cat /sys/class/dma/dma0chan6/in_use \n0\nroot@raspberrypi4-64:~#\n```", "```\nint dmaengine_slave_config(struct dma_chan *chan,\n                         struct dma_slave_config *config)\n```", "```\nstruct dma_slave_config {\n     enum dma_transfer_direction direction;\n     phys_addr_t src_addr;\n     phys_addr_t dst_addr;\n     enum dma_slave_buswidth src_addr_width;\n     enum dma_slave_buswidth dst_addr_width;\n     u32 src_maxburst;\n     u32 dst_maxburst;\n     [...]\n};\n```", "```\n    /* dma transfer mode and direction indicator */\n    enum dma_transfer_direction {\n        DMA_MEM_TO_MEM, /* Async/Memcpy mode */\n        DMA_MEM_TO_DEV, /* From Memory to Device */\n        DMA_DEV_TO_MEM, /* From Device to Memory */\n        DMA_DEV_TO_DEV, /* From Device to Device */\n        DMA_TRANS_NONE, \n    };\n    ```", "```\nenum dma_slave_buswidth {\n    DMA_SLAVE_BUSWIDTH_UNDEFINED = 0,\n    DMA_SLAVE_BUSWIDTH_1_BYTE = 1,\n    DMA_SLAVE_BUSWIDTH_2_BYTES = 2,\n    DMA_SLAVE_BUSWIDTH_3_BYTES = 3,\n    DMA_SLAVE_BUSWIDTH_4_BYTES = 4,\n    DMA_SLAVE_BUSWIDTH_8_BYTES = 8,\n    DMA_SLAVE_BUSWIDTH_16_BYTES = 16,\n    DMA_SLAVE_BUSWIDTH_32_BYTES = 32,\n    DMA_SLAVE_BUSWIDTH_64_BYTES = 64,\n};\n```", "```\nstruct dma_chan *my_dma_chan;\ndma_addr_t dma_src_addr, dma_dst_addr;\nstruct dma_slave_config channel_cfg = {0};\n/* No filter callback, neither filter param */\nmy_dma_chan = dma_request_channel(my_dma_cap_mask,\n                                   NULL, NULL);\n/* scr_addr and dst_addr are ignored for mem to mem copy */\nchannel_cfg.direction = DMA_MEM_TO_MEM;\nchannel_cfg.dst_addr_width = DMA_SLAVE_BUSWIDTH_32_BYTES;\ndmaengine_slave_config(my_dma_chan, &channel_cfg);\n```", "```\nstruct dma_device *dma_dev = my_dma_chan->device;\nstruct dma_async_tx_descriptor *tx_desc = NULL;\ntx_desc = dma_dev->device_prep_dma_memcpy(\n                          my_dma_chan, dma_dst_addr,\n                          dma_src_addr, BUFFER_SIZE, 0);\nif (!tx_desc) {\n    /* dma_unmap_* the buffer */\n    handle_error();\n}\n```", "```\ntx_desc = dmaengine_prep_dma_memcpy(my_dma_chan,\n             dma_dst_addr, dma_src_addr, BUFFER_SIZE, 0);\n```", "```\ndma_cookie_t dmaengine_submit(\n                  struct dma_async_tx_descriptor *desc)\n```", "```\nstruct completion transfer_ok;\ninit_completion(&transfer_ok);\n/*\n * you can also set the parameter to be given to this \n * callback in tx->callback_param\n */\nTx_desc->callback = my_dma_callback;\n/* Submitting our DMA transfer */\ndma_cookie_t cookie = dmaengine_submit(tx);\nif (dma_submit_error(cookie)) {\n    /* handle error */\n    [...]\n}\n```", "```\nvoid dma_async_issue_pending(struct dma_chan *chan);\n```", "```\ndma_async_issue_pending(my_dma_chan);\nwait_for_completion(&transfer_ok);\n/* may be unmap buffer if necessary and if it is not\n * done in the completion callback yet\n */\n[...]\n/* Process buffer through rx_data and tx_data virtual addresses. */\n[...]\n```", "```\nstatic void my_dma_complete_callback (void *param)\n{\n    complete(transfer_ok);\n[...]\n}\n```", "```\n#define pr_fmt(fmt) \"DMA-TEST: \" fmt\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/dma-mapping.h>\n#include <linux/fs.h>\n#include <linux/dmaengine.h>\n#include <linux/device.h>\n#include <linux/io.h>\n#include <linux/delay.h>\n```", "```\n/* we need page aligned buffers */\n#define DMA_BUF_SIZE  2 * PAGE_SIZE\nstatic u32 *wbuf;\nstatic u32 *rbuf;\nstatic int dma_result;\nstatic int gMajor; /* major number of device */\nstatic struct class *dma_test_class;\nstatic struct completion dma_m2m_ok;\nstatic struct dma_chan *dma_m2m_chan;\n```", "```\nstatic void dev_release(struct device *dev)\n{\n    pr_info( \"releasing dma capable device\\n\");\n}\nstatic struct device dev = {\n    .release = dev_release,\n    .coherent_dma_mask = ~0, // allow any address\n    .dma_mask = &dev.coherent_dma_mask,// use the same mask\n};\n```", "```\nint dma_open(struct inode * inode, struct file * filp)\n{     \n     init_completion(&dma_m2m_ok);\n     wbuf = kzalloc(DMA_BUF_SIZE, GFP_KERNEL | GFP_DMA);\n     if(!wbuf) {\n           pr_err(\"Failed to allocate wbuf!\\n\");\n           return -ENOMEM;\n     }\n     rbuf = kzalloc(DMA_BUF_SIZE, GFP_KERNEL | GFP_DMA);\n     if(!rbuf) {\n           kfree(wbuf);\n           pr_err(\"Failed to allocate rbuf!\\n\");\n           return -ENOMEM;\n     }\n     return 0;\n}\n```", "```\nint dma_release(struct inode * inode, struct file * filp)\n{\n     kfree(wbuf);\n     kfree(rbuf);\n     return 0;\n}\n```", "```\nssize_t dma_read (struct file *filp, char __user * buf,\n                   size_t count, loff_t * offset)\n{\n     pr_info(\"DMA result: %d!\\n\", dma_result);\n     return 0;\n}\n```", "```\nstatic void dma_m2m_callback(void *data)\n{\n    pr_info(\"in %s\\n\",__func__);\n    complete(&dma_m2m_ok);\n}\n```", "```\nssize_t dma_write(struct file * filp,\n                  const char __user * buf,\n                  size_t count, loff_t * offset)\n{\n    u32 *index, i;\n    size_t err = count;\n    dma_cookie_t cookie;\n    dma_cap_mask_t dma_m2m_mask;\n    dma_addr_t dma_src, dma_dst;\n    struct dma_slave_config dma_m2m_config = {0};\n    struct dma_async_tx_descriptor *dma_m2m_desc;\n```", "```\n    pr_info(\"Initializing buffer\\n\");\n    index = wbuf;\n    for (i = 0; i < DMA_BUF_SIZE/4; i++) {\n        *(index + i) = 0x56565656;\n    }\n    data_dump(\"WBUF initialized buffer\", (u8*)wbuf,\n               DMA_BUF_SIZE);\n    pr_info(\"Buffer initialized\\n\");\n```", "```\n     dma_cap_zero(dma_m2m_mask);\n     dma_cap_set(DMA_MEMCPY, dma_m2m_mask);\n     dma_m2m_chan = dma_request_channel(dma_m2m_mask,\n                                         NULL, NULL);\n     if (!dma_m2m_chan) {\n           pr_err(\"Error requesting the DMA channel\\n\");\n           return -EINVAL;\n     } else {\n           pr_info(\"Got DMA channel %d\\n\",\n                    dma_m2m_chan->chan_id);\n     }\n```", "```\n     dma_m2m_config.direction = DMA_MEM_TO_MEM;\n     dma_m2m_config.dst_addr_width =\n                     DMA_SLAVE_BUSWIDTH_4_BYTES;\n     dmaengine_slave_config(dma_m2m_chan,\n                            &dma_m2m_config);\n     pr_info(\"DMA channel configured\\n\");\n     /* Grab bus addresses to prepare the DMA transfer */\n     dma_src = dma_map_single(&dev, wbuf, DMA_BUF_SIZE,    \n                               DMA_TO_DEVICE);\n     if (dma_mapping_error(&dev, dma_src)) {\n           pr_err(\"Could not map src buffer\\n\");\n           err = -ENOMEM;\n           goto channel_release;\n     }\n     dma_dst = dma_map_single(&dev, rbuf, DMA_BUF_SIZE,\n                               DMA_FROM_DEVICE);\n     if (dma_mapping_error(&dev, dma_dst)) {\n           dma_unmap_single(&dev, dma_src,\n                            DMA_BUF_SIZE, DMA_TO_DEVICE);\n           err = -ENOMEM;\n           goto channel_release;\n     }\n     pr_info(\"DMA mappings created\\n\");\n```", "```\n    dma_m2m_desc = \n        dmaengine_prep_dma_memcpy(dma_m2m_chan,\n                      dma_dst, dma_src, DMA_BUF_SIZE,0);\n     if (!dma_m2m_desc) {\n           pr_err(\"error in prep_dma_sg\\n\");\n           err = -EINVAL;\n           goto dma_unmap;\n     }\n     dma_m2m_desc->callback = dma_m2m_callback;\n```", "```\n     cookie = dmaengine_submit(dma_m2m_desc);\n     if (dma_submit_error(cookie)) {\n           pr_err(\"Unable to submit the DMA coockie\\n\");\n           err = -EINVAL;\n           goto dma_unmap;\n     }\n     pr_info(\"Got this cookie: %d\\n\", cookie);\n```", "```\n     dma_async_issue_pending(dma_m2m_chan);\n     pr_info(\"waiting for DMA transaction...\\n\");\n     /* you also can use wait_for_completion_timeout() */\n     wait_for_completion(&dma_m2m_ok);\n```", "```\ndma_unmap:\n    /* we do not care about the source anymore */\n    dma_unmap_single(&dev, dma_src, DMA_BUF_SIZE,\n                       DMA_TO_DEVICE);\n    /* unmap the DMA memory destination for CPU access.\n     * This will sync the buffer */\n    dma_unmap_single(&dev, dma_dst, DMA_BUF_SIZE,\n                       DMA_FROM_DEVICE);\n    /* \n     * if no error occured, then we are safe to access \n     * the buffer. The buffer must be synced first, and \n     * thanks to dma_unmap_single(), it is.\n     */\n    if (err >= 0) {\n        pr_info(\"Checking if DMA succeed ...\\n\");\n        for (i = 0; i < DMA_BUF_SIZE/4; i++) {\n            if (*(rbuf+i) != *(wbuf+i)) {\n                pr_err(\"Single DMA buffer copy falled!, \n                        r=%x,w=%x,%d\\n\",\n                        *(rbuf+i), *(wbuf+i), i);\n                return err;\n            }\n        }\n        pr_info(\"buffer copy passed!\\n\");\n        dma_result = 1;\n        data_dump(\"RBUF DMA buffer\", (u8*)rbuf, \n                  DMA_BUF_SIZE);\n    }\nchannel_release:\n     dma_release_channel(dma_m2m_chan);\n     dma_m2m_chan = NULL;\n     return err;\n}\n```", "```\nstruct file_operations dma_fops = {\n     .open = dma_open,\n     .read = dma_read,\n     .write = dma_write,\n     .release = dma_release,\n};\n```", "```\nint __init dma_init_module(void)\n{\n    int error;\n    struct device *dma_test_dev;\n    /* register a character device */\n    error = register_chrdev(0, \"dma_test\", &dma_fops);\n    if (error < 0) {\n      pr_err(\"DMA test driver can't get major number\\n\");\n        return error;\n    }\n    gMajor = error;\n    pr_info(\"DMA test major number = %d\\n\",gMajor);\n    dma_test_class = class_create(THIS_MODULE, \n                                  \"dma_test\");\n    if (IS_ERR(dma_test_class)) {\n       pr_err(\"Error creating dma test module class.\\n\");\n       unregister_chrdev(gMajor, \"dma_test\");\n       return PTR_ERR(dma_test_class);\n    }\n    dma_test_dev = device_create(dma_test_class, NULL,\n                     MKDEV(gMajor, 0), NULL, \"dma_test\");\n    if (IS_ERR(dma_test_dev)) {\n       pr_err(\"Error creating dma test class device.\\n\");\n       class_destroy(dma_test_class);\n       unregister_chrdev(gMajor, \"dma_test\");\n       return PTR_ERR(dma_test_dev);\n    }\n     dev_set_name(&dev, \"dmda-test-dev\");\n     device_register(&dev);\n     pr_info(\"DMA test Driver Module loaded\\n\");\n     return 0;\n}\n```", "```\nstatic void dma_cleanup_module(void)\n{\n    unregister_chrdev(gMajor, \"dma_test\");\n    device_destroy(dma_test_class, MKDEV(gMajor, 0));\n    class_destroy(dma_test_class);\n    device_unregister(&dev);\n    pr_info(\"DMA test Driver Module Unloaded\\n\");\n}\n```", "```\nmodule_init(dma_init_module);\nmodule_exit(dma_cleanup_module);\nMODULE_AUTHOR(\"John Madieu, <john.madieu@laabcsmart.com>\");\nMODULE_DESCRIPTION(\"DMA test driver\");\nMODULE_LICENSE(\"GPL\");\n```", "```\nstruct dma_async_tx_descriptor \n     *dmaengine_prep_dma_cyclic(\n             struct dma_chan *chan, dma_addr_t buf_addr,\n             size_t buf_len, size_t period_len,\n             enum dma_transfer_direction dir,\n             unsigned long flags)\n```", "```\nstatic int atmel_prepare_rx_dma(struct uart_port *port)\n{\n    struct atmel_uart_port *atmel_port = \n                       to_atmel_uart_port(port);\n     struct device *mfd_dev = port->dev->parent;\n     struct dma_async_tx_descriptor *desc;\n     dma_cap_mask_t        mask;\n     struct dma_slave_config config;\n     struct circ_buf       *ring;\n     int ret, nent;\n     ring = &atmel_port->rx_ring;\n     dma_cap_zero(mask);\n     dma_cap_set(DMA_CYCLIC, mask);\n    atmel_port->chan_rx = \n              dma_request_slave_channel(mfd_dev, \"rx\");\n    sg_init_one(&atmel_port->sg_rx, ring->buf,\n                  sizeof(struct atmel_uart_char) *\n                    ATMEL_SERIAL_RINGSIZE);\n    nent = dma_map_sg(port->dev, &atmel_port->sg_rx, 1,\n                       DMA_FROM_DEVICE);\n    /* Configure the slave DMA */\n    [...]\n    ret = dmaengine_slave_config(atmel_port->chan_rx,\n                           &config);\n    /* Prepare a cyclic dma transfer, assign 2\n     * descriptors, each one is half ring buffer size */\n     desc =\n       dmaengine_prep_dma_cyclic(atmel_port->chan_rx,\n           sg_dma_address(&atmel_port->sg_rx),\n           sg_dma_len(&atmel_port->sg_rx),\n           sg_dma_len(&atmel_port->sg_rx)/2,\n           DMA_DEV_TO_MEM, DMA_PREP_INTERRUPT);\n    desc->callback = atmel_complete_rx_dma;\n    desc->callback_param = port;\n    atmel_port->desc_rx = desc;\n    atmel_port->cookie_rx = dmaengine_submit(desc);\n    dma_async_issue_pending(chan);\n    return 0;\nchan_err:\n[...]\n}\n```", "```\nstatic void atmel_rx_from_dma(struct uart_port *port)\n{\n    struct atmel_uart_port *atmel_port =\n                               to_atmel_uart_port(port);\n    struct tty_port *tport = &port->state->port;\n    struct circ_buf *ring = &atmel_port->rx_ring;\n    struct dma_chan *chan = atmel_port->chan_rx;\n    struct dma_tx_state state;\n    enum dma_status dmastat;\n    size_t count;\n    dmastat = dmaengine_tx_status(chan,\n                  atmel_port->cookie_rx, &state);\n    /* CPU claims ownership of RX DMA buffer */\n    dma_sync_sg_for_cpu(port->dev, &atmel_port->sg_rx, 1,\n                        DMA_FROM_DEVICE);\n    /* The current transfer size should not be larger \n     * than the dma buffer length.\n     */\n    ring->head =\n         sg_dma_len(&atmel_port->sg_rx) - state.residue;\n\n    /* we first read from tail to the end of the buffer\n     * then reset tail */\n    if (ring->head < ring->tail) {\n        count =\n            sg_dma_len(&atmel_port->sg_rx) - ring->tail;\n        tty_insert_flip_string(tport,\n                          ring->buf + ring->tail, count);\n           ring->tail = 0;\n           port->icount.rx += count;\n     }\n     /* Finally we read data from tail to head */\n     if (ring->tail < ring->head) {\n           count = ring->head - ring->tail;\n        tty_insert_flip_string(tport,\n                         ring->buf + ring->tail, count);\n        /* Wrap ring->head if needed */\n        if (ring->head >= sg_dma_len(&atmel_port->sg_rx))\n            ring->head = 0;\n        ring->tail = ring->head;\n        port->icount.rx += count;\n     }\n    /* USART retrieves ownership of RX DMA buffer */\n    dma_sync_sg_for_device(port->dev, &atmel_port->sg_rx,\n                            1, DMA_FROM_DEVICE);\n     [...]\n     tty_flip_buffer_push(tport);\n[...]\n}\n```", "```\nuart1: serial@02020000 {\n    compatible = \"fsl,imx6sx-uart\", \"fsl,imx21-uart\";\n    reg = <0x02020000 0x4000>;\n    interrupts = <GIC_SPI 26 IRQ_TYPE_LEVEL_HIGH>;\n    clocks = <&clks IMX6SX_CLK_UART_IPG>,\n                <&clks IMX6SX_CLK_UART_SERIAL>;\n    clock-names = \"ipg\", \"per\";\n    dmas = <&sdma 25 4 0>, <&sdma 26 4 0>;\n    dma-names = \"rx\", \"tx\";\n    status = \"disabled\";\n};\n```", "```\nstatic int imx_uart_dma_init(struct imx_port *sport)\n{\n    struct dma_slave_config slave_config = {};\n    struct device *dev = sport->port.dev;\n    int ret;\n    /* Prepare for RX : */\n    sport->dma_chan_rx =\n               dma_request_slave_channel(dev, \"rx\");\n    if (!sport->dma_chan_rx)\n        /* cannot get the DMA channel. handle error */\n        [...]\n    [...] /* configure the slave channel */\n    ret = dmaengine_slave_config(sport->dma_chan_rx,\n                                 &slave_config);\n[...]\n    /* Prepare for TX */\n    sport->dma_chan_tx =\n                 dma_request_slave_channel(dev, \"tx\");\n    if (!sport->dma_chan_tx) {\n        /* cannot get the DMA channel. handle error */\n        [...]\n    [...] /* configure the slave channel */\n    ret = dmaengine_slave_config(sport->dma_chan_tx,\n                                 &slave_config);\n    if (ret) {\n        [...] /* handle error */\n    }\n    [...]\n}\n```"]