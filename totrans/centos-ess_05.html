<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Building a Development Environment"><div class="titlepage" id="aid-VF2I2"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Building a Development Environment</h1></div></div></div><p>In this chapter, we will cover how to set up a local CoreOS environment for development on a personal computer, and a test and staging environment cluster on the VM instances of Google Cloud's Compute Engine. These are the topics we will cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Setting up a local development environment</li><li class="listitem">Bootstrapping a remote test/staging cluster on GCE</li></ul></div><div class="section" title="Setting up the local development environment"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec27"/>Setting up the local development environment</h1></div></div></div><p>We are going to learn<a id="id89" class="indexterm"/> how to set up a development environment on our personal computer with the help of VirtualBox and Vagrant, as we did in an earlier chapter. Building and testing <code class="literal">docker</code> images and coding locally makes you more productive, it saves time, and Docker repository can be pushed to the docker registry (private or not) when your docker images are ready. The same goes for the code; you just work on it and test it locally. When it is ready, you can merge it with the git test branch where your team/client can test it further.</p><div class="section" title="Setting up the development VM"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec20"/>Setting up the development VM</h2></div></div></div><p>In the previous<a id="id90" class="indexterm"/> chapters, you learned how to install CoreOS via Vagrant on your PC. Here, we have prepared installation scripts for Linux and OS X to go straight to the point. You can download the latest <span class="emphasis"><em>CoreOS Essentials</em></span> book example files from GitHub repository:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git clone https://github.com/rimusz/coreos-essentials-book/</strong></span>
</pre></div><p>To install a local Vagrant-based development VM, type this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd coreos-essentials-book/chapter5/Local_Development_VM</strong></span>
<span class="strong"><strong>$ ./install_local_dev.sh</strong></span>
</pre></div><p>You should see an output similar to this:</p><div class="mediaobject"><img src="../Images/image00130.jpeg" alt="Setting up the development VM"/></div><p style="clear:both; height: 1em;"> </p><p>Hang on! There's<a id="id91" class="indexterm"/> more!</p><div class="mediaobject"><img src="../Images/image00131.jpeg" alt="Setting up the development VM"/></div><p style="clear:both; height: 1em;"> </p><p>This will perform a VM installation similar to the installation that we did in <a class="link" title="Chapter 1. CoreOS – Overview and Installation" href="part0014.xhtml#aid-DB7S1">Chapter 1</a>, <span class="emphasis"><em>CoreOS – Overview and Installation</em></span>, but in a <a id="id92" class="indexterm"/>more automated way this time.</p></div><div class="section" title="What happened during the VM installation?"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec21"/>What happened during the VM installation?</h2></div></div></div><p>Let's check<a id="id93" class="indexterm"/> out what happened during the VM installation. To sum up:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">A new CoreOS VM (VirtualBox/Vagrant-based) was installed</li><li class="listitem">A new folder called <code class="literal">coreos-dev-env</code> was created in your <code class="literal">Home</code> folder</li></ul></div><p>Run the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd ~/coreos-dev-env</strong></span>
<span class="strong"><strong>$ ls</strong></span>
<span class="strong"><strong>bin </strong></span>
<span class="strong"><strong>fleet       </strong></span>
<span class="strong"><strong>share        </strong></span>
<span class="strong"><strong>vm         </strong></span>
<span class="strong"><strong>vm_halt.sh </strong></span>
<span class="strong"><strong>vm_ssh.sh  </strong></span>
<span class="strong"><strong>vm_up.sh</strong></span>
</pre></div><p>As a result, this is what we see:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Four folders, which consist of the following list:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">bin</code>: <code class="literal">docker</code>, <code class="literal">etcdctl</code> and <code class="literal">fleetctl</code> files</li><li class="listitem"><code class="literal">fleet</code>: The <code class="literal">nginx.service fleet</code> unit is stored here</li><li class="listitem"><code class="literal">share</code>: This is shared folder between the host and VM</li><li class="listitem"><code class="literal">vm</code>: Vagrantfile, <code class="literal">config.rb</code> and <code class="literal">user-data</code> files</li></ul></div></li><li class="listitem">We also<a id="id94" class="indexterm"/> have three files:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">vm_halt.sh</code>: This is used to shut down the CoreOS VM</li><li class="listitem"><code class="literal">vm_ssh.sh</code>: This is used to <code class="literal">ssh</code> to the CoreOS VM</li><li class="listitem"><code class="literal">vm_up.sh</code>: This is used to start the CoreOS VM, with the OS shell preset to the following:<div class="informalexample"><pre class="programlisting"># Set the environment variable for the docker daemon
export DOCKER_HOST=tcp://127.0.0.1:2375
# path to the bin folder where we store our binary files
export PATH=${HOME}/coreos-dev-env/bin:$PATH
# set etcd endpoint
export ETCDCTL_PEERS=http://172.19.20.99:2379
# set fleetctl endpoint
export FLEETCTL_ENDPOINT=http://172.19.20.99:2379
export FLEETCTL_DRIVER=etcd
export FLEETCTL_STRICT_HOST_KEY_CHECKING=false</pre></div></li></ul></div></li></ul></div><p>Now that we have installed our CoreOS VM, let's run <code class="literal">vm_up.sh</code>. We should see this output in the <span class="strong"><strong>Terminal</strong></span> window:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd ~/coreos-dev-env</strong></span>
<span class="strong"><strong>$ ./vm_up.sh</strong></span>
</pre></div><p>You should see output similar to this:</p><div class="mediaobject"><img src="../Images/image00132.jpeg" alt="What happened during the VM installation?"/></div><p style="clear:both; height: 1em;"> </p><p>As we can see<a id="id95" class="indexterm"/> in the preceding screenshot, we do not have any errors. Only <code class="literal">fleetctl list-machines</code> shows our CoreOS VM machine, and we have no <code class="literal">docker</code> containers and <code class="literal">fleet</code> units running there yet.</p></div><div class="section" title="Deploying the fleet units"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec22"/>Deploying the fleet units</h2></div></div></div><p>Let's deploy some fleet<a id="id96" class="indexterm"/> units to verify that our development environment works fine. Run the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd fleet</strong></span>
<span class="strong"><strong>$ fleetctl start nginx.service</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title"><a id="note06"/>Note</h3><p>It can take a bit of time for docker to download the <code class="literal">nginx</code> image.</p></div><p>You can check out the <code class="literal">nginx.service</code> unit's status:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ fleetctl status nginx.service</strong></span>
</pre></div><p>You should see output similar to this:</p><div class="mediaobject"><img src="../Images/image00133.jpeg" alt="Deploying the fleet units"/></div><p style="clear:both; height: 1em;"> </p><p>Once the <code class="literal">nginx fleet</code> unit is deployed, open in your browser <code class="literal">http://172.19.20.99</code>. You should see the<a id="id97" class="indexterm"/> following message:</p><div class="mediaobject"><img src="../Images/image00134.jpeg" alt="Deploying the fleet units"/></div><p style="clear:both; height: 1em;"> </p><p>Let's check out what happened there. We scheduled this <code class="literal">nginx.service</code> unit with <code class="literal">fleetctl</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat ~/coreos-dev-env/fleet/nginx.service</strong></span>

[Unit]
Description=nginx

[Service]
User=core
TimeoutStartSec=0
EnvironmentFile=/etc/environment
ExecStartPre=-/usr/bin/docker rm nginx
ExecStart=/usr/bin/docker run --rm --name nginx -p 80:80 \
 -v /home/core/share/nginx/html:/usr/share/nginx/html \
 nginx:latest
#
ExecStop=/usr/bin/docker stop nginx
ExecStopPost=-/usr/bin/docker rm nginx

Restart=always
RestartSec=10s

[X-Fleet]</pre></div><p>Then, we used the official <code class="literal">nginx</code> image from the docker registry, and shared our local <code class="literal">~/coreos-dev-env/share</code> folder with <code class="literal">/home/core/share</code>, which was mounted afterwards as a docker volume <code class="literal">/home/core/share/nginx/html:/usr/share/nginx/html</code>.</p><p>So, whatever <code class="literal">html</code> files we put into our local <code class="literal">~/coreos-dev-env/share/nginx/html</code> folder will be<a id="id98" class="indexterm"/> picked up automatically by <code class="literal">nginx</code>.</p><p>Let's overview what advantages such a setup gives us:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">We can build and test docker containers locally, and then push them to the docker registry (private or public).</li><li class="listitem">Test our code locally <a id="id99" class="indexterm"/>and push it to the git repository when we are done with it.</li><li class="listitem">By having a local development setup, productivity really increases, as everything is done much faster. We do not have build new docker containers upon every code change, push them to the remote docker registry, pull them at some remote test servers, and so on.</li><li class="listitem">It is very easy to clean up the setup and get it working from a clean start again, reusing the configured <code class="literal">fleet</code> units to start the all required docker containers.</li></ul></div><p>Very good! So, now, we have a fully operational local development setup!</p><div class="note" title="Note"><h3 class="title"><a id="note07"/>Note</h3><p>This setup is as per the CoreOS documentation at <a class="ulink" href="https://coreos.com/docs/cluster-management/setup/cluster-architectures/">https://coreos.com/docs/cluster-management/setup/cluster-architectures/</a>, in the <span class="emphasis"><em>Docker Dev Environment on Laptop</em></span> section.</p><p>Go through<a id="id100" class="indexterm"/> the <code class="literal">coreos-dev-install.sh</code> bash script, which sets up your local development VM. It is a simple script and is well commented, so it should not be too hard to understand its logic.</p></div><p>If you are a Mac <a id="id101" class="indexterm"/>user, you can download from <a class="ulink" href="https://github.com/rimusz/coreos-osx-gui">https://github.com/rimusz/coreos-osx-gui</a> and use my Mac App <span class="strong"><strong>CoreOS-Vagrant GUI for Mac OS X</strong></span>, which has a nice UI to manage CoreOS VM. It will automatically set up the CoreOS VM environment.</p><div class="mediaobject"><img src="../Images/image00135.jpeg" alt="Deploying the fleet units"/></div><p style="clear:both; height: 1em;"> </p></div></div></div>
<div class="section" title="Bootstrapping a remote test/staging cluster on GCE"><div class="titlepage" id="aid-10DJ42"><div><div><h1 class="title"><a id="ch05lvl1sec28"/>Bootstrapping a remote test/staging cluster on GCE</h1></div></div></div><p>So, we have<a id="id102" class="indexterm"/> successfully built our local <a id="id103" class="indexterm"/>development setup. Let's get to the next level, that is, building our test/staging environment on the cloud.</p><p>We are going to use Google Cloud's Compute Engine, so you need a Google Cloud account for this. If you do not have it, for<a id="id104" class="indexterm"/> the purpose of running the examples in the book, you can open a trial account at <a class="ulink" href="https://cloud.google.com/compute/">https://cloud.google.com/compute/</a>. A trial account lasts for <a id="id105" class="indexterm"/>60 days and has $300 as credits, enough to run all of this book's examples. When you are done with opening the account, Google <a id="id106" class="indexterm"/>Cloud SDK needs to be installed from <a class="ulink" href="https://cloud.google.com/sdk/">https://cloud.google.com/sdk/</a>.</p><p>In this topic, we<a id="id107" class="indexterm"/> will follow the recommendations on how to set up CoreOS cluster by referring to <span class="emphasis"><em>Easy Development/Testing Cluster</em></span> from <a class="ulink" href="https://coreos.com/docs/cluster-management/setup/cluster-architectures/">https://coreos.com/docs/cluster-management/setup/cluster-architectures/</a>.</p><div class="section" title="Test/staging cluster setup"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec23"/>Test/staging cluster setup</h2></div></div></div><p>Okay, let's get our cloud cluster installed, as you have already downloaded this book's code examples. Carry<a id="id108" class="indexterm"/> out these steps in the shown order:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Run the following commands:<div class="informalexample"><pre class="programlisting">$ cd coreos-essentials-book/chapter5/Test_Staging_Cluster
$ ls
cloud-config
create_cluster_control.sh
create_cluster_workers.sh
files
fleet
install_fleetctl_and_scripts.sh
settings

Let's check "settings" file first:
$ cat settings
### CoreOS Test/Staging Cluster on GCE settings

## change Google Cloud settings as per your requirements
# GC settings

# CoreOS RELEASE CHANNEL
channel=beta

# SET YOUR PROJECT AND ZONE !!!
project=my-cloud-project
zone=europe-west1-d

# ETCD CONTROL AND NODES MACHINES TYPE
#
control_machine_type=g1-small
#
worker_machine_type=n1-standard-1
##

###</pre></div></li><li class="listitem">Update the <code class="literal">settings</code> with your Google Cloud project ID and zone where you want the CoreOS instances to be deployed:<div class="informalexample"><pre class="programlisting"># SET YOUR PROJECT AND ZONE !!!
project=my-cloud-project
zone=europe-west1-d</pre></div></li><li class="listitem">Next, let's install<a id="id109" class="indexterm"/> our control server, which is our <code class="literal">etcd</code> cluster node:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./create_cluster_control.sh</strong></span>
</pre></div><div class="mediaobject"><img src="../Images/image00136.jpeg" alt="Test/staging cluster setup"/></div><p style="clear:both; height: 1em;"> </p></li></ol><div style="height:10px; width: 1px"/></div><p>We just created our new cluster <code class="literal">etcd</code> control node.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's check out what we have in this script:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#!/bin/bash</strong></span>
<span class="strong"><strong># Create TS cluster control</strong></span>

<span class="strong"><strong># Update required settings in "settings" file before running this script</strong></span>

<span class="strong"><strong>function pause(){</strong></span>
<span class="strong"><strong>read -p "$*"</strong></span>
<span class="strong"><strong>}</strong></span>

<span class="strong"><strong>## Fetch GC settings</strong></span>
<span class="strong"><strong># project and zone</strong></span>
<span class="strong"><strong>project=$(cat settings | grep project= | head -1 | cut -f2 -d"=")</strong></span>
<span class="strong"><strong>zone=$(cat settings | grep zone= | head -1 | cut -f2 -d"=")</strong></span>
<span class="strong"><strong># CoreOS release channel</strong></span>
<span class="strong"><strong>channel=$(cat settings | grep channel= | head -1 | cut -f2 -d"=")</strong></span>
<span class="strong"><strong># control instance type</strong></span>
<span class="strong"><strong>control_machine_type=$(cat settings | grep control_machine_type= | head -1 | cut -f2 -d"=")</strong></span>
<span class="strong"><strong># get the latest full image name</strong></span>
<span class="strong"><strong>image=$(gcloud compute images list --project=$project | grep -v grep | grep coreos-$channel | awk {'print $1'})</strong></span>
<span class="strong"><strong>##</strong></span>

<span class="strong"><strong># create an instance</strong></span>
<span class="strong"><strong>gcloud compute instances create tsc-control1 --project=$project --image=$image --image-project=coreos-cloud \</strong></span>
<span class="strong"><strong> --boot-disk-size=10 --zone=$zone --machine-type=$control_machine_type \</strong></span>
<span class="strong"><strong> --metadata-from-file user-data=cloud-config/control1.yaml --can-ip-forward --tags tsc-control1 tsc</strong></span>

<span class="strong"><strong># create a static IP for the new instance</strong></span>
<span class="strong"><strong>gcloud compute routes create ip-10-200-1-1-tsc-control1 --project=$project \</strong></span>
<span class="strong"><strong> --next-hop-instance tsc-control1 \</strong></span>
<span class="strong"><strong> --next-hop-instance-zone $zone \</strong></span>
<span class="strong"><strong> --destination-range 10.200.1.1/32</strong></span>

<span class="strong"><strong>echo " "</strong></span>
<span class="strong"><strong>echo "Setup has finished !!!"</strong></span>
<span class="strong"><strong>pause 'Press [Enter] key to continue...'</strong></span>
<span class="strong"><strong># end of bash script</strong></span>
</pre></div></li></ol><div style="height:10px; width: 1px"/></div><p>It fetches the settings<a id="id110" class="indexterm"/> needed for Google Cloud from the <code class="literal">settings</code> file. With the help of <code class="literal">gcloud</code> utility from the Google Cloud SDK, it sets up the <code class="literal">tsld-control1</code> instance and assigns to it a static internal IP <code class="literal">10.200.1.1</code>. This IP will be used by workers to connect the <code class="literal">etcd</code> cluster, which will run on <code class="literal">tsc-control1</code>.</p><p>In the <code class="literal">cloud-config</code> folder, we have the <code class="literal">cloud-config</code> files needed to create CoreOS instances on GCE.</p><p>Open <code class="literal">control1.yaml</code> and check<a id="id111" class="indexterm"/> out what is there in it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat control1.yaml</strong></span>
<span class="strong"><strong>#cloud-config</strong></span>

<span class="strong"><strong>coreos:</strong></span>

<span class="strong"><strong>etcd2:</strong></span>
<span class="strong"><strong>    name: control1</strong></span>
<span class="strong"><strong>    initial-advertise-peer-urls: http://10.200.1.1:2380</strong></span>
<span class="strong"><strong>    initial-cluster-token: control_etcd</strong></span>
<span class="strong"><strong>    initial-cluster: control1=http://10.200.1.1:2380</strong></span>
<span class="strong"><strong>    initial-cluster-state: new</strong></span>
<span class="strong"><strong>    listen-peer-urls: http://10.200.1.1:2380,http://10.200.1.1:7001</strong></span>
<span class="strong"><strong>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001</strong></span>
<span class="strong"><strong>    advertise-client-urls: http://10.200.1.1:2379,http://10.200.1.1:4001</strong></span>
<span class="strong"><strong>  fleet:</strong></span>
<span class="strong"><strong>    metadata: "role=services,cpeer=tsc-control1"</strong></span>
<span class="strong"><strong>  units:</strong></span>
<span class="strong"><strong>    - name: 00-ens4v1.network</strong></span>
<span class="strong"><strong>      runtime: true</strong></span>
<span class="strong"><strong>      content: |</strong></span>
<span class="strong"><strong>        [Match]</strong></span>
<span class="strong"><strong>        Name=ens4v1</strong></span>

<span class="strong"><strong>        [Network]</strong></span>
<span class="strong"><strong>        Address=10.200.1.1/24</strong></span>
<span class="strong"><strong>    - name: etcd2.service</strong></span>
<span class="strong"><strong>      command: start</strong></span>
<span class="strong"><strong>    - name: fleet.service</strong></span>
<span class="strong"><strong>      command: start</strong></span>
<span class="strong"><strong>    - name: docker.service</strong></span>
<span class="strong"><strong>      command: start</strong></span>
<span class="strong"><strong>      drop-ins:</strong></span>
<span class="strong"><strong>        - name: 50-insecure-registry.conf</strong></span>
<span class="strong"><strong>          content: |</strong></span>
<span class="strong"><strong>            [Unit]</strong></span>
<span class="strong"><strong>            [Service]</strong></span>
<span class="strong"><strong>            Environment=DOCKER_OPTS='--insecure-registry="0.0.0.0/0"'</strong></span>
<span class="strong"><strong>write_files:</strong></span>
<span class="strong"><strong> - path: /etc/resolv.conf</strong></span>
<span class="strong"><strong>   permissions: 0644</strong></span>
<span class="strong"><strong>   owner: root</strong></span>
<span class="strong"><strong>   content: |</strong></span>
<span class="strong"><strong>     nameserver 169.254.169.254</strong></span>
<span class="strong"><strong>     nameserver 10.240.0.1</strong></span>
<span class="strong"><strong>#end of cloud-config</strong></span>
</pre></div><p>As you see, we have <code class="literal">cloud-config</code> file for the control machine, which does the following:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">It creates a node <code class="literal">etcd</code> cluster with a static IP of <code class="literal">10.200.1.1</code>, which will be used to connect to <code class="literal">etcd</code> cluster.</li><li class="listitem">It sets the <code class="literal">fleet</code> metadata to <code class="literal">role=services,cpeer=tsc-control1</code>.</li><li class="listitem"><code class="literal">Unit 00-ens4v1.network</code> assigns a static IP of <code class="literal">10.200.1.1</code>.</li><li class="listitem">The <code class="literal">docker.service</code> drop-in <code class="literal">50-insecure-registry.conf</code> sets <code class="literal">--insecure-registry="0.0.0.0/0"</code>, which allows you to connect to any privately hosted docker registry.</li><li class="listitem">In the <code class="literal">write_files</code> part, we update <code class="literal">/etc/resolv.conf</code> with Google Cloud DNS<a id="id112" class="indexterm"/> servers, which sometimes do not get automatically put there if the static IP is assigned to the instance.</li></ol><div style="height:10px; width: 1px"/></div><div class="section" title="Creating our cluster workers"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec01"/>Creating our cluster workers</h3></div></div></div><p>In order to create<a id="id113" class="indexterm"/> the cluster workers, the command to be used is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./create_cluster_workers.sh</strong></span>
</pre></div><div class="mediaobject"><img src="../Images/image00137.jpeg" alt="Creating our cluster workers"/></div><p style="clear:both; height: 1em;"> </p><p>Make a note of the workers' external IPs, as shown in the previous screenshot; we will need them later.</p><p>Of course, you <a id="id114" class="indexterm"/>can always check them at the Google Developers Console too.</p><div class="mediaobject"><img src="../Images/image00138.jpeg" alt="Creating our cluster workers"/></div><p style="clear:both; height: 1em;"> </p><p>Let's check out what we have inside the <code class="literal">test1.yaml</code> and <code class="literal">staging1.yaml</code> files in the cloud-<code class="literal">config</code> folder. Run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat test1.yaml</strong></span>
<span class="strong"><strong>#cloud-config</strong></span>

<span class="strong"><strong>coreos:</strong></span>
<span class="strong"><strong>  etcd2:</strong></span>
<span class="strong"><strong>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001</strong></span>
<span class="strong"><strong>    initial-cluster: control1=http://10.200.1.1:2380</strong></span>
<span class="strong"><strong>    proxy: on</strong></span>
<span class="strong"><strong>  fleet:</strong></span>
<span class="strong"><strong>    public-ip: $public_ipv4</strong></span>
<span class="strong"><strong>    metadata: "role=worker,cpeer=tsc-test1"</strong></span>
<span class="strong"><strong>  units:</strong></span>
<span class="strong"><strong>    - name: etcd2.service</strong></span>
<span class="strong"><strong>      command: start</strong></span>
<span class="strong"><strong>    - name: fleet.service</strong></span>
<span class="strong"><strong>      command: start</strong></span>
<span class="strong"><strong>    - name: docker.service</strong></span>
<span class="strong"><strong>      command: start</strong></span>
<span class="strong"><strong>      drop-ins:</strong></span>
<span class="strong"><strong>        - name: 50-insecure-registry.conf</strong></span>
<span class="strong"><strong>          content: |</strong></span>
<span class="strong"><strong>            [Unit]</strong></span>
<span class="strong"><strong>            [Service]</strong></span>
<span class="strong"><strong>            Environment=DOCKER_OPTS='--insecure-registry="0.0.0.0/0"'</strong></span>
<span class="strong"><strong># end of cloud-config</strong></span>
</pre></div><p>As we can see, we<a id="id115" class="indexterm"/> have <code class="literal">cloud-config</code> file for the <code class="literal">test1</code> machine:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">It connects to the <code class="literal">etcd</code> cluster machine <code class="literal">control1</code> and enables <code class="literal">etcd2</code> in proxy mode, which allows anything running on the host to access the <code class="literal">etcd</code> cluster via the <code class="literal">127.0.0.1</code> address</li><li class="listitem">It sets the <code class="literal">fleet</code> metadata <code class="literal">role=services,cpeer=tsc-test1</code></li><li class="listitem">The <code class="literal">docker.service</code> drop-in <code class="literal">50-insecure-registry.conf</code> sets <code class="literal">--insecure-registry="0.0.0.0/0"</code>, which will allow you to connect to any privately hosted docker registry</li></ul></div><p>That's it!</p><p>If you check out the <code class="literal">tsc-staging1.yaml</code> cloud-config file, you will see that it is almost identical to <code class="literal">test1.yaml</code>, except that the <code class="literal">fleet</code> metadata has <code class="literal">cpeer=tsc-staging1</code> in it. But we are not done yet! </p><p>Let's now install the OS X/Linux clients, which will allow us to manage the cloud development cluster from our local computer.</p><p>Let's run this installation script:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./install_fleetctl_and_scripts.sh</strong></span>
</pre></div><p>You should see the following output:</p><div class="mediaobject"><img src="../Images/image00139.jpeg" alt="Creating our cluster workers"/></div><p style="clear:both; height: 1em;"> </p><p>So, what has the last script done?</p><p>In your home folder, it<a id="id116" class="indexterm"/> created a new folder called <code class="literal">~/coreos-tsc-gce</code>, which has two folders:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">bin</code><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">etcdctl</code>: This is the shell script used to access the <code class="literal">etcdctl</code> client on a remote cluster <code class="literal">control1</code> node</li><li class="listitem"><code class="literal">fleetctl</code>: The local <code class="literal">fleetctl</code> client is used to control the remote cluster</li><li class="listitem"><code class="literal">staging1.sh</code>: Make <code class="literal">ssh</code> connection to remote <code class="literal">staging1</code> worker</li><li class="listitem"><code class="literal">test1.sh</code>: Make <code class="literal">ssh</code> connection to remote <code class="literal">test1</code> worker</li><li class="listitem"><code class="literal">set_cluster_access.sh</code>: This sets up shell access to the remote cluster</li></ul></div></li><li class="listitem"><code class="literal">fleet</code><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">test1_webserver.service</code>: Our <code class="literal">test1</code> server's <code class="literal">fleet</code> unit</li><li class="listitem"><code class="literal">staging1_webserver.service</code>: Our <code class="literal">staging1</code> server's <code class="literal">fleet</code> unit</li></ul></div></li></ul></div><p>Now, let's take a look at <code class="literal">set_cluster_access.sh</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd ~/coreos-tsc-gce/bin</strong></span>
<span class="strong"><strong>$ cat set_cluster_access.sh</strong></span>
<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong># Setup Client SSH Tunnels</strong></span>
<span class="strong"><strong>ssh-add ~/.ssh/google_compute_engine &amp;&gt;/dev/null</strong></span>

<span class="strong"><strong># SET</strong></span>
<span class="strong"><strong># path to the cluster folder where we store our binary files</strong></span>
<span class="strong"><strong>export PATH=${HOME}/coreos-tsc-gce/bin:$PATH</strong></span>
<span class="strong"><strong># fleet tunnel</strong></span>
<span class="strong"><strong>export FLEETCTL_TUNNEL=104.155.61.42 # our control1 external IP</strong></span>
<span class="strong"><strong>export FLEETCTL_STRICT_HOST_KEY_CHECKING=false</strong></span>

<span class="strong"><strong>echo "etcd cluster:"</strong></span>
<span class="strong"><strong>etcdctl --no-sync ls /</strong></span>

<span class="strong"><strong>echo "list fleet units:"</strong></span>
<span class="strong"><strong>fleetctl list-units</strong></span>

<span class="strong"><strong>/bin/bash</strong></span>
</pre></div><p>This script is<a id="id117" class="indexterm"/> preset by <code class="literal">./install_fleetctl_and_scripts.sh</code> with the remote <code class="literal">control1</code> external IP, and allows us to issue remote <code class="literal">fleet</code> control commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./set_cluster_access.sh</strong></span>
</pre></div><div class="mediaobject"><img src="../Images/image00140.jpeg" alt="Creating our cluster workers"/></div><p style="clear:both; height: 1em;"> </p><p>Very good! Our cluster is up and running, and the workers are connected to the <code class="literal">etcd</code> cluster.</p><p>Now we can run <code class="literal">fleetctl</code> commands on the remote cluster from our local computer.</p></div><div class="section" title="Running fleetctl commands on the remote cluster"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec02"/>Running fleetctl commands on the remote cluster</h3></div></div></div><p>Let's now<a id="id118" class="indexterm"/> install the <a id="id119" class="indexterm"/><code class="literal">nginx</code> fleet units we have in the <code class="literal">~/coreos-tsc-gce/fleet</code> folder. Run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd ~/coreos-tsc-gce/fleet</strong></span>
</pre></div><p>Let's first submit the <code class="literal">fleet</code> units to the cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ fleetctl submit *.service</strong></span>
</pre></div><p>Now, let's start them:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ fleetctl start *.service</strong></span>
</pre></div><p>You should see <a id="id120" class="indexterm"/>something like what is shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00141.jpeg" alt="Running fleetctl commands on the remote cluster"/></div><p style="clear:both; height: 1em;"> </p><p>Give some time to <a id="id121" class="indexterm"/>docker to download the nginx image from the docker registry. We can then check the status of our newly deployed <code class="literal">fleet</code> units using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ fleetctl status *.service</strong></span>
</pre></div><div class="mediaobject"><img src="../Images/image00142.jpeg" alt="Running fleetctl commands on the remote cluster"/></div><p style="clear:both; height: 1em;"> </p><p>Then<a id="id122" class="indexterm"/>, run this<a id="id123" class="indexterm"/> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ fleetctl list-units</strong></span>
</pre></div><div class="mediaobject"><img src="../Images/image00143.jpeg" alt="Running fleetctl commands on the remote cluster"/></div><p style="clear:both; height: 1em;"> </p><p>Perfect!</p><p>Now, in your web browser, open the workers' external IPs, and you should see this:</p><div class="mediaobject"><img src="../Images/image00144.jpeg" alt="Running fleetctl commands on the remote cluster"/></div><p style="clear:both; height: 1em;"> </p><p>The <code class="literal">nginx</code> servers are now working. The reason they are showing this error message is that we have not<a id="id124" class="indexterm"/> provided any <code class="literal">index.html</code> file yet. We will do that in the next chapter.</p><p>But, before we<a id="id125" class="indexterm"/> finish this chapter, let's check out our <code class="literal">test/staging nginx fleet</code> units:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd ~/coreos-tsc-gce/fleet</strong></span>
<span class="strong"><strong>$ cat test1_webserver.service</strong></span>
</pre></div><p>You should see something like the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[Unit]</strong></span>
<span class="strong"><strong>Description=nginx</strong></span>

<span class="strong"><strong>[Service]</strong></span>
<span class="strong"><strong>User=core</strong></span>
<span class="strong"><strong>TimeoutStartSec=0 </strong></span>
<span class="strong"><strong>EnvironmentFile=/etc/environment</strong></span>
<span class="strong"><strong>ExecStartPre=-/usr/bin/docker rm nginx</strong></span>
<span class="strong"><strong>ExecStart=/usr/bin/docker run --rm --name test1-webserver -p 80:80 \</strong></span>
<span class="strong"><strong>-v /home/core/share/nginx/html:/usr/share/nginx/html \</strong></span>
<span class="strong"><strong>nginx:latest</strong></span>
<span class="strong"><strong>#</strong></span>
<span class="strong"><strong>ExecStop=/usr/bin/docker stop nginx</strong></span>
<span class="strong"><strong>ExecStopPost=-/usr/bin/docker rm nginx</strong></span>

<span class="strong"><strong>Restart=always</strong></span>
<span class="strong"><strong>RestartSec=10s</strong></span>
<span class="strong"><strong>[X-Fleet]</strong></span>
<span class="strong"><strong>MachineMetadata=cpeer=tsc-test1 # this where our fleet unit gets scheduled</strong></span>
</pre></div><p>There are a few things to note here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">Staging1</code> has an almost identical unit; instead of <code class="literal">test1</code>, it has <code class="literal">staging1</code> there. So, we reused the <a id="id126" class="indexterm"/>same fleet unit as we used for our local development machine, with a few changes.</li><li class="listitem">At <code class="literal">ExecStart</code>, we used <code class="literal">test1-webserver</code> and <code class="literal">staging1-webserver</code>, so by using <code class="literal">fleetctl list-units</code>, we can see which one is which.<p>We added this bit:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[X-Fleet]</strong></span>
<span class="strong"><strong>MachineMetadata=cpeer=tsc-test1</strong></span>
</pre></div></li></ul></div><p>This will schedule<a id="id127" class="indexterm"/> the unit to the particular cluster worker.</p><p>If you are a Mac user, you can<a id="id128" class="indexterm"/> download from <a class="ulink" href="https://github.com/rimusz/coreos-osx-gui-cluster">https://github.com/rimusz/coreos-osx-gui-cluster</a> and use my Mac App <span class="strong"><strong>CoreOS-Vagrant Cluster GUI for Mac OS X</strong></span>, which has a nice UI for managing CoreOS VMs on your computer.</p><div class="mediaobject"><img src="../Images/image00145.jpeg" alt="Running fleetctl commands on the remote cluster"/></div><p style="clear:both; height: 1em;"> </p><p>This app will set up a <a id="id129" class="indexterm"/>small <code class="literal">control+</code> two-node local cluster, which makes easier to test cluster things on local computer before pushing them to the cloud.</p></div></div></div>
<div class="section" title="References" id="aid-11C3M1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec29"/>References</h1></div></div></div><p>You can read more about the CoreOS cluster architectures that we used for the local and cloud test/staging<a id="id130" class="indexterm"/> setup at <a class="ulink" href="https://coreos.com/docs/cluster-management/setup/cluster-architectures/">https://coreos.com/docs/cluster-management/setup/cluster-architectures/</a>.</p></div>
<div class="section" title="Summary" id="aid-12AK81"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Summary</h1></div></div></div><p>In this chapter, you learned how to set up a CoreOS local development environment and a remote test/staging cluster on GCE. We scheduled fleet units based on different metadata tags.</p><p>In the next chapter, we will see how to deploy code to our cloud servers.</p></div></body></html>