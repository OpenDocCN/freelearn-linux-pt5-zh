<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Building a Development Environment</h1></div></div></div><p>In this chapter, we will cover how to set up a local CoreOS environment for development on a personal computer, and a test and staging environment cluster on the VM instances of Google Cloud's Compute Engine. These are the topics we will cover:</p><div><ul class="itemizedlist"><li class="listitem">Setting up a local development environment</li><li class="listitem">Bootstrapping a remote test/staging cluster on GCE</li></ul></div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec27"/>Setting up the local development environment</h1></div></div></div><p>We are going to learn<a id="id89" class="indexterm"/> how to set up a development environment on our personal computer with the help of VirtualBox and Vagrant, as we did in an earlier chapter. Building and testing <code class="literal">docker</code> images and coding locally makes you more productive, it saves time, and Docker repository can be pushed to the docker registry (private or not) when your docker images are ready. The same goes for the code; you just work on it and test it locally. When it is ready, you can merge it with the git test branch where your team/client can test it further.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec20"/>Setting up the development VM</h2></div></div></div><p>In the previous<a id="id90" class="indexterm"/> chapters, you learned how to install CoreOS via Vagrant on your PC. Here, we have prepared installation scripts for Linux and OS X to go straight to the point. You can download the latest <em>CoreOS Essentials</em> book example files from GitHub repository:</p><div><pre class="programlisting">
<strong>$ git clone https://github.com/rimusz/coreos-essentials-book/</strong>
</pre></div><p>To install a local Vagrant-based development VM, type this:</p><div><pre class="programlisting">
<strong>$ cd coreos-essentials-book/chapter5/Local_Development_VM</strong>
<strong>$ ./install_local_dev.sh</strong>
</pre></div><p>You should see an output similar to this:</p><div><img src="img/image00130.jpeg" alt="Setting up the development VM"/></div><p style="clear:both; height: 1em;"> </p><p>Hang on! There's<a id="id91" class="indexterm"/> more!</p><div><img src="img/image00131.jpeg" alt="Setting up the development VM"/></div><p style="clear:both; height: 1em;"> </p><p>This will perform a VM installation similar to the installation that we did in <a class="link" title="Chapter 1. CoreOS – Overview and Installation" href="part0014.xhtml#aid-DB7S1">Chapter 1</a>, <em>CoreOS – Overview and Installation</em>, but in a <a id="id92" class="indexterm"/>more automated way this time.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec21"/>What happened during the VM installation?</h2></div></div></div><p>Let's check<a id="id93" class="indexterm"/> out what happened during the VM installation. To sum up:</p><div><ul class="itemizedlist"><li class="listitem">A new CoreOS VM (VirtualBox/Vagrant-based) was installed</li><li class="listitem">A new folder called <code class="literal">coreos-dev-env</code> was created in your <code class="literal">Home</code> folder</li></ul></div><p>Run the following commands:</p><div><pre class="programlisting">
<strong>$ cd ~/coreos-dev-env</strong>
<strong>$ ls</strong>
<strong>bin </strong>
<strong>fleet       </strong>
<strong>share        </strong>
<strong>vm         </strong>
<strong>vm_halt.sh </strong>
<strong>vm_ssh.sh  </strong>
<strong>vm_up.sh</strong>
</pre></div><p>As a result, this is what we see:</p><div><ul class="itemizedlist"><li class="listitem">Four folders, which consist of the following list:<div><ul class="itemizedlist"><li class="listitem"><code class="literal">bin</code>: <code class="literal">docker</code>, <code class="literal">etcdctl</code> and <code class="literal">fleetctl</code> files</li><li class="listitem"><code class="literal">fleet</code>: The <code class="literal">nginx.service fleet</code> unit is stored here</li><li class="listitem"><code class="literal">share</code>: This is shared folder between the host and VM</li><li class="listitem"><code class="literal">vm</code>: Vagrantfile, <code class="literal">config.rb</code> and <code class="literal">user-data</code> files</li></ul></div></li><li class="listitem">We also<a id="id94" class="indexterm"/> have three files:<div><ul class="itemizedlist"><li class="listitem"><code class="literal">vm_halt.sh</code>: This is used to shut down the CoreOS VM</li><li class="listitem"><code class="literal">vm_ssh.sh</code>: This is used to <code class="literal">ssh</code> to the CoreOS VM</li><li class="listitem"><code class="literal">vm_up.sh</code>: This is used to start the CoreOS VM, with the OS shell preset to the following:<div><pre class="programlisting"># Set the environment variable for the docker daemon
export DOCKER_HOST=tcp://127.0.0.1:2375
# path to the bin folder where we store our binary files
export PATH=${HOME}/coreos-dev-env/bin:$PATH
# set etcd endpoint
export ETCDCTL_PEERS=http://172.19.20.99:2379
# set fleetctl endpoint
export FLEETCTL_ENDPOINT=http://172.19.20.99:2379
export FLEETCTL_DRIVER=etcd
export FLEETCTL_STRICT_HOST_KEY_CHECKING=false</pre></div></li></ul></div></li></ul></div><p>Now that we have installed our CoreOS VM, let's run <code class="literal">vm_up.sh</code>. We should see this output in the <strong>Terminal</strong> window:</p><div><pre class="programlisting">
<strong>$ cd ~/coreos-dev-env</strong>
<strong>$ ./vm_up.sh</strong>
</pre></div><p>You should see output similar to this:</p><div><img src="img/image00132.jpeg" alt="What happened during the VM installation?"/></div><p style="clear:both; height: 1em;"> </p><p>As we can see<a id="id95" class="indexterm"/> in the preceding screenshot, we do not have any errors. Only <code class="literal">fleetctl list-machines</code> shows our CoreOS VM machine, and we have no <code class="literal">docker</code> containers and <code class="literal">fleet</code> units running there yet.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec22"/>Deploying the fleet units</h2></div></div></div><p>Let's deploy some fleet<a id="id96" class="indexterm"/> units to verify that our development environment works fine. Run the following commands:</p><div><pre class="programlisting">
<strong>$ cd fleet</strong>
<strong>$ fleetctl start nginx.service</strong>
</pre></div><div><h3 class="title"><a id="note06"/>Note</h3><p>It can take a bit of time for docker to download the <code class="literal">nginx</code> image.</p></div><p>You can check out the <code class="literal">nginx.service</code> unit's status:</p><div><pre class="programlisting">
<strong>$ fleetctl status nginx.service</strong>
</pre></div><p>You should see output similar to this:</p><div><img src="img/image00133.jpeg" alt="Deploying the fleet units"/></div><p style="clear:both; height: 1em;"> </p><p>Once the <code class="literal">nginx fleet</code> unit is deployed, open in your browser <code class="literal">http://172.19.20.99</code>. You should see the<a id="id97" class="indexterm"/> following message:</p><div><img src="img/image00134.jpeg" alt="Deploying the fleet units"/></div><p style="clear:both; height: 1em;"> </p><p>Let's check out what happened there. We scheduled this <code class="literal">nginx.service</code> unit with <code class="literal">fleetctl</code>:</p><div><pre class="programlisting">
<strong>$ cat ~/coreos-dev-env/fleet/nginx.service</strong>

[Unit]
Description=nginx

[Service]
User=core
TimeoutStartSec=0
EnvironmentFile=/etc/environment
ExecStartPre=-/usr/bin/docker rm nginx
ExecStart=/usr/bin/docker run --rm --name nginx -p 80:80 \
 -v /home/core/share/nginx/html:/usr/share/nginx/html \
 nginx:latest
#
ExecStop=/usr/bin/docker stop nginx
ExecStopPost=-/usr/bin/docker rm nginx

Restart=always
RestartSec=10s

[X-Fleet]</pre></div><p>Then, we used the official <code class="literal">nginx</code> image from the docker registry, and shared our local <code class="literal">~/coreos-dev-env/share</code> folder with <code class="literal">/home/core/share</code>, which was mounted afterwards as a docker volume <code class="literal">/home/core/share/nginx/html:/usr/share/nginx/html</code>.</p><p>So, whatever <code class="literal">html</code> files we put into our local <code class="literal">~/coreos-dev-env/share/nginx/html</code> folder will be<a id="id98" class="indexterm"/> picked up automatically by <code class="literal">nginx</code>.</p><p>Let's overview what advantages such a setup gives us:</p><div><ul class="itemizedlist"><li class="listitem">We can build and test docker containers locally, and then push them to the docker registry (private or public).</li><li class="listitem">Test our code locally <a id="id99" class="indexterm"/>and push it to the git repository when we are done with it.</li><li class="listitem">By having a local development setup, productivity really increases, as everything is done much faster. We do not have build new docker containers upon every code change, push them to the remote docker registry, pull them at some remote test servers, and so on.</li><li class="listitem">It is very easy to clean up the setup and get it working from a clean start again, reusing the configured <code class="literal">fleet</code> units to start the all required docker containers.</li></ul></div><p>Very good! So, now, we have a fully operational local development setup!</p><div><h3 class="title"><a id="note07"/>Note</h3><p>This setup is as per the CoreOS documentation at <a class="ulink" href="https://coreos.com/docs/cluster-management/setup/cluster-architectures/">https://coreos.com/docs/cluster-management/setup/cluster-architectures/</a>, in the <em>Docker Dev Environment on Laptop</em> section.</p><p>Go through<a id="id100" class="indexterm"/> the <code class="literal">coreos-dev-install.sh</code> bash script, which sets up your local development VM. It is a simple script and is well commented, so it should not be too hard to understand its logic.</p></div><p>If you are a Mac <a id="id101" class="indexterm"/>user, you can download from <a class="ulink" href="https://github.com/rimusz/coreos-osx-gui">https://github.com/rimusz/coreos-osx-gui</a> and use my Mac App <strong>CoreOS-Vagrant GUI for Mac OS X</strong>, which has a nice UI to manage CoreOS VM. It will automatically set up the CoreOS VM environment.</p><div><img src="img/image00135.jpeg" alt="Deploying the fleet units"/></div><p style="clear:both; height: 1em;"> </p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec28"/>Bootstrapping a remote test/staging cluster on GCE</h1></div></div></div><p>So, we have<a id="id102" class="indexterm"/> successfully built our local <a id="id103" class="indexterm"/>development setup. Let's get to the next level, that is, building our test/staging environment on the cloud.</p><p>We are going to use Google Cloud's Compute Engine, so you need a Google Cloud account for this. If you do not have it, for<a id="id104" class="indexterm"/> the purpose of running the examples in the book, you can open a trial account at <a class="ulink" href="https://cloud.google.com/compute/">https://cloud.google.com/compute/</a>. A trial account lasts for <a id="id105" class="indexterm"/>60 days and has $300 as credits, enough to run all of this book's examples. When you are done with opening the account, Google <a id="id106" class="indexterm"/>Cloud SDK needs to be installed from <a class="ulink" href="https://cloud.google.com/sdk/">https://cloud.google.com/sdk/</a>.</p><p>In this topic, we<a id="id107" class="indexterm"/> will follow the recommendations on how to set up CoreOS cluster by referring to <em>Easy Development/Testing Cluster</em> from <a class="ulink" href="https://coreos.com/docs/cluster-management/setup/cluster-architectures/">https://coreos.com/docs/cluster-management/setup/cluster-architectures/</a>.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec23"/>Test/staging cluster setup</h2></div></div></div><p>Okay, let's get our cloud cluster installed, as you have already downloaded this book's code examples. Carry<a id="id108" class="indexterm"/> out these steps in the shown order:</p><div><ol class="orderedlist arabic"><li class="listitem">Run the following commands:<div><pre class="programlisting">$ cd coreos-essentials-book/chapter5/Test_Staging_Cluster
$ ls
cloud-config
create_cluster_control.sh
create_cluster_workers.sh
files
fleet
install_fleetctl_and_scripts.sh
settings

Let's check "settings" file first:
$ cat settings
### CoreOS Test/Staging Cluster on GCE settings

## change Google Cloud settings as per your requirements
# GC settings

# CoreOS RELEASE CHANNEL
channel=beta

# SET YOUR PROJECT AND ZONE !!!
project=my-cloud-project
zone=europe-west1-d

# ETCD CONTROL AND NODES MACHINES TYPE
#
control_machine_type=g1-small
#
worker_machine_type=n1-standard-1
##

###</pre></div></li><li class="listitem">Update the <code class="literal">settings</code> with your Google Cloud project ID and zone where you want the CoreOS instances to be deployed:<div><pre class="programlisting"># SET YOUR PROJECT AND ZONE !!!
project=my-cloud-project
zone=europe-west1-d</pre></div></li><li class="listitem">Next, let's install<a id="id109" class="indexterm"/> our control server, which is our <code class="literal">etcd</code> cluster node:<div><pre class="programlisting">
<strong>$ ./create_cluster_control.sh</strong>
</pre></div><div><img src="img/image00136.jpeg" alt="Test/staging cluster setup"/></div><p style="clear:both; height: 1em;"> </p></li></ol><div></div><p>We just created our new cluster <code class="literal">etcd</code> control node.</p><div><ol class="orderedlist arabic"><li class="listitem">Let's check out what we have in this script:<div><pre class="programlisting">
<strong>#!/bin/bash</strong>
<strong># Create TS cluster control</strong>

<strong># Update required settings in "settings" file before running this script</strong>

<strong>function pause(){</strong>
<strong>read -p "$*"</strong>
<strong>}</strong>

<strong>## Fetch GC settings</strong>
<strong># project and zone</strong>
<strong>project=$(cat settings | grep project= | head -1 | cut -f2 -d"=")</strong>
<strong>zone=$(cat settings | grep zone= | head -1 | cut -f2 -d"=")</strong>
<strong># CoreOS release channel</strong>
<strong>channel=$(cat settings | grep channel= | head -1 | cut -f2 -d"=")</strong>
<strong># control instance type</strong>
<strong>control_machine_type=$(cat settings | grep control_machine_type= | head -1 | cut -f2 -d"=")</strong>
<strong># get the latest full image name</strong>
<strong>image=$(gcloud compute images list --project=$project | grep -v grep | grep coreos-$channel | awk {'print $1'})</strong>
<strong>##</strong>

<strong># create an instance</strong>
<strong>gcloud compute instances create tsc-control1 --project=$project --image=$image --image-project=coreos-cloud \</strong>
<strong> --boot-disk-size=10 --zone=$zone --machine-type=$control_machine_type \</strong>
<strong> --metadata-from-file user-data=cloud-config/control1.yaml --can-ip-forward --tags tsc-control1 tsc</strong>

<strong># create a static IP for the new instance</strong>
<strong>gcloud compute routes create ip-10-200-1-1-tsc-control1 --project=$project \</strong>
<strong> --next-hop-instance tsc-control1 \</strong>
<strong> --next-hop-instance-zone $zone \</strong>
<strong> --destination-range 10.200.1.1/32</strong>

<strong>echo " "</strong>
<strong>echo "Setup has finished !!!"</strong>
<strong>pause 'Press [Enter] key to continue...'</strong>
<strong># end of bash script</strong>
</pre></div></li></ol><div></div><p>It fetches the settings<a id="id110" class="indexterm"/> needed for Google Cloud from the <code class="literal">settings</code> file. With the help of <code class="literal">gcloud</code> utility from the Google Cloud SDK, it sets up the <code class="literal">tsld-control1</code> instance and assigns to it a static internal IP <code class="literal">10.200.1.1</code>. This IP will be used by workers to connect the <code class="literal">etcd</code> cluster, which will run on <code class="literal">tsc-control1</code>.</p><p>In the <code class="literal">cloud-config</code> folder, we have the <code class="literal">cloud-config</code> files needed to create CoreOS instances on GCE.</p><p>Open <code class="literal">control1.yaml</code> and check<a id="id111" class="indexterm"/> out what is there in it:</p><div><pre class="programlisting">
<strong>$ cat control1.yaml</strong>
<strong>#cloud-config</strong>

<strong>coreos:</strong>

<strong>etcd2:</strong>
<strong>    name: control1</strong>
<strong>    initial-advertise-peer-urls: http://10.200.1.1:2380</strong>
<strong>    initial-cluster-token: control_etcd</strong>
<strong>    initial-cluster: control1=http://10.200.1.1:2380</strong>
<strong>    initial-cluster-state: new</strong>
<strong>    listen-peer-urls: http://10.200.1.1:2380,http://10.200.1.1:7001</strong>
<strong>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001</strong>
<strong>    advertise-client-urls: http://10.200.1.1:2379,http://10.200.1.1:4001</strong>
<strong>  fleet:</strong>
<strong>    metadata: "role=services,cpeer=tsc-control1"</strong>
<strong>  units:</strong>
<strong>    - name: 00-ens4v1.network</strong>
<strong>      runtime: true</strong>
<strong>      content: |</strong>
<strong>        [Match]</strong>
<strong>        Name=ens4v1</strong>

<strong>        [Network]</strong>
<strong>        Address=10.200.1.1/24</strong>
<strong>    - name: etcd2.service</strong>
<strong>      command: start</strong>
<strong>    - name: fleet.service</strong>
<strong>      command: start</strong>
<strong>    - name: docker.service</strong>
<strong>      command: start</strong>
<strong>      drop-ins:</strong>
<strong>        - name: 50-insecure-registry.conf</strong>
<strong>          content: |</strong>
<strong>            [Unit]</strong>
<strong>            [Service]</strong>
<strong>            Environment=DOCKER_OPTS='--insecure-registry="0.0.0.0/0"'</strong>
<strong>write_files:</strong>
<strong> - path: /etc/resolv.conf</strong>
<strong>   permissions: 0644</strong>
<strong>   owner: root</strong>
<strong>   content: |</strong>
<strong>     nameserver 169.254.169.254</strong>
<strong>     nameserver 10.240.0.1</strong>
<strong>#end of cloud-config</strong>
</pre></div><p>As you see, we have <code class="literal">cloud-config</code> file for the control machine, which does the following:</p><div><ol class="orderedlist arabic"><li class="listitem">It creates a node <code class="literal">etcd</code> cluster with a static IP of <code class="literal">10.200.1.1</code>, which will be used to connect to <code class="literal">etcd</code> cluster.</li><li class="listitem">It sets the <code class="literal">fleet</code> metadata to <code class="literal">role=services,cpeer=tsc-control1</code>.</li><li class="listitem"><code class="literal">Unit 00-ens4v1.network</code> assigns a static IP of <code class="literal">10.200.1.1</code>.</li><li class="listitem">The <code class="literal">docker.service</code> drop-in <code class="literal">50-insecure-registry.conf</code> sets <code class="literal">--insecure-registry="0.0.0.0/0"</code>, which allows you to connect to any privately hosted docker registry.</li><li class="listitem">In the <code class="literal">write_files</code> part, we update <code class="literal">/etc/resolv.conf</code> with Google Cloud DNS<a id="id112" class="indexterm"/> servers, which sometimes do not get automatically put there if the static IP is assigned to the instance.</li></ol><div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec01"/>Creating our cluster workers</h3></div></div></div><p>In order to create<a id="id113" class="indexterm"/> the cluster workers, the command to be used is as follows:</p><div><pre class="programlisting">
<strong>$ ./create_cluster_workers.sh</strong>
</pre></div><div><img src="img/image00137.jpeg" alt="Creating our cluster workers"/></div><p style="clear:both; height: 1em;"> </p><p>Make a note of the workers' external IPs, as shown in the previous screenshot; we will need them later.</p><p>Of course, you <a id="id114" class="indexterm"/>can always check them at the Google Developers Console too.</p><div><img src="img/image00138.jpeg" alt="Creating our cluster workers"/></div><p style="clear:both; height: 1em;"> </p><p>Let's check out what we have inside the <code class="literal">test1.yaml</code> and <code class="literal">staging1.yaml</code> files in the cloud-<code class="literal">config</code> folder. Run the following command:</p><div><pre class="programlisting">
<strong>$ cat test1.yaml</strong>
<strong>#cloud-config</strong>

<strong>coreos:</strong>
<strong>  etcd2:</strong>
<strong>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001</strong>
<strong>    initial-cluster: control1=http://10.200.1.1:2380</strong>
<strong>    proxy: on</strong>
<strong>  fleet:</strong>
<strong>    public-ip: $public_ipv4</strong>
<strong>    metadata: "role=worker,cpeer=tsc-test1"</strong>
<strong>  units:</strong>
<strong>    - name: etcd2.service</strong>
<strong>      command: start</strong>
<strong>    - name: fleet.service</strong>
<strong>      command: start</strong>
<strong>    - name: docker.service</strong>
<strong>      command: start</strong>
<strong>      drop-ins:</strong>
<strong>        - name: 50-insecure-registry.conf</strong>
<strong>          content: |</strong>
<strong>            [Unit]</strong>
<strong>            [Service]</strong>
<strong>            Environment=DOCKER_OPTS='--insecure-registry="0.0.0.0/0"'</strong>
<strong># end of cloud-config</strong>
</pre></div><p>As we can see, we<a id="id115" class="indexterm"/> have <code class="literal">cloud-config</code> file for the <code class="literal">test1</code> machine:</p><div><ul class="itemizedlist"><li class="listitem">It connects to the <code class="literal">etcd</code> cluster machine <code class="literal">control1</code> and enables <code class="literal">etcd2</code> in proxy mode, which allows anything running on the host to access the <code class="literal">etcd</code> cluster via the <code class="literal">127.0.0.1</code> address</li><li class="listitem">It sets the <code class="literal">fleet</code> metadata <code class="literal">role=services,cpeer=tsc-test1</code></li><li class="listitem">The <code class="literal">docker.service</code> drop-in <code class="literal">50-insecure-registry.conf</code> sets <code class="literal">--insecure-registry="0.0.0.0/0"</code>, which will allow you to connect to any privately hosted docker registry</li></ul></div><p>That's it!</p><p>If you check out the <code class="literal">tsc-staging1.yaml</code> cloud-config file, you will see that it is almost identical to <code class="literal">test1.yaml</code>, except that the <code class="literal">fleet</code> metadata has <code class="literal">cpeer=tsc-staging1</code> in it. But we are not done yet! </p><p>Let's now install the OS X/Linux clients, which will allow us to manage the cloud development cluster from our local computer.</p><p>Let's run this installation script:</p><div><pre class="programlisting">
<strong>$ ./install_fleetctl_and_scripts.sh</strong>
</pre></div><p>You should see the following output:</p><div><img src="img/image00139.jpeg" alt="Creating our cluster workers"/></div><p style="clear:both; height: 1em;"> </p><p>So, what has the last script done?</p><p>In your home folder, it<a id="id116" class="indexterm"/> created a new folder called <code class="literal">~/coreos-tsc-gce</code>, which has two folders:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">bin</code><div><ul class="itemizedlist"><li class="listitem"><code class="literal">etcdctl</code>: This is the shell script used to access the <code class="literal">etcdctl</code> client on a remote cluster <code class="literal">control1</code> node</li><li class="listitem"><code class="literal">fleetctl</code>: The local <code class="literal">fleetctl</code> client is used to control the remote cluster</li><li class="listitem"><code class="literal">staging1.sh</code>: Make <code class="literal">ssh</code> connection to remote <code class="literal">staging1</code> worker</li><li class="listitem"><code class="literal">test1.sh</code>: Make <code class="literal">ssh</code> connection to remote <code class="literal">test1</code> worker</li><li class="listitem"><code class="literal">set_cluster_access.sh</code>: This sets up shell access to the remote cluster</li></ul></div></li><li class="listitem"><code class="literal">fleet</code><div><ul class="itemizedlist"><li class="listitem"><code class="literal">test1_webserver.service</code>: Our <code class="literal">test1</code> server's <code class="literal">fleet</code> unit</li><li class="listitem"><code class="literal">staging1_webserver.service</code>: Our <code class="literal">staging1</code> server's <code class="literal">fleet</code> unit</li></ul></div></li></ul></div><p>Now, let's take a look at <code class="literal">set_cluster_access.sh</code>:</p><div><pre class="programlisting">
<strong>$ cd ~/coreos-tsc-gce/bin</strong>
<strong>$ cat set_cluster_access.sh</strong>
<strong>#!/bin/bash</strong>

<strong># Setup Client SSH Tunnels</strong>
<strong>ssh-add ~/.ssh/google_compute_engine &amp;&gt;/dev/null</strong>

<strong># SET</strong>
<strong># path to the cluster folder where we store our binary files</strong>
<strong>export PATH=${HOME}/coreos-tsc-gce/bin:$PATH</strong>
<strong># fleet tunnel</strong>
<strong>export FLEETCTL_TUNNEL=104.155.61.42 # our control1 external IP</strong>
<strong>export FLEETCTL_STRICT_HOST_KEY_CHECKING=false</strong>

<strong>echo "etcd cluster:"</strong>
<strong>etcdctl --no-sync ls /</strong>

<strong>echo "list fleet units:"</strong>
<strong>fleetctl list-units</strong>

<strong>/bin/bash</strong>
</pre></div><p>This script is<a id="id117" class="indexterm"/> preset by <code class="literal">./install_fleetctl_and_scripts.sh</code> with the remote <code class="literal">control1</code> external IP, and allows us to issue remote <code class="literal">fleet</code> control commands:</p><div><pre class="programlisting">
<strong>$ ./set_cluster_access.sh</strong>
</pre></div><div><img src="img/image00140.jpeg" alt="Creating our cluster workers"/></div><p style="clear:both; height: 1em;"> </p><p>Very good! Our cluster is up and running, and the workers are connected to the <code class="literal">etcd</code> cluster.</p><p>Now we can run <code class="literal">fleetctl</code> commands on the remote cluster from our local computer.</p></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec02"/>Running fleetctl commands on the remote cluster</h3></div></div></div><p>Let's now<a id="id118" class="indexterm"/> install the <a id="id119" class="indexterm"/><code class="literal">nginx</code> fleet units we have in the <code class="literal">~/coreos-tsc-gce/fleet</code> folder. Run the following command:</p><div><pre class="programlisting">
<strong>$ cd ~/coreos-tsc-gce/fleet</strong>
</pre></div><p>Let's first submit the <code class="literal">fleet</code> units to the cluster:</p><div><pre class="programlisting">
<strong>$ fleetctl submit *.service</strong>
</pre></div><p>Now, let's start them:</p><div><pre class="programlisting">
<strong>$ fleetctl start *.service</strong>
</pre></div><p>You should see <a id="id120" class="indexterm"/>something like what is shown in the following screenshot:</p><div><img src="img/image00141.jpeg" alt="Running fleetctl commands on the remote cluster"/></div><p style="clear:both; height: 1em;"> </p><p>Give some time to <a id="id121" class="indexterm"/>docker to download the nginx image from the docker registry. We can then check the status of our newly deployed <code class="literal">fleet</code> units using the following command:</p><div><pre class="programlisting">
<strong>$ fleetctl status *.service</strong>
</pre></div><div><img src="img/image00142.jpeg" alt="Running fleetctl commands on the remote cluster"/></div><p style="clear:both; height: 1em;"> </p><p>Then<a id="id122" class="indexterm"/>, run this<a id="id123" class="indexterm"/> command:</p><div><pre class="programlisting">
<strong>$ fleetctl list-units</strong>
</pre></div><div><img src="img/image00143.jpeg" alt="Running fleetctl commands on the remote cluster"/></div><p style="clear:both; height: 1em;"> </p><p>Perfect!</p><p>Now, in your web browser, open the workers' external IPs, and you should see this:</p><div><img src="img/image00144.jpeg" alt="Running fleetctl commands on the remote cluster"/></div><p style="clear:both; height: 1em;"> </p><p>The <code class="literal">nginx</code> servers are now working. The reason they are showing this error message is that we have not<a id="id124" class="indexterm"/> provided any <code class="literal">index.html</code> file yet. We will do that in the next chapter.</p><p>But, before we<a id="id125" class="indexterm"/> finish this chapter, let's check out our <code class="literal">test/staging nginx fleet</code> units:</p><div><pre class="programlisting">
<strong>$ cd ~/coreos-tsc-gce/fleet</strong>
<strong>$ cat test1_webserver.service</strong>
</pre></div><p>You should see something like the following code:</p><div><pre class="programlisting">
<strong>[Unit]</strong>
<strong>Description=nginx</strong>

<strong>[Service]</strong>
<strong>User=core</strong>
<strong>TimeoutStartSec=0 </strong>
<strong>EnvironmentFile=/etc/environment</strong>
<strong>ExecStartPre=-/usr/bin/docker rm nginx</strong>
<strong>ExecStart=/usr/bin/docker run --rm --name test1-webserver -p 80:80 \</strong>
<strong>-v /home/core/share/nginx/html:/usr/share/nginx/html \</strong>
<strong>nginx:latest</strong>
<strong>#</strong>
<strong>ExecStop=/usr/bin/docker stop nginx</strong>
<strong>ExecStopPost=-/usr/bin/docker rm nginx</strong>

<strong>Restart=always</strong>
<strong>RestartSec=10s</strong>
<strong>[X-Fleet]</strong>
<strong>MachineMetadata=cpeer=tsc-test1 # this where our fleet unit gets scheduled</strong>
</pre></div><p>There are a few things to note here:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">Staging1</code> has an almost identical unit; instead of <code class="literal">test1</code>, it has <code class="literal">staging1</code> there. So, we reused the <a id="id126" class="indexterm"/>same fleet unit as we used for our local development machine, with a few changes.</li><li class="listitem">At <code class="literal">ExecStart</code>, we used <code class="literal">test1-webserver</code> and <code class="literal">staging1-webserver</code>, so by using <code class="literal">fleetctl list-units</code>, we can see which one is which.<p>We added this bit:</p><div><pre class="programlisting">
<strong>[X-Fleet]</strong>
<strong>MachineMetadata=cpeer=tsc-test1</strong>
</pre></div></li></ul></div><p>This will schedule<a id="id127" class="indexterm"/> the unit to the particular cluster worker.</p><p>If you are a Mac user, you can<a id="id128" class="indexterm"/> download from <a class="ulink" href="https://github.com/rimusz/coreos-osx-gui-cluster">https://github.com/rimusz/coreos-osx-gui-cluster</a> and use my Mac App <strong>CoreOS-Vagrant Cluster GUI for Mac OS X</strong>, which has a nice UI for managing CoreOS VMs on your computer.</p><div><img src="img/image00145.jpeg" alt="Running fleetctl commands on the remote cluster"/></div><p style="clear:both; height: 1em;"> </p><p>This app will set up a <a id="id129" class="indexterm"/>small <code class="literal">control+</code> two-node local cluster, which makes easier to test cluster things on local computer before pushing them to the cloud.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec29"/>References</h1></div></div></div><p>You can read more about the CoreOS cluster architectures that we used for the local and cloud test/staging<a id="id130" class="indexterm"/> setup at <a class="ulink" href="https://coreos.com/docs/cluster-management/setup/cluster-architectures/">https://coreos.com/docs/cluster-management/setup/cluster-architectures/</a>.</p></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Summary</h1></div></div></div><p>In this chapter, you learned how to set up a CoreOS local development environment and a remote test/staging cluster on GCE. We scheduled fleet units based on different metadata tags.</p><p>In the next chapter, we will see how to deploy code to our cloud servers.</p></div></body></html>