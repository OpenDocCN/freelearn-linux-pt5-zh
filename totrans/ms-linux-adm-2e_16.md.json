["```\nsudo apt install snapd\n```", "```\nsudo snap install core\n```", "```\nsudo snap install microk8s --classic\n```", "```\nsudo usermod -aG microk8s $USER\nsudo chown -f -R $USER ~/.kube\n```", "```\nmicrok8s help\n```", "```\nmicrok8s status\n```", "```\nsudo swapoff -a\n```", "```\nsudo sed -i '/\\s*swap\\s*/s/^\\(.*\\)$/# \\1/g' /etc/fstab\n```", "```\ncat /etc/fstab\n```", "```\n    sudo modprobe br_netfilter\n    sudo modprobe overlay\n    ```", "```\n    cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf\n    br_netfilter\n    overlay\n    sysctl parameters, also persisted across system reboots:\n\n    ```", "```\n\n    ```", "```\n    sudo sysctl --system\n    ```", "```\n    sudo sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward\n    ```", "```\n    containerd:\n\n    ```", "```\n    sudo mkdir -p /etc/containerd\n    config.toml inside it. The output of the command is too large to show, but it will show you on the screen the automatically generated contents of the new file we created.\n    ```", "```\n\n    ```", "```\n    [plugins] section of the file):\n\n    ```", "```\n\n    ```", "```\n    [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n      ...\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n      SystemdCgroup = true\n    ```", "```\n    containerd Service, by using the following command:\n\n    ```", "```\n\n    The output should show that the system is running and there are no issues.\n    ```", "```\n    apt repository GNU Privacy Guard (GPG) public signing key (for the latest Kubernetes 1.28 at the time of writing):\n\n    ```", "```\n    echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\n    ```", "```\n\n    ```", "```\n    sudo apt update -y\n    ```", "```\n    apt-mark hold command to pin the version of the Kubernetes packages, including containerd:\n\n    ```", "```\n    sudo systemctl enable containerd\n    containerd Service first:\n\n    ```", "```\n\n    ```", "```\n\n    ```", "```\n    exited:\n    ```", "```\nkubeadm help\n```", "```\n    calico.yaml file in the current directory (/home/packt/) that we’ll use with kubectl to configure pod networking later in the process.\n    ```", "```\n    # - name: CALICO_IPV4POOL_CIDR\n    CALICO_IPV4POOL_CIDR points to the network range associated with the pods. If the related subnet conflicts in any way with your local environment, you’ll have to change it here. We’ll leave the setting as is.\n    ```", "```\n    k8s-config.yaml file we just generated and mention a few changes that we’ll have to make. We will open it using the localAPIEndpoint.advertiseAddress configuration parameter – the IP address of the API server endpoint. The default value is 1.2.3.4, and we need to change it to the IP address of the VM running the CP node (k8s-cp1), in our case, 192.168.122.104. Refer to the *Preparing the lab environment* section earlier in this chapter. You’ll have to enter the IP address matching your environment:\n    ```", "```\nkubeadm version\n```", "```\n    cat <<EOF | cat >> k8s-config.yaml\n    ---\n    apiVersion: kubelet.config.k8s.io/v1beta1\n    kind: KubeletConfiguration\n    cgroupDriver: systemd\n    kubeadm init command with the --config option pointing to the cluster configuration file (k8s-config.yaml), and with the --cri-socket option parameter pointing to the containerd socket:\n\n    ```", "```\n\n    The preceding command takes a couple of minutes to run. A successful bootstrap of the Kubernetes cluster completes with the following output:\n    ```", "```\n    mkdir -p ~/.kube\n    sudo cp -i /etc/kubernetes/admin.conf ~/.kube/config\n    sudo chown $(id -u):$(id -g) ~/.kube/config\n    ```", "```\n    kubectl command to list all the pods in the system:\n\n    ```", "```\n\n    The command yields the following output:\n    ```", "```\n    k8s-cp1 as the only node configured in the Kubernetes cluster, running as a CP node:\n    ```", "```\n    sudo systemctl status kubelet\n    ```", "```\n    ls /etc/kubernetes/manifests/\n    ```", "```\n    ls /etc/kubernetes/\n    ```", "```\n    abcdef.0123456789abcdef):\n    ```", "```\n    openssl x509 -pubkey \\\n        -in /etc/kubernetes/pki/ca.crt | \\\n        openssl rsa -pubin -outform der 2>/dev/null | \\\n        openssl dgst -sha256 -hex | sed 's/^.* //'\n    ```", "```\n    kubeadm join command with the required parameters:\n\n    ```", "```\n\n    ```", "```\n    sudo kubeadm join 192.168.122.104:6443 \\\n        --token abcdef.0123456789abcdef \\\n    k8s-cp1):\n\n    ```", "```\n\n    ```", "```\nkubectl [command] [TYPE] [NAME] [flags]\n```", "```\ncurl -LO \"kubectl with the following command:\n\n```", "```\n\n Now that the package is installed, we can test the installation using the following command:\n\n```", "```\n\n The output of the preceding commands is shown in the following screenshot:\n![Figure 16.25 – Installing kubectl locally on a Debian system](img/B19682_16_25.jpg)\n\nFigure 16.25 – Installing kubectl locally on a Debian system\nWe want to add (merge) yet another cluster configuration to our environment. This time, we connect to an on-premises Kubernetes CP, and we’ll use `kubectl` to update kubeconfig. Here are the steps we’ll be taking:\n\n1.  We first copy kubeconfig from the CP node (`k8s-cp1`, `192.168.122.104`) to a temporary location `(/tmp/config.cp`):\n\n    ```", "```\n\n     2.  Finally, we can move the new kubeconfig file to the new location:\n\n    ```", "```\n\n     3.  Optionally, we can clean up the temporary files created in the process:\n\n    ```", "```\n\n     4.  Let’s get a view of the current kubeconfig contexts:\n\n    ```", "```\n\n![Figure 16.26 – The new kubeconfig contexts](img/B19682_16_26.jpg)\n\nFigure 16.26 – The new kubeconfig contexts\n\n1.  For consistency, let’s change the on-premises cluster’s context name to `k8s-local` and make it the default context in our `kubectl` environment:\n\n    ```", "```\n\n![Figure 16.27 – The current context set to the on-premises Kubernetes cluster](img/B19682_16_27.jpg)\n\nFigure 16.27 – The current context set to the on-premises Kubernetes cluster\nNext, we look at some of the most common `kubectl` commands used with everyday Kubernetes administration tasks.\nWorking with kubectl\nOne of the first commands we run when connected to a Kubernetes cluster is the following:\n\n```", "```\n\n The command shows the IP address and port of the API server listening on the CP node, among other information:\n![Figure 16.28 – The Kubernetes cluster information shown](img/B19682_16_28.jpg)\n\nFigure 16.28 – The Kubernetes cluster information shown\nThe `cluster-info` command can also help to debug and diagnose cluster-related issues:\n\n```", "```\n\n To get a detailed view of the cluster nodes, we run the following command:\n\n```", "```\n\n The `--output=wide` (or `-o wide`) flag yields detailed information about cluster nodes. The output in the following illustration has been cropped to show it more clearly:\n![Figure 16.29 – Getting detailed information about cluster nodes](img/B19682_16_29.jpg)\n\nFigure 16.29 – Getting detailed information about cluster nodes\nThe following command retrieves the pods running in the default namespace:\n\n```", "```\n\n As of now, we don’t have any user pods running, and the command returns the following:\n\n```", "```\n\n To list all the pods, we append the `--all-namespaces` flag to the preceding command:\n\n```", "```\n\n The output shows all pods running in the system. Since these are exclusively system pods, they are associated with the `kube-system` namespace:\n![Figure 16.30 – Getting all pods in the system](img/B19682_16_30.jpg)\n\nFigure 16.30 – Getting all pods in the system\nWe would get the same output if we specified `kube-system` with the `--``namespace` flag:\n\n```", "```\n\n For a comprehensive view of all resources running in the system, we run the following command:\n\n```", "```\n\n So far, we have only mentioned some of the more common object types, such as nodes, pods, and Services. There are many others, and we can view them with the following command:\n\n```", "```\n\n The output includes the name of the API object types (such as `nodes`), their short name or alias (such as `no`), and whether they can be organized in namespaces (such as `false`):\n![Figure 16.31 – Getting all API object types (cropped)](img/B19682_16_31.jpg)\n\nFigure 16.31 – Getting all API object types (cropped)\nSuppose you want to find out more about specific API objects, such as `nodes`. Here’s where the `explain` command comes in handy:\n\n```", "```\n\n The output is as follows:\n![Figure 16.32 – Showing nodes’ detailed information (cropped)](img/B19682_16_32.jpg)\n\nFigure 16.32 – Showing nodes’ detailed information (cropped)\nThe output provides detailed documentation about the `nodes` API object type, including the related API fields. One of the API fields is `apiVersion`, describing the versioning schema of an object. You may view the related documentation with the following command:\n\n```", "```\n\n The output is as follows:\n![Figure 16.33 – Details about the apiVersion field (cropped)](img/B19682_16_33.jpg)\n\nFigure 16.33 – Details about the apiVersion field (cropped)\nWe encourage you to use the `explain` command to learn about the various Kubernetes API object types in a cluster. Please note that the `explain` command provides documentation about *resource types*. It should not be confused with the `describe` command, which shows detailed information about the *resources* in the system.\nThe following commands display cluster node-related information about *all* nodes, and then node `k8s-n1` in particular:\n\n```", "```\n\n For every `kubectl` command, you can invoke `--help` (or `-h`) to get context-specific help. Here are a few examples:\n\n```", "```\n\n The `kubectl` CLI is relatively rich in commands, and becoming proficient with it may take some time. Occasionally, you may find yourself looking for a specific command or remembering its correct spelling or use. The `auto-complete` bash for `kubectl` comes to the rescue. We’ll show you how to enable this next.\nEnabling kubectl autocompletion\nWith `kubectl` autocompletion, you’ll get context-sensitive suggestions when you hit the *Tab* key twice while typing the `kubectl` commands.\nThe `kubectl` autocompletion feature depends on `bash-completion`. Most Linux platforms have `bash-completion` enabled by default. Otherwise, you’ll have to install the related package manually. On Ubuntu, for example, you install it with the following command:\n\n```", "```\n\n Next, you need to source the `kubectl` autocompletion in your shell (or similar) profile:\n\n```", "```\n\n The changes will take effect on your next login to the terminal or immediately if you source the `bash` profile:\n\n```", "```\n\n With the `kubectl` autocomplete active, you’ll get context-sensitive suggestions when you hit the *Tab* key twice while typing the command. For example, the following sequence provides all the available resources when you try to create one:\n\n```", "```\n\n When typing the `kubectl create` command and pressing the *Tab* key twice, the result will be a list of resources available for the command:\n![Figure 16.34 – Autocompletion use with kubectl](img/B19682_16_34.jpg)\n\nFigure 16.34 – Autocompletion use with kubectl\nThe `kubectl` autocompletion reaches every part of the syntax: command, resource (type and name), and flags.\nNow that we know more about using the `kubectl` command, it’s time to turn our attention to deploying applications in Kubernetes.\nDeploying applications\nWhen we introduced the `kubectl` command and its usage pattern at the beginning of the *Using kubectl* section, we touched upon the two ways of creating application resources in Kubernetes: imperative and declarative.\nWe’ll look at both of these models closely in this section while deploying a simple web application. Let’s start with the imperative model first.\nWorking with imperative Deployments\nAs a quick refresher, with imperative Deployments, we follow a sequence of `kubectl` commands to create the required resources and get to the cluster’s desired state, such as running the application. Declarative Deployments accomplish the same, usually with a single `kubectl` `apply` command using a manifest file describing multiple resources.\nCreating a Deployment\nLet’s begin by creating a Deployment first. We’ll name our Deployment `packt`, based on a demo Nginx container we’re pulling from the public Docker registry (`docker.io/nginxdemos/hello`):\n\n```", "```\n\n The command output shows that our Deployment was created successfully:\n\n```", "```\n\n We just created a Deployment with a ReplicaSet containing a single pod running a web server application. We should note that our application is managed by the controller manager within an app Deployment stack (`deployment.apps`). Alternatively, we could just deploy a simple application pod (`packt-web`) with the following command:\n\n```", "```\npod/packt-web created\n```", "```\nkubectl get pods -o wide\n```", "```\nkubectl describe pod packt-5dc77bb9bf-bnzsc\n```", "```\nkubectl describe pod packt-web\n```", "```\nssh packt@192.168.122.163\n```", "```\nsudo crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps\n```", "```\nkubectl exec -it packt-web -- /bin/sh\n```", "```\nps aux\n```", "```\nifconfig | grep 'inet addr:' | cut -d: -f2 | awk '{print $1}' | grep -v '127.0.0.1'\n```", "```\n172.16.57.193\n```", "```\nhostname\n```", "```\npackt-web\n```", "```\nkubectl get pods packt-web -o jsonpath='{.status.podIP}{\"\\n\"}'\n```", "```\n    sleep command due to the Docker entry point of the corresponding image, which simply runs a curl command and then exits. Without sleep, the pod would keep coming up and crashing. With the sleep command, we delay the execution of the curl entry point to prevent the exit. The output is shown in the following screenshot:\n    ```", "```\n    kubectl exec test -- curl http://172.16.57.193\n    ```", "```\n    kubectl logs packt-web\n    ```", "```\n    kubectl exec packt-web -- ls -la /var/log/nginx\n    ```", "```\n    kubectl delete pods test\n    ```", "```\nkubectl create deployment packt --image=nginxdemos/hello\n```", "```\nkubectl get deployments -l app=packt\nkubectl get replicasets -l app=packt\nkubectl get pods -l app=packt\n```", "```\nkubectl describe deployment packt | more\nkubectl describe replicaset packt | more\nkubectl describe pod packt-5dc77bb9bf-bnzsc | more\n```", "```\n    kubectl expose deployment packt \\\n        --port=80 \\\n        --target-port=80 \\\n    --type=NodePort flag, the Service type would be ClusterIP by default, and the Service endpoint would only be accessible within the cluster.\n    ```", "```\n    10.105.111.243) and the ports the Service is listening on for TCP traffic (80:32664/TCP):*   port `80`: Within the cluster*   port `32664`: Outside the cluster, on any of the nodesWe should note that the cluster IP is only accessible within the cluster and not from the outside:\n    ```", "```\n    kubectl get nodes -o jsonpath='{range .items[*]}{.status.addresses[*].address}{\"\\n\"}'\n    ```", "```\n    kubectl get pod packt-579bb9c999-rtvzr -o jsonpath='{.status.podIP}{\"\\n\"}{.metadata.name}{\"\\n\"}'\n    ```", "```\n    kubectl describe deployment packt\n    ```", "```\n    packt Deployment, we’ll see 10 pods running:\n\n    ```", "```\n\n    The output is as follows:\n    ```", "```\n    sudo apt-get install -y lynx\n    ```", "```\n    lynx 172.16.191.6:32081\n    ```", "```\n    packt application pods, we can see the surplus pods terminating until only three pods are remaining:\n\n    ```", "```\n\n    The output is as follows:\n    ```", "```\n    kubectl delete service packt\n    kubectl delete deployment packt\n    kubectl delete pod packt-web\n    ```", "```\n    kubectl get all\n    ```", "```\nkubectl apply -f MANIFEST\n```", "```\nkubectl create deployment packt --image=nginxdemos/hello\n```", "```\nkubectl create deployment packt --image=nginxdemos/hello \\\n    --dry-run=client --output=yaml\n```", "```\nkubectl create deployment packt --image=nginxdemos/hello \\\n    --dry-run=client --output=yaml > packt.yaml. From here, we can edit the file to accommodate more complex configurations. For now, we’ll leave the manifest as is and proceed with the next stage in our declarative Deployment workflow.\nValidating a manifest\nBefore deploying a manifest, we recommend validating the Deployment, especially if you edited the file manually. Editing mistakes can happen, particularly when working with complex YAML files with multiple indentation levels.\nThe following command validates the `packt.yaml` Deployment manifest:\n\n```", "```\n\n A successful validation yields the following output:\n\n```", "```\n\n If there are any errors, we should edit the manifest file and correct them prior to deployment. Our manifest looks good, so let’s go ahead and deploy it.\nDeploying a manifest\nTo deploy the `packt.yaml` manifest, we use the following command:\n\n```", "```\n\n A successful Deployment shows the following message:\n\n```", "```\n\n We can check the deployed resources with the following command:\n\n```", "```\n\n The output shows that the `packt` Deployment resources created declaratively are up and running:\n![Figure 16.55 – The deployment resources created declaratively](img/B19682_16_55.jpg)\n\nFigure 16.55 – The Deployment resources created declaratively\nNext, we want to expose our Deployment using a Service.\nExposing the Deployment with a Service\nWe’ll repeat the preceding workflow by creating, validating, and deploying the Service manifest (`packt-svc.yaml`). For brevity, we simply enumerate the related commands:\n\n1.  Create the manifest file (`packt-svc.yaml`) for the Service exposing our Deployment (`packt`):\n\n    ```", "```\n\n    We explained the preceding command previously in the *Exposing Deployments as* *Services* section.\n\n     2.  Next, we’ll validate the Service Deployment manifest:\n\n    ```", "```\n\n     3.  If the validation is successful, we deploy the Service manifest:\n\n    ```", "```\n    packt application resources, including the Service endpoint (service/packt) listening on port 31380:\n    ```", "```\n\n![Figure 16.56 – The packt application resources deployed](img/B19682_16_56.jpg)\n\nFigure 16.56 – The packt application resources deployed\n\n1.  Using a browser, `curl`, or Lynx, we can access our application by targeting any of the cluster nodes on port `31380`. Let’s use the CP node (`k8s-cp1`, `192.168.122.104`) by pointing our browser to `http://192.168.122.104:31380`:\n\n![Figure 16.57 – Accessing the packt application endpoint](img/B19682_16_57.jpg)\n\nFigure 16.57 – Accessing the packt application endpoint\nIf we want to change the existing configuration of a resource in our application Deployment, we can update the related manifest and redeploy it. In the next section, we’ll modify the Deployment to accommodate a scale-out scenario.\nUpdating a manifest\nSuppose our application is taking a high number of requests, and we’d like to add more pods to our Deployment to handle the traffic. We need to change the `spec.replicas` configuration setting in the `pack.yaml` manifest:\n\n1.  Using your editor of choice, edit the `packt.yaml` file and locate the following configuration section:\n\n    ```", "```\n\n    Change the value from `1` to `10` for additional application pods in the ReplicaSet controlled by the `packt` Deployment. The configuration becomes the following:\n\n    ```", "```\n\n     2.  Save the manifest file and redeploy with the following command:\n\n    ```", "```\n    packt resources in the cluster, we should see the new pods up and running:\n\n    ```", "```\n\n    ```", "```\n\n![Figure 16.58 – The additional pods added for application scale-out](img/B19682_16_58.jpg)\n\nFigure 16.58 – The additional pods added for application scale-out\nWe encourage you to test with the scale-out environment and verify the load balancing workload described in the *Scaling application Deployments* section earlier in this chapter.\n\n1.  Let’s scale back our Deployment to three pods, but this time by updating the related manifest on the fly with the following command:\n\n    ```", "```\n\n    The command will open our default editor in the system (**vi**) to make the desired change:\n\n![Figure 16.59 – Making deployment changes on the fly](img/B19682_16_59.jpg)\n\nFigure 16.59 – Making Deployment changes on the fly\n\n1.  After saving and exiting the editor, we’ll get a message suggesting that our Deployment (`packt`) has been updated:\n\n    ```", "```\n\n     2.  We can verify our updated Deployment with the help of the following command:\n\n    ```", "```\n\n    The output now shows only three pods running in our Deployment:\n\n![Figure 16.60 – Showing the number of deployments](img/B19682_16_60.jpg)\n\nFigure 16.60 – Showing the number of Deployments\n\n1.  Before wrapping up, let’s clean up our resources once again with the following commands to bring the cluster back to the default state:\n\n    ```", "```\n\nWe have shown you how to use Kubernetes on bare metal, and in the next section, we will briefly point you to some useful resources for using Kubernetes in the cloud.\nRunning Kubernetes in the cloud\nManaged Kubernetes Services are fairly common among public cloud providers. Amazon **Elastic Kubernetes Service** (**EKS**), **Azure Kubernetes Services** (**AKS**), and **Google Kubernetes Engine** (**GKE**) are the major cloud offerings of Kubernetes at the time of this writing. In this section, we’ll not focus on any of these solutions, but we will provide you with solid resources on how to use Kubernetes in the cloud. For more advanced titles on this subject, please check the *Further reading* section of this chapter.\nWe should note that we just scratched the surface of deploying and managing Kubernetes clusters. Yet, here we are, at a significant milestone, where we deployed our first Kubernetes clusters on-premises. We have reached the end of this journey here, but we trust that you’ll take it to the next level and further explore the exciting domain of application Deployment and scaling with Kubernetes. Let’s now summarize what we have learned in this chapter.\nSummary\nWe began this chapter with a high-level overview of the Kubernetes architecture and API object model, introducing the most common cluster resources, such as pods, Deployments, and Services. Next, we took on the relatively challenging task of building an on-premises Kubernetes cluster from scratch using VMs. We explored various CLI tools for managing Kubernetes cluster resources on-premises. At the high point of our journey, we focused on deploying and scaling applications in Kubernetes using imperative and declarative Deployment scenarios.\nWe believe that novice Linux administrators will benefit greatly from the material covered in this chapter and become more knowledgeable in managing resources across hybrid clouds and on-premises distributed environments, deploying applications at scale, and working with CLI tools. We believe that the structured information in this chapter will also help seasoned system administrators refresh some of their knowledge and skills in the areas covered.\nIt’s been a relatively long chapter, and we barely skimmed the surface of the related field. We encourage you to explore some resources captured in the *Further reading* section and strengthen your knowledge regarding some key areas of Kubernetes environments, such as networking, security, and scale.\nIn the next chapter, we’ll stay within the application deployment realm and look at **Ansible**, a platform for accelerating application delivery on-premises and in the cloud.\nQuestions\nHere are a few questions for refreshing or pondering upon some of the concepts you’ve learned in this chapter:\n\n1.  Enumerate some of the essential Services of a Kubernetes CP node. How do the worker nodes differ?\n2.  What command did we use to bootstrap a Kubernetes cluster?\n3.  What is the difference between imperative and declarative Deployments in Kubernetes?\n4.  What is the `kubectl` command for deploying a pod? How about the command for creating a Deployment?\n5.  What is the `kubectl` command to access the shell within a pod container?\n6.  What is the `kubectl` command to query all resources related to a Deployment?\n7.  How do you scale out a Deployment in Kubernetes? Can you think of the different ways (commands) in which to accomplish the task?\n8.  How do you delete all resources related to a Deployment in Kubernetes?\n\nFurther reading\nThe following resources may help you to consolidate your knowledge of Kubernetes further:\n\n*   Kubernetes documentation online: [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)\n*   The `kubectl` cheat sheet: [https://kubernetes.io/docs/reference/kubectl/cheatsheet/](https://kubernetes.io/docs/reference/kubectl/cheatsheet/)\n*   *Kubernetes and Docker: The Container Masterclass [Video]*, *Cerulean Canvas*, Packt Publishing\n*   *Mastering Kubernetes – Third Edition*, Gigi Sayfan, Packt Publishing\n\nThe following is a short list of useful links for deploying Kubernetes on Azure, Amazon, and Google:\n\n*   Amazon EKS:\n    *   [https://docs.aws.amazon.com/eks/index.html](https://docs.aws.amazon.com/eks/index.html)\n    *   [https://docs.aws.amazon.com/eks/latest/userguide/sample-deployment.html](https://docs.aws.amazon.com/eks/latest/userguide/sample-deployment.html)\n*   AKS:\n    *   [https://azure.microsoft.com/en-us/services/kubernetes-service/](https://azure.microsoft.com/en-us/services/kubernetes-service/)\n    *   [https://learn.microsoft.com/en-us/azure/aks/tutorial-kubernetes-deploy-cluster?tabs=azure-cli](https://learn.microsoft.com/en-us/azure/aks/tutorial-kubernetes-deploy-cluster?tabs=azure-cli)\n    *   [https://learn.microsoft.com/en-us/azure/aks/learn/quick-kubernetes-deploy-portal?tabs=azure-cli](https://learn.microsoft.com/en-us/azure/aks/learn/quick-kubernetes-deploy-portal?tabs=azure-cli)\n*   GKE:\n    *   [https://cloud.google.com/kubernetes-engine](https://cloud.google.com/kubernetes-engine)\n    *   [https://cloud.google.com/build/docs/deploying-builds/deploy-gke](https://cloud.google.com/build/docs/deploying-builds/deploy-gke)\n    *   [https://cloud.google.com/kubernetes-engine/docs/deploy-app-cluster](https://cloud.google.com/kubernetes-engine/docs/deploy-app-cluster)\n\n```", "```\n\n```"]