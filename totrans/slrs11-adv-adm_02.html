<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02" class="calibre1"/>Chapter 2. ZFS</h1></div></div></div><p class="calibre7">In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Creating ZFS storage pools and filesystems</li><li class="listitem">Playing with ZFS faults and properties</li><li class="listitem">Creating a ZFS snapshot and clone</li><li class="listitem">Performing a backup in a ZFS filesystem</li><li class="listitem">Handling logs and caches</li><li class="listitem">Managing devices in storage pools</li><li class="listitem">Configuring spare disks</li><li class="listitem">Handling ZFS snapshots and clones</li><li class="listitem">Playing with COMSTAR</li><li class="listitem">Mirroring the root pool</li><li class="listitem">ZFS shadowing</li><li class="listitem">Configuring ZFS sharing with the SMB share</li><li class="listitem">Setting and getting other ZFS properties</li><li class="listitem">Playing with the ZFS swap</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch02lvl1sec31" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre7">ZFS is a 128-bit <a id="id198" class="calibre1"/>transactional filesystem offered by Oracle Solaris 11, and it supports 256 trillion directory entries, does not have any upper limit of files, and is always consistent on disk. Oracle Solaris 11 makes ZFS its default filesystem, which provides some features such as storage pool, snapshots, clones, and volumes. When administering ZFS objects, the first step is to create a ZFS storage pool. It can be made from full disks, files, and slices, considering that the minimum size of any mentioned block device is 128 MB. Furthermore, when creating a ZFS pool, the possible RAID configurations are stripe (Raid 0), mirror (Raid 1), and RAID-Z (a kind of RAID-5). Both the mirror and RAID-Z configurations support a feature named self-healing data that works by protecting data. In this case, when a bad block arises in a disk, the ZFS framework fetches the same block from another replicated disk to repair the original bad block. RAID-Z presents three <a id="id199" class="calibre1"/>variants: raidz1 (similar to RAID-5) that uses at least three disks (two data and one parity), raidz2 (similar to RAID-6) that uses at least five disks (3D and 2P), and raidz3 (similar to RAID-6, but with an additional level of parity) that uses at least eight disks (5D and 3P).</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec32" class="calibre1"/>Creating ZFS storage pools and filesystems</h1></div></div></div><p class="calibre7">To start playing <a id="id200" class="calibre1"/>with ZFS, the first step is to create a storage pool, and <a id="id201" class="calibre1"/>afterwards, all filesystems will be created inside these storage pools. To accomplish the creation of a storage pool, we have to decide which raid configuration we will use (stripe, mirror, or RAID-Z) to create the storage pool and, afterwards, the filesystems on it.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec46" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">To follow this recipe, it is necessary to use a virtual machine (VMware or VirtualBox) that runs Oracle Solaris 11 with 4 GB RAM and eight 4 GB disks. Once the virtual machine is up and running, log in as the root user and open a terminal.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec47" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">A storage pool is a logical object, and it represents the physical characteristics of the storage and must be created before anything else. To create a storage pool, the first step is to list all the available disks on the system and choose what disks will be used by running the following command as the root role:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">format</strong>
Searching for disks...done

AVAILABLE DISK SELECTIONS:
       0. c8t0d0 &lt;VBOX-HARDDISK-1.0-80.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@0,0
       1. c8t1d0 &lt;VBOX-HARDDISK-1.0-16.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@1,0
       2. c8t2d0 &lt;VBOX-HARDDISK-1.0-4.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@2,0
       3. c8t3d0 &lt;VBOX-HARDDISK-1.0 cyl 2046 alt 2 hd 128 sec 32&gt;
          /pci@0,0/pci1000,8000@14/sd@3,0
       4. c8t4d0 &lt;VBOX-HARDDISK-1.0 cyl 2046 alt 2 hd 128 sec 32&gt;
          /pci@0,0/pci1000,8000@14/sd@4,0
       5. c8t5d0 &lt;VBOX-HARDDISK-1.0 cyl 2046 alt 2 hd 128 sec 32&gt;
          /pci@0,0/pci1000,8000@14/sd@5,0
       6. c8t6d0 &lt;VBOX-HARDDISK-1.0 cyl 2046 alt 2 hd 128 sec 32&gt;
          /pci@0,0/pci1000,8000@14/sd@6,0
       7. c8t8d0 &lt;VBOX-HARDDISK-1.0 cyl 2046 alt 2 hd 128 sec 32&gt;
          /pci@0,0/pci1000,8000@14/sd@8,0
       8. c8t9d0 &lt;VBOX-HARDDISK-1.0 cyl 2046 alt 2 hd 128 sec 32&gt;
          /pci@0,0/pci1000,8000@14/sd@9,0
       9. c8t10d0 &lt;VBOX-HARDDISK-1.0 cyl 2046 alt 2 hd 128 sec 32&gt;
          /pci@0,0/pci1000,8000@14/sd@a,0
      10. c8t11d0 &lt;VBOX-HARDDISK-1.0 cyl 2046 alt 2 hd 128 sec 32&gt;
          /pci@0,0/pci1000,8000@14/sd@b,0
Specify disk (enter its number):</pre></div><p class="calibre7">Following the selection of disks, create a <code class="email">zpool create</code> storage pool and verify the information about this pool using the <code class="email">zpool list</code> and <code class="email">zpool status</code> commands. Before these steps, we have to decide the pool configuration: stripe (default), mirror, raidz, raidz2, or raidz3. If the configuration isn't specified, stripe (raid0) will be assumed as default. Then, a <a id="id202" class="calibre1"/>pool is created by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create oracle_stripe_1 c8t3d0 c8t4d0</strong>
'oracle_stripe_1' successfully created, but with no redundancy; failure of one device will cause loss of the pool</pre></div><p class="calibre7">To list the <a id="id203" class="calibre1"/>pool, execute the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool list oracle_stripe_1</strong>
NAME              SIZE   ALLOC  FREE   CAP  DEDUP  HEALTH  ALTROOT
oracle_stripe_1   7.94G  122K   7.94G  0%   1.00x  ONLINE  -</pre></div><p class="calibre7">To verify the status of the pool, run the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool status oracle_stripe_1</strong>
  pool: oracle_stripe_1
 state: ONLINE
  scan: none requested
config:

  NAME             STATE     READ WRITE CKSUM
  oracle_stripe_1  ONLINE       0     0     0
  c8t3d0           ONLINE       0     0     0
  c8t4d0           ONLINE       0     0     0

errors: No known data errors</pre></div><p class="calibre7">Although it's out of the scope of this chapter, we can list some related performance information by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool iostat -v oracle_stripe_1</strong>
                  capacity       operations    bandwidth
pool              alloc   free   read  write   read  write
----------------  -----  -----  -----  -----  -----  -----
oracle_stripe_1   128K    7.94G    0      0    794     56
  c8t3d0          53K     3.97G    0      0    391     24
  c8t4d0          74.5K   3.97G    0      0    402     32
----------------  -----  -----  -----  -----  -----  -----</pre></div><p class="calibre7">If necessary, a second and third storage pool can be created using the same commands but taking different disks and, in this case, by changing to the <code class="email">mirror</code> and <code class="email">raidz</code> configurations, respectively. This <a id="id204" class="calibre1"/>task is accomplished by running <a id="id205" class="calibre1"/>the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create oracle_mirror_1 mirror c8t5d0 c8t6d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool list oracle_mirror_1</strong>
NAME              SIZE   ALLOC  FREE   CAP  DEDUP  HEALTH  ALTROOT
oracle_mirror_1   3.97G  85K    3.97G  0%   1.00x  ONLINE  -
root@solaris11-1:~# <strong class="calibre8">zpool status oracle_mirror_1</strong>
  pool: oracle_mirror_1
 state: ONLINE
  scan: none requested
config:
  NAME             STATE      READ  WRITE  CKSUM
  oracle_mirror_1  ONLINE       0     0      0
  mirror-0         ONLINE       0     0      0
  c8t5d0           ONLINE       0     0      0
  c8t6d0           ONLINE       0     0      0

errors: No known data errors
root@solaris11-1:~# <strong class="calibre8">zpool create oracle_raidz_1 raidz c8t8d0 c8t9d0 c8t10d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool list oracle_raidz_1</strong>
NAME             SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
oracle_raidz_1  11.9G   176K  11.9G   0%  1.00x  ONLINE  -
root@solaris11-1:~# <strong class="calibre8">zpool status oracle_raidz_1</strong>
pool: oracle_raidz_1
 state: ONLINE
  scan: none requested
config:

  NAME            STATE     READ WRITE CKSUM
  oracle_raidz_1  ONLINE       0     0     0
  raidz1-0        ONLINE       0     0     0
  c8t8d0          ONLINE       0     0     0
  c8t9d0          ONLINE       0     0     0
  c8t10d0         ONLINE       0     0     0

errors: No known data errors</pre></div><p class="calibre7">Once the storage pools are created, it's time to create filesystems in these pools. First, let's create a filesystem named <code class="email">zfs_stripe_1</code> in the <code class="email">oracle_stripe_1</code> pool. Execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs create oracle_stripe_1/zfs_stripe_1</strong>
</pre></div><p class="calibre7">Repeating the same syntax, it's easy to create two new filesystems named <code class="email">zfs_mirror_1</code> and <code class="email">zfs_raidz_1</code> in <code class="email">oracle_mirror_1</code> and <code class="email">oracle_raidz_1</code>, respectively:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs create oracle_mirror_1/zfs_mirror_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs create oracle_raidz_1/zfs_raidz_1</strong>
</pre></div><p class="calibre7">The listing of recently created <a id="id206" class="calibre1"/>filesystems is done by running <a id="id207" class="calibre1"/>the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list</strong>
NAME                            USED   AVAIL  REFER  MOUNTPOINT
<strong class="calibre8">(truncated output)</strong>
oracle_mirror_1                 124K   3.91G  32K    /oracle_mirror_1
oracle_mirror_1/zfs_mirror_1    31K    3.91G  31K  /oracle_mirror_1/zfs_mirror_1
oracle_raidz_1                  165K   7.83G  36.0K  /oracle_raidz_1
oracle_raidz_1/zfs_raidz_1      34.6K  7.83G  34.6K  /oracle_raidz_1/zfs_raidz_1
oracle_stripe_1                 128K   7.81G  32K    /oracle_stripe_1
oracle_stripe_1/zfs_stripe_1    31K    7.81G  31K  /oracle_stripe_1/zfs_stripe_1
<strong class="calibre8">(truncated output)</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list oracle_stripe_1 oracle_mirror_1 oracle_raidz_1</strong>
NAME             USED  AVAIL  REFER  MOUNTPOINT
oracle_mirror_1  124K  3.91G    32K  /oracle_mirror_1
oracle_raidz_1   165K  7.83G  36.0K  /oracle_raidz_1
oracle_stripe_1  128K  7.81G    32K  /oracle_stripe_1</pre></div><p class="calibre7">The ZFS engine has automatically created the mount-point directory for all the created filesystems, and it has been mounted on them. This can also be verified by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs mount</strong>
rpool/ROOT/solaris              /
rpool/ROOT/solaris/var          /var
rpool/VARSHARE                  /var/share
rpool/export                    /export
rpool/export/home               /export/home
oracle_mirror_1                 /oracle_mirror_1
oracle_mirror_1/zfs_mirror_1    /oracle_mirror_1/zfs_mirror_1
oracle_stripe_1                 /oracle_stripe_1
oracle_stripe_1/zfs_stripe_1    /oracle_stripe_1/zfs_stripe_1
rpool                           /rpool
oracle_raidz_1                  /oracle_raidz_1
oracle_raidz_1/zfs_raidz_1      /oracle_raidz_1/zfs_raidz_1</pre></div><p class="calibre7">The last two lines confirm that the <a id="id208" class="calibre1"/>ZFS filesystems that we <a id="id209" class="calibre1"/>created are already mounted and ready to use.</p><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec22" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">This recipe has taught us how to create a storage pool with different configurations such as stripe, mirror, and raidz. Additionally, we learned how to create filesystems in these pools.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec33" class="calibre1"/>Playing with ZFS faults and properties</h1></div></div></div><p class="calibre7">ZFS is completely <a id="id210" class="calibre1"/>oriented by properties that can change the behavior of <a id="id211" class="calibre1"/>storage pools and filesystems. This <a id="id212" class="calibre1"/>recipe will touch upon important properties from ZFS, and we <a id="id213" class="calibre1"/>will learn how to handle them.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec48" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">To follow this recipe, it is necessary to use a virtual machine (VMware or VirtualBox) that runs Oracle Solaris 11 with 4 GB RAM and eight 4 GB disks. Once the virtual machine is up and running, log in as the root user and open a terminal.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec49" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">Every ZFS object has properties that can be accessed and, most of the time, changed. For example, to get the pool properties, we must execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool get all oracle_mirror_1</strong>
NAME                PROPERTY       VALUE               SOURCE
<strong class="calibre8">(truncated output)</strong>
oracle_mirror_1     bootfs         -                   default
oracle_mirror_1     cachefile      -                   default
oracle_mirror_1     capacity       0%                  -
oracle_mirror_1     dedupditto     0                   default
oracle_mirror_1     dedupratio     1.00x               -
oracle_mirror_1     delegation     on                  default
oracle_mirror_1     failmode       wait                default
oracle_mirror_1     free           3.97G               -
oracle_mirror_1     guid           730796695846862911  -
<strong class="calibre8">(truncated output)</strong>
</pre></div><p class="calibre7">Some useful information from the previous output is that the free space is 3.97 GB (the <code class="email">free</code> property), the <a id="id214" class="calibre1"/>pool is online (the <code class="email">health</code> property), and <code class="email">0%</code> of the total <a id="id215" class="calibre1"/>capacity was used (the <code class="email">capacity</code> property). If <a id="id216" class="calibre1"/>we need to know about any problem related to <a id="id217" class="calibre1"/>the pool (referring to the <code class="email">health</code> property), it's recommended that you get this information by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool status -x </strong>
all pools are health
root@solaris11-1:~# <strong class="calibre8">zpool status -x oracle_mirror_1</strong>
pool 'oracle_mirror_1' is healthy
root@solaris11-1:~# <strong class="calibre8">zpool status oracle_mirror_1</strong>
  pool: oracle_mirror_1
 state: ONLINE
  scan: none requested
config:

    NAME             STATE     READ WRITE CKSUM
    oracle_mirror_1  ONLINE       0     0     0
    mirror-0         ONLINE       0     0     0
    c8t5d0           ONLINE       0     0     0
    c8t6d0           ONLINE       0     0     0</pre></div><p class="calibre7">Another fantastic method to check whether all data in the specified storage pool is okay is using the <code class="email">zpool scrub</code> command that examines whether the checksums are correct, and for replicated devices (such as mirror and raidz configurations), the <code class="email">zpool scrub</code> command repairs any discovered problem. To follow the <code class="email">zpool scrub</code> results, the <code class="email">zpool status</code> command can be used as follows:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool scrub oracle_mirror_1</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status oracle_mirror_1</strong>
  pool: oracle_mirror_1
 state: ONLINE
scan: scrub in progress since Tue Jun 10 04:04:56 2014
    2.53G scanned out of 3.91G at 24.0M/s, 0h1m to go
    0 repaired, 64.71% done
config:

    NAME             STATE     READ WRITE CKSUM
    oracle_mirror_1  ONLINE       0     0     0
    mirror-0         ONLINE       0     0     0
    c8t5d0           ONLINE       0     0     0
    c8t6d0           ONLINE       0     0     0</pre></div><p class="calibre7">After some time, if everything went well, the same <code class="email">zpool</code> status command should show the following output:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool status oracle_mirror_1</strong>
  pool: oracle_mirror_1
 state: ONLINE
scan: scrub repaired 0 in 0h4m with 0 errors on Tue Jun 10 04:09:48 2014
config:

    NAME             STATE     READ WRITE CKSUM
    oracle_mirror_1  ONLINE    0     0     0
        mirror-0  ONLINE       0     0     0
          c8t5d0  ONLINE       0     0     0
          c8t6d0  ONLINE       0     0     0</pre></div><p class="calibre7">During an analysis of <a id="id218" class="calibre1"/>possible disk errors, the following <code class="email">zpool history</code> <a id="id219" class="calibre1"/>command, which shows all the events <a id="id220" class="calibre1"/>that occurred on the pool, could be interesting and <a id="id221" class="calibre1"/>suitable:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool history oracle_mirror_1</strong>
History for 'oracle_mirror_1':
2013-11-27.19:14:15 zpool create oracle_mirror_1 mirror c8t5d0 c8t6d0
2013-11-27.19:57:31 zfs create oracle_mirror_1/zfs_mirror_1
<strong class="calibre8">(truncated output)</strong>
</pre></div><p class="calibre7">The Oracle Solaris Fault Manager, through its <code class="email">fmd</code> daemon, is a framework that receives any information related to potential problems that were detected by the system, diagnoses these problems and, eventually, takes a proactive action to keep the system integrity such as disabling a memory module. Therefore, this framework offers the following <code class="email">fmadm</code> command that, when used with the <code class="email">faulty</code> argument, displays information about resources that the Oracle Solaris Fault Manager believes to be faulty:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">fmadm faulty</strong>
</pre></div><p class="calibre7">The following <code class="email">dmesg</code> command confirms any suspicious hardware error:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">dmesg</strong>
</pre></div><p class="calibre7">From the <code class="email">zpool status</code> command, there are some possible values for the <code class="email">status</code> field:</p><div><ul class="itemizedlist"><li class="listitem"><code class="email">ONLINE</code>:. This means that the pool is good</li><li class="listitem"><code class="email">FAULTED</code>: This means that the pool is bad</li><li class="listitem"><code class="email">OFFLINE</code>: This means that the pool was disabled by the administrator</li><li class="listitem"><code class="email">DEGRADED</code>: This means that something (likely a disk) is bad, but the pool is still working</li><li class="listitem"><code class="email">REMOVED</code>: This means that a disk was hot-swapped</li><li class="listitem"><code class="email">UNAVAIL</code>: This means that the device or virtual device can be opened</li></ul></div><p class="calibre7">Returning to ZFS properties, it's easy to get property information from a ZFS filesystem by running the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list -r oracle_mirror_1</strong>
NAME                          USED  AVAIL  REFER  MOUNTPOINT
oracle_mirror_1               124K  3.91G    32K  /oracle_mirror_1
oracle_mirror_1/zfs_mirror_1   31K  3.91G    31K  /oracle_mirror_1/zfs_mirror_1
root@solaris11-1:~# <strong class="calibre8">zfs get all oracle_mirror_1/zfs_mirror_1</strong>
NAME                          PROPERTY          VALUE         SOURCE
oracle_mirror_1/zfs_mirror_1  aclinherit        restricted    default
oracle_mirror_1/zfs_mirror_1  aclmode           discard       default
oracle_mirror_1/zfs_mirror_1  atime             on            default
oracle_mirror_1/zfs_mirror_1  available         3.91G         -
oracle_mirror_1/zfs_mirror_1  canmount          on            default
oracle_mirror_1/zfs_mirror_1  casesensitivity   mixed         -
oracle_mirror_1/zfs_mirror_1  checksum          on            default
<strong class="calibre8">(truncated output)</strong>
</pre></div><p class="calibre7">The previous two <a id="id222" class="calibre1"/>commands deserve an explanation—<code class="email">zfs list –r</code> shows all <a id="id223" class="calibre1"/>the datasets (filesystems, snapshots, clones, and so on) <a id="id224" class="calibre1"/>under the <code class="email">oracle_mirror_1</code> storage <a id="id225" class="calibre1"/>pool. Additionally, <code class="email">zfs get all oracle_mirror_1/zfs_mirror_1</code> displays all the properties from the <code class="email">zfs_mirror_1</code> filesystem.</p><p class="calibre7">There are many filesystem properties (some of them are read-only and others read-write), and it's advisable to know some of them. Almost all are inheritable—a child (for example, a snapshot or clone object) inherits a configured value for a parent object (for example, a filesystem).</p><p class="calibre7">Setting a property value is done by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs set mountpoint=/oracle_mirror_1/another_point oracle_mirror_1/zfs_mirror_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r oracle_mirror_1</strong>
NAME                          USED  AVAIL  REFER  MOUNTPOINT
oracle_mirror_1               134K  3.91G    32K  /oracle_mirror_1
oracle_mirror_1/zfs_mirror_1   31K  3.91G    31K  /oracle_mirror_1/another_point</pre></div><p class="calibre7">The old mount point was renamed to the <code class="email">/oracle_mirror_1/another_point</code> directory and remounted again. Later, we'll return to this point and review some properties.</p><p class="calibre7">When it's necessary, a ZFS filesystem has to be renamed by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs rename oracle_stripe_1/zfs_stripe_1 oracle_stripe_1/zfs_test_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r oracle_stripe_1</strong>
NAME                         USED  AVAIL  REFER  MOUNTPOINT
oracle_stripe_1              128K  7.81G    32K  /oracle_stripe_1
oracle_stripe_1/zfs_test_1   31K   7.81G    31K  /oracle_stripe_1/zfs_test_1
root@solaris11-1:~# <strong class="calibre8">df -h /oracle_stripe_1/*</strong>
Filesystem             Size   Used  Available Capacity  Mounted on
oracle_stripe_1/zfs_test_1
                       7.8G    31K       7.8G     1%    /oracle_stripe_1/zfs_test_1</pre></div><p class="calibre7">Oracle Solaris 11 automatically <a id="id226" class="calibre1"/>altered the mount point of the renamed <a id="id227" class="calibre1"/>filesystem and remounted it again.</p><p class="calibre7">To destroy a ZFS filesystem or <a id="id228" class="calibre1"/>storage pool, there can't be any process that <a id="id229" class="calibre1"/>accesses the dataset. For example, if we try to delete the <code class="email">zfs_test</code> filesystem when a process is using the directory, we get an error:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">cd /oracle_stripe_1/zfs_test_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r oracle_stripe_1</strong>
NAME                          USED  AVAIL  REFER  MOUNTPOINT
oracle_stripe_1               128K  7.81G    32K  /oracle_stripe_1
oracle_stripe_1/zfs_test_1  31.5K  7.81G  31.5K  /oracle_stripe_1/zfs_test_1
root@solaris11-1:~# <strong class="calibre8">zfs destroy oracle_stripe_1/zfs_test_1</strong>
cannot unmount '/oracle_stripe_1/zfs_test_1': Device busy</pre></div><p class="calibre7">This case presents several possibilities—first (and the most recommended) is to understand what processes or applications are using the mentioned filesystem. Once the guilty processes or applications are found, the next step is to stop them. Therefore, everything is solved without <a id="id230" class="calibre1"/>losing any data. However, if there isn't any possibility to <a id="id231" class="calibre1"/>find the guilty processes, then killing the offending <a id="id232" class="calibre1"/>process(es) would be a feasible and unpredictable <a id="id233" class="calibre1"/>option, where data loss would be probable. Finally, using the <code class="email">-f</code> option would cause a <em class="calibre11">forced destroy</em>, which, obviously, is not advisable and would probably cause data loss. The following is the second procedure (killing the problematic process) by running the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">fuser -cu /oracle_stripe_1/zfs_test_1</strong>
/oracle_stripe_1/zfs_test_1:     1977c(root)
root@solaris11-1:~# <strong class="calibre8">ps -ef | grep 1977</strong>
    root  1977  1975   0 07:03:14 pts/1       0:00 bash</pre></div><p class="calibre7">We used the <code class="email">fuser</code> command that enables us to look for processes that access a specific file or directory. Therefore, according to the previous two outputs, there's a process using the <code class="email">/oracle_stripe_1/zfs_test_1</code> filesystem, and the <code class="email">ps –ef</code> command reveals that <code class="email">bash</code> is the guilty process, which is correct because we changed the mount point before trying to delete it. To solve this, it would be enough to leave the <code class="email">/oracle_stripe_1/zfs_test_1</code> directory. Nonetheless, if we didn't know how to solve the problem, the last resource would be to kill the offending process by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <code class="email">kill -9 1977</code>
</pre></div><p class="calibre7">At this time, there isn't a process accessing the filesystem, so it's possible to destroy it:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs destroy oracle_stripe_1/zfs_test_1</strong>
</pre></div><p class="calibre7">To verify whether the filesystem was correctly destroyed, execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list -r oracle_stripe_1</strong>
NAME              USED   AVAIL  REFER  MOUNTPOINT
oracle_stripe_1   89.5K  7.81G    31K  /oracle_stripe_1</pre></div><p class="calibre7">Everything worked fine, and the filesystem was destroyed. Nonetheless, if there was a snapshot or clone under this filesystem (we'll review and learn about them in the next recipe), we wouldn't have been able to delete the filesystem, and we should use the same command with the<code class="email">–r</code> option (for snapshots inside) or <code class="email">–R</code> (for snapshots and clones inside). From here, it's also possible to destroy the whole pool using the <code class="email">zpool destroy</code> command. Nevertheless, we should take care of a single detail—if there isn't any process using any filesystem from the pool to be destroyed, Oracle Solaris 11 doesn't prompt any question about the pool destruction. Everything inside the pool is destroyed without any question (so different from the Windows system, which prompts a warning before a dangerous action). To prove this statement, in the next example, we're going to create one filesystem in the <code class="email">oracle_stripe_1</code> pool, put some information into it, and, at the end, we're going to destroy all pools:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list -r oracle_stripe_1</strong>
NAME              USED  AVAIL  REFER  MOUNTPOINT
oracle_stripe_1  89.5K  7.81G    31K  /oracle_stripe_1
root@solaris11-1:~# <strong class="calibre8">zfs create oracle_stripe_1/fs_1</strong>
root@solaris11-1:~# <strong class="calibre8">cp /etc/[a-e]* /oracle_stripe_1/fs_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r oracle_stripe_1</strong>     
NAME                   USED  AVAIL  REFER  MOUNTPOINT
oracle_stripe_1       4.01M  7.81G    35K  /oracle_stripe_1
oracle_stripe_1/fs_1  82.5K  7.81G  82.5K  /oracle_stripe_1/fs_1
root@solaris11-1:~# <strong class="calibre8">zpool list oracle_stripe_1</strong>
NAME              SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
oracle_stripe_1  7.94G  4.01M  7.93G   0%  1.00x  ONLINE  -
root@solaris11-1:~# <strong class="calibre8">zpool destroy oracle_stripe_1</strong>
root@solaris11-1:~# <strong class="calibre8">zpool list</strong>
NAME              SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
iscsi_pool       3.97G  2.62M  3.97G   0%  1.00x  ONLINE  -
oracle_mirror_1  3.97G   134K  3.97G   0%  1.00x  ONLINE  -
oracle_raidz_1   11.9G   248K  11.9G   0%  1.00x  ONLINE  -
repo_pool        15.9G  7.64G  8.24G  48%  1.00x  ONLINE  -
rpool            79.5G  31.8G</pre></div><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec23" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">Taking the <code class="email">zpool</code> and <code class="email">zfs</code> commands, we created, listed, renamed, and destroyed pools and filesystems. Furthermore, we learned how to view properties and alter them, especially the <a id="id234" class="calibre1"/>mount point property that's very essential for daily ZFS <a id="id235" class="calibre1"/>administration. We also learned how to see the pool history, monitor <a id="id236" class="calibre1"/>the pool, and gather important information <a id="id237" class="calibre1"/>about related pool failures.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec34" class="calibre1"/>Creating a ZFS snapshot and clone</h1></div></div></div><p class="calibre7">A ZFS snapshot and <a id="id238" class="calibre1"/>clone play fundamental roles in the ZFS framework and in <a id="id239" class="calibre1"/>Oracle Solaris 11, as there are many uses for these <a id="id240" class="calibre1"/>features, and one of them is to execute backup and restore files <a id="id241" class="calibre1"/>from the ZFS filesystem. For example, a snapshot could be handy when either there is some corruption in the ZFS filesystem or a user loses a specific file. Using ZFS snapshots makes it possible to completely rollback the ZFS filesystem to a specific point or date.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec50" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">To follow this recipe, it is necessary to use a virtual machine (VMware or VirtualBox) that runs Oracle Solaris 11 with 4 GB RAM and eight 4 GB disks. Once the virtual machine is up and running, log in as the root user and open a terminal.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec51" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">Creating a snapshot is a <a id="id242" class="calibre1"/>fundamental task that can be executed by <a id="id243" class="calibre1"/>running the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create pool_1 c8t3d0</strong>
root@solaris11-1:~# <strong class="calibre8">zfs create pool_1/fs_1</strong>
</pre></div><p class="calibre7">Before continuing, I <a id="id244" class="calibre1"/>suggest that we copy some big files to the <code class="email">pool_1/fs_1</code> filesystem. In this case, I used files that I already had on my system, but you can copy anything into the filesystem. Run the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">cp -r mh* jo* /pool_1/fs_1/</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r pool_1/fs_1 </strong>
NAME          USED  AVAIL  REFER  MOUNTPOINT
pool_1/fs_1  63.1M  3.85G  63.1M  /pool_1/fs_1</pre></div><p class="calibre7">Finally, we create the snapshot by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs snapshot pool_1/fs_1@snap1</strong>
</pre></div><p class="calibre7">By default, snapshots aren't <a id="id245" class="calibre1"/>shown even when using the <code class="email">zfs list -r</code> command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list -r pool_1</strong>
NAME          USED  AVAIL  REFER  MOUNTPOINT
pool_1       63.2M  3.85G    32K  /pool_1
pool_1/fs_1  63.1M  3.85G  63.1M  /pool_1/fs_1</pre></div><p class="calibre7">This behavior is controlled by the <code class="email">listsnapshots</code> property (its value is <code class="email">off</code> by default) from the pool:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool get listsnapshots pool_1</strong>
NAME    PROPERTY       VALUE  SOURCE
pool_1  listsnapshots  off    local</pre></div><p class="calibre7">It's necessary to alter <code class="email">listsnapshots</code> to <code class="email">on</code> to change this behavior:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool set listsnapshots=on pool_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r pool_1</strong>
NAME                USED  AVAIL  REFER  MOUNTPOINT
pool_1             63.2M  3.85G    32K  /pool_1
pool_1/fs_1        63.1M  3.85G  63.1M  /pool_1/fs_1
pool_1/fs_1@snap1      0      -  63.1M  -</pre></div><p class="calibre7">It worked as planned. However, when executing the previous command, all datasets (filesystems and snapshots) are listed. To list only snapshots, it is necessary to specify a filter using the<code class="email">–t</code> option as follows:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list -t snapshot</strong>
NAME                                         USED  AVAIL  REFER  MOUNTPOINT
pool_1/fs_1@snap1                               0      -  63.1M  -
rpool/ROOT/solaris@install                   106M      -  3.52G  -
rpool/ROOT/solaris@2013-10-10-22:27:20       219M      -  3.77G  -
rpool/ROOT/solaris@2013-11-26-08:38:27      1.96G      -  24.2G  -
rpool/ROOT/solaris/var@install              63.0M      -   189M  -
rpool/ROOT/solaris/var@2013-10-10-22:27:20  66.5M      -   200M  -
rpool/ROOT/solaris/var@2013-11-26-08:38:27   143M      -   291M  -</pre></div><p class="calibre7">The previous command has shown only the existing snapshots as expected. An interesting fact is that snapshots live inside filesystems, and initially, they don't take any space on disk. However, as the filesystem is being altered, snapshots take free space, and this could be a big concern. Considering this, the <code class="email">SIZE</code> property equals zero and <code class="email">REFER</code> equals <code class="email">63.1M</code>, which is the exact size of the <code class="email">pool_1/fs_1</code> filesystem.</p><p class="calibre7">The <code class="email">REFER</code> field deserves an explanation—when snapshots are explained in any IT area, the classification is the same. There are physical snapshots and logical snapshots. Physical snapshots take the same space from a reference filesystem, and both don't have any impact on each other <a id="id246" class="calibre1"/>during the read/write operations. The creation of the snapshot <a id="id247" class="calibre1"/>takes a long time, because it's a kind of "copy" of <a id="id248" class="calibre1"/>everything from the reference filesystem. In this case, the <a id="id249" class="calibre1"/>snapshot is a static picture that represents the filesystem at the exact time when the snapshot was created. After this initial time, snapshots won't be synchronized with the reference filesystem anymore. If the administrator wants both synchronized, they should do it manually.</p><p class="calibre7">The other classification, logical snapshots, is very different from the first one. When a logical snapshot is made, only pointers to data from the reference filesystem are created, but there is no data inside the snapshot. This process is very fast and takes little disk space. The disadvantage is that any read operation impacts the reference filesystem. There are two additional effects—when some data changes in the reference filesystem, the operating system copies the data to be modified to the snapshot before being modified itself (this process is called <strong class="calibre8">copy </strong>
<a id="id250" class="calibre1"/>
<strong class="calibre8">on write</strong> (<strong class="calibre8">COW</strong>)). Why? Because of our previous explanation that snapshots are a static picture of an exact time from the reference filesystem. If some data changes, the snapshot has to be unaltered, and it must contain the same data from the time that it was created. A second and worse effect is that if the reference filesystem is lost, every snapshot becomes invalid. Why? Because the reference filesystem doesn't exist anymore, and all pointers become invalid.</p><p class="calibre7">Return to the <code class="email">REFER</code> field explanation; it means how much data in the reference filesystem is being referenced by a pointer in the snapshot. A clone is a copy of a filesystem, and it's based on snapshots, so to create a clone, a snapshot must be made first. However, there's a fundamental difference between a clone and snapshot—a snapshot is a read-only object, and a clone is a read/write object. Therefore, it's possible to write in a clone as we're able to write in a filesystem. Other interesting facts are that as the snapshot must exist before creating a clone, the clone is dependent on the snapshot, and both must be created in the same pool. Create a pool by executing the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs clone pool_1/fs_1@snap1 pool_1/clone_1   </strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r pool_1</strong>
NAME                USED  AVAIL  REFER  MOUNTPOINT
pool_1             63.2M  3.85G    33K  /pool_1
pool_1/clone_1       25K  3.85G  63.1M  /pool_1/clone_1
pool_1/fs_1        63.1M  3.85G  63.1M  /pool_1/fs_1
pool_1/fs_1@snap1      0      -  63.1M  -</pre></div><p class="calibre7">If we look at this output, it's <a id="id251" class="calibre1"/>complicated to distinguish a clone from a <a id="id252" class="calibre1"/>filesystem. Nonetheless, we could gather enough details to <a id="id253" class="calibre1"/>be able to distinguish the datasets:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get origin pool_1/fs_1</strong>
NAME         PROPERTY  VALUE  SOURCE
pool_1/fs_1  origin    -      -
root@solaris11-1:~# <strong class="calibre8">zfs get origin pool_1/fs_1@snap1</strong>
NAME               PROPERTY  VALUE  SOURCE
pool_1/fs_1@snap1  origin    -      -
root@solaris11-1:~# <strong class="calibre8">zfs get origin pool_1/clone_1</strong>   
NAME            PROPERTY  VALUE              SOURCE
pool_1/clone_1  origin    pool_1/fs_1@snap1  -</pre></div><p class="calibre7">The <code class="email">origin</code> property <a id="id254" class="calibre1"/>doesn't show anything relevant to pools and snapshots, but when this property is analyzed on a clone context, it shows us that the clone originated from the <code class="email">pool1_/fs_1@snap1</code> snapshot. Therefore, it's feasible to confirm that <code class="email">pool_1/fs_1@snap1</code> is indeed a snapshot by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get type pool_1/fs_1@snap1</strong>
NAME               PROPERTY  VALUE     SOURCE
pool_1/fs_1@snap1  type      snapshot  -</pre></div><p class="calibre7">In ZFS, the object creation order is <code class="email">pool</code> | <code class="email">filesystem</code> | <code class="email">snapshot</code> | <code class="email">clone</code>. So, the destruction order should be the inverse: <code class="email">clone</code> | <code class="email">snapshot</code> | <code class="email">filesystem</code> | <code class="email">pool</code>. It's possible to skip steps using special options that we'll learn about later.</p><p class="calibre7">For example, if we try to destroy a filesystem that contains a snapshot, the following error will be shown:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs destroy pool_1/fs_1</strong>
cannot destroy 'pool_1/fs_1': 
filesystem has children
use '-r' to destroy the following datasets:
pool_1/fs_1@snap1</pre></div><p class="calibre7">In the same way, if we try <a id="id255" class="calibre1"/>to destroy a snapshot without removing the clone first, the <a id="id256" class="calibre1"/>following message will be shown:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs destroy pool_1/fs_1@snap1</strong>
cannot destroy 'pool_1/fs_1@snap1': 
snapshot has dependent clones
use '-R' to destroy the following datasets:
pool_1/clone_1</pre></div><p class="calibre7">The last two cases have <a id="id257" class="calibre1"/>shown that it's necessary to follow the right order to <a id="id258" class="calibre1"/>destroy datasets in ZFS. Execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list -r pool_1</strong>
NAME                USED  AVAIL  REFER  MOUNTPOINT
pool_1             63.2M  3.85G    33K  /pool_1
pool_1/clone_1       25K  3.85G  63.1M  /pool_1/clone_1
pool_1/fs_1        63.1M  3.85G  63.1M  /pool_1/fs_1
pool_1/fs_1@snap1      0      -  63.1M  -
root@solaris11-1:~# <strong class="calibre8">zfs destroy pool_1/clone_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs destroy pool_1/fs_1@snap1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs destroy pool_1/fs_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r pool_1</strong> 
NAME     USED  AVAIL  REFER  MOUNTPOINT
pool_1  98.5K  3.91G    31K  /pool_1</pre></div><p class="calibre7">When the correct sequence is followed, it's possible to destroy each dataset one by one, although, as we mentioned earlier, it would be possible to skip steps. The next sequence shows how this is possible. Execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs destroy -R pool_1/fs_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r pool_1</strong>
NAME    USED  AVAIL  REFER  MOUNTPOINT
pool_1   91K  3.91G    31K  /pool_1
root@solaris11-1:~#</pre></div><p class="calibre7">Finally, we used the <code class="email">-R</code> option, and everything was destroyed—including the clone, snapshot, and filesystem.</p><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec24" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">We learned how to manage <a id="id259" class="calibre1"/>snapshots and clones, including <a id="id260" class="calibre1"/>how <a id="id261" class="calibre1"/>to create, list, distinguish, and <a id="id262" class="calibre1"/>destroy them. Finally, this closes our review about the fundamentals of ZFS.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec35" class="calibre1"/>Performing a backup in a ZFS filesystem</h1></div></div></div><p class="calibre7">Ten years ago, I <a id="id263" class="calibre1"/>didn't think about learning how to use any backup software, and honestly, I didn't like this kind of software because I thought it was so simple. Nowadays, I can see why I was so wrong.</p><p class="calibre7">Administering and managing <a id="id264" class="calibre1"/>backup software is the most fundamental activity in IT, acting as the last line of defense against hackers. By the way, hackers are winning the war using all types of resources—malwares, Trojans, viruses, worms, and spywares, and only backups of file servers and applications can save a company.</p><p class="calibre7">Oracle Solaris 11 offers a <a id="id265" class="calibre1"/>simple solution composed of two commands (<code class="email">zfs send</code> and <code class="email">zfs recv</code>) to back up ZFS filesystem data. During the backup operation, data is generated as a stream and sent (using the <code class="email">zfs send</code> command) through the network to another Oracle Solaris 11 system that receives this stream (using <code class="email">zfs recv</code>).</p><p class="calibre7">Oracle Solaris 11 is able to produce two kinds of streams: the replication stream, which includes the filesystem and all its dependent datasets (snapshots and clones), and the recursive stream, which includes the filesystems and clones, but excludes snapshots. The default stream type is the replication stream.</p><p class="calibre7">This recipe will show you how to execute a backup and restore operation.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec52" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">To follow this recipe, it's necessary to have two virtual machines (VMware or VirtualBox) that run Oracle Solaris 11, with 4 GB RAM each and eight 4 GB disks. The systems used in this recipe are named <code class="email">solaris11-1</code> and <code class="email">solaris11-2</code>.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec53" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">All the ZFS backup operations are based on snapshots. This procedure will do everything from the beginning—creating a pool, filesystem, and snapshot and then executing the backup. Execute the <a id="id266" class="calibre1"/>following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create backuptest_pool c8t5d0</strong>
root@solaris11-1:~# <strong class="calibre8">zfs create backuptest_pool/zfs1</strong>
root@solaris11-1:~# <strong class="calibre8">cp /etc/[a-p]* /backuptest_pool/zfs1</strong>
root@solaris11-1:/# <strong class="calibre8">ls -l /backuptest_pool/zfs1/</strong>
total 399
-rw-r--r--   1 root     root        1436 Dec 13 03:30 aliases
-rw-r--r--   1 root     root         182 Dec 13 03:30 auto_home
-rw-r--r--   1 root     root         220 Dec 13 03:30 auto_master
-rw-r--r--   1 root     root        1931 Dec 13 03:30 dacf.conf
<strong class="calibre8">(truncated output)</strong>
root@solaris11-1:/# <strong class="calibre8">zfs list backuptest_pool/zfs1</strong> 
NAME                  USED  AVAIL  REFER  MOUNTPOINT
backuptest_pool/zfs1  214K  3.91G   214K  /backuptest_pool/zfs1
root@solaris11-1:/# <strong class="calibre8">zfs snapshot backuptest_pool/zfs1@backup1</strong>
root@solaris11-1:/# <strong class="calibre8">zpool listsnapshots=on backuptest_pool</strong> 
root@solaris11-1:/# <strong class="calibre8">zfs list -r backuptest_pool</strong>
NAME                          USED  AVAIL  REFER  MOUNTPOINT
backuptest_pool               312K  3.91G    32K  /backuptest_pool
backuptest_pool/zfs1          214K  3.91G   214K  /backuptest_pool/zfs1
backuptest_pool/zfs1@backup1     0      -   214K  -</pre></div><p class="calibre7">The following commands <a id="id267" class="calibre1"/>remove some files from the <code class="email">backuptest_pool/zfs1</code> filesystem:</p><div><pre class="programlisting">root@solaris11-1:/# <strong class="calibre8">cd /backuptest_pool/zfs1/</strong>
root@solaris11-1:/backuptest_pool/zfs1# <strong class="calibre8">rm [a-k]*</strong>
root@solaris11-1:/backuptest_pool/zfs1# <strong class="calibre8">ls -l</strong>
total 125
-rw-r--r--   1 root     root        2986 Dec 13 03:30 name_to_major
-rw-r--r--   1 root     root        3090 Dec 13 03:30 name_to_sysnum
-rw-r--r--   1 root     root        7846 Dec 13 03:30 nanorc
-rw-r--r--   1 root     root        1321 Dec 13 03:30 netconfig
-rw-r--r--   1 root     root         487 Dec 13 03:30 netmasks
-rw-r--r--   1 root     root         462 Dec 13 03:30 networks
-rw-r--r--   1 root     root        1065 Dec 13 03:30 nfssec.conf
……….
<strong class="calibre8">(truncated output)</strong>
</pre></div><p class="calibre7">We omitted a very interesting fact about snapshots—when any file is deleted from the filesystem, it doesn't disappear forever. There is a hidden directory named <code class="email">.zfs</code> inside each filesystem; it <a id="id268" class="calibre1"/>contains snapshots, and all the removed files go to a subdirectory inside this hidden directory. Let's look at the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">cd /backuptest_pool/zfs1/.zfs</strong>
root@solaris11-1:/backuptest_pool/zfs1/.zfs# <strong class="calibre8">ls</strong>
shares    snapshot
root@solaris11-1:/backuptest_pool/zfs1/.zfs# <strong class="calibre8">cd snapshot/</strong>
root@solaris11-1:/backuptest_pool/zfs1/.zfs/snapshot# <strong class="calibre8">ls</strong>
backup1
root@solaris11-1:/backuptest_pool/zfs1/.zfs/snapshot# <strong class="calibre8">cd backup1/</strong>
root@solaris11-1:/backuptest_pool/zfs1/.zfs/snapshot/backup1# <strong class="calibre8">ls -l</strong>
total 399
-rw-r--r--   1 root     root        1436 Dec 13 03:30 aliases
-rw-r--r--   1 root     root         182 Dec 13 03:30 auto_home
-rw-r--r--   1 root     root         220 Dec 13 03:30 auto_master
-rw-r--r--   1 root     root        1931 Dec 13 03:30 dacf.conf
-r--r--r--   1 root     root         516 Dec 13 03:30 datemsk
-rw-r--r--   1 root     root        2670 Dec 13 03:30 devlink.tab
-rw-r--r--   1 root     root       38237 Dec 13 03:30 driver_aliases
………
<strong class="calibre8">(truncated output)</strong>

root@solaris11-1:/backuptest_pool/zfs1/.zfs/snapshot/backup1# <strong class="calibre8">cd</strong>
</pre></div><p class="calibre7">Using this information <a id="id269" class="calibre1"/>about the localization of deleted files, any file could be restored, and even better, it would <a id="id270" class="calibre1"/>be possible to revert the filesystem to the same content as when the snapshot was taken. This operation is named <code class="email">rollback</code>, and it can be executed using the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs rollback backuptest_pool/zfs1@backup1</strong>
root@solaris11-1:~# <strong class="calibre8">cd /backuptest_pool/zfs1/</strong>
root@solaris11-1:/backuptest_pool/zfs1# <strong class="calibre8">ls -l</strong>    
total 399
-rw-r--r--   1 root     root        1436 Dec 13 03:30 aliases
-rw-r--r--   1 root     root         182 Dec 13 03:30 auto_home
-rw-r--r--   1 root     root         220 Dec 13 03:30 auto_master
-rw-r--r--   1 root     root        1931 Dec 13 03:30 dacf.conf
-r--r--r--   1 root     root         516
 Dec 13 03:30 datemsk
-rw-r--r--   1 root     root        2670 Dec 13 03:30 devlink.tab
-rw-r--r--   1 root     root       38237 Dec 13 03:30 driver_aliases
<strong class="calibre8">(truncated output)</strong>
</pre></div><p class="calibre7">Every single file was restored to the filesystem, as nothing had happened.</p><p class="calibre7">Going a step ahead, let's <a id="id271" class="calibre1"/>see how to back up the filesystem data to another system that runs Oracle Solaris 11. The first step is to connect <a id="id272" class="calibre1"/>to another system (<code class="email">solaris 11-2</code>) and create and prepare a pool to receive the backup stream from the <code class="email">solaris11-1</code> source system by running the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">ssh solaris11-2</strong>
Password: 
Last login: Fri Dec 13 04:29:41 2013
Oracle Corporation      SunOS 5.11      11.1    September 2012
root@solaris11-2:~# <strong class="calibre8">zpool create away_backup c8t3d0</strong>
root@solaris11-2:~# <strong class="calibre8">zpool set readonly=on away_backup</strong>
root@solaris11-2:~# <strong class="calibre8">zfs list away_backup</strong>
NAME         USED  AVAIL  REFER  MOUNTPOINT
away_backup   85K  3.91G    31K  /away_backup</pre></div><p class="calibre7">We enabled the <code class="email">readonly</code> property from <code class="email">away_pool</code>. Why? Because we have to keep the metadata consistent while receiving data from another host and afterwards too.</p><p class="calibre7">Continuing this procedure, the <a id="id273" class="calibre1"/>next step is to execute the remote backup from the <code class="email">solaris11-1</code> source machine, sending all filesystem data to the <code class="email">solaris11-2</code> target machine:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs send backuptest_pool/zfs1@backup1 | ssh solaris11-2 zfs recv -F away_backup/saved_backup</strong>
Password:</pre></div><p class="calibre7">We used the <code class="email">ssh</code> command <a id="id274" class="calibre1"/>to send all data through a secure tunnel, but we could have used the <code class="email">netcat</code> command (it's included in Oracle Solaris, and <a id="id275" class="calibre1"/>there's more information about it on <a class="calibre1" href="http://netcat.sourceforge.net/">http://netcat.sourceforge.net/</a>) if security isn't a requirement.</p><p class="calibre7">You can verify that all data is present on the target machine by executing the following command:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">zfs list -r away_backup</strong>
NAME                      USED  AVAIL  REFER  MOUNTPOINT
away_backup               311K  3.91G    32K  /away_backup
away_backup/saved_backup  214K  3.91G   214K  /away_backup/saved_backup
root@solaris11-2:~# <strong class="calibre8">ls -l /away_backup/saved_backup/</strong>
total 399
-rw-r--r--   1 root     root        1436 Dec 13 03:30 aliases
-rw-r--r--   1 root     root         182 Dec 13 03:30 auto_home
-rw-r--r--   1 root     root         220 Dec 13 03:30 auto_master
-rw-r--r--   1 root     root        1931 Dec 13 03:30 dacf.conf
-r--r--r--   1 root     root         516 Dec 13 03:30 datemsk
-rw-r--r--   1 root     root        2670 Dec 13 03:30 devlink.tab
-rw-r--r--   1 root     root       38237 Dec 13 03:30 driver_aliases
-rw-r--r--   1 root     root         557 Dec 13 03:30 driver_classes
-rwxr--r--   1 root     root        1661 Dec 13 03:30 dscfg_format
……..
<strong class="calibre8">(truncated output)</strong>
</pre></div><p class="calibre7">According to this output, the remote backup, using the <code class="email">zfs send</code> and <code class="email">zfs recv</code> commands, has worked as expected. The restore operation is similar, so let's destroy every file from the <code class="email">backuptest_pool/zfs1</code> filesystem in the first system (<code class="email">solaris11-1</code>) as well as its snapshot by running the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">cd /backuptest_pool/zfs1/</strong>
root@solaris11-1:/backuptest_pool/zfs1# <strong class="calibre8">rm *</strong>
root@solaris11-1:/backuptest_pool/zfs1# <strong class="calibre8">cd</strong>
root@solaris11-1:~# <strong class="calibre8">zfs destroy backuptest_pool/zfs1@backup1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r backuptest_pool/zfs1</strong>
NAME                  USED  AVAIL  REFER  MOUNTPOINT
backuptest_pool/zfs1   31K  3.91G    31K  /backuptest_pool/zfs1
root@solaris11-1:~# </pre></div><p class="calibre7">From the second machine (<code class="email">solaris11-2</code>), the restore procedure can be executed by running the following commands:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">zpool set listsnapshots=on away_backup</strong>
root@solaris11-2:~# <strong class="calibre8">zfs list -r away_backup</strong>
NAME                              USED  AVAIL  REFER  MOUNTPOINT
away_backup                       312K  3.91G    32K  /away_backup
away_backup/saved_backup          214K  3.91G   214K  /away_backup/saved_backup
away_backup/saved_backup@backup1     0      -   214K  -</pre></div><p class="calibre7">The restore operation is similar <a id="id276" class="calibre1"/>to what we did during the <a id="id277" class="calibre1"/>backup, but we have to change the direction of the command where the <code class="email">solaris11-1</code> system is the target and <code class="email">solaris11-2</code> is the source now:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">zfs send -Rv away_backup/saved_backup@backup1 | ssh solaris11-1 zfs recv -F backuptest_pool/zfs1</strong>
sending from @ to away_backup/saved_backup@backup1
Password:
root@solaris11-2:~#</pre></div><p class="calibre7">You can see that we used the <code class="email">ssh</code> command to make a secure transmission between the systems. Again, we <a id="id278" class="calibre1"/>could have used another tool such as <code class="email">netcat</code> and the methodology would have done the same thing.</p><p class="calibre7">Returning to the <code class="email">solaris11-1</code> system, verify that all data was recovered by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list -r backuptest_pool/zfs1</strong>
NAME                          USED  AVAIL  REFER  MOUNTPOINT
backuptest_pool/zfs1          214K  3.91G   214K  /backuptest_pool/zfs1
backuptest_pool/zfs1@backup1     0      -   214K  -
root@solaris11-1:~# <strong class="calibre8">cd /backuptest_pool/zfs1/</strong>
root@solaris11-1:/backuptest_pool/zfs1# <strong class="calibre8">ls -al</strong>
total 407
drwxr-xr-x   2 root     root          64 Dec 13 03:30 .
drwxr-xr-x   3 root     root           3 Dec 13 05:12 ..
-rw-r--r--   1 root     root        1436 Dec 13 03:30 aliases
-rw-r--r--   1 root     root         182 Dec 13 03:30 auto_home
-rw-r--r--   1 root     root         220 Dec 13 03:30 auto_master
-rw-r--r--   1 root     root        1931 Dec 13 03:30 dacf.conf
………
<strong class="calibre8">(truncated output)</strong>
</pre></div><p class="calibre7">ZFS is amazing. The backup <a id="id279" class="calibre1"/>and restore operations are simple to <a id="id280" class="calibre1"/>execute, and everything has <a id="id281" class="calibre1"/>worked so well. The removed files are back.</p><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec25" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">On ZFS, the restore and backup operations are done through <a id="id282" class="calibre1"/>two commands: <code class="email">zfs send</code> and <code class="email">zfs recv</code>. Both operations are based on snapshots, and they make it possible to save data on the <a id="id283" class="calibre1"/>same machine or on another machine. During the explanation, we also learned about the snapshot rollback procedure.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec36" class="calibre1"/>Handling logs and caches</h1></div></div></div><p class="calibre7">ZFS has some very interesting internal structures that can greatly improve the performance of the <a id="id284" class="calibre1"/>pool and filesystem. One of them is <strong class="calibre8">ZFS intent log</strong> (<strong class="calibre8">ZIL</strong>), <a id="id285" class="calibre1"/>which <a id="id286" class="calibre1"/>was created to get more intensive and sequential write request performance, making more <strong class="calibre8">Input/Output </strong>
<a id="id287" class="calibre1"/>
<strong class="calibre8">Operations Per Second</strong> (<strong class="calibre8">IOPS</strong>) possible and <a id="id288" class="calibre1"/>saving any transaction record in the memory until transaction <a id="id289" class="calibre1"/>groups (known as TXG) are flushed to the disk or a request is received. When using ZIL, all of the write operations are done on ZIL, and afterwards, they are committed to the filesystem, helping prevent any data loss.</p><p class="calibre7">Usually, the ZIL space is allocated from the main storage pool, but this could fragment data. Oracle Solaris 11 allows us to decide where ZIL will be held. Most implementations put ZIL on a dedicated disk or, even better, on a mirrored configuration using SSD disks or flash memory devices, being appropriated to highlight that log devices for ZIL shouldn't be confused with database logfiles' disks. Usually, ZIL device logs don't have a size bigger than half of the RAM size, but other aspects must be considered to provide a consistent guideline when making its sizing.</p><p class="calibre7">Another very <a id="id290" class="calibre1"/>popular structure of ZFS is the <strong class="calibre8">Adaptive Replacement Cache</strong> (<strong class="calibre8">ARC</strong>), which increases to occupy almost all free memory (RAM minus 1 GB) of Oracle Solaris 11, but without pushing the application data out of memory. A very positive aspect of ARC is that it improves the reading performance a lot, because if data can be found in the memory (ARC), there isn't a necessity of taking any information from disks.</p><p class="calibre7">Beyond ARC, there's another type of <a id="id291" class="calibre1"/>cache named L2ARC, which is similar to a cache level 2 between the main memory and the disk. L2ARC complements ARC, and using SSD disks is suitable for this type of cache, given that one of the more productive scenarios is when L2ARC is deployed as an accelerator for random reads. Here's a very important fact to be remembered—L2ARC writes data to the cache devices (SSD disks) in an asynchronous way, so L2ARC <a id="id292" class="calibre1"/>is not recommended for intensive (sequential) writes.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec54" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">This recipe is going to use a <a id="id293" class="calibre1"/>virtual machine (from VirtualBox or VMware) with 4 GB of memory, Oracle Solaris 11 (installed), and at least eight 4 GB disks.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec55" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">There are two methods to configure a log object in a pool—either the pool is created with log devices (at the same time) or log devices are added after the pool's creation. The latter method is used more often, so <a id="id294" class="calibre1"/>the following procedure takes this approach:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create raid1_pool mirror c8t3d0 c8t4d0</strong>
</pre></div><p class="calibre7">In the next command, we'll add a log in the mirror mode, which is very appropriate to prevent a single point <a id="id295" class="calibre1"/>of failure. So, execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool add raid1_pool log mirror c8t5d0 c8t6d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status raid1_pool</strong>
  pool: raid1_pool
 state: ONLINE
  scan: none requested
config:

  NAME        STATE     READ WRITE CKSUM
  raid1_pool  ONLINE       0     0     0
    mirror-0  ONLINE       0     0     0
      c8t3d0  ONLINE       0     0     0
      c8t4d0  ONLINE       0     0     0
  logs
    mirror-1  ONLINE       0     0     0
      c8t5d0  ONLINE       0     0     0
      c8t6d0  ONLINE       0     0     0

errors: No known data errors</pre></div><p class="calibre7">Perfect! The mirrored log was added as expected. It's appropriate to explain about the <code class="email">mirror-0</code> and <code class="email">mirror-1</code> objects from <code class="email">zpool status</code>. Both objects are virtual devices. When a pool is created, the disks that were chosen are organized under a structure named virtual devices (<code class="email">vdev</code>), and then, this <code class="email">vdev</code> object is presented to the pool. In a rough way, a pool is composed of virtual <a id="id296" class="calibre1"/>devices, and each virtual device is composed of disks, slices, files, or any volume presented by other software or storage. Virtual devices are <a id="id297" class="calibre1"/>generated when the <code class="email">stripe</code>, <code class="email">mirror</code>, and <code class="email">raidz</code> pools are <a id="id298" class="calibre1"/>created. Additionally, they are also created when a log and cache are inserted into the pool.</p><p class="calibre7">If a disk log removal is <a id="id299" class="calibre1"/>necessary, execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool detach raid1_pool c8t6d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status raid1_pool</strong>
  pool: raid1_pool
 state: ONLINE
  scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
  raid1_pool    ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
        c8t3d0  ONLINE       0     0     0
        c8t4d0  ONLINE       0     0     0
    logs
      c8t5d0    ONLINE       0     0     0

errors: No known data errors</pre></div><p class="calibre7">It would be possible to remove both log disks at once by specifying <code class="email">mirror-1</code> (the virtual device), which represents the logs:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool remove raid1_pool mirror-1</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status raid1_pool</strong>
  pool: raid1_pool
 state: ONLINE
  scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
  raid1_pool    ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
        c8t3d0  ONLINE       0     0     0
        c8t4d0  ONLINE       0     0     0

errors: No known data errors
root@solaris11-1:~#</pre></div><p class="calibre7">As we explained at the beginning <a id="id300" class="calibre1"/>of this procedure, it's usual to add logs after a pool has <a id="id301" class="calibre1"/>been created, but it would be possible and easy to create a <a id="id302" class="calibre1"/>pool and, at the same time, include the log devices during the <a id="id303" class="calibre1"/>creation process by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create mir_pool mirror c8t3d0 c8t4d0 log mirror c8t5d0 c8t6d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool</strong>
  pool: mir_pool
 state: ONLINE
  scan: none requested
config:

  NAME        STATE     READ WRITE CKSUM
  mir_pool    ONLINE       0     0     0
    mirror-0  ONLINE       0     0     0
      c8t3d0  ONLINE       0     0     0
      c8t4d0  ONLINE       0     0     0
  logs
    mirror-1  ONLINE       0     0     0
      c8t5d0  ONLINE       0     0     0
      c8t6d0  ONLINE       0     0     0

errors: No known data errors
root@solaris11-1:~#</pre></div><p class="calibre7">According to the explanation about the L2ARC cache at the beginning of the recipe, it's also possible to add a cache object (L2ARC) into the ZFS pool using a syntax very similar to the one used when adding log objects by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create mircache_pool mirror c8t3d0 c8t4d0 cache c8t5d0 c8t6d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status mircache_pool</strong>
  pool: mircache_pool
 state: ONLINE
  scan: none requested
config:

  NAME           STATE     READ WRITE CKSUM
  mircache_pool  ONLINE       0     0     0
       mirror-0  ONLINE       0     0     0
         c8t3d0  ONLINE       0     0     0
         c8t4d0  ONLINE       0     0     0
  cache
       c8t5d0    ONLINE       0     0     0
       c8t6d0    ONLINE       0     0     0
errors: No known data errors</pre></div><p class="calibre7">Similarly, like <a id="id304" class="calibre1"/>log <a id="id305" class="calibre1"/>devices, a pool <a id="id306" class="calibre1"/>could be <a id="id307" class="calibre1"/>created including cache devices in a single step:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create mircache_pool mirror c8t3d0 c8t4d0 cache c8t5d0 c8t6d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status mircache_pool</strong>
  pool: mircache_pool
 state: ONLINE
  scan: none requested
config:

     NAME        STATE     READ WRITE CKSUM
  mircache_pool  ONLINE       0     0     0
       mirror-0  ONLINE       0     0     0
         c8t3d0  ONLINE       0     0     0
         c8t4d0  ONLINE       0     0     0
  cache
       c8t5d0    ONLINE       0     0     0
       c8t6d0    ONLINE       0     0     0

errors: No known data errors</pre></div><p class="calibre7">It worked as expected! However, it's necessary to note that cache objects can't be mirrored as we did when adding log devices, and they can't be part of a RAID-Z configuration.</p><p class="calibre7">Removing a <a id="id308" class="calibre1"/>cache device from a pool is done by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool remove mircache_pool c8t5d0</strong>
</pre></div><p class="calibre7">A final <a id="id309" class="calibre1"/>and important warning—every time <code class="email">cache</code> objects are added into a pool, wait until the <a id="id310" class="calibre1"/>data comes into cache (the warm-up phase). It <a id="id311" class="calibre1"/>usually takes around 2 hours.</p><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec26" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">ARC, L2ARC, and ZIL are common structures in ZFS administration, and we learned how to create and remove both logs and cache from the ZFS pool. There are very interesting procedures and recommendations about performance and tuning that includes these objects, but it's out of the scope of this book.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec37" class="calibre1"/>Managing devices in storage pools</h1></div></div></div><p class="calibre7">Manipulating and <a id="id312" class="calibre1"/>managing devices are common tasks when working <a id="id313" class="calibre1"/>with a ZFS storage pool, and more maintenance activities involve adding, deleting, attaching, and detaching disks. According to Oracle, ZFS supports raid0 (<code class="email">stripe</code>), raid1 (<code class="email">mirror</code>), raidz (similar to raid5, with one parity disk), raidz2 (similar to raid6, but uses two parity disks), and raidz3 (three parity disks), and additionally, there could be a combination such as raid 0+1 or raid 1+0.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec56" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">This recipe is going to use a virtual machine (from VirtualBox or VMware) with 4 GB of memory, a running Oracle Solaris 11 installation, and at least eight 4 GB disks.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec57" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">According to the previous recipes, the structure of a mirrored pool is <code class="email">pool</code> | <code class="email">vdev</code> | <code class="email">disks</code>, and the next command shouldn't be new to us:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create mir_pool2 mirror c8t3d0 c8t4d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool2</strong>
  pool: mir_pool2
 state: ONLINE
  scan: none requested
config:

  NAME        STATE     READ WRITE CKSUM
  mir_pool2   ONLINE       0     0     0
    mirror-0  ONLINE       0     0     0
      c8t3d0  ONLINE       0     0     0
      c8t4d0  ONLINE       0     0     0

errors: No known data errors</pre></div><p class="calibre7">Eventually, in a critical environment, it could be necessary to increase the size of the pool, given that there are some ways to accomplish it. However, not all of them are correct, because this procedure must be done with care to keep the redundancy. For example, the next command fails to increase the redundancy because only one disk is added, and in this case, we would have two vdevs, the first being <code class="email">vdev</code> (<code class="email">mirror-0</code>) with two disks concatenated and <a id="id314" class="calibre1"/>a second <code class="email">vdev</code> that doesn't have any redundancy. If the second <code class="email">vdev</code> fails, the entire pool is lost. Oracle Solaris <a id="id315" class="calibre1"/>notifies us about the problem when we try this wrong configuration:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool add mir_pool2 c8t5d0</strong>
vdev verification failed: use -f to override the following errors:
mismatched replication level: pool uses mirror and new vdev is disk
Unable to build pool from specified devices: invalid vdev configuration</pre></div><p class="calibre7">If we wanted to proceed even with this notification, it would be enough to add the <code class="email">-f</code> option, but this isn't recommended.</p><p class="calibre7">The second example is very similar to the first one, and we tried to add two disks instead of only one:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool add mir_pool2 c8t5d0 c8t6d0</strong>
vdev verification failed: use -f to override the following errors:
mismatched replication level: pool uses mirror and new vdev is disk
Unable to build pool from specified devices: invalid vdev configuration</pre></div><p class="calibre7">Again, the error remains because we added two disks, but we haven't mirrored them. In this case, the explanation is the same, and we would have a single point of failure if we tried to proceed.</p><p class="calibre7">Therefore, the correct method to expand the pool and keep the tolerance against failure is by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool add mir_pool2 mirror c8t5d0 c8t6d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool2</strong>
  pool: mir_pool2
 state: ONLINE
  scan: none requested
config:

     NAME        STATE     READ WRITE CKSUM
    mir_pool2   ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
        c8t3d0  ONLINE       0     0     0
        c8t4d0  ONLINE       0     0     0
      mirror-1  ONLINE       0     0     0
        c8t5d0  ONLINE       0     0     0
        c8t6d0  ONLINE       0     0     0

errors: No known data errors</pre></div><p class="calibre7">It worked! The final configuration is one that is similar to RAID 1+0, where there are two mirrored vdevs and all the data is spread over them. In this case, if the pool has a failure disk in any vdevs, data information is preserved. Furthermore, there are two vdevs in the pool: <code class="email">mirror-0</code> and <code class="email">mirror-1</code>. If we wished to remove a single disk from a mirror, it could be done by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool detach mir_pool3 c8t6d0</strong>
</pre></div><p class="calibre7">If the plan is to remove the whole mirror (<code class="email">vdev</code>), execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool remove mir_pool3 mirror-1</strong>
</pre></div><p class="calibre7">All deletions were done successfully.</p><p class="calibre7">A mirrored pool with two disks is fine and is used very often, but some companies require a more resilient configuration <a id="id316" class="calibre1"/>with three disks. To use a more realistic case, let's create a mirrored pool with two disks, create a filesystem inside it, copy some <a id="id317" class="calibre1"/>aleatory data into this filesystem (the reader can choose any data), and finally, add a third disk. Perform the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create mir_pool3  mirror c8t8d0 c8t9d0</strong>
root@solaris11-1:~# <strong class="calibre8">zfs create mir_pool3/zfs1</strong>
root@solaris11-1:~# <strong class="calibre8">cp -r mhvtl-* DTraceToolkit-0.99* dtbook_scripts* john* /mir_pool3/zfs1/</strong>
</pre></div><p class="calibre7">Again, in the preceding command, we could have copied any data. Finally, the command that executes our task is as follows:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool attach mir_pool3 c8t9d0 c8t10d0</strong>
</pre></div><p class="calibre7">In the preceding command, we attached a new disk (<code class="email">c8t10d0</code>) to a mirrored pool and specified where the current data would be copied from (<code class="email">c8t9d0</code>). After resilvering (resynchronization), the pool organization is as follows:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool3</strong>
  pool: mir_pool3
 state: ONLINE
  scan: resilvered 70.7M in 0h0m with 0 errors on Sat Dec 14 02:49:08 2013
config:

    NAME         STATE     READ WRITE CKSUM
   mir_pool3    ONLINE       0     0     0
     mirror-0   ONLINE       0     0     0
       c8t8d0   ONLINE       0     0     0
       c8t9d0   ONLINE       0     0     0
       c8t10d0  ONLINE       0     0     0

errors: No known data errors</pre></div><p class="calibre7">Now, the <code class="email">mir_pool3</code> pool is a three-way mirror pool, and all data is resilvered (resynchronized).</p><p class="calibre7">Some maintenance procedures require that we disable a disk to prevent any reading or writing operation on this <a id="id318" class="calibre1"/>device. Thus, when this disk is put to the <code class="email">offline</code> <a id="id319" class="calibre1"/>state, it remains <code class="email">offline</code> even after a reboot. Considering our existing three-way mirrored pool, the last device can be put in <code class="email">offline</code>:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool offline mir_pool3 c8t10d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool3</strong>
  pool: mir_pool3
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
  Sufficient replicas exist for the pool to continue functioning in a
  degraded state.
action: Online the device using 'zpool online' or replace the device with 'zpool replace'.
  scan: resilvered 70.7M in 0h0m with 0 errors on Sat Dec 14 02:49:08 2013
config:

   NAME         STATE     READ WRITE CKSUM
  mir_pool3    DEGRADED     0     0     0
    mirror-0   DEGRADED     0     0     0
      c8t8d0   ONLINE       0     0     0
      c8t9d0   ONLINE       0     0     0
      c8t10d0  OFFLINE      0     0     0

errors: No known data errors</pre></div><p class="calibre7">There are some interesting findings—the <code class="email">c8t10d0</code> disk is <code class="email">OFFLINE</code>, <code class="email">vdev</code> (<code class="email">mirror-0</code>) is in the <code class="email">DEGRADED</code> state, and the <code class="email">mir_pool3</code> pool is in the <code class="email">DEGRADED</code> state too.</p><p class="calibre7">The opposite operation to change the status of a disk to <code class="email">ONLINE</code> is very easy, and while the pool is being resilvered, its status will be <code class="email">DEGRADED</code>:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool online mir_pool3 c8t10d0</strong>
warning: device 'c8t10d0' onlined, but remains in degraded state
root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool3</strong>
  pool: mir_pool3
 state: ONLINE
  scan: resilvered 18K in 0h0m with 0 errors on Sat Dec 14 04:50:03 2013
config:
<strong class="calibre8">(truncated output)</strong>
</pre></div><p class="calibre7">One of the most useful and interesting tasks when managing pools is disk replacement, which only happens when there are pools using one of the following configurations: <code class="email">raid1</code>, <code class="email">raidz</code>, <code class="email">raidz2</code>, or <code class="email">raid3</code>. Why? Because a disk replacement couldn't compromise the <a id="id320" class="calibre1"/>data availability, and only these <a id="id321" class="calibre1"/>configurations can ensure this premise.</p><p class="calibre7">Two kinds of replacement exist:</p><div><ul class="itemizedlist"><li class="listitem">Replacement of a failed device by another in the same slot</li><li class="listitem">Replacement of a failed device by another from another slot</li></ul></div><p class="calibre7">Both methods are straight and easy to execute. For example, we're using VirtualBox in this example, and to simulate the first case, we're going to power off Oracle Solaris 11 (<code class="email">solaris11-1</code>), remove the disk that will be replaced (<code class="email">c8t10d0</code>), create a new one in the same slot, and power on the virtual machine again (<code class="email">solaris11-1</code>).</p><p class="calibre7">Before performing all these steps, we'll copy more data (here, it can be any data of your choice) to the <code class="email">zfs1</code> filesystem inside the <code class="email">mir_pool3</code> pool:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">cp -r /root/SFHA601/ /mir_pool3/zfs1/</strong>
root@solaris11-1:~# <strong class="calibre8">zpool list mir_pool3</strong>
NAME        SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
mir_pool3  3.97G  2.09G  1.88G  52%  1.00x  ONLINE  -
root@solaris11-1:~# <strong class="calibre8">shutdown –y –g0</strong>
</pre></div><p class="calibre7">On the VirtualBox Manager, click on the virtual machine with <code class="email">solaris11-1</code>, go to <strong class="calibre8">Settings</strong>, and then go to <a id="id322" class="calibre1"/>
<strong class="calibre8">Storage</strong>. Once there, remove the disks from <a id="id323" class="calibre1"/>slot 10 and create another disk at the same place (slot 10). After the physical replacement is done, power on the virtual machine (<code class="email">solaris11-1</code>) again. After the login, open a terminal and execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool3</strong>
  pool: mir_pool3
 state: DEGRADED
status: One or more devices are unavailable in response to persistent errors.
  Sufficient replicas exist for the pool to continue functioning in a
  degraded state.
action: Determine if the device needs to be replaced, and clear the errors
  using 'zpool clear' or 'fmadm repaired', or replace the device
  with 'zpool replace'.
  Run 'zpool status -v' to see device specific details.
  scan: resilvered 18K in 0h0m with 0 errors on Sat Dec 14 04:50:03 2013
config:

   NAME         STATE     READ WRITE CKSUM
   mir_pool3    DEGRADED     0     0     0
     mirror-0   DEGRADED     0     0     0
       c8t8d0   ONLINE       0     0     0
       c8t9d0   ONLINE       0     0     0
       c8t10d0  UNAVAIL      0     0     0

errors: No known data errors
root@solaris11-1:~#</pre></div><p class="calibre7">As the <code class="email">c8t10d0</code> device was exchanged for a new one, the <code class="email">zpool status mir_pool3</code> command shows that it's unavailable (<code class="email">UNAVAIL</code>). This is the expected status. According to the previous explanation, the idea is that the failed disk is exchanged for another one in the same slot. Execute the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool replace mir_pool3 c8t10d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool3</strong> 
  pool: mir_pool3
 state: DEGRADED
status: One or more devices is currently being resilvered.  The pool will
  scan: resilver in progress since Sat Dec 14 05:56:15 2013
    139M scanned out of 2.09G at 3.98M/s, 0h8m to go
    136M resilvered, 6.51% done
config:

   NAME               STATE     READ WRITE CKSUM
   mir_pool3          DEGRADED     0     0     0
     mirror-0         DEGRADED     0     0     0
       c8t8d0         ONLINE       0     0     0
       c8t9d0         ONLINE       0     0     0
       replacing-2    DEGRADED     0     0     0
         c8t10d0/old  UNAVAIL      0     0     0
         c8t10d0      DEGRADED     0     0     0  (resilvering)

errors: No known data errors
root@solaris11-1:~#</pre></div><p class="calibre7">The <code class="email">c8t10d0</code> disk was replaced and is being resilvered now. This time, we need to wait for the resilvering to complete.</p><p class="calibre7">If we're executing the <a id="id324" class="calibre1"/>replacement for a disk from another slot, the <a id="id325" class="calibre1"/>procedure is easier. For example, in the following steps, we're replacing the <code class="email">c8t9d0</code> disk with <code class="email">c8t3d0</code> by executing the following steps:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool replace mir_pool3 c8t9d0 c8t3d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool3</strong>
  pool: mir_pool3
 state: DEGRADED
status: One or more devices is currently being resilvered.  The pool will
  continue to function in a degraded state.
    576M scanned out of 2.09G at 4.36M/s, 0h5m to go
    572M resilvered, 26.92% done
config:

   NAME             STATE     READ WRITE CKSUM
   mir_pool3        DEGRADED     0     0     0
     mirror-0       DEGRADED     0     0     0
       c8t8d0       ONLINE       0     0     0
       replacing-1  DEGRADED     0     0     0
         c8t9d0     ONLINE       0     0     0
         c8t3d0     DEGRADED     0     0     0  (resilvering)
       c8t10d0      ONLINE       0     0     0</pre></div><p class="calibre7">Again, after the resync process is over, everything will be okay.</p><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec27" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">Managing disks is the most <a id="id326" class="calibre1"/>important task when working with ZFS. In <a id="id327" class="calibre1"/>this section, we learned how to add, remove, attach, detach, and replace a disk. All these processes will take a long time on a normal daily basis.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec38" class="calibre1"/>Configuring spare disks</h1></div></div></div><p class="calibre7">In a big company <a id="id328" class="calibre1"/>environment, there are a hundred disks working 24/7, and literally, it's impossible to know when a disk will fail. Imagine lots of disks failing <a id="id329" class="calibre1"/>during the day and how much time the replacement operations would take. This pictured context is useful to show the importance of spare disks. When deploying spare disks in a pool in a system, if any disk fails, the spare disk will take its place automatically, and data availability won't be impacted.</p><p class="calibre7">In the ZFS framework, spare disks are configured per storage pool, and after the appropriate configuration, even when a disk fails, nothing is necessary. The ZFS makes the entire replacement job automatic.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec58" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">This recipe requires a virtual machine (VirtualBox or VMware) that runs Oracle Solaris 11 with 4 GB RAM and at least eight disks of 4 GB each.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec59" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">A real situation using spare disks is where there's a mirrored pool, so to simulate this scenario, let's execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create mir_pool4 mirror c8t3d0 c8t4d0</strong>
</pre></div><p class="calibre7">Adding spare disks in this pool is done by executing the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool add mir_pool4 spare c8t5d0 c8t6d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool4</strong>
  pool: mir_pool4
 state: ONLINE
  scan: none requested
config:

   NAME        STATE     READ WRITE CKSUM
   mir_pool4   ONLINE       0     0     0
     mirror-0  ONLINE       0     0     0
       c8t3d0  ONLINE       0     0     0
       c8t4d0  ONLINE       0     0     0
   spares
     c8t5d0    AVAIL   
     c8t6d0    AVAIL   </pre></div><p class="calibre7">As we mentioned earlier, spare disks will be used only when something wrong happens to the disks. To test the <a id="id330" class="calibre1"/>environment with spare disks, a good practice is shutting down Oracle Solaris 11 (<code class="email">shutdown –y –g0</code>), removing the <code class="email">c8t3d0</code> disk (SCSI slot 3) from the virtual machine's configuration, and turning on the virtual machine again. The <a id="id331" class="calibre1"/>status of <code class="email">mir_pool4</code> presented by Oracle Solaris 11 is as follows:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool4</strong>
  pool: mir_pool4
 state: DEGRADED
status: One or more devices are unavailable in response to persistent errors.
  Sufficient replicas exist for the pool to continue functioning in a
  degraded state.
action: Determine if the device needs to be replaced, and clear the errors
  using 'zpool clear' or 'fmadm repaired', or replace the device
  with 'zpool replace'.
  Run 'zpool status -v' to see device specific details.
  scan: resilvered 94K in 0h0m with 0 errors on Sat Dec 14 18:00:26 2013
config:

   NAME          STATE     READ WRITE CKSUM
   mir_pool4     DEGRADED     0     0     0
     mirror-0    DEGRADED     0     0     0
       spare-0   DEGRADED     0     0     0
         c8t3d0  UNAVAIL      0     0     0
         c8t5d0  ONLINE       0     0     0
       c8t4d0    ONLINE       0     0     0
   spares
     c8t5d0      INUSE   
     c8t6d0      AVAIL   

errors: No known data errors</pre></div><p class="calibre7">Perfect! The disk that was removed is being shown as unavailable (<code class="email">UNAVAIL</code>), and the <code class="email">c8t5d0</code> spare disk has taken its place (<code class="email">INUSE</code>). The pool is shown as <code class="email">DEGRADED</code> to notify the administrator that a main disk is facing problems.</p><p class="calibre7">Finally, let's return to the configuration—power off the virtual machine, reinsert the removed disk again to the same SCSI slot 3, and power on the virtual machine. After completing all the steps, run <a id="id332" class="calibre1"/>the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool4</strong>
  pool: mir_pool4
 state: ONLINE
  scan: resilvered 27K in 0h0m with 0 errors on Sat Dec 14 16:49:29 2013
config:

   NAME          STATE     READ WRITE CKSUM
   mir_pool4     ONLINE       0     0     0
     mirror-0    ONLINE       0     0     0
       spare-0   ONLINE       0     0     0
         c8t3d0  ONLINE       0     0     0
         c8t5d0  ONLINE       0     0     0
       c8t4d0    ONLINE       0     0     0
   spares
     c8t5d0      INUSE   
     c8t6d0      AVAIL   

errors: No known data errors</pre></div><p class="calibre7">According to the output, the <code class="email">c8d5d0</code> spare disk continues to show its status as <code class="email">INUSE</code> even when the <code class="email">c8t3d0</code> disk is online again. To signal to the spare disk that <code class="email">c8t3d0</code> is online again before Oracle Solaris <a id="id333" class="calibre1"/>updates it, execute the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool online mir_pool4 c8t3d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool4</strong>
  pool: mir_pool4
 state: ONLINE
  scan: resilvered 27K in 0h0m with 0 errors on Sat Dec 14 16:49:29 2013
config:

   NAME        STATE     READ WRITE CKSUM
   mir_pool4   ONLINE       0     0     0
     mirror-0  ONLINE       0     0     0
       c8t3d0  ONLINE       0     0     0
       c8t4d0  ONLINE       0     0     0
   spares
     c8t5d0    AVAIL   
     c8t6d0    AVAIL   

errors: No known data errors</pre></div><p class="calibre7">ZFS is amazing. Initially, the <code class="email">c8t3d0</code> disk has come online again, but the <code class="email">c8t5d0</code> spare disk was still in use (<code class="email">INUSE</code>). Afterwards, we ran the <code class="email">zpool online mir_pool4 c8t3d0</code> command to confirm <a id="id334" class="calibre1"/>the online status of <code class="email">c8t3d0</code>, and the spare disk (<code class="email">c8t5d0</code>) became available and started acting as a spare disk.</p><p class="calibre7">Finally, remove the <a id="id335" class="calibre1"/>spare disk by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool remove mir_pool4 c8t5d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool status mir_pool4</strong>
  pool: mir_pool4
 state: ONLINE
  scan: resilvered 27K in 0h0m with 0 errors on Sat Dec 14 16:49:29 2013
config:

   NAME        STATE     READ WRITE CKSUM
   mir_pool4   ONLINE       0     0     0
     mirror-0  ONLINE       0     0     0
       c8t3d0  ONLINE       0     0     0
       c8t4d0  ONLINE       0     0     0
   spares
     c8t6d0    AVAIL   </pre></div><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec28" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">In this section, you saw how to configure spare disks, and some experiments were done to explain its exact working.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec39" class="calibre1"/>Handling ZFS snapshots and clones</h1></div></div></div><p class="calibre7">ZFS snapshot is a <a id="id336" class="calibre1"/>complex theme that can have its functionality extended <a id="id337" class="calibre1"/>using the hold and release operations. Additionally, other tasks such as renaming snapshots, promoting clones, and executing differential <a id="id338" class="calibre1"/>snapshots are crucial in daily administration. All <a id="id339" class="calibre1"/>these points will be covered in this recipe.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec60" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">This recipe can be followed using a virtual machine (VirtualBox or VMware) with 4 GB RAM, a running Oracle Solaris 11 application, and at least eight disks with 4 GB each.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec61" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">From what we learned in the previous recipes, let's create a pool and a filesystem, and populate this filesystem with <a id="id340" class="calibre1"/>any data (readers can copy any data into this filesystem) <a id="id341" class="calibre1"/>and two snapshots by executing the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create simple_pool_1 c8t3d0</strong>
root@solaris11-1:~# <strong class="calibre8">zfs create simple_pool_1/zfs1</strong>
root@solaris11-1:~# <strong class="calibre8">cp -r /root/mhvtl-* /root/john* /simple_pool_1/zfs1 </strong>
root@solaris11-1:~# <strong class="calibre8">zpool list simple_pool_1</strong>
NAME            SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
simple_pool_1  3.97G  63.1M  3.91G   1%  1.00x  ONLINE  -

root@solaris11-1:~# <strong class="calibre8">zfs snapshot simple_pool_1/zfs1@today</strong>
root@solaris11-1:~# <strong class="calibre8">zfs snapshot simple_pool_1/zfs1@today_2</strong>
root@solaris11-1:~# <strong class="calibre8">zpool set listsnapshots=on simple_pool_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r simple_pool_1</strong>
NAME                         USED  AVAIL  REFER  MOUNTPOINT
simple_pool_1               63.2M  3.85G    32K  /simple_pool_1
simple_pool_1/zfs1          63.1M  3.85G  63.1M  /simple_pool_1/zfs1
simple_pool_1/zfs1@today        0      -  63.1M  -
simple_pool_1/zfs1@today_2      0      -  63.1M  -</pre></div><p class="calibre7">Deleting a snapshot is easy as <a id="id342" class="calibre1"/>we already saw it previously in the chapter, and <a id="id343" class="calibre1"/>if it's necessary, it can be done by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs destroy simple_pool_1/zfs1@today_2</strong>
</pre></div><p class="calibre7">Like the operation of removing a snapshot, renaming it is done by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs rename simple_pool_1/zfs1@today simple_pool_1/zfs1@today_2</strong>
</pre></div><p class="calibre7">Both actions (renaming and destroying) are common operations that are done when handling snapshots. Nonetheless, the big question that comes up is whether it would be possible to prevent a snapshot from being deleted. This is where a new snapshot operation named <code class="email">hold</code> can help us. When a snapshot is put in <code class="email">hold</code> status, it can't be removed. This behavior can be configured by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list -r simple_pool_1</strong>
NAME                         USED  AVAIL  REFER  MOUNTPOINT
simple_pool_1               63.1M  3.85G    32K  /simple_pool_1
simple_pool_1/zfs1          63.1M  3.85G  63.1M  /simple_pool_1/zfs1
simple_pool_1/zfs1@today_2      0      -  63.1M  -
root@solaris11-1:~# <strong class="calibre8">zfs hold keep simple_pool_1/zfs1@today_2</strong>
</pre></div><p class="calibre7">To list the snapshots on hold, execute the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs holds simple_pool_1/zfs1@today_2</strong>
NAME                        TAG   TIMESTAMP                 
simple_pool_1/zfs1@today_2  keep  Sat Dec 14 21:51:26 2013  
root@solaris11-1:~# <strong class="calibre8">zfs destroy simple_pool_1/zfs1@today_2</strong>
cannot destroy 'simple_pool_1/zfs1@today_2': snapshot is busy
root@solaris11-1:~#</pre></div><p class="calibre7">Through the <code class="email">zfs hold keep</code> command, the snapshot was left in suspension, and afterwards, we tried to remove it <a id="id344" class="calibre1"/>without success because of the hold. If there <a id="id345" class="calibre1"/>were other descendants from the <code class="email">simple_pool/zfs1</code> filesystem, it would be possible to hold all of them by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs hold –r keep simple_pool_1/zfs1@today_2</strong>
</pre></div><p class="calibre7">An important detail must <a id="id346" class="calibre1"/>be reinforced here—a snapshot can only be destroyed <a id="id347" class="calibre1"/>when it's released, and there's a property named <code class="email">userrefs</code> that tells whether the snapshot is being held or not. Using this information, the releasing and destruction operations can be executed in a row by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get userrefs simple_pool_1/zfs1@today_2</strong>
NAME                        PROPERTY  VALUE  SOURCE
simple_pool_1/zfs1@today_2  userrefs  1   
root@solaris11-1:~# <strong class="calibre8">zfs release keep simple_pool_1/zfs1@today_2</strong>
root@solaris11-1:~# <strong class="calibre8">zfs get userrefs simple_pool_1/zfs1@today_2</strong>
NAME                        PROPERTY  VALUE  SOURCE
simple_pool_1/zfs1@today_2  userrefs  0      -
root@solaris11-1:~# <strong class="calibre8">zfs destroy simple_pool_1/zfs1@today_2</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r simple_pool_1</strong>             
NAME                 USED  AVAIL  REFER  MOUNTPOINT
simple_pool_1       63.2M  3.85G    32K  /simple_pool_1
simple_pool_1/zfs1  63.1M  3.85G  63.1M  /simple_pool_1/zfs1</pre></div><p class="calibre7">Going a little further, Oracle Solaris 11 allows us to determine what has changed in a filesystem when comparing two snapshots. To understand how it works, the first step is to take a new snapshot named <code class="email">snap_1</code>. Afterwards, we have to alter the content of the <code class="email">simple_pool/zfs1</code> filesystem to take a new snapshot (<code class="email">snap_2</code>) and determine what has changed in the filesystem. The entire procedure is accomplished by executing the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list -r simple_pool_1</strong>
NAME                 USED  AVAIL  REFER  MOUNTPOINT
simple_pool_1       63.2M  3.85G    32K  /simple_pool_1
simple_pool_1/zfs1  63.1M  3.85G  63.1M  /simple_pool_1/zfs1
root@solaris11-1:~# <strong class="calibre8">zfs snapshot simple_pool_1/zfs1@snap1</strong>
root@solaris11-1:~# <strong class="calibre8">cp /etc/hosts /simple_pool_1/zfs1/</strong>
root@solaris11-1:~# <strong class="calibre8">zfs snapshot simple_pool_1/zfs1@snap2</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r simple_pool_1</strong>
NAME                       USED  AVAIL  REFER  MOUNTPOINT
simple_pool_1             63.4M  3.84G    32K  /simple_pool_1
simple_pool_1/zfs1        63.1M  3.84G  63.1M  /simple_pool_1/zfs1
simple_pool_1/zfs1@snap1    32K      -  63.1M  -
simple_pool_1/zfs1@snap2      0      -  63.1M  -</pre></div><p class="calibre7">The following command is the most important from this procedure because it takes the differential snapshot:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs diff simple_pool_1/zfs1@snap1 simple_pool_1/zfs1@snap2</strong>
M  /simple_pool_1/zfs1/
+  /simple_pool_1/zfs1/hosts
root@solaris11-1:~#</pre></div><p class="calibre7">The previous command <a id="id348" class="calibre1"/>has shown that the new file in <code class="email">/simple_pool_1/zfs1</code> <a id="id349" class="calibre1"/>is the <code class="email">hosts</code> file, and it was expected according to our previous setup. The <code class="email">+</code> identifier indicates that a file or directory was added, the <code class="email">-</code> identifier indicates that a file or directory was removed, the <code class="email">M</code> identifier indicates that a file or directory was modified, and the <code class="email">R</code> identifier indicates that a file or directory was renamed.</p><p class="calibre7">Now that we are reaching the <a id="id350" class="calibre1"/>end of this section, we should remember that earlier <a id="id351" class="calibre1"/>in this chapter, we reviewed how to make a clone from a snapshot, but not all operations were shown. The fact about clone is that it is possible to promote it to a normal filesystem and, eventually, remove the original filesystem (if necessary) because there isn't a clone as a descendant anymore. Let's verify the preceding sentence by running the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs snapshot simple_pool_1/zfs1@snap3</strong>
root@solaris11-1:~# <strong class="calibre8">zfs clone simple_pool_1/zfs1@snap3 simple_pool_1/zfs1_clone1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r simple_pool_1</strong>
NAME                        USED  AVAIL  REFER  MOUNTPOINT
simple_pool_1              63.3M  3.84G    33K  /simple_pool_1
simple_pool_1/zfs1         63.1M  3.84G  63.1M  /simple_pool_1/zfs1
simple_pool_1/zfs1@snap1     32K      -  63.1M  -
simple_pool_1/zfs1@snap2       0      -  63.1M  -
simple_pool_1/zfs1@snap3       0      -  63.1M  -
simple_pool_1/zfs1_clone1    25K  3.84G  63.1M  /simple_pool_1/zfs1_clone1</pre></div><p class="calibre7">Until this point, everything is okay. The next command shows us that <code class="email">simple_pool_1/zfs1_clone</code> is indeed a clone:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get origin simple_pool_1/zfs1_clone1</strong>
NAME                       PROPERTY  VALUE                     SOURCE
simple_pool_1/zfs1_clone1  origin    simple_pool_1/zfs1@snap3  -</pre></div><p class="calibre7">The next command <a id="id352" class="calibre1"/>promotes the existing clone to an independent <a id="id353" class="calibre1"/>filesystem:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs promote simple_pool_1/zfs1_clone1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r simple_pool_1</strong>
NAME                              USED  AVAIL  REFER  MOUNTPOINT
simple_pool_1                    63.3M  3.84G    33K  /simple_pool_1
simple_pool_1/zfs1                   0  3.84G  63.1M  /simple_pool_1/zfs1
simple_pool_1/zfs1_clone1        63.1M  3.84G  63.1M  /simple_pool_1/zfs1_clone1
simple_pool_1/zfs1_clone1@snap1    32K      -  63.1M  -
simple_pool_1/zfs1_clone1@snap2      0      -  63.1M  -
simple_pool_1/zfs1_clone1@snap3      0      -  63.1M  -
root@solaris11-1:~# <strong class="calibre8">zfs get origin simple_pool_1/zfs1_clone1</strong>
NAME                       PROPERTY  VALUE  SOURCE
simple_pool_1/zfs1_clone1  origin    -      -
root@solaris11-1:~#</pre></div><p class="calibre7">We're able to prove that <code class="email">simple_pool_1/zfs1_clone1</code> is a new filesystem because the clone didn't require any <a id="id354" class="calibre1"/>space (size of <code class="email">25K</code>), and the recently promoted clone to filesystem takes 63.1M <a id="id355" class="calibre1"/>now. Moreover, the <code class="email">origin</code> property doesn't point to a snapshot object anymore.</p><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec29" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">This section has explained how to create, destroy, hold, and release a snapshot, as well as how to promote a clone to a real filesystem. Furthermore, you saw how to determine the difference between two snapshots.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec40" class="calibre1"/>Playing with COMSTAR</h1></div></div></div><p class="calibre7">
<strong class="calibre8">Common Protocol SCSI Target</strong> (<strong class="calibre8">COMSTAR</strong>) is a framework that was introduced in Oracle Solaris 11; this makes it possible for Oracle Solaris 11 to access disks in another system that is running any operating system (Oracle Solaris, Oracle Enterprise Linux, and so on). This <a id="id356" class="calibre1"/>access <a id="id357" class="calibre1"/>happens through the network using protocols such as <strong class="calibre8">iSCSI</strong>, <strong class="calibre8">Fibre </strong>
<a id="id358" class="calibre1"/>
<strong class="calibre8">Channel over Ethernet</strong> (<strong class="calibre8">FCoE</strong>), or <a id="id359" class="calibre1"/>
<strong class="calibre8">Fibre Channel</strong> (<strong class="calibre8">FC</strong>).</p><p class="calibre7">One big advantage of using COMSTAR is that Oracle Solaris 11 is able to reach the disks on another machine without using a HBA board (very expensive) for an FC channel access. There are also disadvantages such as the fact that dump devices don't support the iSCSI disks offered by COMSTAR and the network infrastructure can become overloaded.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec62" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">This section requires two <a id="id360" class="calibre1"/>virtual machines that run Oracle Solaris 11, both with 4 GB RAM and eight 4 GB disks. Additionally, both virtual machines must be in the same network and have access to each other.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec63" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">A good approach when configuring iSCSI is to have an initial plan, a well-defined list of disks that will be accessed using iSCSI, and to determine which system will be the initiator (<code class="email">solaris11-2</code>) and the target (<code class="email">solaris11-1</code>). Therefore, let's list the existing disks by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">format</strong>
AVAILABLE DISK SELECTIONS:
       0. c8t0d0 &lt;VBOX-HARDDISK-1.0-80.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@0,0
       1. c8t1d0 &lt;VBOX-HARDDISK-1.0-16.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@1,0
       2. c8t2d0 &lt;VBOX-HARDDISK-1.0-4.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@2,0
       3. c8t3d0 &lt;VBOX-HARDDISK-1.0-4.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@3,0
       4. c8t4d0 &lt;VBOX-HARDDISK-1.0-4.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@4,0
       5. c8t5d0 &lt;VBOX-HARDDISK-1.0-4.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@5,0
       6. c8t6d0 &lt;VBOX-HARDDISK-1.0-4.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@6,0
       7. c8t8d0 &lt;VBOX-HARDDISK-1.0-4.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@8,0
       8. c8t9d0 &lt;VBOX-HARDDISK-1.0-4.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@9,0
       9. c8t10d0 &lt;VBOX-HARDDISK-1.0-4.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@a,0
      10. c8t11d0 &lt;VBOX-HARDDISK-1.0-4.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@b,0
      11. c8t12d0 &lt;VBOX-HARDDISK-1.0 cyl 2045 alt 2 hd 128 sec 32&gt;
          /pci@0,0/pci1000,8000@14/sd@c,0
   root@solaris11-1:~# <strong class="calibre8">zpool status | grep d0</strong>
    c8t2d0  ONLINE       0     0     0
    c8t1d0  ONLINE       0     0     0
    c8t0d0  ONLINE       0     0     0</pre></div><p class="calibre7">According to the previous two commands, the <code class="email">c8t3d0</code> and <code class="email">c8t12d0</code> disks are available for use. Nevertheless, unfortunately, the COMSTAR software isn't installed in Oracle Solaris 11 by default; we have to install it to use the iSCSI protocol on the <code class="email">solaris11-1</code> system. Consequently, using the IPS framework that was configured in <a class="calibre1" title="Chapter 1. IPS and Boot Environments" href="part0015_split_000.html#page">Chapter 1</a>, <em class="calibre11">IPS and Boot Environments</em>, we <a id="id361" class="calibre1"/>can confirm whether the appropriate package is or isn't installed on the system by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">pkg search storage-server</strong>
INDEX       ACTION VALUE                                PACKAGE
incorporate depend pkg:/storage-server@0.1,5.11-0.133   pkg:/consolidation/osnet/osnet-incorporation@0.5.11-0.175.1.0.0.24.2
pkg.fmri    set    solaris/storage-server               pkg:/storage-server@0.1-0.133
pkg.fmri    set    solaris/storage/storage-server       pkg:/storage/storage-server@0.1-0.173.0.0.0.1.0
pkg.fmri    set    solaris/group/feature/storage-server pkg:/group/feature/storage-server@0.5.11-0.175.1.0.0.24.2
root@solaris11-1:~# <strong class="calibre8">pkg install storage-server</strong>
root@solaris11-1:~# <strong class="calibre8">pkg list storage-server</strong>
NAME (PUBLISHER)                       VERSION                    IFO
group/feature/storage-server           0.5.11-0.175.1.0.0.24.2    i—
root@solaris11-1:~# <strong class="calibre8">pkg info storage-server</strong>
</pre></div><p class="calibre7">The iSCSI target feature was installed through a package named <code class="email">storage-server</code>, but the feature is only enabled if the <code class="email">stmf</code> service is also enabled. Therefore, let's enable the service by executing the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">svcs -a | grep stmf</strong>
disabled        09:11:13 svc:/system/stmf:default
root@solaris11-1:~# <strong class="calibre8">svcadm enable svc:/system/stmf:default</strong>
root@solaris11-1:~# <strong class="calibre8">svcs -a | grep stmf</strong>
online          09:14:19 svc:/system/stmf:default</pre></div><p class="calibre7">At this point, the system is ready to be configured as an iSCSI target. Before proceeding, let's learn a new concept about ZFS.</p><p class="calibre7">ZFS has a nice feature named ZFS volumes that represent and work as block devices. ZFS volumes are identified as devices in <code class="email">/dev/zvol/dsk/rdsk/pool/[volume_name]</code>. The other nice thing about ZFS volumes is that after they are created, the size of the volume is reserved in the pool.</p><p class="calibre7">It's necessary to create a ZFS <a id="id362" class="calibre1"/>volume and, afterwards, a <strong class="calibre8">Logical Unit</strong> (<strong class="calibre8">LUN</strong>) from this ZFS volume to use iSCSI <a id="id363" class="calibre1"/>in Oracle Solaris 11. Eventually, less experienced administrators don't know that the LUN concept comes from the storage world (Oracle, EMC, and Hitachi). A storage box presents a volume (configured as raid0, raid1, raid5, and so on) to the operating system, and this volume is known as LUN, but from the operating system's point view, it's only a simple disk.</p><p class="calibre7">So, let's create a ZFS volume. The first step is to create a pool:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create mypool_iscsi c8t5d0</strong>
</pre></div><p class="calibre7">Now, it's time to create a volume (in this case, using a size of 2 GB) by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs create -V 2Gb mypool_iscsi/myvolume</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list mypool_iscsi/myvolume</strong>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
mypool_iscsi/myvolume  2.06G  3.91G    16K  -</pre></div><p class="calibre7">Next, as a requirement to present the volume through the network using iSCSI, it's necessary to create LUN from the <code class="email">mypool_iscsi/myvolume</code> volume:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">stmfadm create-lu /dev/zvol/rdsk/mypool_iscsi/myvolume</strong>
Logical unit created: 600144F0991C8E00000052ADD63B0001

root@solaris11-1:~# <strong class="calibre8">stmfadm list-lu</strong>
LU Name: 600144F0991C8E00000052ADD63B0001</pre></div><p class="calibre7">Our main concern is to make the recently created LUN viewable from any host that needs to access it. So, let's configure the access that is available and permitted from all hosts by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">stmfadm add-view 600144F0991C8E00000052ADD63B0001</strong>
root@solaris11-1:~# <strong class="calibre8">stmfadm list-view -l 600144F0991C8E00000052ADD63B0001</strong>
View Entry: 0
    Host group   : All
    Target Group : All
    LUN          : Auto</pre></div><p class="calibre7">Currently, the iSCSI target service can be disabled; now, it must be checked and enabled if necessary:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">svcs -a | grep target</strong>
disabled       16:48:34 svc:/system/fcoe_target:default
disabled       16:48:34 svc:/system/ibsrp/target:default
disabled       14:30:51 svc:/network/iscsi/target:default
root@solaris11-1:~# <strong class="calibre8">svcadm enable svc:/network/iscsi/target:default</strong>
root@solaris11-1:~# <strong class="calibre8">svcs svc:/network/iscsi/target:default</strong>
STATE          STIME    FMRI
online         14:31:47 svc:/network/iscsi/target:default</pre></div><p class="calibre7">It's important to realize the dependencies from this service by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">svcs -l svc:/network/iscsi/target:default</strong>
fmri         svc:/network/iscsi/target:default
name         iscsi target
enabled      true
state        online
next_state   none
state_time   Sun Dec 15 14:31:47 2013
logfile      /var/svc/log/network-iscsi-target:default.log
restarter    svc:/system/svc/restarter:default
manifest     /lib/svc/manifest/network/iscsi/iscsi-target.xml
dependency   require_any/error svc:/milestone/network (online)
dependency   require_all/none svc:/system/stmf:default (online)</pre></div><p class="calibre7">Now that the iSCSI target <a id="id364" class="calibre1"/>service is enabled, let's create a new iSCSI target. Remember that to access the available disks through the network and using iSCSI, we have to create a target (something like an access port or an iSCSI server) to enable this access. Then, to create a target in the <code class="email">solaris11-1</code> machine, execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">itadm create-target</strong>
Target iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d successfully created
root@solaris11-1:~# <strong class="calibre8">itadm list-target -v</strong>
TARGET NAME                                                  STATE    SESSIONS 
iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d  online   0        
  alias:                -
  auth:                 none (defaults)
  targetchapuser:       -
  targetchapsecret:     unset
  tpg-tags:             default</pre></div><p class="calibre7">The iSCSI target has some important default properties, and one of them determines whether an authentication scheme will be required or not. The following output confirms that authentication (<code class="email">auth</code>) isn't enabled:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">itadm list-defaults</strong>
iSCSI Target Default Properties:
alias:           &lt;none&gt;
auth:            &lt;none&gt;
radiusserver:    &lt;none&gt;
radiussecret:    unset
isns:            disabled
isnsserver:      &lt;none&gt;</pre></div><p class="calibre7">From here, we are handling <a id="id365" class="calibre1"/>two systems—<code class="email">solaris11-1</code> (<code class="email">192.168.1.106</code>), which was configured as the iSCSI target, and <code class="email">solaris11-2</code> (<code class="email">192.168.1.109</code>), which will be used as an initiator. By the way, we should remember that an iSCSI initiator is a kind of iSCS client that's necessary to access iSCSI disks offered by other systems.</p><p class="calibre7">To configure an initiator, the first task is to verify that the iSCSI initiator service and its dependencies are enabled by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">ssh solaris11-2</strong>
Password: 
Last login: Sun Dec 15 14:13:08 2013
Oracle Corporation      SunOS 5.11      11.1    September 2012
root@solaris11-2:~# <strong class="calibre8">svcs -a | grep initiator</strong>
online         12:10:22 svc:/system/fcoe_initiator:default
online         12:10:25 svc:/network/iscsi/initiator:default
root@solaris11-2:~# <strong class="calibre8">svcs -l svc:/network/iscsi/initiator:default</strong>
fmri         svc:/network/iscsi/initiator:default
name         iSCSI initiator daemon
enabled      true
state        online
next_state   none
state_time   Sun Dec 15 12:10:25 2013
logfile      /var/svc/log/network-iscsi-initiator:default.log
restarter    svc:/system/svc/restarter:default
contract_id  89 
manifest     /lib/svc/manifest/network/iscsi/iscsi-initiator.xml
dependency   require_any/error svc:/milestone/network (online)
dependency   require_all/none svc:/network/service (online)
dependency   require_any/error svc:/network/loopback (online)</pre></div><p class="calibre7">The configured initiator has some very interesting properties:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">iscsiadm list initiator-node</strong> 
Initiator node name: iqn.1986-03.com.sun:01:e00000000000.5250ac8e
Initiator node alias: solaris11
        Login Parameters (Default/Configured):
                Header Digest: NONE/-
                Data Digest: NONE/-
                Max Connections: 65535/-
        Authentication Type: NONE
        RADIUS Server: NONE
        RADIUS Access: disabled
        Tunable Parameters (Default/Configured):
                Session Login Response Time: 60/-
                Maximum Connection Retry Time: 180/-
                Login Retry Time Interval: 60/-
        Configured Sessions: 1</pre></div><p class="calibre7">According to the preceding output, <code class="email">Authentication Type</code> is configured to <code class="email">NONE</code>; this is the same configuration for the <a id="id366" class="calibre1"/>target. For now, it's appropriate because both systems must have the same authentication scheme.</p><p class="calibre7">Before the iSCSI configuration procedure, there are three methods to find an iSCSI disk on another system: static, send target, and iSNS. However, while all of them certainly have a specific use for different scenarios, a complete explanation about these methods is out of scope. Therefore, we will choose the <em class="calibre11">send target</em> method that is a kind of automatic mechanism to find iSCSI disks in internal networks.</p><p class="calibre7">To verify the configured method and to enable the send targets methods, execute the following commands:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">iscsiadm list discovery</strong>
Discovery:
        Static: disabled
        Send Targets: disabled
        iSNS: disabled
root@solaris11-2:~# <strong class="calibre8">iscsiadm modify discovery --sendtargets enable</strong> 
root@solaris11-2:~# <strong class="calibre8">iscsiadm list discovery</strong>
Discovery:
        Static: disabled
        Send Targets: enabled
        iSNS: disabled</pre></div><p class="calibre7">The <code class="email">solaris11-1</code> system was configured as an iSCSI target, and we created a LUN in this system to be accessed by the network. On the <code class="email">solaris11-2</code> system (iSCSI initiator), we have to register the iSCSI target system (<code class="email">solaris11-1</code>) to discover which LUNs are available to be accessed. To accomplish these tasks, execute the following commands:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">iscsiadm add discovery-address 192.168.1.106</strong>
root@solaris11-2:~# <strong class="calibre8">iscsiadm list discovery-address</strong>
Discovery Address: 192.168.1.106:3260
root@solaris11-2:~# <strong class="calibre8">iscsiadm list target</strong>
Target: iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d
  Alias: -
  TPGT: 1
  ISID: 4000002a0000
  Connections: 1</pre></div><p class="calibre7">The previous command shows the configured target on the <code class="email">solaris11-1</code> system (first line of the output).</p><p class="calibre7">To confirm the successfully <a id="id367" class="calibre1"/>added target, iSCSI LUNs available from the iSCSI target (<code class="email">solaris11-1</code>) are shown by the following command:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">devfsadm</strong>
root@solaris11-2:~# <strong class="calibre8">format</strong>
Searching for disks...done

AVAILABLE DISK SELECTIONS:
       0. c0t600144F0991C8E00000052ADD63B0001d0 &lt;SUN-COMSTAR-1.0 cyl 1022 alt 2 hd 128 sec 32&gt;
          /scsi_vhci/disk@g600144f0991c8e00000052add63b0001
       1. c8t0d0 &lt;VBOX-HARDDISK-1.0-80.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@0,0

<strong class="calibre8">(truncated output)</strong>
</pre></div><p class="calibre7">The iSCSI volume (presented as a disk for the iSCSI initiator) from the <code class="email">solaris11-1</code> system was found, and it can be used normally as it is a local device. To test it, execute the following command:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">zpool create new_iscsi c0t600144F0991C8E00000052ADD63B0001d0</strong>
root@solaris11-2:~# <strong class="calibre8">zfs create new_iscsi/fs_iscsi</strong>
root@solaris11-2:~# <strong class="calibre8">zfs list -r new_iscsi</strong>
NAME                USED  AVAIL  REFER  MOUNTPOINT
new_iscsi           124K  1.95G    32K  /new_iscsi
new_iscsi/fs_iscsi   31K  1.95G    31K  /new_iscsi/fs_iscsi
root@solaris11-2:~# <strong class="calibre8">zpool status new_iscsi</strong>
  pool: new_iscsi
 state: ONLINE
  scan: none requested
config:

  NAME                                     STATE     READ WRITE CKSUM
  new_iscsi                                ONLINE       0     0     0
    c0t600144F0991C8E00000052ADD63B0001d0  ONLINE       0     0     0</pre></div><p class="calibre7">Normally, this configuration (without authentication) is the configuration that we'll see in most companies, although it isn't recommended.</p><p class="calibre7">Some businesses require that all data communication be authenticated, requiring both the iSCSI target and initiator to be configured with an authentication scheme where a password is set on the iSCSi target (<code class="email">solaris11-1</code>), forcing the same credential to be set on the iSCSI initiator (<code class="email">solaris11-2</code>).</p><p class="calibre7">When managing authentication, it's possible to configure the iSCSI authentication scheme using the CHAP method (unidirectional or bidirectional) or even RADIUS. As an example, we're going to use CHAP unidirectional where the client (<code class="email">solaris 11-2</code>, the iSCSI initiator) executes the <a id="id368" class="calibre1"/>login to the server (<code class="email">solaris11-1</code>, the iSCSI target) to access the iSCSI target devices (LUNs or, at the end, ZFS volumes). However, if a bidirectional authentication was used, both the target and initiator should present a CHAP password to authenticate each other.</p><p class="calibre7">On the <code class="email">solaris11-1</code> system, list the current target's configuration by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">itadm list-target</strong>
TARGET NAME                                                  STATE    SESSIONS 
iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d  online   1    
root@solaris11-1:~# <strong class="calibre8">itadm list-target iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d –v</strong>
TARGET NAME                                                  STATE    SESSIONS 
iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d  online   1        
  alias:                -
  auth:                 none (defaults)
  targetchapuser:       -
  targetchapsecret:     unset
  tpg-tags:             default</pre></div><p class="calibre7">According to the output, currently, the authentication isn't configured to use the CHAP authentication. Therefore, it can be done by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">itadm modify-target -a chap iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d</strong>
Target iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d successfully modified</pre></div><p class="calibre7">That's great, but there isn't any enabled password to make the authentication happen. Thus, we have to set a password (<code class="email">packt1234567</code>) to complete the target configuration. By the way, the password is long because the CHAP password must have 12 characters at least:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">itadm modify-target -s iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d</strong>
Enter CHAP secret: <strong class="calibre8">packt1234567</strong>
Re-enter secret: <strong class="calibre8">packt1234567</strong>
Target iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d successfully modified</pre></div><p class="calibre7">On the <code class="email">solaris11-2</code> system, the <a id="id369" class="calibre1"/>CHAP authentication must be set up to make it possible for the initiator to log in to the target; now, execute the following command:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">iscsiadm list initiator-node</strong>
Initiator node name: iqn.1986-03.com.sun:01:e00000000000.5250ac8e
Initiator node alias: solaris11
        Login Parameters (Default/Configured):
                Header Digest: NONE/-
                Data Digest: NONE/-
                Max Connections: 65535/-
        Authentication Type: NONE
        RADIUS Server: NONE
        RADIUS Access: disabled
        Tunable Parameters (Default/Configured):
                Session Login Response Time: 60/-
                Maximum Connection Retry Time: 180/-
                Login Retry Time Interval: 60/-
        Configured Sessions: 1</pre></div><p class="calibre7">On the <code class="email">solaris11-2</code> system (initiator), we have to confirm that it continues using the iSCSI dynamic discovery (<code class="email">sendtargets</code>):</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">iscsiadm list discovery</strong>
Discovery:
  Static: disabled
  Send Targets: enabled
  iSNS: disabled</pre></div><p class="calibre7">The same password from the target (<code class="email">packt1234567</code>) must be set on the <code class="email">solaris11-2</code> system (initiator). Moreover, the CHAP authentication also must be configured by running the following command:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">iscsiadm modify initiator-node --CHAP-secret</strong>
Enter secret: <strong class="calibre8">packt1234567</strong>
Re-enter secret: <strong class="calibre8">packt1234567</strong>
root@solaris11-2:~# <strong class="calibre8">iscsiadm modify initiator-node --authentication CHAP</strong>
</pre></div><p class="calibre7">Verifying the authentication <a id="id370" class="calibre1"/>configuration from the initiator node and available targets can be done using the following command:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">iscsiadm list initiator-node</strong>
Initiator node name: iqn.1986-03.com.sun:01:e00000000000.5250ac8e
Initiator node alias: solaris11
        Login Parameters (Default/Configured):
                Header Digest: NONE/-
                Data Digest: NONE/-
                Max Connections: 65535/-
        Authentication Type: CHAP
                CHAP Name: iqn.1986-03.com.sun:01:e00000000000.5250ac8e
        RADIUS Server: NONE
        RADIUS Access: disabled
        Tunable Parameters (Default/Configured):
                Session Login Response Time: 60/-
                Maximum Connection Retry Time: 180/-
                Login Retry Time Interval: 60/-
        Configured Sessions: 1

root@solaris11-2:~# <strong class="calibre8">iscsiadm list discovery-address</strong>
Discovery Address: 192.168.1.106:3260
root@solaris11-2:~# <strong class="calibre8">iscsiadm list target</strong> 
Target: iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d
        Alias: -
        TPGT: 1
        ISID: 4000002a0000
        Connections: 1</pre></div><p class="calibre7">Finally, we have to update the device tree configuration using the <code class="email">devfsadm</code> command to confirm that the target is available for the initiator (<code class="email">solaris11-2</code>) access. If everything has gone well, the iSCSI disk will be visible using the <code class="email">format</code> command:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">devfsadm</strong>
root@solaris11-2:~# <strong class="calibre8">format</strong>
Searching for disks...done

AVAILABLE DISK SELECTIONS:
       0. c0t600144F0991C8E00000052ADD63B0001d0 &lt;SUN-COMSTAR-1.0-2.00GB&gt;
          /scsi_vhci/disk@g600144f0991c8e00000052add63b0001
       1. c8t0d0 &lt;VBOX-HARDDISK-1.0-80.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@0,0
<strong class="calibre8">(truncated output)</strong>
</pre></div><p class="calibre7">As a simple example, the following commands create a pool and filesystem using the iSCSI disk that was discovered and configured in the previous steps:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">zpool create new_iscsi_chap c0t600144F0991C8E00000052ADD63B0001d0</strong>
root@solaris11-2:~# <strong class="calibre8">zfs create new_iscsi_chap/zfs1</strong>
root@solaris11-2:~# <strong class="calibre8">zfs list -r new_iscsi_chap</strong> 
NAME                 USED  AVAIL  REFER  MOUNTPOINT
new_iscsi_chap       124K  1.95G    32K  /new_iscsi_chap
new_iscsi_chap/zfs1   31K  1.95G    31K  /new_iscsi_chap/zfs1
root@solaris11-2:~# <strong class="calibre8">zpool list new_iscsi_chap</strong>
NAME             SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
new_iscsi_chap  1.98G   124K  1.98G   0%  1.00x  ONLINE  -
root@solaris11-2:~# <strong class="calibre8">zpool status new_iscsi_chap</strong>
  pool: new_iscsi_chap
 state: ONLINE
  scan: none requested
config:

  NAME                                   STATE     READ WRITE CKSUM
  new_iscsi_chap                         ONLINE       0     0     0
  c0t600144F0991C8E00000052ADD63B0001d0  ONLINE       0     0     0</pre></div><p class="calibre7">Great! The iSCSI configuration with the CHAP authentication has worked smoothly. Now, to consolidate all the <a id="id371" class="calibre1"/>acquired knowledge, the following commands undo all the iSCSI configurations, first on the initiator (<code class="email">solaris11-2</code>) and afterwards on the target (<code class="email">solaris11-1</code>), as follows:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">zpool destroy new_iscsi_chap</strong>
root@solaris11-2:~# <strong class="calibre8">iscsiadm list initiator-node</strong>
Initiator node name: iqn.1986-03.com.sun:01:e00000000000.5250ac8e
Initiator node alias: solaris11
        Login Parameters (Default/Configured):
                Header Digest: NONE/-
                Data Digest: NONE/-
                Max Connections: 65535/-
        Authentication Type: CHAP
                CHAP Name: iqn.1986-03.com.sun:01:e00000000000.5250ac8e
        RADIUS Server: NONE
        RADIUS Access: disabled
        Tunable Parameters (Default/Configured):
                Session Login Response Time: 60/-
                Maximum Connection Retry Time: 180/-
                Login Retry Time Interval: 60/-
        Configured Sessions: 1

root@solaris11-2:~# <strong class="calibre8">iscsiadm remove discovery-address 192.168.1.106</strong>
root@solaris11-2:~# i<strong class="calibre8">scsiadm modify initiator-node --authentication none</strong>
root@solaris11-2:~# <strong class="calibre8">iscsiadm list initiator-node</strong>
Initiator node name: iqn.1986-03.com.sun:01:e00000000000.5250ac8e
Initiator node alias: solaris11
        Login Parameters (Default/Configured):
                Header Digest: NONE/-
                Data Digest: NONE/-
                Max Connections: 65535/-
        Authentication Type: NONE
        RADIUS Server: NONE
        RADIUS Access: disabled
        Tunable Parameters (Default/Configured):
                Session Login Response Time: 60/-
                Maximum Connection Retry Time: 180/-
                Login Retry Time Interval: 60/-
        Configured Sessions: 1</pre></div><p class="calibre7">By updating the device <a id="id372" class="calibre1"/>tree (the <code class="email">devfsadm</code> and <code class="email">format</code> commands), we can see that the iSCSI disk has disappeared:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">devfsadm</strong>
root@solaris11-2:~# <strong class="calibre8">format</strong>
Searching for disks...done

AVAILABLE DISK SELECTIONS:
       0. c8t0d0 &lt;VBOX-HARDDISK-1.0-80.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@0,0
       
<strong class="calibre8">(truncated output)</strong>
</pre></div><p class="calibre7">Now, the unconfiguring process must be done on the target (<code class="email">solaris11-2</code>). First, list the existing LUNs:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">stmfadm list-lu</strong>
LU Name: 600144F0991C8E00000052ADD63B0001</pre></div><p class="calibre7">Remove the existing LUN:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">stmfadm delete-lu 600144F0991C8E00000052ADD63B0001</strong>
</pre></div><p class="calibre7">List the currently configured targets:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">itadm list-target -v</strong>
TARGET NAME                                                  STATE    SESSIONS 
iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d  online   0        
  alias:                -
  auth:                 chap 
  targetchapuser:       -
  targetchapsecret:     set
  tpg-tags:             default</pre></div><p class="calibre7">Delete the existing targets:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">itadm delete-target -f iqn.1986-03.com.sun:02:51d113f3-39a0-cead-e602-ea9aafdaad3d</strong>
root@solaris11-1:~# <strong class="calibre8">itadm list-target –v</strong>
</pre></div><p class="calibre7">Destroy the pool that contains the iSCSI disk:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool destroy mypool_iscsi</strong>
</pre></div><p class="calibre7">Finally, we did it. There isn't an iSCSI configuration anymore.</p><p class="calibre7">A few months ago, I wrote a <a id="id373" class="calibre1"/>tutorial that explains how to configure a free VTL software that emulates a tape robot, and at the end of document, I explained how to connect to this VTL from Oracle Solaris 11 using the iSCSI protocol. It's very interesting to see a real case about how to use the iSCSI initiator to access an external application. Check the references at the end of this chapter to learn more about this VTL document.</p><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec30" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">In this section, you learned about all the iSCSI configurations using COMSTAR with and without the CHAP authentication. Moreover, the undo configuration steps were also provided.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec41" class="calibre1"/>Mirroring the root pool</h1></div></div></div><p class="calibre7">Nowadays, systems <a id="id374" class="calibre1"/>running very critical applications without a working mirrored boot disk is something unthinkable. However, when working with ZFS, the mirroring process of the boot disk is smooth and requires few steps to accomplish it.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec64" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">To follow this recipe, it's necessary to have a virtual machine (VirtualBox or VMware) that runs Oracle Solaris 11 with 4 GB RAM and a disk the same size as the existing boot disk. This example uses an 80 GB disk.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec65" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">Before thinking about boot disk mirroring, the first thing to do is check is the <code class="email">rpool</code> health:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool status rpool</strong>
  pool: rpool
 state: ONLINE
  scan: none requested
config:

  NAME      STATE     READ WRITE CKSUM
  rpool     ONLINE       0     0     0
    c8t0d0  ONLINE       0     0     0</pre></div><p class="calibre7">According to this output, <code class="email">rpool</code> is healthy, so the next step is to choose a disk with a size that is equal to or bigger <a id="id375" class="calibre1"/>than the original <code class="email">rpool</code> disk. Then, we need to call the <code class="email">format</code> tool and prepare it to receive the same data from the original disk as follows:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">format</strong>
Searching for disks...done

AVAILABLE DISK SELECTIONS:
       0. c8t0d0 &lt;VBOX-HARDDISK-1.0-80.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@0,0
       1. c8t1d0 &lt;VBOX-HARDDISK-1.0-16.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@1,0
       2. c8t2d0 &lt;VBOX-HARDDISK-1.0-4.00GB&gt;
          /pci@0,0/pci1000,8000@14/sd@2,0
       3. c8t3d0 &lt;VBOX-HARDDISK-1.0 cyl 10441 alt 2 hd 255 sec 63&gt;
          /pci@0,0/pci1000,8000@14/sd@3,0
<strong class="calibre8">…..(truncated)</strong>

Specify disk (enter its number): <strong class="calibre8">3</strong>
selecting c8t3d0
[disk formatted]
No Solaris fdisk partition found.

format&gt; <strong class="calibre8">fdisk</strong>
No fdisk table exists. The default partition for the disk is:

  a 100% "SOLARIS System" partition

Type "y" to accept the default partition,  otherwise type "n" to edit the
 partition table.
y
format&gt; <strong class="calibre8">p</strong>
partition&gt; <strong class="calibre8">p</strong>

Current partition table (default):
Total disk cylinders available: 10440 + 2 (reserved cylinders)

Part      Tag   Flag    Cylinders    Size            Blocks
  0 unassigned   wm     0            0         (0/0/0)             0
  1 unassigned   wm     0            0         (0/0/0)             0
  2     backup   wu     0 - 10439   79.97GB    (10440/0/0) 167718600
  
<strong class="calibre8">(truncated output)</strong>
partition&gt; <strong class="calibre8">q</strong>

root@solaris11-1:~#</pre></div><p class="calibre7">Once we've chosen which will be the mirrored disk, the second disk has to be attached to the existing root pool (<code class="email">rpool</code>) <a id="id376" class="calibre1"/>to mirror the boot and system files. Remember that the mirroring process will include all the snapshots from the filesystem under the <code class="email">rpool</code> disk. The mirroring process is initiated by running:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool attach rpool c8t0d0 c8t3d0</strong>
</pre></div><div><h3 class="title2"><a id="note06" class="calibre1"/>Note</h3><p class="calibre7">Make sure that you wait until resilvering is done before rebooting.</p></div><p class="calibre7">To follow the mirroring process, execute the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool status rpool</strong>
  pool: rpool
 state: DEGRADED
status: One or more devices is currently being resilvered.  The pool will
  continue to function in a degraded state.
action: Wait for the resilver to complete.
  Run 'zpool status -v' to see device specific details.
  scan: resilver in progress since Tue Dec 10 02:32:22 2013
    4.19M scanned out of 38.2G at 82.0K/s, 30h42m to go
    4.15M resilvered, 0.02% done
config:

  NAME        STATE     READ WRITE CKSUM
  rpool       DEGRADED     0     0     0
    mirror-0  DEGRADED     0     0     0
      c8t0d0  ONLINE       0     0     0
      c8t3d0  DEGRADED     0     0     0  (resilvering)

errors: No known data errors</pre></div><p class="calibre7">To avoid executing the previous <a id="id377" class="calibre1"/>command several times, it would be simpler to make a script as follows:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">while true</strong>
&gt; do
&gt; zpool status | grep done
&gt; sleep 2
&gt; done
    2.15G resilvered, 5.54% done
    2.19G resilvered, 5.70% done
    
…………
<strong class="calibre8">(truncated output)</strong>
………..
    38.1G resilvered, 99.95% done
    38.2G resilvered, 100.00% done</pre></div><p class="calibre7">Finally, the <code class="email">rpool</code> pool is completely mirrored as follows:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool status rpool</strong>
  pool: rpool
 state: ONLINE
  scan: resilvered 38.2G in 1h59m with 0 errors on Mon Dec 16 08:37:11 2013
config:

  NAME        STATE     READ WRITE CKSUM
  rpool       ONLINE       0     0     0
    mirror-0  ONLINE       0     0     0
      c8t0d0  ONLINE       0     0     0
      c8t3d0  ONLINE       0     0     0</pre></div><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec31" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">After adding the second disk (mirror disk) into the <code class="email">rpool</code> pool and after the entire mirroring process has finished, the system can be booted using the alternative disk (through BIOS, we're able to initialize <a id="id378" class="calibre1"/>the system from the mirrored disk). For example, this example was done using VirtualBox, so the alternative disk can be chosen using the <em class="calibre11">F12</em> key.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec42" class="calibre1"/>ZFS shadowing</h1></div></div></div><p class="calibre7">Most companies have very <a id="id379" class="calibre1"/>heterogeneous environments where some machines are outdated and others are new. Usually, it's required to copy data from the old machine to <a id="id380" class="calibre1"/>a new machine that runs Oracle Solaris 11, and it's a perfect time to use an excellent feature named Shadow Migration. This feature can be used to copy (migrate) data through NFS or locally (between two machines), and the filesystem types that can be used as the origin are UFS, VxFS (from Symantec), and surely, the fantastic ZFS.</p><p class="calibre7">An additional and very attractive characteristic of this feature is the fact that a client application doesn't need to wait for the data migration to be complete at the target, and it can access all data that was already migrated. If the required data wasn't copied to the new machine (target) while being accessed, then ZFS will fail through to the source (original data).</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec66" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">This recipe requires two virtual machines (<code class="email">solaris11-1</code> and <code class="email">solaris11-2</code>) with Oracle Solaris 11 installed and 4 GB RAM each. Furthermore, the example will show you how to migrate data from an existing filesystem (<code class="email">/shadowing_pool/origin_filesystem</code>) in the <code class="email">solaris11-2</code> system (source) to the <code class="email">solaris11-1</code> system (target or destination).</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec67" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">Remember that the source machine is the <code class="email">solaris11-2</code> system (from where the data will be migrated), and the <code class="email">solaris11-1</code> system is the destination or target. Therefore, the first step to handle shadowing is to install the <code class="email">shadow-migration</code> package on the destination machine to where the data will be migrated, by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">pkg install shadow-migration</strong>
</pre></div><p class="calibre7">After the installation of the package, it's suggested that you check whether the shadowing service is enabled, by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">svcs -a | grep shadow</strong>
disabled       18:35:00 svc:/system/filesystem/shadowd:default</pre></div><p class="calibre7">As the shadowing service isn't enabled, run the following command to enable it:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">svcadm enable</strong>
svc:/system/filesystem/shadowd:default</pre></div><p class="calibre7">On the second machine (<code class="email">solaris11-2</code>, the source host), the filesystem to be migrated must be shared in a read-only mode using NFS. Why must it be read-only ? Because the content can't change during the migration.</p><p class="calibre7">Let's set up a test ZFS filesystem to be migrated using Shadow Migration and to make the filesystem read-only:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">zpool create shadowing_pool c8t3d0</strong>
root@solaris11-2:~# <strong class="calibre8">zfs create shadowing_pool/origin_filesystem</strong>
root@solaris11-2:~# <strong class="calibre8">zfs list -r shadowing_pool</strong>
NAME                              USED  AVAIL  REFER  MOUNTPOINT
shadowing_pool                    124K  3.91G    32K  /shadowing_pool
shadowing_pool/origin_filesystem   31K  3.91G    31K  /shadowing_pool/origin_filesystem</pre></div><p class="calibre7">The following command <a id="id381" class="calibre1"/>copies some data (readers can copy anything) to the <code class="email">shadowing_pool/origin_filesystem</code> filesystem from <code class="email">solaris11-2</code> to simulate a real case of migration:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">cp -r * /shadowing_pool/origin_filesystem/</strong>
</pre></div><p class="calibre7">Share the origin filesystem as read-only data (<code class="email">-o ro</code>) using the NFS service by executing the following command:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">share -F nfs -o ro /shadowing_pool/origin_filesystem</strong> 
root@solaris11-2:~# <strong class="calibre8">share</strong>
shadowing_pool_origin_filesystem  /shadowing_pool/origin_filesystem  nfs  sec=sys,ro</pre></div><p class="calibre7">On the first machine (<code class="email">solaris11-1</code>), which is the destination where data will be migrated (copied), check whether the <a id="id382" class="calibre1"/>NFS share is okay and reachable by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">dfshares solaris11-2</strong>
RESOURCE                                   SERVER ACCESS    TRANSPORT
solaris11-2:/shadowing_pool/origin_filesystem  solaris11-2  -</pre></div><p class="calibre7">The system is all in place. The shadowing process is ready to start from the second system (<code class="email">solaris11-2</code>) to the first system (<code class="email">solaris11-1</code>). This process will create the <code class="email">shadowed_pool/shad_filesystem</code> filesystem by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create shadowed_pool c8t3d0</strong>
root@solaris11-1:~# <strong class="calibre8">zfs create -o shadow=nfs://solaris11-2/shadowing_pool/origin_filesystem shadowed_pool/shad_filesystem</strong>
</pre></div><p class="calibre7">The shadowing process can be tracked by running the <code class="email">shadowstat</code> command:</p><div><pre class="programlisting">root@solaris11-1:~/Desktop# <strong class="calibre8">shadowstat</strong>
                               EST
                        BYTES  BYTES    ELAPSED
DATASET                        XFRD    LEFT  ERRORS  TIME
shadowed_pool/shad_filesystem   -       -     -      00:00:13
shadowed_pool/shad_filesystem   -       -     -      00:00:23
shadowed_pool/shad_filesystem   -       -     -      00:00:33
shadowed_pool/shad_filesystem   -       -     -      00:00:43
shadowed_pool/shad_filesystem   -       -     -      00:00:53
shadowed_pool/shad_filesystem   -       -     -      00:01:03
<strong class="calibre8">(truncated output)</strong>
shadowed_pool/shad_filesystem   -       -     -      00:07:33
shadowed_pool/shad_filesystem   -       -     -      00:07:43
shadowed_pool/shad_filesystem   -       -     -      00:07:53
shadowed_pool/shad_filesystem   1.57G   -     -      00:08:03
No migrations in progress</pre></div><p class="calibre7">The finished shadowing task is verified by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~/Desktop# <strong class="calibre8">zfs list -r shadowed_pool</strong> 
NAME                            USED  AVAIL  REFER  MOUNTPOINT
shadowed_pool                  1.58G  2.33G    32K  /shadowed_pool
shadowed_pool/shad_filesystem  1.58G  2.33G  1.58G  /shadowed_pool/shad_filesystem
root@solaris11-1:~/Desktop# <strong class="calibre8">zfs get -r shadow shadowed_pool/shad_filesystem</strong>
NAME                           PROPERTY  VALUE  SOURCE
shadowed_pool/shad_filesystem  shadow    none   -</pre></div><p class="calibre7">The shadowing process worked! Moreover, the same operation is feasible to be accomplished using two <a id="id383" class="calibre1"/>local ZFS filesystems (the previous process was done through NFS between the <code class="email">solaris11-2</code> and <code class="email">solaris11-1</code> systems). Thus, the entire recipe can be repeated to copy some files <a id="id384" class="calibre1"/>to the source filesystem (it can be any data we want) and to start the shadowing activity by running the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs create rpool/shad_source</strong>
root@solaris11-1:~# <strong class="calibre8">cp /root/kali-linux-1.0.5-amd64.iso /root/john* /root/mh* /rpool/shad_source/</strong>
root@solaris11-1:~# <strong class="calibre8">zfs set readonly=on rpool/shad_source</strong>
root@solaris11-1:~# <strong class="calibre8">zfs create -o shadow=file:///rpool/shad_source rpool/shad_target</strong>
root@solaris11-1:~# <strong class="calibre8">shadowstat</strong>
                              EST
        BYTES                 BYTES       ELAPSED
DATASET                       XFRD  LEFT  ERRORS  TIME
rpool/shad_target               -      -      -  00:00:08
rpool/shad_target               -      -      -  00:00:18
rpool/shad_target               -      -      -  00:00:28
rpool/shad_target               -      -      -  00:00:38
rpool/shad_target               -      -      -  00:00:48
rpool/shad_target               -      -      -  00:00:58
rpool/shad_target               -      -      -  00:01:08
rpool/shad_target               -      -      -  00:01:18
rpool/shad_target               -      -      -  00:01:28
rpool/shad_target               -      -      -  00:01:38
rpool/shad_target               -      -      -  00:01:48
rpool/shad_target               -      -      -  00:01:58
rpool/shad_target               -      -      -  00:02:08
rpool/shad_target               -      -      -  00:02:18
rpool/shad_target               -      -      -  00:02:28
rpool/shad_target               1.58G  2.51G  -  00:02:38
rpool/shad_target               1.59G  150M   -  00:02:48
rpool/shad_target               1.59G  8E     -  00:02:58
No migrations in progress</pre></div><p class="calibre7">Everything has worked perfectly as expected, but in this case, we used two local ZFS filesystems instead of using the NFS service. Therefore, the completed process can be checked and finished by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get shadow rpool/shad_source</strong>
NAME               PROPERTY  VALUE  SOURCE
rpool/shad_source  shadow    none   -
root@solaris11-1:~# <strong class="calibre8">zfs set readonly=off rpool/shad_source</strong>
</pre></div><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec32" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">The shadow migration procedure was <a id="id385" class="calibre1"/>explained in two contexts—using a remote filesystem through NFS and using local filesystems. In both cases, it's necessary to set the read-only mode for the source <a id="id386" class="calibre1"/>filesystem. Furthermore, you learned how to monitor the shadowing using <code class="email">shadowstat</code> and even the <code class="email">shadow</code> property.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec43" class="calibre1"/>Configuring ZFS sharing with the SMB share</h1></div></div></div><p class="calibre7">Oracle Solaris 11 <a id="id387" class="calibre1"/>has introduced a new feature that <a id="id388" class="calibre1"/>enables a system to share its filesystems through the <a id="id389" class="calibre1"/>
<strong class="calibre8">Server Message Block</strong> (<strong class="calibre8">SMB</strong>) and <a id="id390" class="calibre1"/>
<strong class="calibre8">Common Internet File System</strong> (<strong class="calibre8">CIFS</strong>) protocols, both being very common in the Windows world. In this section, we're going to configure two filesystems and access these using CIFS.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec68" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">This recipe requires two virtual machines (VMware or VirtualBox) that run Oracle Solaris 11, with 4 GB memory each, and some test disks with 4 GB. Furthermore, we'll require an additional machine that runs Windows (for example, Windows 7) to test the CIFS shares offered by Oracle Solaris 11.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec69" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">To begin the recipe, it's necessary to install the smb service by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">pkg install service/file-system/smb</strong>
</pre></div><p class="calibre7">Let's create a pool and two filesystems inside it by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create cifs_pool c8t4d0</strong>
root@solaris11-1:~# <strong class="calibre8">zfs create cifs_pool/zfs_cifs_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs create cifs_pool/zfs_cifs_2</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r cifs_pool</strong>
NAME                  USED  AVAIL  REFER  MOUNTPOINT
cifs_pool             162K  3.91G    33K  /cifs_pool
cifs_pool/zfs_cifs_1   31K  3.91G    31K  /cifs_pool/zfs_cifs_1
cifs_pool/zfs_cifs_2   31K  3.91G    31K  /cifs_pool/zfs_cifs_2</pre></div><p class="calibre7">Another crucial configuration is to set mandatory locking (the <code class="email">nbmand</code> property) for each filesystem, which will be offered by CIFS, because Unix usually uses advisory locking and SMB uses <a id="id391" class="calibre1"/>mandatory locking. A very quick explanation about these kinds of locks is that an advisory lock doesn't prevent non-cooperating <a id="id392" class="calibre1"/>clients (or processes) from having read or write access to a shared file. On the other hand, mandatory clients prevent any non-cooperating clients (or processes) from having read or write access to shared file.</p><p class="calibre7">We can accomplish this task by running the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs set nbmand=on cifs_pool/zfs_cifs_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs set nbmand=on cifs_pool/zfs_cifs_2</strong>
</pre></div><p class="calibre7">Our initial setup is ready. The following step shares the <code class="email">cifs_pool/zfs_cifs_1</code> and <code class="email">cifs_pool/zfs_cifs_2</code> filesystems through the SMB protocol and configures a share name (<code class="email">name</code>), protocol (<code class="email">prot</code>), and path (<code class="email">file system path</code>). Moreover, a cache client (<code class="email">csc</code>) is also configured to smooth the performance when the filesystem is overused:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs set share=name=zfs_cifs_1,path=/cifs_pool/zfs_cifs_1,prot=smb,csc=auto cifs_pool/zfs_cifs_1</strong>
name=zfs_cifs_1,path=/cifs_pool/zfs_cifs_1,prot=smb,csc=auto
root@solaris11-1:~# <strong class="calibre8">zfs set share=name=zfs_cifs_2,path=/cifs_pool/zfs_cifs_2,prot=smb,csc=auto cifs_pool/zfs_cifs_2</strong>
name=zfs_cifs_2,path=/cifs_pool/zfs_cifs_2,prot=smb,csc=auto</pre></div><p class="calibre7">Finally, to enable the SMB share feature for each filesystem, we must set the <code class="email">sharesmb</code> attribute to <code class="email">on</code>:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs set sharesmb=on cifs_pool/zfs_cifs_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs set sharesmb=on cifs_pool/zfs_cifs_2</strong>
root@solaris11-1:~# <strong class="calibre8">zfs get sharesmb  cifs_pool/zfs_cifs_1</strong>
NAME                  PROPERTY   VALUE  SOURCE
cifs_pool/zfs_cifs_1  share.smb  on     local
root@solaris11-1:~# <strong class="calibre8">zfs get sharesmb  cifs_pool/zfs_cifs_2</strong>
NAME                  PROPERTY   VALUE  SOURCE
cifs_pool/zfs_cifs_2  share.smb  on     local</pre></div><p class="calibre7">The SMB Server <a id="id393" class="calibre1"/>service isn't enabled by default. By the way, the <strong class="calibre8">Service Management Facility</strong> (<strong class="calibre8">SMF</strong>) still wasn't <a id="id394" class="calibre1"/>introduced, but the <code class="email">svcs –a</code> command lists all the installed services and shows which services are online, offline, or disabled. As we are interested only in the <code class="email">smb/server</code> service, we <a id="id395" class="calibre1"/>can use the <code class="email">grep</code> command to filter the target service by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">svcs -a | grep smb/server</strong>
disabled          7:13:51 svc:/network/smb/server:default</pre></div><p class="calibre7">The <code class="email">smb/server</code> service is disabled, and to enable it, you need to execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">svcadm enable -r smb/server</strong>
root@solaris11-1:~# <strong class="calibre8">svcs -a | grep smb</strong>       
online          7:12:50 svc:/network/smb:default
online          7:13:47 svc:/network/smb/client:default
online          7:13:51 svc:/network/smb/server:default</pre></div><p class="calibre7">A suitable test is to list the shares provided by the SMB server either by getting the value of the <code class="email">share</code> filesystem property or by executing the <code class="email">share</code> command as follows:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get share</strong>
NAME                                        PROPERTY  VALUE  SOURCE
cifs_pool/zfs_cifs_1                        share     name=zfs_cifs_1,path=/cifs_pool/zfs_cifs_1,prot=smb,csc=auto  local
cifs_pool/zfs_cifs_2                        share     name=zfs_cifs_2,path=/cifs_pool/zfs_cifs_2,prot=smb,csc=auto  local

root@solaris11-1:~# <strong class="calibre8">share</strong>
IPC$    smb  -  Remote IPC
c$  /var/smb/cvol  smb  -  Default Share
zfs_cifs_1  /cifs_pool/zfs_cifs_1  smb  csc=auto
zfs_cifs_2  /cifs_pool/zfs_cifs_2  smb  csc=auto
root@solaris11-1:~#</pre></div><p class="calibre7">To proceed with a real test that accesses an SMB share, let's create a regular user named <code class="email">aborges</code> and assign a password to him by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">useradd aborges</strong>  
root@solaris11-1:~# <strong class="calibre8">passwd aborges</strong>
New Password: 
Re-enter new Password: 
passwd: password successfully changed for aborges</pre></div><p class="calibre7">The user <code class="email">aborges</code> needs to be enabled in the SMB service, so execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">smbadm enable-user aborges</strong>
aborges is enabled.
root@solaris11-1:~#</pre></div><p class="calibre7">To confirm that the <a id="id396" class="calibre1"/>user <code class="email">aborges</code> was created <a id="id397" class="calibre1"/>and enabled for the SMB service, run the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">smbadm lookup-user aborges</strong>
aborges: S-1-5-21-3351362105-248310137-3301682468-1104</pre></div><p class="calibre7">According to the previous output, a <a id="id398" class="calibre1"/>
<strong class="calibre8">security identifier</strong> (<strong class="calibre8">SID</strong>) was assigned to the user <code class="email">aborges</code>. The next step is to enable the SMB authentication by adding a new library (<code class="email">pam_smb_passwd.so.1</code>) in the authentication scheme by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">vi /etc/pam.d/other</strong> 
……………………..
<strong class="calibre8">(truncated)</strong>
……………………….
password include  pam_authtok_common
password required  pam_authtok_store.so.1
password required  pam_smb_passwd.so.1  nowarn</pre></div><p class="calibre7">The best way to test all the steps until here is to verify that the shares are currently being offered to the other machine (<code class="email">solaris11-2</code>) by running the following command:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">smbadm lookup-server //solaris11-1</strong>
Workgroup: WORKGROUP
Server: SOLARIS11-1
IP address: 192.168.1.119</pre></div><p class="calibre7">To show which shares are available from the <code class="email">solaris11-1</code> host, run the following command:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">smbadm show-shares -u aborges solaris11-1</strong>
Enter password:
c$                  Default Share
IPC$                Remote IPC
zfs_cifs_1          
zfs_cifs_2          
4 shares (total=4, read=4)</pre></div><p class="calibre7">To mount the first ZFS share (<code class="email">zfs_cifs_1</code>) using the SMB service on <code class="email">solaris11-2</code> from <code class="email">solaris11-1</code>, execute the following command:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">mount -o user=aborges -F smbfs //solaris11-1/zfs_cifs_1 /mnt</strong>
</pre></div><p class="calibre7">The mounted filesystem is an SMB filesystem (<code class="email">-F smbfs</code>), and it's easy to check its content by executing the <a id="id399" class="calibre1"/>following commands:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">df -h /mnt</strong>
Filesystem             Size   Used  Available Capacity  Mounted on
//solaris11-1/zfs_cifs_1
                       3.9G    40K       3.9G     1%    /mnt
root@solaris11-2:~# <strong class="calibre8">ls -l /mnt</strong>
total 10
-rwxr-x---+  1 2147483649 2147483650     893 Dec 17 21:04 zfsslower.d
-rwxr-x---+  1 2147483649 2147483650     956 Dec 17 21:04 zfssnoop.d
-rwxr-x---+  1 2147483649 2147483650     466 Dec 17 21:04 zioprint.d
-rwxr-x---+  1 2147483649 2147483650    1255 Dec 17 21:04 ziosnoop.d
-rwxr-x---+  1 2147483649 2147483650     650 Dec 17 21:04 ziotype.d</pre></div><p class="calibre7">SMB is very common in <a id="id400" class="calibre1"/>Windows environments, and then, it would be nice to access these shares from a Windows machine (Windows 7 in this case) by accessing the network shares by going to the <strong class="calibre8">Start</strong> menu and typing <code class="email">\\192.168.1.119</code> as shown in the following screenshot:</p><div><img src="img/00008.jpeg" alt="How to do it…" class="calibre12"/></div><p class="calibre13"> </p><p class="calibre7">From the previous screenshot, there are two shares being offered to us: <code class="email">zfs_cifs_1</code> and <code class="email">zfs_cifs_2</code>. Therefore, we can try to access one of them by double-clicking it and filling out the credentials as shown in the following screenshot:</p><div><img src="img/00009.jpeg" alt="How to do it…" class="calibre12"/></div><p class="calibre13"> </p><p class="calibre7">As expected, the username <a id="id401" class="calibre1"/>and password are required <a id="id402" class="calibre1"/>according to the rules from the Windows system that enforce the <code class="email">[Workgroup][Domain]\[user]</code> syntax. So, after we fill the textboxes, the <code class="email">zfs_cifs_1 file system </code>content is shown as seen in the following screenshot:</p><div><img src="img/00010.jpeg" alt="How to do it…" class="calibre12"/></div><p class="calibre13"> </p><p class="calibre7">Everything has worked as we expected, and if we need to undo the SMB sharing offered by the <code class="email">solaris11-1</code> system, it's easy to do so by executing the following command:</p><div><pre class="programlisting">root@solaris11-2:~# <strong class="calibre8">umount /mnt</strong>
root@solaris11-1:~# <strong class="calibre8">zfs set -c share=name=zfs_cifs_1 cifs_pool/zfs_cifs_1</strong>
share 'zfs_cifs_1' was removed.
root@solaris11-1:~# <strong class="calibre8">zfs set -c share=name=zfs_cifs_2 cifs_pool/zfs_cifs_2</strong>
share 'zfs_cifs_2' was removed.
root@solaris11-1:~# <strong class="calibre8">share</strong>
IPC$            smb     -       Remote IPC
c$      /var/smb/cvol   smb     -       Default Share
root@solaris11-1:~# <strong class="calibre8">zfs get share</strong>
root@solaris11-1:~#</pre></div><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec33" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">In this section, the <a id="id403" class="calibre1"/>CIFS sharing in Oracle Solaris 11 was <a id="id404" class="calibre1"/>also explained in a step-by-step procedure that showed us how to configure and access CIFS shares.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec44" class="calibre1"/>Setting and getting other ZFS properties</h1></div></div></div><p class="calibre7">Managing ZFS <a id="id405" class="calibre1"/>properties is one of the secrets when we are working <a id="id406" class="calibre1"/>with <a id="id407" class="calibre1"/>the ZFS filesystem, and this is the reason why <a id="id408" class="calibre1"/>understanding the inherence concept is very important.</p><p class="calibre7">One ZFS property can usually have three origins as source: <code class="email">local</code> (the property value was set locally), <code class="email">default</code> (the property wasn't set either locally or by inheritance), and <code class="email">inherited</code> (the property was inherited from an ancestor). Additionally, two other values are possible: <code class="email">temporary</code> (the value isn't persistent) and <code class="email">none</code> (the property is read-only, and its value was generated by ZFS). Based on these key concepts, the sections are going to present different and interesting properties for daily administration.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec70" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">This recipe can be followed using two virtual machines (VirtualBox or VMware) with Oracle Solaris 11 installed, 4 GB RAM, and eight disks of at least 4 GB.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec71" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">Working as a small review, datasets such as pools, filesystems, snapshots, and clones have several properties that administrators are able to list, handle, and configure. Therefore, the following <a id="id409" class="calibre1"/>commands will create a pool and three filesystems <a id="id410" class="calibre1"/>under this pool. Additionally, we are going <a id="id411" class="calibre1"/>to copy some data (a reminder again—we could use any data) into <a id="id412" class="calibre1"/>the first filesystem as follows:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create prop_pool c8t5d0</strong>
root@solaris11-1:~# <strong class="calibre8">zfs create prop_pool/zfs_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs create prop_pool/zfs_2</strong>
root@solaris11-1:~# <strong class="calibre8">zfs create prop_pool/zfs_3</strong>
root@solaris11-1:~# <strong class="calibre8">cp -r socat-2.0.0-b6.tar.gz dtbook_scripts* /prop_pool/zfs_1</strong>
</pre></div><p class="calibre7">To get all the properties from a pool and filesystem, execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool get all prop_pool</strong>
NAME       PROPERTY       VALUE                 SOURCE
prop_pool  allocated      1.13M                 -
prop_pool  altroot        -                     default
prop_pool  autoexpand     off                   default
prop_pool  autoreplace    off                   default
prop_pool  bootfs         -                     default
prop_pool  cachefile      -                     default
prop_pool  capacity       0%                    -
prop_pool  dedupditto     0                     default
prop_pool  dedupratio     1.00x                 -
prop_pool  delegation     on                    default
prop_pool  failmode       wait                  default
prop_pool  free           3.97G                 -
prop_pool  guid           10747479388132741479  -
prop_pool  health         ONLINE                -
prop_pool  listshares     off                   default
prop_pool  listsnapshots  off                   default
prop_pool  readonly       off                   -
prop_pool  size           3.97G                 -
prop_pool  version        34                    default
root@solaris11-1:~# <strong class="calibre8">zfs get all prop_pool/zfs_1</strong>
NAME             PROPERTY              VALUE                  SOURCE
prop_pool/zfs_1  aclinherit            restricted             default
prop_pool/zfs_1  aclmode               discard                default
prop_pool/zfs_1  atime                 on                     default
prop_pool/zfs_1  available             3.91G   
<strong class="calibre8">(truncated output)</strong>
</pre></div><p class="calibre7">Both commands have a similar syntax, and we've got all the properties from the <code class="email">prop_pool</code> pool and the <code class="email">prop_pool/zfs_1</code> filesystem.</p><p class="calibre7">In the <em class="calibre11">ZFS shadowing</em> section, we touched the NFS subject, and some filesystems were shared using the <code class="email">share</code> command. Nonetheless, they could have been shared using ZFS properties, such as <code class="email">sharenfs</code>, that have a value equal to <code class="email">off</code> by default (when we use this value, it isn't managed by ZFS <a id="id413" class="calibre1"/>and is still using <code class="email">/etc/dfs/dfstab</code>). Let's <a id="id414" class="calibre1"/>take the <code class="email">sharenfs</code> property, which will be used to highlight some basic concepts about properties.</p><p class="calibre7">As usual, the property <a id="id415" class="calibre1"/>listing is too long; it is faster to get only one property's <a id="id416" class="calibre1"/>value by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get sharenfs prop_pool</strong>
NAME       PROPERTY   VALUE  SOURCE
prop_pool  share.nfs  off    default
root@solaris11-1:~# <strong class="calibre8">zfs get sharenfs prop_pool/zfs_1</strong>
NAME             PROPERTY   VALUE  SOURCE
prop_pool/zfs_1  share.nfs  off    default</pre></div><p class="calibre7">Moreover, the same property can be got recursively by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get -r sharenfs prop_pool</strong>      
NAME             PROPERTY   VALUE  SOURCE
prop_pool        share.nfs  off    default
prop_pool/zfs_1  share.nfs  off    default
prop_pool/zfs_2  share.nfs  off    default
prop_pool/zfs_3  share.nfs  off    default </pre></div><p class="calibre7">From the last three outputs, we noticed that the <code class="email">sharenfs</code> property is disabled on the pool and filesystems, and this is the default value set by Oracle Solaris 11.</p><p class="calibre7">The <code class="email">sharenfs</code> property can be enabled by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs set sharenfs=on prop_pool/zfs_1</strong>
root@solaris11-1:~# <strong class="calibre8">zfs get -r sharenfs prop_pool/zfs_1</strong>
NAME              PROPERTY   VALUE  SOURCE
prop_pool/zfs_1   share.nfs  on     local
prop_pool/zfs_1%  share.nfs  on     inherited from prop_pool/zfs_1</pre></div><p class="calibre7">As <code class="email">sharenfs</code> was set to <code class="email">on</code> for <code class="email">prop_pool/zfs_1</code>, the source value has changed to <code class="email">local</code>, indicating that this value wasn't inherited, but it was set locally. Therefore, execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get -s local all prop_pool/zfs_1</strong>
NAME             PROPERTY     VALUE  SOURCE
prop_pool/zfs_1  share.*      ...    local
root@solaris11-1:~# <strong class="calibre8">zfs get -r sharenfs prop_pool</strong>
NAME              PROPERTY   VALUE  SOURCE
prop_pool         share.nfs  off    default
prop_pool/zfs_1   share.nfs  on     local
prop_pool/zfs_1%  share.nfs  on     inherited from prop_pool/zfs_1
prop_pool/zfs_2   share.nfs  off    default
prop_pool/zfs_3   share.nfs  off    default</pre></div><p class="calibre7">The NFS sharing can be <a id="id417" class="calibre1"/>confirmed by running the following <a id="id418" class="calibre1"/>command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">share</strong>
IPC$    smb  -  Remote IPC
c$  /var/smb/cvol  smb  -  Default Share
prop_pool_zfs_1  /prop_pool/zfs_1  nfs  sec=sys,rw</pre></div><p class="calibre7">Creating a new file <a id="id419" class="calibre1"/>stem under <code class="email">zfs_1</code> shows us an interesting <a id="id420" class="calibre1"/>characteristic. Execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs create prop_pool/zfs_1/zfs_4</strong>
root@solaris11-1:~# <strong class="calibre8">zfs get -r sharenfs prop_pool</strong>
NAME                    PROPERTY   VALUE  SOURCE
prop_pool               share.nfs  off    default
prop_pool/zfs_1         share.nfs  on     local
prop_pool/zfs_1%        share.nfs  on     inherited from prop_pool/zfs_1
prop_pool/zfs_1/zfs_4   share.nfs  on     inherited from prop_pool/zfs_1
prop_pool/zfs_1/zfs_4%  share.nfs  on     inherited from prop_pool/zfs_1
prop_pool/zfs_2         share.nfs  off    default
prop_pool/zfs_3         share.nfs  off    default</pre></div><p class="calibre7">The new <code class="email">zfs_4</code> filesystem has the <code class="email">sharenfs</code> property inherited from the <code class="email">upper zfs_1</code> filesystem; now execute the following command to list all the inherited properties:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get -s inherited all prop_pool/zfs_1/zfs_4</strong>
NAME                   PROPERTY     VALUE  SOURCE
prop_pool/zfs_1/zfs_4  share.*      ...    inherited
root@solaris11-1:~# <strong class="calibre8">share</strong>
IPC$    smb  -  Remote IPC
c$  /var/smb/cvol  smb  -  Default Share
prop_pool_zfs_1  /prop_pool/zfs_1  nfs  sec=sys,rw
prop_pool_zfs_1_zfs_4  /prop_pool/zfs_1/zfs_4  nfs  sec=sys,rw</pre></div><p class="calibre7">That's great! The new <code class="email">zfs_4</code> filesystem has inherited the <code class="email">sharenfs</code> property, and it appears in the <code class="email">share</code> output command.</p><p class="calibre7">A good question is whether a <a id="id421" class="calibre1"/>filesystem will be able to fill all the space of a pool. Yes, it <a id="id422" class="calibre1"/>will be able to! Now, this is the reason for ZFS <a id="id423" class="calibre1"/>having several properties related to the amount of space on the disk. The first <a id="id424" class="calibre1"/>of them, the <code class="email">quota</code> property, is a well-known property that limits how much space a dataset (filesystem in this case) can fill in a pool. Let's take an example:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list -r prop_pool</strong>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
prop_pool               399M  3.52G   391M  /prop_pool
prop_pool/zfs_1        8.09M  3.52G  8.06M  /prop_pool/zfs_1
prop_pool/zfs_1/zfs_4    31K  3.52G    31K  /prop_pool/zfs_1/zfs_4
prop_pool/zfs_2          31K  3.52G    31K  /prop_pool/zfs_2
prop_pool/zfs_3          31K  3.52G    31K  /prop_pool/zfs_3</pre></div><p class="calibre7">All filesystems struggle to use the same space (<code class="email">3.52G</code>), and one of them can fill more space than the other (or all the free space), so it is possible that a filesystem suffered a "run out space" error. A solution would be to limit the space a filesystem can take up by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs quota=1G prop_pool/zfs_3</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r prop_pool</strong>      
NAME                    USED  AVAIL  REFER  MOUNTPOINT
prop_pool               399M  3.52G   391M  /prop_pool
prop_pool/zfs_1        8.09M  3.52G  8.06M  /prop_pool/zfs_1
prop_pool/zfs_1/zfs_4    31K  3.52G    31K  /prop_pool/zfs_1/zfs_4
prop_pool/zfs_2          31K  3.52G    31K  /prop_pool/zfs_2
prop_pool/zfs_3          31K  1024M    31K  /prop_pool/zfs_3</pre></div><p class="calibre7">The <code class="email">zfs_3</code> filesystem space was limited to 1 GB, and it can't exceed this threshold. Nonetheless, there isn't any additional guarantee that it has 1 GB to fill. This is subtle—it can't exceed 1 GB, but there is no guarantee that even 1 GB is enough for doing it. Another serious detail—this quota space is shared by the filesystem and all the descendants such as snapshots and clones. Finally and obviously, it isn't possible to set a quota value lesser than the currently used space of the dataset.</p><p class="calibre7">A solution for this apparent problem is the <code class="email">reservation</code> property. When using <code class="email">reservation</code>, the space is guaranteed for the filesystem, and nobody else can take this space. Sure, it isn't possible to make a reservation above the quota or maximum free space, and the same rule is <a id="id425" class="calibre1"/>followed—the reservation is for a filesystem and its descendants.</p><p class="calibre7">When the <code class="email">reservation</code> property <a id="id426" class="calibre1"/>is set to a value, this amount is discounted from the total <a id="id427" class="calibre1"/>available pool space, and the used pool space is <a id="id428" class="calibre1"/>increased by the same value:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list -r prop_pool</strong>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
prop_pool               399M  3.52G   391M  /prop_pool
prop_pool/zfs_1        8.09M  3.52G  8.06M  /prop_pool/zfs_1
prop_pool/zfs_1/zfs_4    31K  3.52G    31K  /prop_pool/zfs_1/zfs_4
prop_pool/zfs_2          31K  3.52G    31K  /prop_pool/zfs_2
prop_pool/zfs_3          31K  1024M    31K  /prop_pool/zfs_3</pre></div><p class="calibre7">Each dataset under <code class="email">prop_pool</code> has its <code class="email">reservation</code> property:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get -r reservation prop_pool</strong>
NAME                    PROPERTY     VALUE  SOURCE
prop_pool               reservation  none   default
prop_pool/zfs_1         reservation  none   default
prop_pool/zfs_1%        reservation  -      -
prop_pool/zfs_1/zfs_4   reservation  none   default
prop_pool/zfs_1/zfs_4%  reservation  -      -
prop_pool/zfs_2         reservation  none   default
prop_pool/zfs_3         reservation  none   default</pre></div><p class="calibre7">The <code class="email">reservation</code> property is configured to a specific value (for example, 512 MB), given that this amount is subtracted from the pool's available space and added to its used space. Now, execute the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs set reservation=512M prop_pool/zfs_3</strong>
root@solaris11-1:~# <strong class="calibre8">zfs list -r prop_pool</strong>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
prop_pool               911M  3.02G   391M  /prop_pool
prop_pool/zfs_1        8.09M  3.02G  8.06M  /prop_pool/zfs_1
prop_pool/zfs_1/zfs_4    31K  3.02G    31K  /prop_pool/zfs_1/zfs_4
prop_pool/zfs_2          31K  3.02G    31K  /prop_pool/zfs_2
prop_pool/zfs_3          31K  1024M    31K  /prop_pool/zfs_3
root@solaris11-1:~# <strong class="calibre8">zfs get -r reservation prop_pool</strong>
NAME                    PROPERTY     VALUE  SOURCE
prop_pool               reservation  none   default
prop_pool/zfs_1         reservation  none   default
prop_pool/zfs_1%        reservation  -      -
prop_pool/zfs_1/zfs_4   reservation  none   default
prop_pool/zfs_1/zfs_4%  reservation  -      -
prop_pool/zfs_2         reservation  none   default
prop_pool/zfs_3         reservation  512M   local</pre></div><p class="calibre7">The concern about space is usually focused on a total value for the whole pool, but it's possible to limit the available space for individual users or groups.</p><p class="calibre7">Setting the quota for users is done <a id="id429" class="calibre1"/>through the <code class="email">userquota</code> property and for <a id="id430" class="calibre1"/>groups using the <code class="email">groupquota</code> property:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs set userquota@aborges=750M </strong>
prop_pool/zfs_3
root@solaris11-1:~# <strong class="calibre8">zfs set userquota@alexandre=1.5G prop_pool/zfs_3</strong>
root@solaris11-1:~# <strong class="calibre8">zfs get userquota@aborges prop_pool/zfs_3</strong>
NAME             PROPERTY           VALUE  SOURCE
prop_pool/zfs_3  userquota@aborges  750M   local
root@solaris11-1:~# <strong class="calibre8">zfs get userquota@alexandre prop_pool/zfs_3</strong>
NAME             PROPERTY             VALUE  SOURCE
prop_pool/zfs_3  userquota@alexandre  1.50G  local
root@solaris11-1:~# <strong class="calibre8">zfs set groupquota@staff=1G prop_pool/zfs_3        </strong>
root@solaris11-1:~# <strong class="calibre8">zfs get groupquota@staff prop_pool/zfs_3</strong>
NAME             PROPERTY          VALUE  SOURCE
prop_pool/zfs_3  groupquota@staff  1G     local</pre></div><p class="calibre7">Getting the used and quota <a id="id431" class="calibre1"/>space from users and groups is done by <a id="id432" class="calibre1"/>executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs userspace prop_pool/zfs_3</strong>
TYPE        NAME       USED  QUOTA  
POSIX User  aborges       0   750M  
POSIX User  alexandre     0     1G  
POSIX User  root         3K   none  
root@solaris11-1:~# <strong class="calibre8">zfs groupspace prop_pool/zfs_3</strong>
TYPE         NAME   USED  QUOTA  
POSIX Group  root     3K   none  
POSIX Group  staff     0     1G</pre></div><p class="calibre7">Removing all the quota values that were set until now is done through the following sequence:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs set quota=none prop_pool/zfs_3</strong>
root@solaris11-1:~# <strong class="calibre8">zfs set userquota@aborges=none prop_pool/zfs_3</strong>
root@solaris11-1:~# <strong class="calibre8">zfs set userquota@alexandre=none prop_pool/zfs_3</strong>
root@solaris11-1:~# <strong class="calibre8">zfs set groupquota@staff=none prop_pool/zfs_3</strong>
root@solaris11-1:~# <strong class="calibre8">zfs userspace prop_pool/zfs_3</strong>
TYPE        NAME  USED  QUOTA  
POSIX User  root    3K   none  
root@solaris11-1:~# <strong class="calibre8">zfs groupspace prop_pool/zfs_3</strong>
TYPE         NAME  USED  QUOTA
POSIX Group  root    3K   none</pre></div><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec34" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">In this section, you saw some properties such as <code class="email">sharenfs</code>, <code class="email">quota</code>, <code class="email">reservation</code>, <code class="email">userquota</code>, and <code class="email">groupquota</code>. All of the properties alter the behavior of the ZFS pool, filesystems, snapshots, and <a id="id433" class="calibre1"/>clones. Moreover, there are other additional properties that <a id="id434" class="calibre1"/>can improve the ZFS functionality, and I suggest <a id="id435" class="calibre1"/>that readers look for all of them <a id="id436" class="calibre1"/>in <em class="calibre11">ZFS Administration Guide</em>.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec45" class="calibre1"/>Playing with the ZFS swap</h1></div></div></div><p class="calibre7">One of the toughest <a id="id437" class="calibre1"/>jobs in Oracle Solaris 11 is to calculate the optimal size of the swap area. Roughly, the operating system's virtual memory is made from a <a id="id438" class="calibre1"/>sum of RAM and swap, and its correct provisioning helps the application's performance. Unfortunately, when Oracle Solaris 11 is initially installed, the correct swap size can be underestimated or overestimated, given that any possible mistake can be corrected easily. This section will show you how to manage this issue.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec72" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">This recipe requires a virtual machine (VMware or VirtualBox) with Oracle Solaris 11 installed and 4 GB RAM. Additionally, it's necessary to have access to eight 4 GB disks.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec73" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre7">According to Oracle, there is an estimate during the installation process that Solaris needs around one-fourth of the RAM space for a swap area in the disk. However, for historical reasons, administrators still believe in the myth that swap space should be equal or bigger than twice the RAM size for any situation. Surely, it should work, but it isn't necessary. Usually (not a rule, but observed many times), it should be something between 0.5 x RAM and 1.5 x RAM, excluding exceptions such as when predicting a database installation. Remember that the swap area can be a dedicated partition or a file; the best way to list the swap areas (and their free space) is by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">swap -l</strong>
swapfile             dev    swaplo   blocks     free
/dev/zvol/dsk/rpool/swap 285,2         8  4194296  4194296
/dev/zvol/dsk/rpool/newswap 285,3         8  4194296  4194296</pre></div><p class="calibre7">From the previous output, the meaning of each column is as follows:</p><div><ul class="itemizedlist"><li class="listitem"><code class="email">swapfile</code>: This shows that swap areas come from two ZFS volumes <code class="email">(/dev/zvol/dsk/rpool/swap</code> and <code class="email">/dev/zvol/dsk/rpool/newswap</code>)</li><li class="listitem"><code class="email">dev</code>: This shows the major and minor number of swap devices</li><li class="listitem"><code class="email">swaplo</code>: This shows the minimum possible swap space, which is limited to the memory page size and its respective value is usually obtained as units of sectors (512 bytes) by executing the <code class="email">pagesize</code> command</li><li class="listitem"><code class="email">blocks</code>: This is the total swap space in sectors</li><li class="listitem"><code class="email">free</code>: This is the free swap space (4 GB)</li></ul></div><p class="calibre7">An alternative way to collect information about the swap area is using the same <code class="email">swap</code> command with the <code class="email">–s</code> option, as shown in the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">swap –s</strong>
total: 519668k bytes allocated + 400928k reserved = 920596k used, 4260372k available</pre></div><p class="calibre7">From this command output, we have:</p><div><ul class="itemizedlist"><li class="listitem"><code class="email">519668k bytes allocated</code>: This is a swap space that indicates the amount of swap space that already has been used earlier but is not necessarily in use this time. Therefore, it's <a id="id439" class="calibre1"/>reserved and available to be used when required.</li><li class="listitem"><code class="email">400928k reserved</code>: This is the virtual swap space that was reserved (heap segment and anonymous memory) for future use, and this time, it isn't allocated yet. Usually, the swap space is reserved when the virtual memory for a process is <a id="id440" class="calibre1"/>created. Anonymous memory refers to pages that don't have a counterpart in the disk (any filesystem). They are moved to a swap area because the shortage of RAM (physical memory) occurs many times because of the sum of stack, shared memory, and process heap, which is larger than the available physical memory.</li><li class="listitem"><code class="email">946696k used</code>: This is total amount of swap space that is reserved or allocated.</li><li class="listitem"><code class="email">4260372k available</code>: This is the amount of swap space available for future allocation.</li></ul></div><p class="calibre7">Until now, you've learned how to monitor swap areas. From now, let's see how to add and delete swap space on Oracle Solaris 11 by executing the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs list -r rpool</strong>
NAME                              USED  AVAIL  REFER  MOUNTPOINT
rpool                            37.0G  41.3G  4.91M  /rpool
rpool/ROOT                       26.7G  41.3G    31K  legacy
<strong class="calibre8">(truncated output)</strong>
rpool/newswap                    2.06G  41.3G  2.00G  -
rpool/shad_source                2.38G  41.3G  2.38G  /rpool/shad_source
rpool/shad_target                1.60G  41.3G  1.60G  /rpool/shad_target
rpool/swap                       2.06G  41.3G  2.00G  -</pre></div><p class="calibre7">Two lines (<code class="email">rpool/newswap</code> and <code class="email">rpool/swap</code>) prove that the swap space has a size of 4 GB (2 GB + 2 GB), and both datasets are ZFS volumes, which can be verified by executing the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">ls -ls /dev/zvol/rdsk/rpool/swap</strong> 
   0 lrwxrwxrwx   1 root     root           0 Dec 17 20:35 /dev/zvol/rdsk/rpool/swap -&gt; ../../../..//devices/pseudo/zfs@0:2,raw
root@solaris11-1:~# <strong class="calibre8">ls -ls /dev/zvol/rdsk/rpool/newswap</strong> 
   0 lrwxrwxrwx   1 root     root           0 Dec 20 19:04 /dev/zvol/rdsk/rpool/newswap -&gt; ../../../..//devices/pseudo/zfs@0:3,raw</pre></div><p class="calibre7">Continuing <a id="id441" class="calibre1"/>from the previous section (getting and setting properties), the swap <a id="id442" class="calibre1"/>space can be changed by altering the <code class="email">volsize</code> property if the pool has free space. Then, run the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get volsize rpool/swap</strong>
NAME        PROPERTY  VALUE  SOURCE
rpool/swap  volsize   2G     local

root@solaris11-1:~# <strong class="calibre8">zfs get volsize rpool/newswap</strong>
NAME           PROPERTY  VALUE  SOURCE
rpool/newswap  volsize   2G     local</pre></div><p class="calibre7">A simple way to increase the swap space would be by changing the <code class="email">volsize</code> value. Then, execute the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs set volsize=3G rpool/newswap</strong>
root@solaris11-1:~# <strong class="calibre8">zfs get volsize rpool/newswap</strong>
NAME           PROPERTY  VALUE  SOURCE

rpool/newswap  volsize   3G     local
root@solaris11-1:~# <strong class="calibre8">swap –l</strong>
swapfile             dev    swaplo   blocks     free
/dev/zvol/dsk/rpool/swap 285,2         8  4194296  4194296
/dev/zvol/dsk/rpool/newswap 285,3         8  4194296  4194296
/dev/zvol/dsk/rpool/newswap 285,3   4194312  2097144  2097144
root@solaris11-1:~# <strong class="calibre8">swap -s</strong>
total: 451556k bytes allocated + 267760k reserved = 719316k used, 5359332k available
root@solaris11-1:~# <strong class="calibre8">zfs list -r rpool/swap</strong>
NAME         USED  AVAIL  REFER  MOUNTPOINT
rpool/swap  2.00G  40.4G  2.00G  -
root@solaris11-1:~# <strong class="calibre8">zfs list -r rpool/newswap</strong>
NAME            USED  AVAIL  REFER  MOUNTPOINT
rpool/newswap  3.00G  40.4G  3.00G  -</pre></div><p class="calibre7">Eventually, it's necessary to add a new volume because the free space on a pool isn't enough, so it can be done by executing the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zpool create swap_pool c8t12d0</strong>
root@solaris11-1:~# <strong class="calibre8">zpool list swap_pool</strong>
NAME        SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT
swap_pool  3.97G    85K  3.97G   0%  1.00x  ONLINE  -
root@solaris11-1:~# <strong class="calibre8">zfs create -V 1G swap_pool/vol_swap_1</strong> 
root@solaris11-1:~# <strong class="calibre8">zfs list -r swap_pool</strong>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
swap_pool             1.03G  2.87G    31K  /swap_pool
swap_pool/vol_swap_1  1.03G  3.91G    16K  -</pre></div><p class="calibre7">Once the swap volume <a id="id443" class="calibre1"/>has been created, the next step is to add it as a swap device <a id="id444" class="calibre1"/>by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">swap -a /dev/zvol/dsk/swap_pool/vol_swap_1</strong> 
root@solaris11-1:~# <strong class="calibre8">swap -l</strong>
swapfile             dev    swaplo   blocks     free
/dev/zvol/dsk/rpool/swap 285,2         8  4194296  4194296
/dev/zvol/dsk/rpool/newswap 285,3         8  4194296  4194296
/dev/zvol/dsk/rpool/newswap 285,3   4194312  2097144  2097144
/dev/zvol/dsk/swap_pool/vol_swap_1 285,4         8  2097144  2097144
root@solaris11-1:~# <strong class="calibre8">swap -s</strong>
total: 456308k bytes allocated + 268024k reserved = 724332k used, 6361756k available
root@solaris11-1:~# <strong class="calibre8">zfs list -r swap_pool</strong>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
swap_pool             1.03G  2.87G    31K  /swap_pool
swap_pool/vol_swap_1  1.03G  2.91G  1.00G  -
root@solaris11-1:~# <strong class="calibre8">zfs list -r rpool | grep swap</strong>
rpool/newswap                    3.00G  40.4G  3.00G  -
rpool/swap                       2.00G  40.4G  2.00G  -</pre></div><p class="calibre7">Finally, the new swap device must be included in the <code class="email">vfstab</code> file under <code class="email">etc</code> to be mounted during the Oracle Solaris 11 boot:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">more /etc/vfstab</strong> 
#device    device    mount    FS  fsck  mount  mount
#to mount  to fsck    point    type  pass  at boot  options
#
/devices  -    /devices  devfs  -  no  -
/proc     -     /proc      proc  -  no  -
<strong class="calibre8">(truncated output)</strong>
swap                                -    /tmp    tmpfs  -  yes  -

/dev/zvol/dsk/rpool/swap            -    -       swap   -  no   -
/dev/zvol/dsk/rpool/newswap         -    -       swap   -  no   -
/dev/zvol/dsk/swap_pool/vol_swap_1  -    -       swap   -  no   -</pre></div><p class="calibre7">Last but not least, the task <a id="id445" class="calibre1"/>of removing the swap area is very simple. First, the entry in <code class="email">/etc/vfstab</code> needs to be deleted. Before removing the swap areas, they need to be listed as follows:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">swap -l</strong>
swapfile             dev    swaplo   blocks     free
/dev/zvol/dsk/rpool/swap 285,2         8  4194296  4194296
/dev/zvol/dsk/rpool/newswap 285,3         8  4194296  4194296
/dev/zvol/dsk/rpool/newswap 285,3   4194312  2097144  2097144
/dev/zvol/dsk/swap_pool/vol_swap_1 285,4         8  2097144  2097144</pre></div><p class="calibre7">Second, the swap volume <a id="id446" class="calibre1"/>must be unregistered from the system by running the following command:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">swap -d /dev/zvol/dsk/swap_pool/vol_swap_1</strong>
root@solaris11-1:~# <strong class="calibre8">zpool destroy swap_pool</strong>
root@solaris11-1:~# <strong class="calibre8">swap -d /dev/zvol/dsk/rpool/newswap</strong>
root@solaris11-1:~# <strong class="calibre8">swap -l</strong>
swapfile             dev    swaplo   blocks     free
/dev/zvol/dsk/rpool/swap 285,2         8  4194296  4194296
/dev/zvol/dsk/rpool/newswap 285,3   4194312  2097144  2097144</pre></div><p class="calibre7">Earlier, the <code class="email">rpool/newswap</code> volume was increased. However, it would be impossible to decrease it because <code class="email">rpool/newswap</code> was in use (busy). Now, as the first 2 GB space from this volume was removed, this 2 GB part isn't in use at this moment, and the total volume (3 GB) can be <a id="id447" class="calibre1"/>reduced. Execute the following commands:</p><div><pre class="programlisting">root@solaris11-1:~# <strong class="calibre8">zfs get volsize rpool/newswap</strong>
NAME           PROPERTY  VALUE  SOURCE
rpool/newswap  volsize   3G     local
root@solaris11-1:~# <strong class="calibre8">zfs set volsize=1G rpool/newswap</strong>
root@solaris11-1:~# <strong class="calibre8">zfs get volsize rpool/newswap</strong>
NAME           PROPERTY  VALUE  SOURCE
rpool/newswap  volsize   1G     local
root@solaris11-1:~# <strong class="calibre8">swap -l</strong>
swapfile             dev    swaplo   blocks     free
/dev/zvol/dsk/rpool/swap 285,2         8  4194296  4194296
/dev/zvol/dsk/rpool/newswap 285,3   4194312  2097144  2097144
root@solaris11-1:~# <strong class="calibre8">swap -s</strong>
total: 456836k bytes allocated + 267580k reserved = 724416k used, 3203464k available</pre></div><div><div><div><div><h3 class="title2"><a id="ch02lvl3sec35" class="calibre1"/>An overview of the recipe</h3></div></div></div><p class="calibre7">You saw how to add, remove, and monitor the swap space using the ZFS framework. Furthermore, You learned some very <a id="id448" class="calibre1"/>important concepts such as reserved, allocated, and free swap.</p></div></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec46" class="calibre1"/>References</h1></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre11">Oracle Solaris Administration - </em><a id="id449" class="calibre1"/><em class="calibre11">ZFS File Systems</em> at <a class="calibre1" href="http://docs.oracle.com/cd/E23824_01/html/821-1448/preface-1.html#scrolltoc">http://docs.oracle.com/cd/E23824_01/html/821-1448/preface-1.html#scrolltoc</a></li><li class="listitem"><em class="calibre11">How to configure a </em><a id="id450" class="calibre1"/><em class="calibre11">free VTL (Virtual Tape Library)</em> at <a class="calibre1" href="http://alexandreborgesbrazil.files.wordpress.com/2013/09/how-to-configure-a-free-vtl1.pdf">http://alexandreborgesbrazil.files.wordpress.com/2013/09/how-to-configure-a-free-vtl1.pdf</a></li><li class="listitem"><em class="calibre11">Oracle Solaris </em><a id="id451" class="calibre1"/><em class="calibre11">Tunable Parameters Reference Manual</em> at <a class="calibre1" href="http://docs.oracle.com/cd/E23823_01/html/817-0404/preface-1.html#scrolltoc">http://docs.oracle.com/cd/E23823_01/html/817-0404/preface-1.html#scrolltoc</a></li><li class="listitem"><em class="calibre11">Oracle Solaris </em><a id="id452" class="calibre1"/><em class="calibre11">Administration: SMB and Windows Interoperability</em> at <a class="calibre1" href="http://docs.oracle.com/cd/E23824_01/html/821-1449/toc.html">http://docs.oracle.com/cd/E23824_01/html/821-1449/toc.html</a></li><li class="listitem"><em class="calibre11">Playing with Swap Monitoring and Increasing Swap Space Using ZFS Volumes In </em><a id="id453" class="calibre1"/><em class="calibre11">Oracle Solaris 11.1</em> (by Alexandre Borges) at <a class="calibre1" href="http://www.oracle.com/technetwork/articles/servers-storage-admin/monitor-swap-solaris-zfs-2216650.html">http://www.oracle.com/technetwork/articles/servers-storage-admin/monitor-swap-solaris-zfs-2216650.html</a></li><li class="listitem"><em class="calibre11">Playing with ZFS Encryption </em><a id="id454" class="calibre1"/><em class="calibre11">In Oracle Solaris 11</em> (by Alexandre Borges) at <a class="calibre1" href="http://www.oracle.com/technetwork/articles/servers-storage-admin/solaris-zfs-encryption-2242161.html">http://www.oracle.com/technetwork/articles/servers-storage-admin/solaris-zfs-encryption-2242161.html</a></li></ul></div></div></body></html>